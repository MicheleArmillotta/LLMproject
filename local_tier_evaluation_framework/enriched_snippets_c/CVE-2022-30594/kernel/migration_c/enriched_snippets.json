[
  {
    "function_name": "__irq_move_irq",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/migration.c",
    "lines": "94-119",
    "snippet": "void __irq_move_irq(struct irq_data *idata)\n{\n\tbool masked;\n\n\t/*\n\t * Get top level irq_data when CONFIG_IRQ_DOMAIN_HIERARCHY is enabled,\n\t * and it should be optimized away when CONFIG_IRQ_DOMAIN_HIERARCHY is\n\t * disabled. So we avoid an \"#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY\" here.\n\t */\n\tidata = irq_desc_get_irq_data(irq_data_to_desc(idata));\n\n\tif (unlikely(irqd_irq_disabled(idata)))\n\t\treturn;\n\n\t/*\n\t * Be careful vs. already masked interrupts. If this is a\n\t * threaded interrupt with ONESHOT set, we can end up with an\n\t * interrupt storm.\n\t */\n\tmasked = irqd_irq_masked(idata);\n\tif (!masked)\n\t\tidata->chip->irq_mask(idata);\n\tirq_move_masked_irq(idata);\n\tif (!masked)\n\t\tidata->chip->irq_unmask(idata);\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/interrupt.h>",
      "#include <linux/irq.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "idata->chip->irq_unmask",
          "args": [
            "idata"
          ],
          "line": 118
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_move_masked_irq",
          "args": [
            "idata"
          ],
          "line": 116
        },
        "resolved": true,
        "details": {
          "function_name": "irq_move_masked_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/migration.c",
          "lines": "38-92",
          "snippet": "void irq_move_masked_irq(struct irq_data *idata)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(idata);\n\tstruct irq_data *data = &desc->irq_data;\n\tstruct irq_chip *chip = data->chip;\n\n\tif (likely(!irqd_is_setaffinity_pending(data)))\n\t\treturn;\n\n\tirqd_clr_move_pending(data);\n\n\t/*\n\t * Paranoia: cpu-local interrupts shouldn't be calling in here anyway.\n\t */\n\tif (irqd_is_per_cpu(data)) {\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (unlikely(cpumask_empty(desc->pending_mask)))\n\t\treturn;\n\n\tif (!chip->irq_set_affinity)\n\t\treturn;\n\n\tassert_raw_spin_locked(&desc->lock);\n\n\t/*\n\t * If there was a valid mask to work with, please\n\t * do the disable, re-program, enable sequence.\n\t * This is *not* particularly important for level triggered\n\t * but in a edge trigger case, we might be setting rte\n\t * when an active trigger is coming in. This could\n\t * cause some ioapics to mal-function.\n\t * Being paranoid i guess!\n\t *\n\t * For correct operation this depends on the caller\n\t * masking the irqs.\n\t */\n\tif (cpumask_any_and(desc->pending_mask, cpu_online_mask) < nr_cpu_ids) {\n\t\tint ret;\n\n\t\tret = irq_do_set_affinity(data, desc->pending_mask, false);\n\t\t/*\n\t\t * If the there is a cleanup pending in the underlying\n\t\t * vector management, reschedule the move for the next\n\t\t * interrupt. Leave desc->pending_mask intact.\n\t\t */\n\t\tif (ret == -EBUSY) {\n\t\t\tirqd_set_move_pending(data);\n\t\t\treturn;\n\t\t}\n\t}\n\tcpumask_clear(desc->pending_mask);\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/interrupt.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n\nvoid irq_move_masked_irq(struct irq_data *idata)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(idata);\n\tstruct irq_data *data = &desc->irq_data;\n\tstruct irq_chip *chip = data->chip;\n\n\tif (likely(!irqd_is_setaffinity_pending(data)))\n\t\treturn;\n\n\tirqd_clr_move_pending(data);\n\n\t/*\n\t * Paranoia: cpu-local interrupts shouldn't be calling in here anyway.\n\t */\n\tif (irqd_is_per_cpu(data)) {\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (unlikely(cpumask_empty(desc->pending_mask)))\n\t\treturn;\n\n\tif (!chip->irq_set_affinity)\n\t\treturn;\n\n\tassert_raw_spin_locked(&desc->lock);\n\n\t/*\n\t * If there was a valid mask to work with, please\n\t * do the disable, re-program, enable sequence.\n\t * This is *not* particularly important for level triggered\n\t * but in a edge trigger case, we might be setting rte\n\t * when an active trigger is coming in. This could\n\t * cause some ioapics to mal-function.\n\t * Being paranoid i guess!\n\t *\n\t * For correct operation this depends on the caller\n\t * masking the irqs.\n\t */\n\tif (cpumask_any_and(desc->pending_mask, cpu_online_mask) < nr_cpu_ids) {\n\t\tint ret;\n\n\t\tret = irq_do_set_affinity(data, desc->pending_mask, false);\n\t\t/*\n\t\t * If the there is a cleanup pending in the underlying\n\t\t * vector management, reschedule the move for the next\n\t\t * interrupt. Leave desc->pending_mask intact.\n\t\t */\n\t\tif (ret == -EBUSY) {\n\t\t\tirqd_set_move_pending(data);\n\t\t\treturn;\n\t\t}\n\t}\n\tcpumask_clear(desc->pending_mask);\n}"
        }
      },
      {
        "call_info": {
          "callee": "idata->chip->irq_mask",
          "args": [
            "idata"
          ],
          "line": 115
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_irq_masked",
          "args": [
            "idata"
          ],
          "line": 113
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "irqd_irq_disabled(idata)"
          ],
          "line": 105
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_irq_disabled",
          "args": [
            "idata"
          ],
          "line": 105
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_desc_get_irq_data",
          "args": [
            "irq_data_to_desc(idata)"
          ],
          "line": 103
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_to_desc",
          "args": [
            "idata"
          ],
          "line": 103
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n\nvoid __irq_move_irq(struct irq_data *idata)\n{\n\tbool masked;\n\n\t/*\n\t * Get top level irq_data when CONFIG_IRQ_DOMAIN_HIERARCHY is enabled,\n\t * and it should be optimized away when CONFIG_IRQ_DOMAIN_HIERARCHY is\n\t * disabled. So we avoid an \"#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY\" here.\n\t */\n\tidata = irq_desc_get_irq_data(irq_data_to_desc(idata));\n\n\tif (unlikely(irqd_irq_disabled(idata)))\n\t\treturn;\n\n\t/*\n\t * Be careful vs. already masked interrupts. If this is a\n\t * threaded interrupt with ONESHOT set, we can end up with an\n\t * interrupt storm.\n\t */\n\tmasked = irqd_irq_masked(idata);\n\tif (!masked)\n\t\tidata->chip->irq_mask(idata);\n\tirq_move_masked_irq(idata);\n\tif (!masked)\n\t\tidata->chip->irq_unmask(idata);\n}"
  },
  {
    "function_name": "irq_move_masked_irq",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/migration.c",
    "lines": "38-92",
    "snippet": "void irq_move_masked_irq(struct irq_data *idata)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(idata);\n\tstruct irq_data *data = &desc->irq_data;\n\tstruct irq_chip *chip = data->chip;\n\n\tif (likely(!irqd_is_setaffinity_pending(data)))\n\t\treturn;\n\n\tirqd_clr_move_pending(data);\n\n\t/*\n\t * Paranoia: cpu-local interrupts shouldn't be calling in here anyway.\n\t */\n\tif (irqd_is_per_cpu(data)) {\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (unlikely(cpumask_empty(desc->pending_mask)))\n\t\treturn;\n\n\tif (!chip->irq_set_affinity)\n\t\treturn;\n\n\tassert_raw_spin_locked(&desc->lock);\n\n\t/*\n\t * If there was a valid mask to work with, please\n\t * do the disable, re-program, enable sequence.\n\t * This is *not* particularly important for level triggered\n\t * but in a edge trigger case, we might be setting rte\n\t * when an active trigger is coming in. This could\n\t * cause some ioapics to mal-function.\n\t * Being paranoid i guess!\n\t *\n\t * For correct operation this depends on the caller\n\t * masking the irqs.\n\t */\n\tif (cpumask_any_and(desc->pending_mask, cpu_online_mask) < nr_cpu_ids) {\n\t\tint ret;\n\n\t\tret = irq_do_set_affinity(data, desc->pending_mask, false);\n\t\t/*\n\t\t * If the there is a cleanup pending in the underlying\n\t\t * vector management, reschedule the move for the next\n\t\t * interrupt. Leave desc->pending_mask intact.\n\t\t */\n\t\tif (ret == -EBUSY) {\n\t\t\tirqd_set_move_pending(data);\n\t\t\treturn;\n\t\t}\n\t}\n\tcpumask_clear(desc->pending_mask);\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/interrupt.h>",
      "#include <linux/irq.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpumask_clear",
          "args": [
            "desc->pending_mask"
          ],
          "line": 91
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_set_move_pending",
          "args": [
            "data"
          ],
          "line": 87
        },
        "resolved": true,
        "details": {
          "function_name": "irqd_set_move_pending",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/internals.h",
          "lines": "203-206",
          "snippet": "static inline void irqd_set_move_pending(struct irq_data *d)\n{\n\t__irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;\n}",
          "includes": [
            "#include <linux/debugfs.h>",
            "#include \"settings.h\"",
            "#include \"debug.h\"",
            "#include <linux/sched/clock.h>",
            "#include <linux/pm_runtime.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/irqdesc.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/debugfs.h>\n#include \"settings.h\"\n#include \"debug.h\"\n#include <linux/sched/clock.h>\n#include <linux/pm_runtime.h>\n#include <linux/kernel_stat.h>\n#include <linux/irqdesc.h>\n\nstatic inline void irqd_set_move_pending(struct irq_data *d)\n{\n\t__irqd_to_state(d) |= IRQD_SETAFFINITY_PENDING;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_do_set_affinity",
          "args": [
            "data",
            "desc->pending_mask",
            "false"
          ],
          "line": 80
        },
        "resolved": true,
        "details": {
          "function_name": "irq_do_set_affinity",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/manage.c",
          "lines": "220-281",
          "snippet": "int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,\n\t\t\tbool force)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tint ret;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If this is a managed interrupt and housekeeping is enabled on\n\t * it check whether the requested affinity mask intersects with\n\t * a housekeeping CPU. If so, then remove the isolated CPUs from\n\t * the mask and just keep the housekeeping CPU(s). This prevents\n\t * the affinity setter from routing the interrupt to an isolated\n\t * CPU to avoid that I/O submitted from a housekeeping CPU causes\n\t * interrupts on an isolated one.\n\t *\n\t * If the masks do not intersect or include online CPU(s) then\n\t * keep the requested mask. The isolated target CPUs are only\n\t * receiving interrupts when the I/O operation was submitted\n\t * directly from them.\n\t *\n\t * If all housekeeping CPUs in the affinity mask are offline, the\n\t * interrupt will be migrated by the CPU hotplug code once a\n\t * housekeeping CPU which belongs to the affinity mask comes\n\t * online.\n\t */\n\tif (irqd_affinity_is_managed(data) &&\n\t    housekeeping_enabled(HK_FLAG_MANAGED_IRQ)) {\n\t\tconst struct cpumask *hk_mask, *prog_mask;\n\n\t\tstatic DEFINE_RAW_SPINLOCK(tmp_mask_lock);\n\t\tstatic struct cpumask tmp_mask;\n\n\t\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\n\t\traw_spin_lock(&tmp_mask_lock);\n\t\tcpumask_and(&tmp_mask, mask, hk_mask);\n\t\tif (!cpumask_intersects(&tmp_mask, cpu_online_mask))\n\t\t\tprog_mask = mask;\n\t\telse\n\t\t\tprog_mask = &tmp_mask;\n\t\tret = chip->irq_set_affinity(data, prog_mask, force);\n\t\traw_spin_unlock(&tmp_mask_lock);\n\t} else {\n\t\tret = chip->irq_set_affinity(data, mask, force);\n\t}\n\tswitch (ret) {\n\tcase IRQ_SET_MASK_OK:\n\tcase IRQ_SET_MASK_OK_DONE:\n\t\tcpumask_copy(desc->irq_common_data.affinity, mask);\n\t\tfallthrough;\n\tcase IRQ_SET_MASK_OK_NOCOPY:\n\t\tirq_validate_effective_affinity(data);\n\t\tirq_set_thread_affinity(desc);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/task_work.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/random.h>",
            "#include <linux/module.h>",
            "#include <linux/kthread.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/task_work.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/task.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/irqdomain.h>\n#include <linux/interrupt.h>\n#include <linux/random.h>\n#include <linux/module.h>\n#include <linux/kthread.h>\n#include <linux/irq.h>\n\nint irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,\n\t\t\tbool force)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tint ret;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If this is a managed interrupt and housekeeping is enabled on\n\t * it check whether the requested affinity mask intersects with\n\t * a housekeeping CPU. If so, then remove the isolated CPUs from\n\t * the mask and just keep the housekeeping CPU(s). This prevents\n\t * the affinity setter from routing the interrupt to an isolated\n\t * CPU to avoid that I/O submitted from a housekeeping CPU causes\n\t * interrupts on an isolated one.\n\t *\n\t * If the masks do not intersect or include online CPU(s) then\n\t * keep the requested mask. The isolated target CPUs are only\n\t * receiving interrupts when the I/O operation was submitted\n\t * directly from them.\n\t *\n\t * If all housekeeping CPUs in the affinity mask are offline, the\n\t * interrupt will be migrated by the CPU hotplug code once a\n\t * housekeeping CPU which belongs to the affinity mask comes\n\t * online.\n\t */\n\tif (irqd_affinity_is_managed(data) &&\n\t    housekeeping_enabled(HK_FLAG_MANAGED_IRQ)) {\n\t\tconst struct cpumask *hk_mask, *prog_mask;\n\n\t\tstatic DEFINE_RAW_SPINLOCK(tmp_mask_lock);\n\t\tstatic struct cpumask tmp_mask;\n\n\t\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\n\t\traw_spin_lock(&tmp_mask_lock);\n\t\tcpumask_and(&tmp_mask, mask, hk_mask);\n\t\tif (!cpumask_intersects(&tmp_mask, cpu_online_mask))\n\t\t\tprog_mask = mask;\n\t\telse\n\t\t\tprog_mask = &tmp_mask;\n\t\tret = chip->irq_set_affinity(data, prog_mask, force);\n\t\traw_spin_unlock(&tmp_mask_lock);\n\t} else {\n\t\tret = chip->irq_set_affinity(data, mask, force);\n\t}\n\tswitch (ret) {\n\tcase IRQ_SET_MASK_OK:\n\tcase IRQ_SET_MASK_OK_DONE:\n\t\tcpumask_copy(desc->irq_common_data.affinity, mask);\n\t\tfallthrough;\n\tcase IRQ_SET_MASK_OK_NOCOPY:\n\t\tirq_validate_effective_affinity(data);\n\t\tirq_set_thread_affinity(desc);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_any_and",
          "args": [
            "desc->pending_mask",
            "cpu_online_mask"
          ],
          "line": 77
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "assert_raw_spin_locked",
          "args": [
            "&desc->lock"
          ],
          "line": 63
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "cpumask_empty(desc->pending_mask)"
          ],
          "line": 57
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_empty",
          "args": [
            "desc->pending_mask"
          ],
          "line": 57
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "1"
          ],
          "line": 53
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_is_per_cpu",
          "args": [
            "data"
          ],
          "line": 52
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_clr_move_pending",
          "args": [
            "data"
          ],
          "line": 47
        },
        "resolved": true,
        "details": {
          "function_name": "irqd_clr_move_pending",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/internals.h",
          "lines": "208-211",
          "snippet": "static inline void irqd_clr_move_pending(struct irq_data *d)\n{\n\t__irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;\n}",
          "includes": [
            "#include <linux/debugfs.h>",
            "#include \"settings.h\"",
            "#include \"debug.h\"",
            "#include <linux/sched/clock.h>",
            "#include <linux/pm_runtime.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/irqdesc.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/debugfs.h>\n#include \"settings.h\"\n#include \"debug.h\"\n#include <linux/sched/clock.h>\n#include <linux/pm_runtime.h>\n#include <linux/kernel_stat.h>\n#include <linux/irqdesc.h>\n\nstatic inline void irqd_clr_move_pending(struct irq_data *d)\n{\n\t__irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!irqd_is_setaffinity_pending(data)"
          ],
          "line": 44
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_is_setaffinity_pending",
          "args": [
            "data"
          ],
          "line": 44
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_to_desc",
          "args": [
            "idata"
          ],
          "line": 40
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n\nvoid irq_move_masked_irq(struct irq_data *idata)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(idata);\n\tstruct irq_data *data = &desc->irq_data;\n\tstruct irq_chip *chip = data->chip;\n\n\tif (likely(!irqd_is_setaffinity_pending(data)))\n\t\treturn;\n\n\tirqd_clr_move_pending(data);\n\n\t/*\n\t * Paranoia: cpu-local interrupts shouldn't be calling in here anyway.\n\t */\n\tif (irqd_is_per_cpu(data)) {\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (unlikely(cpumask_empty(desc->pending_mask)))\n\t\treturn;\n\n\tif (!chip->irq_set_affinity)\n\t\treturn;\n\n\tassert_raw_spin_locked(&desc->lock);\n\n\t/*\n\t * If there was a valid mask to work with, please\n\t * do the disable, re-program, enable sequence.\n\t * This is *not* particularly important for level triggered\n\t * but in a edge trigger case, we might be setting rte\n\t * when an active trigger is coming in. This could\n\t * cause some ioapics to mal-function.\n\t * Being paranoid i guess!\n\t *\n\t * For correct operation this depends on the caller\n\t * masking the irqs.\n\t */\n\tif (cpumask_any_and(desc->pending_mask, cpu_online_mask) < nr_cpu_ids) {\n\t\tint ret;\n\n\t\tret = irq_do_set_affinity(data, desc->pending_mask, false);\n\t\t/*\n\t\t * If the there is a cleanup pending in the underlying\n\t\t * vector management, reschedule the move for the next\n\t\t * interrupt. Leave desc->pending_mask intact.\n\t\t */\n\t\tif (ret == -EBUSY) {\n\t\t\tirqd_set_move_pending(data);\n\t\t\treturn;\n\t\t}\n\t}\n\tcpumask_clear(desc->pending_mask);\n}"
  },
  {
    "function_name": "irq_fixup_move_pending",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/migration.c",
    "lines": "18-36",
    "snippet": "bool irq_fixup_move_pending(struct irq_desc *desc, bool force_clear)\n{\n\tstruct irq_data *data = irq_desc_get_irq_data(desc);\n\n\tif (!irqd_is_setaffinity_pending(data))\n\t\treturn false;\n\n\t/*\n\t * The outgoing CPU might be the last online target in a pending\n\t * interrupt move. If that's the case clear the pending move bit.\n\t */\n\tif (cpumask_any_and(desc->pending_mask, cpu_online_mask) >= nr_cpu_ids) {\n\t\tirqd_clr_move_pending(data);\n\t\treturn false;\n\t}\n\tif (force_clear)\n\t\tirqd_clr_move_pending(data);\n\treturn true;\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/interrupt.h>",
      "#include <linux/irq.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "irqd_clr_move_pending",
          "args": [
            "data"
          ],
          "line": 34
        },
        "resolved": true,
        "details": {
          "function_name": "irqd_clr_move_pending",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/internals.h",
          "lines": "208-211",
          "snippet": "static inline void irqd_clr_move_pending(struct irq_data *d)\n{\n\t__irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;\n}",
          "includes": [
            "#include <linux/debugfs.h>",
            "#include \"settings.h\"",
            "#include \"debug.h\"",
            "#include <linux/sched/clock.h>",
            "#include <linux/pm_runtime.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/irqdesc.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/debugfs.h>\n#include \"settings.h\"\n#include \"debug.h\"\n#include <linux/sched/clock.h>\n#include <linux/pm_runtime.h>\n#include <linux/kernel_stat.h>\n#include <linux/irqdesc.h>\n\nstatic inline void irqd_clr_move_pending(struct irq_data *d)\n{\n\t__irqd_to_state(d) &= ~IRQD_SETAFFINITY_PENDING;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_any_and",
          "args": [
            "desc->pending_mask",
            "cpu_online_mask"
          ],
          "line": 29
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_is_setaffinity_pending",
          "args": [
            "data"
          ],
          "line": 22
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_desc_get_irq_data",
          "args": [
            "desc"
          ],
          "line": 20
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n\nbool irq_fixup_move_pending(struct irq_desc *desc, bool force_clear)\n{\n\tstruct irq_data *data = irq_desc_get_irq_data(desc);\n\n\tif (!irqd_is_setaffinity_pending(data))\n\t\treturn false;\n\n\t/*\n\t * The outgoing CPU might be the last online target in a pending\n\t * interrupt move. If that's the case clear the pending move bit.\n\t */\n\tif (cpumask_any_and(desc->pending_mask, cpu_online_mask) >= nr_cpu_ids) {\n\t\tirqd_clr_move_pending(data);\n\t\treturn false;\n\t}\n\tif (force_clear)\n\t\tirqd_clr_move_pending(data);\n\treturn true;\n}"
  }
]