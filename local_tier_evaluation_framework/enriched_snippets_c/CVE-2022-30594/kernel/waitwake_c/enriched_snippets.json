[
  {
    "function_name": "futex_wait_restart",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "694-707",
    "snippet": "static long futex_wait_restart(struct restart_block *restart)\n{\n\tu32 __user *uaddr = restart->futex.uaddr;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\n\treturn (long)futex_wait(uaddr, restart->futex.flags,\n\t\t\t\trestart->futex.val, tp, restart->futex.bitset);\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static long futex_wait_restart(struct restart_block *restart);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "futex_wait",
          "args": [
            "uaddr",
            "restart->futex.flags",
            "restart->futex.val",
            "tp",
            "restart->futex.bitset"
          ],
          "line": 705
        },
        "resolved": true,
        "details": {
          "function_name": "futex_wait",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "632-692",
          "snippet": "int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val, ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tto = futex_setup_timer(abs_time, &timeout, flags,\n\t\t\t       current->timer_slack_ns);\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, it holds hb->lock and q\n\t * is initialized.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* futex_queue and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\tif (!futex_unqueue(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current->restart_block;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = *abs_time;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = set_restart_fn(restart, futex_wait_restart);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static long futex_wait_restart(struct restart_block *restart);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic long futex_wait_restart(struct restart_block *restart);\n\nint futex_wait(u32 __user *uaddr, unsigned int flags, u32 val, ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tto = futex_setup_timer(abs_time, &timeout, flags,\n\t\t\t       current->timer_slack_ns);\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, it holds hb->lock and q\n\t * is initialized.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* futex_queue and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\tif (!futex_unqueue(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current->restart_block;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = *abs_time;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = set_restart_fn(restart, futex_wait_restart);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic long futex_wait_restart(struct restart_block *restart);\n\nstatic long futex_wait_restart(struct restart_block *restart)\n{\n\tu32 __user *uaddr = restart->futex.uaddr;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\n\treturn (long)futex_wait(uaddr, restart->futex.flags,\n\t\t\t\trestart->futex.val, tp, restart->futex.bitset);\n}"
  },
  {
    "function_name": "futex_wait",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "632-692",
    "snippet": "int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val, ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tto = futex_setup_timer(abs_time, &timeout, flags,\n\t\t\t       current->timer_slack_ns);\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, it holds hb->lock and q\n\t * is initialized.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* futex_queue and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\tif (!futex_unqueue(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current->restart_block;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = *abs_time;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = set_restart_fn(restart, futex_wait_restart);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static long futex_wait_restart(struct restart_block *restart);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "destroy_hrtimer_on_stack",
          "args": [
            "&to->timer"
          ],
          "line": 689
        },
        "resolved": true,
        "details": {
          "function_name": "destroy_hrtimer_on_stack",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/time/hrtimer.c",
          "lines": "450-453",
          "snippet": "void destroy_hrtimer_on_stack(struct hrtimer *timer)\n{\n\tdebug_object_free(timer, &hrtimer_debug_descr);\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nvoid destroy_hrtimer_on_stack(struct hrtimer *timer)\n{\n\tdebug_object_free(timer, &hrtimer_debug_descr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_cancel",
          "args": [
            "&to->timer"
          ],
          "line": 688
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_cancel",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/time/hrtimer.c",
          "lines": "1438-1449",
          "snippet": "int hrtimer_cancel(struct hrtimer *timer)\n{\n\tint ret;\n\n\tdo {\n\t\tret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret < 0)\n\t\t\thrtimer_cancel_wait_running(timer);\n\t} while (ret < 0);\n\treturn ret;\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nint hrtimer_cancel(struct hrtimer *timer)\n{\n\tint ret;\n\n\tdo {\n\t\tret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret < 0)\n\t\t\thrtimer_cancel_wait_running(timer);\n\t} while (ret < 0);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_restart_fn",
          "args": [
            "restart",
            "futex_wait_restart"
          ],
          "line": 684
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "signal_pending",
          "args": [
            "current"
          ],
          "line": 670
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_unqueue",
          "args": [
            "&q"
          ],
          "line": 660
        },
        "resolved": true,
        "details": {
          "function_name": "futex_unqueue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "620-627",
          "snippet": "void futex_unqueue_pi(struct futex_q *q)\n{\n\t__futex_unqueue(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid futex_unqueue_pi(struct futex_q *q)\n{\n\t__futex_unqueue(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_wait_queue",
          "args": [
            "hb",
            "&q",
            "to"
          ],
          "line": 656
        },
        "resolved": true,
        "details": {
          "function_name": "futex_wait_queue",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "328-358",
          "snippet": "void futex_wait_queue(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t    struct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using smp_store_mb() and\n\t * futex_queue() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tfutex_queue(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout)\n\t\thrtimer_sleeper_start_expires(timeout, HRTIMER_MODE_ABS);\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nvoid futex_wait_queue(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t    struct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using smp_store_mb() and\n\t * futex_queue() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tfutex_queue(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout)\n\t\thrtimer_sleeper_start_expires(timeout, HRTIMER_MODE_ABS);\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_wait_setup",
          "args": [
            "uaddr",
            "val",
            "flags",
            "&q",
            "&hb"
          ],
          "line": 651
        },
        "resolved": true,
        "details": {
          "function_name": "futex_wait_setup",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "577-630",
          "snippet": "int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t     struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = futex_q_lock(q);\n\n\tret = futex_get_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tfutex_q_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tfutex_q_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nint futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t     struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = futex_q_lock(q);\n\n\tret = futex_get_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tfutex_q_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tfutex_q_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_setup_timer",
          "args": [
            "abs_time",
            "&timeout",
            "flags",
            "current->timer_slack_ns"
          ],
          "line": 644
        },
        "resolved": true,
        "details": {
          "function_name": "futex_setup_timer",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "134-151",
          "snippet": "struct hrtimer_sleeper *\nfutex_setup_timer(ktime_t *time, struct hrtimer_sleeper *timeout,\n\t\t  int flags, u64 range_ns)\n{\n\tif (!time)\n\t\treturn NULL;\n\n\thrtimer_init_sleeper_on_stack(timeout, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t/*\n\t * If range_ns is 0, calling hrtimer_set_expires_range_ns() is\n\t * effectively the same as calling hrtimer_set_expires().\n\t */\n\thrtimer_set_expires_range_ns(&timeout->timer, *time, range_ns);\n\n\treturn timeout;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nstruct hrtimer_sleeper *\nfutex_setup_timer(ktime_t *time, struct hrtimer_sleeper *timeout,\n\t\t  int flags, u64 range_ns)\n{\n\tif (!time)\n\t\treturn NULL;\n\n\thrtimer_init_sleeper_on_stack(timeout, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t/*\n\t * If range_ns is 0, calling hrtimer_set_expires_range_ns() is\n\t * effectively the same as calling hrtimer_set_expires().\n\t */\n\thrtimer_set_expires_range_ns(&timeout->timer, *time, range_ns);\n\n\treturn timeout;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic long futex_wait_restart(struct restart_block *restart);\n\nint futex_wait(u32 __user *uaddr, unsigned int flags, u32 val, ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tto = futex_setup_timer(abs_time, &timeout, flags,\n\t\t\t       current->timer_slack_ns);\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, it holds hb->lock and q\n\t * is initialized.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* futex_queue and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\tif (!futex_unqueue(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current->restart_block;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = *abs_time;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = set_restart_fn(restart, futex_wait_restart);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}"
  },
  {
    "function_name": "futex_wait_setup",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "577-630",
    "snippet": "int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t     struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = futex_q_lock(q);\n\n\tret = futex_get_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tfutex_q_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tfutex_q_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\n\treturn ret;\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "futex_q_unlock",
          "args": [
            "*hb"
          ],
          "line": 625
        },
        "resolved": true,
        "details": {
          "function_name": "futex_q_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "536-541",
          "snippet": "void futex_q_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\tfutex_hb_waiters_dec(hb);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid futex_q_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\tfutex_hb_waiters_dec(hb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_user",
          "args": [
            "uval",
            "uaddr"
          ],
          "line": 614
        },
        "resolved": true,
        "details": {
          "function_name": "bpf_obj_get_user",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/inode.c",
          "lines": "530-557",
          "snippet": "int bpf_obj_get_user(const char __user *pathname, int flags)\n{\n\tenum bpf_type type = BPF_TYPE_UNSPEC;\n\tint f_flags;\n\tvoid *raw;\n\tint ret;\n\n\tf_flags = bpf_get_file_flag(flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\traw = bpf_obj_do_get(pathname, &type, f_flags);\n\tif (IS_ERR(raw))\n\t\treturn PTR_ERR(raw);\n\n\tif (type == BPF_TYPE_PROG)\n\t\tret = bpf_prog_new_fd(raw);\n\telse if (type == BPF_TYPE_MAP)\n\t\tret = bpf_map_new_fd(raw, f_flags);\n\telse if (type == BPF_TYPE_LINK)\n\t\tret = (f_flags != O_RDWR) ? -EINVAL : bpf_link_new_fd(raw);\n\telse\n\t\treturn -ENOENT;\n\n\tif (ret < 0)\n\t\tbpf_any_put(raw, type);\n\treturn ret;\n}",
          "includes": [
            "#include \"preload/bpf_preload.h\"",
            "#include <linux/bpf_trace.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/kdev_t.h>",
            "#include <linux/fs_parser.h>",
            "#include <linux/fs_context.h>",
            "#include <linux/fs.h>",
            "#include <linux/namei.h>",
            "#include <linux/mount.h>",
            "#include <linux/major.h>",
            "#include <linux/magic.h>",
            "#include <linux/init.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"preload/bpf_preload.h\"\n#include <linux/bpf_trace.h>\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <linux/kdev_t.h>\n#include <linux/fs_parser.h>\n#include <linux/fs_context.h>\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/major.h>\n#include <linux/magic.h>\n#include <linux/init.h>\n\nint bpf_obj_get_user(const char __user *pathname, int flags)\n{\n\tenum bpf_type type = BPF_TYPE_UNSPEC;\n\tint f_flags;\n\tvoid *raw;\n\tint ret;\n\n\tf_flags = bpf_get_file_flag(flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\traw = bpf_obj_do_get(pathname, &type, f_flags);\n\tif (IS_ERR(raw))\n\t\treturn PTR_ERR(raw);\n\n\tif (type == BPF_TYPE_PROG)\n\t\tret = bpf_prog_new_fd(raw);\n\telse if (type == BPF_TYPE_MAP)\n\t\tret = bpf_map_new_fd(raw, f_flags);\n\telse if (type == BPF_TYPE_LINK)\n\t\tret = (f_flags != O_RDWR) ? -EINVAL : bpf_link_new_fd(raw);\n\telse\n\t\treturn -ENOENT;\n\n\tif (ret < 0)\n\t\tbpf_any_put(raw, type);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_get_value_locked",
          "args": [
            "&uval",
            "uaddr"
          ],
          "line": 609
        },
        "resolved": true,
        "details": {
          "function_name": "futex_get_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "451-460",
          "snippet": "int futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_q_lock",
          "args": [
            "q"
          ],
          "line": 607
        },
        "resolved": true,
        "details": {
          "function_name": "futex_q_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "513-534",
          "snippet": "struct futex_hash_bucket *futex_q_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = futex_hash(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all futex_q_lock()\n\t * users end up calling futex_queue(). Similarly, for housekeeping,\n\t * decrement the counter at futex_q_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\tfutex_hb_waiters_inc(hb); /* implies smp_mb(); (A) */\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock);\n\treturn hb;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nstruct futex_hash_bucket *futex_q_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = futex_hash(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all futex_q_lock()\n\t * users end up calling futex_queue(). Similarly, for housekeeping,\n\t * decrement the counter at futex_q_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\tfutex_hb_waiters_inc(hb); /* implies smp_mb(); (A) */\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock);\n\treturn hb;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret != 0"
          ],
          "line": 603
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_futex_key",
          "args": [
            "uaddr",
            "flags & FLAGS_SHARED",
            "&q->key",
            "FUTEX_READ"
          ],
          "line": 602
        },
        "resolved": true,
        "details": {
          "function_name": "get_futex_key",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "220-395",
          "snippet": "int get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__read_mostly __aligned(2*sizeof(long));"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n__read_mostly __aligned(2*sizeof(long));\n\nint get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nint futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t     struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = futex_q_lock(q);\n\n\tret = futex_get_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tfutex_q_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tfutex_q_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\n\treturn ret;\n}"
  },
  {
    "function_name": "futex_wait_multiple",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "524-559",
    "snippet": "int futex_wait_multiple(struct futex_vector *vs, unsigned int count,\n\t\t\tstruct hrtimer_sleeper *to)\n{\n\tint ret, hint = 0;\n\n\tif (to)\n\t\thrtimer_sleeper_start_expires(to, HRTIMER_MODE_ABS);\n\n\twhile (1) {\n\t\tret = futex_wait_multiple_setup(vs, count, &hint);\n\t\tif (ret) {\n\t\t\tif (ret > 0) {\n\t\t\t\t/* A futex was woken during setup */\n\t\t\t\tret = hint;\n\t\t\t}\n\t\t\treturn ret;\n\t\t}\n\n\t\tfutex_sleep_multiple(vs, count, to);\n\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\tret = unqueue_multiple(vs, count);\n\t\tif (ret >= 0)\n\t\t\treturn ret;\n\n\t\tif (to && !to->task)\n\t\t\treturn -ETIMEDOUT;\n\t\telse if (signal_pending(current))\n\t\t\treturn -ERESTARTSYS;\n\t\t/*\n\t\t * The final case is a spurious wakeup, for\n\t\t * which just retry.\n\t\t */\n\t}\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "signal_pending",
          "args": [
            "current"
          ],
          "line": 552
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unqueue_multiple",
          "args": [
            "vs",
            "count"
          ],
          "line": 546
        },
        "resolved": true,
        "details": {
          "function_name": "unqueue_multiple",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "371-381",
          "snippet": "static int unqueue_multiple(struct futex_vector *v, int count)\n{\n\tint ret = -1, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!futex_unqueue(&v[i].q))\n\t\t\tret = i;\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic int unqueue_multiple(struct futex_vector *v, int count)\n{\n\tint ret = -1, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!futex_unqueue(&v[i].q))\n\t\t\tret = i;\n\t}\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 544
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_sleep_multiple",
          "args": [
            "vs",
            "count",
            "to"
          ],
          "line": 542
        },
        "resolved": true,
        "details": {
          "function_name": "futex_sleep_multiple",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "496-508",
          "snippet": "static void futex_sleep_multiple(struct futex_vector *vs, unsigned int count,\n\t\t\t\t struct hrtimer_sleeper *to)\n{\n\tif (to && !to->task)\n\t\treturn;\n\n\tfor (; count; count--, vs++) {\n\t\tif (!READ_ONCE(vs->q.lock_ptr))\n\t\t\treturn;\n\t}\n\n\tfreezable_schedule();\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic void futex_sleep_multiple(struct futex_vector *vs, unsigned int count,\n\t\t\t\t struct hrtimer_sleeper *to)\n{\n\tif (to && !to->task)\n\t\treturn;\n\n\tfor (; count; count--, vs++) {\n\t\tif (!READ_ONCE(vs->q.lock_ptr))\n\t\t\treturn;\n\t}\n\n\tfreezable_schedule();\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_wait_multiple_setup",
          "args": [
            "vs",
            "count",
            "&hint"
          ],
          "line": 533
        },
        "resolved": true,
        "details": {
          "function_name": "futex_wait_multiple_setup",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "399-485",
          "snippet": "static int futex_wait_multiple_setup(struct futex_vector *vs, int count, int *woken)\n{\n\tstruct futex_hash_bucket *hb;\n\tbool retry = false;\n\tint ret, i;\n\tu32 uval;\n\n\t/*\n\t * Enqueuing multiple futexes is tricky, because we need to enqueue\n\t * each futex on the list before dealing with the next one to avoid\n\t * deadlocking on the hash bucket. But, before enqueuing, we need to\n\t * make sure that current->state is TASK_INTERRUPTIBLE, so we don't\n\t * lose any wake events, which cannot be done before the get_futex_key\n\t * of the next key, because it calls get_user_pages, which can sleep.\n\t * Thus, we fetch the list of futexes keys in two steps, by first\n\t * pinning all the memory keys in the futex key, and only then we read\n\t * each key and queue the corresponding futex.\n\t *\n\t * Private futexes doesn't need to recalculate hash in retry, so skip\n\t * get_futex_key() when retrying.\n\t */\nretry:\n\tfor (i = 0; i < count; i++) {\n\t\tif ((vs[i].w.flags & FUTEX_PRIVATE_FLAG) && retry)\n\t\t\tcontinue;\n\n\t\tret = get_futex_key(u64_to_user_ptr(vs[i].w.uaddr),\n\t\t\t\t    !(vs[i].w.flags & FUTEX_PRIVATE_FLAG),\n\t\t\t\t    &vs[i].q.key, FUTEX_READ);\n\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\tfor (i = 0; i < count; i++) {\n\t\tu32 __user *uaddr = (u32 __user *)(unsigned long)vs[i].w.uaddr;\n\t\tstruct futex_q *q = &vs[i].q;\n\t\tu32 val = (u32)vs[i].w.val;\n\n\t\thb = futex_q_lock(q);\n\t\tret = futex_get_value_locked(&uval, uaddr);\n\n\t\tif (!ret && uval == val) {\n\t\t\t/*\n\t\t\t * The bucket lock can't be held while dealing with the\n\t\t\t * next futex. Queue each futex at this moment so hb can\n\t\t\t * be unlocked.\n\t\t\t */\n\t\t\tfutex_queue(q, hb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfutex_q_unlock(hb);\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\t/*\n\t\t * Even if something went wrong, if we find out that a futex\n\t\t * was woken, we don't return error and return this index to\n\t\t * userspace\n\t\t */\n\t\t*woken = unqueue_multiple(vs, i);\n\t\tif (*woken >= 0)\n\t\t\treturn 1;\n\n\t\tif (ret) {\n\t\t\t/*\n\t\t\t * If we need to handle a page fault, we need to do so\n\t\t\t * without any lock and any enqueued futex (otherwise\n\t\t\t * we could lose some wakeup). So we do it here, after\n\t\t\t * undoing all the work done so far. In success, we\n\t\t\t * retry all the work.\n\t\t\t */\n\t\t\tif (get_user(uval, uaddr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tretry = true;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (uval != val)\n\t\t\treturn -EWOULDBLOCK;\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic int futex_wait_multiple_setup(struct futex_vector *vs, int count, int *woken)\n{\n\tstruct futex_hash_bucket *hb;\n\tbool retry = false;\n\tint ret, i;\n\tu32 uval;\n\n\t/*\n\t * Enqueuing multiple futexes is tricky, because we need to enqueue\n\t * each futex on the list before dealing with the next one to avoid\n\t * deadlocking on the hash bucket. But, before enqueuing, we need to\n\t * make sure that current->state is TASK_INTERRUPTIBLE, so we don't\n\t * lose any wake events, which cannot be done before the get_futex_key\n\t * of the next key, because it calls get_user_pages, which can sleep.\n\t * Thus, we fetch the list of futexes keys in two steps, by first\n\t * pinning all the memory keys in the futex key, and only then we read\n\t * each key and queue the corresponding futex.\n\t *\n\t * Private futexes doesn't need to recalculate hash in retry, so skip\n\t * get_futex_key() when retrying.\n\t */\nretry:\n\tfor (i = 0; i < count; i++) {\n\t\tif ((vs[i].w.flags & FUTEX_PRIVATE_FLAG) && retry)\n\t\t\tcontinue;\n\n\t\tret = get_futex_key(u64_to_user_ptr(vs[i].w.uaddr),\n\t\t\t\t    !(vs[i].w.flags & FUTEX_PRIVATE_FLAG),\n\t\t\t\t    &vs[i].q.key, FUTEX_READ);\n\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\tfor (i = 0; i < count; i++) {\n\t\tu32 __user *uaddr = (u32 __user *)(unsigned long)vs[i].w.uaddr;\n\t\tstruct futex_q *q = &vs[i].q;\n\t\tu32 val = (u32)vs[i].w.val;\n\n\t\thb = futex_q_lock(q);\n\t\tret = futex_get_value_locked(&uval, uaddr);\n\n\t\tif (!ret && uval == val) {\n\t\t\t/*\n\t\t\t * The bucket lock can't be held while dealing with the\n\t\t\t * next futex. Queue each futex at this moment so hb can\n\t\t\t * be unlocked.\n\t\t\t */\n\t\t\tfutex_queue(q, hb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfutex_q_unlock(hb);\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\t/*\n\t\t * Even if something went wrong, if we find out that a futex\n\t\t * was woken, we don't return error and return this index to\n\t\t * userspace\n\t\t */\n\t\t*woken = unqueue_multiple(vs, i);\n\t\tif (*woken >= 0)\n\t\t\treturn 1;\n\n\t\tif (ret) {\n\t\t\t/*\n\t\t\t * If we need to handle a page fault, we need to do so\n\t\t\t * without any lock and any enqueued futex (otherwise\n\t\t\t * we could lose some wakeup). So we do it here, after\n\t\t\t * undoing all the work done so far. In success, we\n\t\t\t * retry all the work.\n\t\t\t */\n\t\t\tif (get_user(uval, uaddr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tretry = true;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (uval != val)\n\t\t\treturn -EWOULDBLOCK;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_sleeper_start_expires",
          "args": [
            "to",
            "HRTIMER_MODE_ABS"
          ],
          "line": 530
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_sleeper_start_expires",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/time/hrtimer.c",
          "lines": "1952-1966",
          "snippet": "void hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,\n\t\t\t\t   enum hrtimer_mode mode)\n{\n\t/*\n\t * Make the enqueue delivery mode check work on RT. If the sleeper\n\t * was initialized for hard interrupt delivery, force the mode bit.\n\t * This is a special case for hrtimer_sleepers because\n\t * hrtimer_init_sleeper() determines the delivery mode on RT so the\n\t * fiddling with this decision is avoided at the call sites.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && sl->timer.is_hard)\n\t\tmode |= HRTIMER_MODE_HARD;\n\n\thrtimer_start_expires(&sl->timer, mode);\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nvoid hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,\n\t\t\t\t   enum hrtimer_mode mode)\n{\n\t/*\n\t * Make the enqueue delivery mode check work on RT. If the sleeper\n\t * was initialized for hard interrupt delivery, force the mode bit.\n\t * This is a special case for hrtimer_sleepers because\n\t * hrtimer_init_sleeper() determines the delivery mode on RT so the\n\t * fiddling with this decision is avoided at the call sites.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && sl->timer.is_hard)\n\t\tmode |= HRTIMER_MODE_HARD;\n\n\thrtimer_start_expires(&sl->timer, mode);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nint futex_wait_multiple(struct futex_vector *vs, unsigned int count,\n\t\t\tstruct hrtimer_sleeper *to)\n{\n\tint ret, hint = 0;\n\n\tif (to)\n\t\thrtimer_sleeper_start_expires(to, HRTIMER_MODE_ABS);\n\n\twhile (1) {\n\t\tret = futex_wait_multiple_setup(vs, count, &hint);\n\t\tif (ret) {\n\t\t\tif (ret > 0) {\n\t\t\t\t/* A futex was woken during setup */\n\t\t\t\tret = hint;\n\t\t\t}\n\t\t\treturn ret;\n\t\t}\n\n\t\tfutex_sleep_multiple(vs, count, to);\n\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\tret = unqueue_multiple(vs, count);\n\t\tif (ret >= 0)\n\t\t\treturn ret;\n\n\t\tif (to && !to->task)\n\t\t\treturn -ETIMEDOUT;\n\t\telse if (signal_pending(current))\n\t\t\treturn -ERESTARTSYS;\n\t\t/*\n\t\t * The final case is a spurious wakeup, for\n\t\t * which just retry.\n\t\t */\n\t}\n}"
  },
  {
    "function_name": "futex_sleep_multiple",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "496-508",
    "snippet": "static void futex_sleep_multiple(struct futex_vector *vs, unsigned int count,\n\t\t\t\t struct hrtimer_sleeper *to)\n{\n\tif (to && !to->task)\n\t\treturn;\n\n\tfor (; count; count--, vs++) {\n\t\tif (!READ_ONCE(vs->q.lock_ptr))\n\t\t\treturn;\n\t}\n\n\tfreezable_schedule();\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "freezable_schedule",
          "args": [],
          "line": 507
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "vs->q.lock_ptr"
          ],
          "line": 503
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic void futex_sleep_multiple(struct futex_vector *vs, unsigned int count,\n\t\t\t\t struct hrtimer_sleeper *to)\n{\n\tif (to && !to->task)\n\t\treturn;\n\n\tfor (; count; count--, vs++) {\n\t\tif (!READ_ONCE(vs->q.lock_ptr))\n\t\t\treturn;\n\t}\n\n\tfreezable_schedule();\n}"
  },
  {
    "function_name": "futex_wait_multiple_setup",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "399-485",
    "snippet": "static int futex_wait_multiple_setup(struct futex_vector *vs, int count, int *woken)\n{\n\tstruct futex_hash_bucket *hb;\n\tbool retry = false;\n\tint ret, i;\n\tu32 uval;\n\n\t/*\n\t * Enqueuing multiple futexes is tricky, because we need to enqueue\n\t * each futex on the list before dealing with the next one to avoid\n\t * deadlocking on the hash bucket. But, before enqueuing, we need to\n\t * make sure that current->state is TASK_INTERRUPTIBLE, so we don't\n\t * lose any wake events, which cannot be done before the get_futex_key\n\t * of the next key, because it calls get_user_pages, which can sleep.\n\t * Thus, we fetch the list of futexes keys in two steps, by first\n\t * pinning all the memory keys in the futex key, and only then we read\n\t * each key and queue the corresponding futex.\n\t *\n\t * Private futexes doesn't need to recalculate hash in retry, so skip\n\t * get_futex_key() when retrying.\n\t */\nretry:\n\tfor (i = 0; i < count; i++) {\n\t\tif ((vs[i].w.flags & FUTEX_PRIVATE_FLAG) && retry)\n\t\t\tcontinue;\n\n\t\tret = get_futex_key(u64_to_user_ptr(vs[i].w.uaddr),\n\t\t\t\t    !(vs[i].w.flags & FUTEX_PRIVATE_FLAG),\n\t\t\t\t    &vs[i].q.key, FUTEX_READ);\n\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\tfor (i = 0; i < count; i++) {\n\t\tu32 __user *uaddr = (u32 __user *)(unsigned long)vs[i].w.uaddr;\n\t\tstruct futex_q *q = &vs[i].q;\n\t\tu32 val = (u32)vs[i].w.val;\n\n\t\thb = futex_q_lock(q);\n\t\tret = futex_get_value_locked(&uval, uaddr);\n\n\t\tif (!ret && uval == val) {\n\t\t\t/*\n\t\t\t * The bucket lock can't be held while dealing with the\n\t\t\t * next futex. Queue each futex at this moment so hb can\n\t\t\t * be unlocked.\n\t\t\t */\n\t\t\tfutex_queue(q, hb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfutex_q_unlock(hb);\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\t/*\n\t\t * Even if something went wrong, if we find out that a futex\n\t\t * was woken, we don't return error and return this index to\n\t\t * userspace\n\t\t */\n\t\t*woken = unqueue_multiple(vs, i);\n\t\tif (*woken >= 0)\n\t\t\treturn 1;\n\n\t\tif (ret) {\n\t\t\t/*\n\t\t\t * If we need to handle a page fault, we need to do so\n\t\t\t * without any lock and any enqueued futex (otherwise\n\t\t\t * we could lose some wakeup). So we do it here, after\n\t\t\t * undoing all the work done so far. In success, we\n\t\t\t * retry all the work.\n\t\t\t */\n\t\t\tif (get_user(uval, uaddr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tretry = true;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (uval != val)\n\t\t\treturn -EWOULDBLOCK;\n\t}\n\n\treturn 0;\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "get_user",
          "args": [
            "uval",
            "uaddr"
          ],
          "line": 473
        },
        "resolved": true,
        "details": {
          "function_name": "bpf_obj_get_user",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/inode.c",
          "lines": "530-557",
          "snippet": "int bpf_obj_get_user(const char __user *pathname, int flags)\n{\n\tenum bpf_type type = BPF_TYPE_UNSPEC;\n\tint f_flags;\n\tvoid *raw;\n\tint ret;\n\n\tf_flags = bpf_get_file_flag(flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\traw = bpf_obj_do_get(pathname, &type, f_flags);\n\tif (IS_ERR(raw))\n\t\treturn PTR_ERR(raw);\n\n\tif (type == BPF_TYPE_PROG)\n\t\tret = bpf_prog_new_fd(raw);\n\telse if (type == BPF_TYPE_MAP)\n\t\tret = bpf_map_new_fd(raw, f_flags);\n\telse if (type == BPF_TYPE_LINK)\n\t\tret = (f_flags != O_RDWR) ? -EINVAL : bpf_link_new_fd(raw);\n\telse\n\t\treturn -ENOENT;\n\n\tif (ret < 0)\n\t\tbpf_any_put(raw, type);\n\treturn ret;\n}",
          "includes": [
            "#include \"preload/bpf_preload.h\"",
            "#include <linux/bpf_trace.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/kdev_t.h>",
            "#include <linux/fs_parser.h>",
            "#include <linux/fs_context.h>",
            "#include <linux/fs.h>",
            "#include <linux/namei.h>",
            "#include <linux/mount.h>",
            "#include <linux/major.h>",
            "#include <linux/magic.h>",
            "#include <linux/init.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"preload/bpf_preload.h\"\n#include <linux/bpf_trace.h>\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <linux/kdev_t.h>\n#include <linux/fs_parser.h>\n#include <linux/fs_context.h>\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/major.h>\n#include <linux/magic.h>\n#include <linux/init.h>\n\nint bpf_obj_get_user(const char __user *pathname, int flags)\n{\n\tenum bpf_type type = BPF_TYPE_UNSPEC;\n\tint f_flags;\n\tvoid *raw;\n\tint ret;\n\n\tf_flags = bpf_get_file_flag(flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\traw = bpf_obj_do_get(pathname, &type, f_flags);\n\tif (IS_ERR(raw))\n\t\treturn PTR_ERR(raw);\n\n\tif (type == BPF_TYPE_PROG)\n\t\tret = bpf_prog_new_fd(raw);\n\telse if (type == BPF_TYPE_MAP)\n\t\tret = bpf_map_new_fd(raw, f_flags);\n\telse if (type == BPF_TYPE_LINK)\n\t\tret = (f_flags != O_RDWR) ? -EINVAL : bpf_link_new_fd(raw);\n\telse\n\t\treturn -ENOENT;\n\n\tif (ret < 0)\n\t\tbpf_any_put(raw, type);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unqueue_multiple",
          "args": [
            "vs",
            "i"
          ],
          "line": 461
        },
        "resolved": true,
        "details": {
          "function_name": "unqueue_multiple",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "371-381",
          "snippet": "static int unqueue_multiple(struct futex_vector *v, int count)\n{\n\tint ret = -1, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!futex_unqueue(&v[i].q))\n\t\t\tret = i;\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic int unqueue_multiple(struct futex_vector *v, int count)\n{\n\tint ret = -1, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!futex_unqueue(&v[i].q))\n\t\t\tret = i;\n\t}\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 454
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_q_unlock",
          "args": [
            "hb"
          ],
          "line": 453
        },
        "resolved": true,
        "details": {
          "function_name": "futex_q_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "536-541",
          "snippet": "void futex_q_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\tfutex_hb_waiters_dec(hb);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid futex_q_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\tfutex_hb_waiters_dec(hb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_queue",
          "args": [
            "q",
            "hb"
          ],
          "line": 449
        },
        "resolved": true,
        "details": {
          "function_name": "__futex_queue",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "543-560",
          "snippet": "void __futex_queue(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid __futex_queue(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_get_value_locked",
          "args": [
            "&uval",
            "uaddr"
          ],
          "line": 441
        },
        "resolved": true,
        "details": {
          "function_name": "futex_get_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "451-460",
          "snippet": "int futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_q_lock",
          "args": [
            "q"
          ],
          "line": 440
        },
        "resolved": true,
        "details": {
          "function_name": "futex_q_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "513-534",
          "snippet": "struct futex_hash_bucket *futex_q_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = futex_hash(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all futex_q_lock()\n\t * users end up calling futex_queue(). Similarly, for housekeeping,\n\t * decrement the counter at futex_q_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\tfutex_hb_waiters_inc(hb); /* implies smp_mb(); (A) */\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock);\n\treturn hb;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nstruct futex_hash_bucket *futex_q_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = futex_hash(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all futex_q_lock()\n\t * users end up calling futex_queue(). Similarly, for housekeeping,\n\t * decrement the counter at futex_q_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\tfutex_hb_waiters_inc(hb); /* implies smp_mb(); (A) */\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock);\n\treturn hb;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "TASK_INTERRUPTIBLE"
          ],
          "line": 433
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret"
          ],
          "line": 429
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_futex_key",
          "args": [
            "u64_to_user_ptr(vs[i].w.uaddr)",
            "!(vs[i].w.flags & FUTEX_PRIVATE_FLAG)",
            "&vs[i].q.key",
            "FUTEX_READ"
          ],
          "line": 425
        },
        "resolved": true,
        "details": {
          "function_name": "get_futex_key",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "220-395",
          "snippet": "int get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__read_mostly __aligned(2*sizeof(long));"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n__read_mostly __aligned(2*sizeof(long));\n\nint get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}"
        }
      },
      {
        "call_info": {
          "callee": "u64_to_user_ptr",
          "args": [
            "vs[i].w.uaddr"
          ],
          "line": 425
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic int futex_wait_multiple_setup(struct futex_vector *vs, int count, int *woken)\n{\n\tstruct futex_hash_bucket *hb;\n\tbool retry = false;\n\tint ret, i;\n\tu32 uval;\n\n\t/*\n\t * Enqueuing multiple futexes is tricky, because we need to enqueue\n\t * each futex on the list before dealing with the next one to avoid\n\t * deadlocking on the hash bucket. But, before enqueuing, we need to\n\t * make sure that current->state is TASK_INTERRUPTIBLE, so we don't\n\t * lose any wake events, which cannot be done before the get_futex_key\n\t * of the next key, because it calls get_user_pages, which can sleep.\n\t * Thus, we fetch the list of futexes keys in two steps, by first\n\t * pinning all the memory keys in the futex key, and only then we read\n\t * each key and queue the corresponding futex.\n\t *\n\t * Private futexes doesn't need to recalculate hash in retry, so skip\n\t * get_futex_key() when retrying.\n\t */\nretry:\n\tfor (i = 0; i < count; i++) {\n\t\tif ((vs[i].w.flags & FUTEX_PRIVATE_FLAG) && retry)\n\t\t\tcontinue;\n\n\t\tret = get_futex_key(u64_to_user_ptr(vs[i].w.uaddr),\n\t\t\t\t    !(vs[i].w.flags & FUTEX_PRIVATE_FLAG),\n\t\t\t\t    &vs[i].q.key, FUTEX_READ);\n\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\tfor (i = 0; i < count; i++) {\n\t\tu32 __user *uaddr = (u32 __user *)(unsigned long)vs[i].w.uaddr;\n\t\tstruct futex_q *q = &vs[i].q;\n\t\tu32 val = (u32)vs[i].w.val;\n\n\t\thb = futex_q_lock(q);\n\t\tret = futex_get_value_locked(&uval, uaddr);\n\n\t\tif (!ret && uval == val) {\n\t\t\t/*\n\t\t\t * The bucket lock can't be held while dealing with the\n\t\t\t * next futex. Queue each futex at this moment so hb can\n\t\t\t * be unlocked.\n\t\t\t */\n\t\t\tfutex_queue(q, hb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfutex_q_unlock(hb);\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\t/*\n\t\t * Even if something went wrong, if we find out that a futex\n\t\t * was woken, we don't return error and return this index to\n\t\t * userspace\n\t\t */\n\t\t*woken = unqueue_multiple(vs, i);\n\t\tif (*woken >= 0)\n\t\t\treturn 1;\n\n\t\tif (ret) {\n\t\t\t/*\n\t\t\t * If we need to handle a page fault, we need to do so\n\t\t\t * without any lock and any enqueued futex (otherwise\n\t\t\t * we could lose some wakeup). So we do it here, after\n\t\t\t * undoing all the work done so far. In success, we\n\t\t\t * retry all the work.\n\t\t\t */\n\t\t\tif (get_user(uval, uaddr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tretry = true;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (uval != val)\n\t\t\treturn -EWOULDBLOCK;\n\t}\n\n\treturn 0;\n}"
  },
  {
    "function_name": "unqueue_multiple",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "371-381",
    "snippet": "static int unqueue_multiple(struct futex_vector *v, int count)\n{\n\tint ret = -1, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!futex_unqueue(&v[i].q))\n\t\t\tret = i;\n\t}\n\n\treturn ret;\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "futex_unqueue",
          "args": [
            "&v[i].q"
          ],
          "line": 376
        },
        "resolved": true,
        "details": {
          "function_name": "futex_unqueue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "620-627",
          "snippet": "void futex_unqueue_pi(struct futex_q *q)\n{\n\t__futex_unqueue(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid futex_unqueue_pi(struct futex_q *q)\n{\n\t__futex_unqueue(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic int unqueue_multiple(struct futex_vector *v, int count)\n{\n\tint ret = -1, i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!futex_unqueue(&v[i].q))\n\t\t\tret = i;\n\t}\n\n\treturn ret;\n}"
  },
  {
    "function_name": "futex_wait_queue",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "328-358",
    "snippet": "void futex_wait_queue(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t    struct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using smp_store_mb() and\n\t * futex_queue() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tfutex_queue(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout)\n\t\thrtimer_sleeper_start_expires(timeout, HRTIMER_MODE_ABS);\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 357
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "freezable_schedule",
          "args": [],
          "line": 355
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!plist_node_empty(&q->list)"
          ],
          "line": 348
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "plist_node_empty",
          "args": [
            "&q->list"
          ],
          "line": 348
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "hrtimer_sleeper_start_expires",
          "args": [
            "timeout",
            "HRTIMER_MODE_ABS"
          ],
          "line": 342
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_sleeper_start_expires",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/time/hrtimer.c",
          "lines": "1952-1966",
          "snippet": "void hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,\n\t\t\t\t   enum hrtimer_mode mode)\n{\n\t/*\n\t * Make the enqueue delivery mode check work on RT. If the sleeper\n\t * was initialized for hard interrupt delivery, force the mode bit.\n\t * This is a special case for hrtimer_sleepers because\n\t * hrtimer_init_sleeper() determines the delivery mode on RT so the\n\t * fiddling with this decision is avoided at the call sites.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && sl->timer.is_hard)\n\t\tmode |= HRTIMER_MODE_HARD;\n\n\thrtimer_start_expires(&sl->timer, mode);\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nvoid hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,\n\t\t\t\t   enum hrtimer_mode mode)\n{\n\t/*\n\t * Make the enqueue delivery mode check work on RT. If the sleeper\n\t * was initialized for hard interrupt delivery, force the mode bit.\n\t * This is a special case for hrtimer_sleepers because\n\t * hrtimer_init_sleeper() determines the delivery mode on RT so the\n\t * fiddling with this decision is avoided at the call sites.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && sl->timer.is_hard)\n\t\tmode |= HRTIMER_MODE_HARD;\n\n\thrtimer_start_expires(&sl->timer, mode);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_queue",
          "args": [
            "q",
            "hb"
          ],
          "line": 338
        },
        "resolved": true,
        "details": {
          "function_name": "__futex_queue",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "543-560",
          "snippet": "void __futex_queue(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid __futex_queue(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "TASK_INTERRUPTIBLE"
          ],
          "line": 337
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nvoid futex_wait_queue(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t    struct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using smp_store_mb() and\n\t * futex_queue() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tfutex_queue(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout)\n\t\thrtimer_sleeper_start_expires(timeout, HRTIMER_MODE_ABS);\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}"
  },
  {
    "function_name": "futex_wake_op",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "238-318",
    "snippet": "int futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,\n\t\t  int nr_wake, int nr_wake2, int op)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tint ret, op_ret;\n\tDEFINE_WAKE_Q(wake_q);\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\thb1 = futex_hash(&key1);\n\thb2 = futex_hash(&key2);\n\nretry_private:\n\tdouble_lock_hb(hb1, hb2);\n\top_ret = futex_atomic_op_inuser(op, uaddr2);\n\tif (unlikely(op_ret < 0)) {\n\t\tdouble_unlock_hb(hb1, hb2);\n\n\t\tif (!IS_ENABLED(CONFIG_MMU) ||\n\t\t    unlikely(op_ret != -EFAULT && op_ret != -EAGAIN)) {\n\t\t\t/*\n\t\t\t * we don't get EFAULT from MMU faults if we don't have\n\t\t\t * an MMU, but we might get them from range checking\n\t\t\t */\n\t\t\tret = op_ret;\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (op_ret == -EFAULT) {\n\t\t\tret = fault_in_user_writeable(uaddr2);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tcond_resched();\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\t\tgoto retry;\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (futex_match (&this->key, &key1)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tfutex_wake_mark(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (op_ret > 0) {\n\t\top_ret = 0;\n\t\tplist_for_each_entry_safe(this, next, &hb2->chain, list) {\n\t\t\tif (futex_match (&this->key, &key2)) {\n\t\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t\tfutex_wake_mark(&wake_q, this);\n\t\t\t\tif (++op_ret >= nr_wake2)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret += op_ret;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\twake_up_q(&wake_q);\n\treturn ret;\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "&wake_q"
          ],
          "line": 316
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_q",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "948-967",
          "snippet": "void wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic __always_inline struct;\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "double_unlock_hb",
          "args": [
            "hb1",
            "hb2"
          ],
          "line": 315
        },
        "resolved": true,
        "details": {
          "function_name": "double_unlock_hb",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/futex.h",
          "lines": "248-254",
          "snippet": "static inline void\ndouble_unlock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tspin_unlock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_unlock(&hb2->lock);\n}",
          "includes": [
            "#include <asm/futex.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/futex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/futex.h>\n#include <linux/rcuwait.h>\n#include <linux/sched/wake_q.h>\n#include <linux/futex.h>\n\nstatic inline void\ndouble_unlock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tspin_unlock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_unlock(&hb2->lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_wake_mark",
          "args": [
            "&wake_q",
            "this"
          ],
          "line": 306
        },
        "resolved": true,
        "details": {
          "function_name": "futex_wake_mark",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "115-138",
          "snippet": "void futex_wake_mark(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\tget_task_struct(p);\n\t__futex_unqueue(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __futex_unqueue().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock.\n\t */\n\twake_q_add_safe(wake_q, p);\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nvoid futex_wake_mark(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\tget_task_struct(p);\n\t__futex_unqueue(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __futex_unqueue().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock.\n\t */\n\twake_q_add_safe(wake_q, p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_match",
          "args": [
            "&this->key",
            "&key2"
          ],
          "line": 301
        },
        "resolved": true,
        "details": {
          "function_name": "futex_match",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/futex.h",
          "lines": "134-140",
          "snippet": "static inline int futex_match(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}",
          "includes": [
            "#include <asm/futex.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/futex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/futex.h>\n#include <linux/rcuwait.h>\n#include <linux/sched/wake_q.h>\n#include <linux/futex.h>\n\nstatic inline int futex_match(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}"
        }
      },
      {
        "call_info": {
          "callee": "plist_for_each_entry_safe",
          "args": [
            "this",
            "next",
            "&hb2->chain",
            "list"
          ],
          "line": 300
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "plist_for_each_entry_safe",
          "args": [
            "this",
            "next",
            "&hb1->chain",
            "list"
          ],
          "line": 286
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cond_resched",
          "args": [],
          "line": 280
        },
        "resolved": true,
        "details": {
          "function_name": "__cond_resched",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8172-8193",
          "snippet": "int __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic void __sched;\nstatic void __sched;\n\nint __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "fault_in_user_writeable",
          "args": [
            "uaddr2"
          ],
          "line": 275
        },
        "resolved": true,
        "details": {
          "function_name": "fault_in_user_writeable",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "409-420",
          "snippet": "int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "op_ret != -EFAULT && op_ret != -EAGAIN"
          ],
          "line": 265
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_MMU"
          ],
          "line": 264
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "op_ret < 0"
          ],
          "line": 261
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_atomic_op_inuser",
          "args": [
            "op",
            "uaddr2"
          ],
          "line": 260
        },
        "resolved": true,
        "details": {
          "function_name": "futex_atomic_op_inuser",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "188-232",
          "snippet": "static int futex_atomic_op_inuser(unsigned int encoded_op, u32 __user *uaddr)\n{\n\tunsigned int op =\t  (encoded_op & 0x70000000) >> 28;\n\tunsigned int cmp =\t  (encoded_op & 0x0f000000) >> 24;\n\tint oparg = sign_extend32((encoded_op & 0x00fff000) >> 12, 11);\n\tint cmparg = sign_extend32(encoded_op & 0x00000fff, 11);\n\tint oldval, ret;\n\n\tif (encoded_op & (FUTEX_OP_OPARG_SHIFT << 28)) {\n\t\tif (oparg < 0 || oparg > 31) {\n\t\t\tchar comm[sizeof(current->comm)];\n\t\t\t/*\n\t\t\t * kill this print and return -EINVAL when userspace\n\t\t\t * is sane again\n\t\t\t */\n\t\t\tpr_info_ratelimited(\"futex_wake_op: %s tries to shift op by %d; fix this program\\n\",\n\t\t\t\t\tget_task_comm(comm, current), oparg);\n\t\t\toparg &= 31;\n\t\t}\n\t\toparg = 1 << oparg;\n\t}\n\n\tpagefault_disable();\n\tret = arch_futex_atomic_op_inuser(op, oparg, &oldval, uaddr);\n\tpagefault_enable();\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (cmp) {\n\tcase FUTEX_OP_CMP_EQ:\n\t\treturn oldval == cmparg;\n\tcase FUTEX_OP_CMP_NE:\n\t\treturn oldval != cmparg;\n\tcase FUTEX_OP_CMP_LT:\n\t\treturn oldval < cmparg;\n\tcase FUTEX_OP_CMP_GE:\n\t\treturn oldval >= cmparg;\n\tcase FUTEX_OP_CMP_LE:\n\t\treturn oldval <= cmparg;\n\tcase FUTEX_OP_CMP_GT:\n\t\treturn oldval > cmparg;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic int futex_atomic_op_inuser(unsigned int encoded_op, u32 __user *uaddr)\n{\n\tunsigned int op =\t  (encoded_op & 0x70000000) >> 28;\n\tunsigned int cmp =\t  (encoded_op & 0x0f000000) >> 24;\n\tint oparg = sign_extend32((encoded_op & 0x00fff000) >> 12, 11);\n\tint cmparg = sign_extend32(encoded_op & 0x00000fff, 11);\n\tint oldval, ret;\n\n\tif (encoded_op & (FUTEX_OP_OPARG_SHIFT << 28)) {\n\t\tif (oparg < 0 || oparg > 31) {\n\t\t\tchar comm[sizeof(current->comm)];\n\t\t\t/*\n\t\t\t * kill this print and return -EINVAL when userspace\n\t\t\t * is sane again\n\t\t\t */\n\t\t\tpr_info_ratelimited(\"futex_wake_op: %s tries to shift op by %d; fix this program\\n\",\n\t\t\t\t\tget_task_comm(comm, current), oparg);\n\t\t\toparg &= 31;\n\t\t}\n\t\toparg = 1 << oparg;\n\t}\n\n\tpagefault_disable();\n\tret = arch_futex_atomic_op_inuser(op, oparg, &oldval, uaddr);\n\tpagefault_enable();\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (cmp) {\n\tcase FUTEX_OP_CMP_EQ:\n\t\treturn oldval == cmparg;\n\tcase FUTEX_OP_CMP_NE:\n\t\treturn oldval != cmparg;\n\tcase FUTEX_OP_CMP_LT:\n\t\treturn oldval < cmparg;\n\tcase FUTEX_OP_CMP_GE:\n\t\treturn oldval >= cmparg;\n\tcase FUTEX_OP_CMP_LE:\n\t\treturn oldval <= cmparg;\n\tcase FUTEX_OP_CMP_GT:\n\t\treturn oldval > cmparg;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "double_lock_hb",
          "args": [
            "hb1",
            "hb2"
          ],
          "line": 259
        },
        "resolved": true,
        "details": {
          "function_name": "double_lock_hb",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/futex.h",
          "lines": "237-246",
          "snippet": "static inline void\ndouble_lock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tif (hb1 > hb2)\n\t\tswap(hb1, hb2);\n\n\tspin_lock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_lock_nested(&hb2->lock, SINGLE_DEPTH_NESTING);\n}",
          "includes": [
            "#include <asm/futex.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/futex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/futex.h>\n#include <linux/rcuwait.h>\n#include <linux/sched/wake_q.h>\n#include <linux/futex.h>\n\nstatic inline void\ndouble_lock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tif (hb1 > hb2)\n\t\tswap(hb1, hb2);\n\n\tspin_lock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_lock_nested(&hb2->lock, SINGLE_DEPTH_NESTING);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_hash",
          "args": [
            "&key2"
          ],
          "line": 256
        },
        "resolved": true,
        "details": {
          "function_name": "futex_hash",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "115-121",
          "snippet": "struct futex_hash_bucket *futex_hash(union futex_key *key)\n{\n\tu32 hash = jhash2((u32 *)key, offsetof(typeof(*key), both.offset) / 4,\n\t\t\t  key->both.offset);\n\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [
            "#define futex_hashsize (__futex_data.hashsize)",
            "#define futex_queues   (__futex_data.queues)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n#define futex_hashsize (__futex_data.hashsize)\n#define futex_queues   (__futex_data.queues)\n\nstruct futex_hash_bucket *futex_hash(union futex_key *key)\n{\n\tu32 hash = jhash2((u32 *)key, offsetof(typeof(*key), both.offset) / 4,\n\t\t\t  key->both.offset);\n\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret != 0"
          ],
          "line": 252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_futex_key",
          "args": [
            "uaddr2",
            "flags & FLAGS_SHARED",
            "&key2",
            "FUTEX_WRITE"
          ],
          "line": 251
        },
        "resolved": true,
        "details": {
          "function_name": "get_futex_key",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "220-395",
          "snippet": "int get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__read_mostly __aligned(2*sizeof(long));"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n__read_mostly __aligned(2*sizeof(long));\n\nint get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret != 0"
          ],
          "line": 249
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 245
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nint futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,\n\t\t  int nr_wake, int nr_wake2, int op)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tint ret, op_ret;\n\tDEFINE_WAKE_Q(wake_q);\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\thb1 = futex_hash(&key1);\n\thb2 = futex_hash(&key2);\n\nretry_private:\n\tdouble_lock_hb(hb1, hb2);\n\top_ret = futex_atomic_op_inuser(op, uaddr2);\n\tif (unlikely(op_ret < 0)) {\n\t\tdouble_unlock_hb(hb1, hb2);\n\n\t\tif (!IS_ENABLED(CONFIG_MMU) ||\n\t\t    unlikely(op_ret != -EFAULT && op_ret != -EAGAIN)) {\n\t\t\t/*\n\t\t\t * we don't get EFAULT from MMU faults if we don't have\n\t\t\t * an MMU, but we might get them from range checking\n\t\t\t */\n\t\t\tret = op_ret;\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (op_ret == -EFAULT) {\n\t\t\tret = fault_in_user_writeable(uaddr2);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tcond_resched();\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\t\tgoto retry;\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (futex_match (&this->key, &key1)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tfutex_wake_mark(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (op_ret > 0) {\n\t\top_ret = 0;\n\t\tplist_for_each_entry_safe(this, next, &hb2->chain, list) {\n\t\t\tif (futex_match (&this->key, &key2)) {\n\t\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t\tfutex_wake_mark(&wake_q, this);\n\t\t\t\tif (++op_ret >= nr_wake2)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret += op_ret;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\twake_up_q(&wake_q);\n\treturn ret;\n}"
  },
  {
    "function_name": "futex_atomic_op_inuser",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "188-232",
    "snippet": "static int futex_atomic_op_inuser(unsigned int encoded_op, u32 __user *uaddr)\n{\n\tunsigned int op =\t  (encoded_op & 0x70000000) >> 28;\n\tunsigned int cmp =\t  (encoded_op & 0x0f000000) >> 24;\n\tint oparg = sign_extend32((encoded_op & 0x00fff000) >> 12, 11);\n\tint cmparg = sign_extend32(encoded_op & 0x00000fff, 11);\n\tint oldval, ret;\n\n\tif (encoded_op & (FUTEX_OP_OPARG_SHIFT << 28)) {\n\t\tif (oparg < 0 || oparg > 31) {\n\t\t\tchar comm[sizeof(current->comm)];\n\t\t\t/*\n\t\t\t * kill this print and return -EINVAL when userspace\n\t\t\t * is sane again\n\t\t\t */\n\t\t\tpr_info_ratelimited(\"futex_wake_op: %s tries to shift op by %d; fix this program\\n\",\n\t\t\t\t\tget_task_comm(comm, current), oparg);\n\t\t\toparg &= 31;\n\t\t}\n\t\toparg = 1 << oparg;\n\t}\n\n\tpagefault_disable();\n\tret = arch_futex_atomic_op_inuser(op, oparg, &oldval, uaddr);\n\tpagefault_enable();\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (cmp) {\n\tcase FUTEX_OP_CMP_EQ:\n\t\treturn oldval == cmparg;\n\tcase FUTEX_OP_CMP_NE:\n\t\treturn oldval != cmparg;\n\tcase FUTEX_OP_CMP_LT:\n\t\treturn oldval < cmparg;\n\tcase FUTEX_OP_CMP_GE:\n\t\treturn oldval >= cmparg;\n\tcase FUTEX_OP_CMP_LE:\n\t\treturn oldval <= cmparg;\n\tcase FUTEX_OP_CMP_GT:\n\t\treturn oldval > cmparg;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "pagefault_enable",
          "args": [],
          "line": 212
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "arch_futex_atomic_op_inuser",
          "args": [
            "op",
            "oparg",
            "&oldval",
            "uaddr"
          ],
          "line": 211
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pagefault_disable",
          "args": [],
          "line": 210
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_info_ratelimited",
          "args": [
            "\"futex_wake_op: %s tries to shift op by %d; fix this program\\n\"",
            "get_task_comm(comm, current)",
            "oparg"
          ],
          "line": 203
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_task_comm",
          "args": [
            "comm",
            "current"
          ],
          "line": 204
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sign_extend32",
          "args": [
            "encoded_op & 0x00000fff",
            "11"
          ],
          "line": 193
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sign_extend32",
          "args": [
            "(encoded_op & 0x00fff000) >> 12",
            "11"
          ],
          "line": 192
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nstatic int futex_atomic_op_inuser(unsigned int encoded_op, u32 __user *uaddr)\n{\n\tunsigned int op =\t  (encoded_op & 0x70000000) >> 28;\n\tunsigned int cmp =\t  (encoded_op & 0x0f000000) >> 24;\n\tint oparg = sign_extend32((encoded_op & 0x00fff000) >> 12, 11);\n\tint cmparg = sign_extend32(encoded_op & 0x00000fff, 11);\n\tint oldval, ret;\n\n\tif (encoded_op & (FUTEX_OP_OPARG_SHIFT << 28)) {\n\t\tif (oparg < 0 || oparg > 31) {\n\t\t\tchar comm[sizeof(current->comm)];\n\t\t\t/*\n\t\t\t * kill this print and return -EINVAL when userspace\n\t\t\t * is sane again\n\t\t\t */\n\t\t\tpr_info_ratelimited(\"futex_wake_op: %s tries to shift op by %d; fix this program\\n\",\n\t\t\t\t\tget_task_comm(comm, current), oparg);\n\t\t\toparg &= 31;\n\t\t}\n\t\toparg = 1 << oparg;\n\t}\n\n\tpagefault_disable();\n\tret = arch_futex_atomic_op_inuser(op, oparg, &oldval, uaddr);\n\tpagefault_enable();\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (cmp) {\n\tcase FUTEX_OP_CMP_EQ:\n\t\treturn oldval == cmparg;\n\tcase FUTEX_OP_CMP_NE:\n\t\treturn oldval != cmparg;\n\tcase FUTEX_OP_CMP_LT:\n\t\treturn oldval < cmparg;\n\tcase FUTEX_OP_CMP_GE:\n\t\treturn oldval >= cmparg;\n\tcase FUTEX_OP_CMP_LE:\n\t\treturn oldval <= cmparg;\n\tcase FUTEX_OP_CMP_GT:\n\t\treturn oldval > cmparg;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n}"
  },
  {
    "function_name": "futex_wake",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "143-186",
    "snippet": "int futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tint ret;\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\thb = futex_hash(&key);\n\n\t/* Make sure we really have tasks to wakeup */\n\tif (!futex_hb_waiters_pending(hb))\n\t\treturn ret;\n\n\tspin_lock(&hb->lock);\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (futex_match (&this->key, &key)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check if one of the bits is set in both bitsets */\n\t\t\tif (!(this->bitset & bitset))\n\t\t\t\tcontinue;\n\n\t\t\tfutex_wake_mark(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&hb->lock);\n\twake_up_q(&wake_q);\n\treturn ret;\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "&wake_q"
          ],
          "line": 184
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_q",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "948-967",
          "snippet": "void wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic __always_inline struct;\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&hb->lock"
          ],
          "line": 183
        },
        "resolved": true,
        "details": {
          "function_name": "__bpf_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/helpers.c",
          "lines": "315-322",
          "snippet": "static inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}",
          "includes": [
            "#include \"../../lib/kstrtox.h\"",
            "#include <linux/security.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/ctype.h>",
            "#include <linux/filter.h>",
            "#include <linux/uidgid.h>",
            "#include <linux/sched.h>",
            "#include <linux/ktime.h>",
            "#include <linux/topology.h>",
            "#include <linux/smp.h>",
            "#include <linux/random.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <linux/bpf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU(unsigned long, irqsave_flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../../lib/kstrtox.h\"\n#include <linux/security.h>\n#include <linux/proc_ns.h>\n#include <linux/pid_namespace.h>\n#include <linux/jiffies.h>\n#include <linux/ctype.h>\n#include <linux/filter.h>\n#include <linux/uidgid.h>\n#include <linux/sched.h>\n#include <linux/ktime.h>\n#include <linux/topology.h>\n#include <linux/smp.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/bpf.h>\n\nstatic DEFINE_PER_CPU(unsigned long, irqsave_flags);\n\nstatic inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_wake_mark",
          "args": [
            "&wake_q",
            "this"
          ],
          "line": 177
        },
        "resolved": true,
        "details": {
          "function_name": "futex_wake_mark",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
          "lines": "115-138",
          "snippet": "void futex_wake_mark(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\tget_task_struct(p);\n\t__futex_unqueue(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __futex_unqueue().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock.\n\t */\n\twake_q_add_safe(wake_q, p);\n}",
          "includes": [
            "#include \"futex.h\"",
            "#include <linux/freezer.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nvoid futex_wake_mark(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\tget_task_struct(p);\n\t__futex_unqueue(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __futex_unqueue().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock.\n\t */\n\twake_q_add_safe(wake_q, p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_match",
          "args": [
            "&this->key",
            "&key"
          ],
          "line": 167
        },
        "resolved": true,
        "details": {
          "function_name": "futex_match",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/futex.h",
          "lines": "134-140",
          "snippet": "static inline int futex_match(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}",
          "includes": [
            "#include <asm/futex.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/futex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/futex.h>\n#include <linux/rcuwait.h>\n#include <linux/sched/wake_q.h>\n#include <linux/futex.h>\n\nstatic inline int futex_match(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}"
        }
      },
      {
        "call_info": {
          "callee": "plist_for_each_entry_safe",
          "args": [
            "this",
            "next",
            "&hb->chain",
            "list"
          ],
          "line": 166
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "&hb->lock"
          ],
          "line": 164
        },
        "resolved": true,
        "details": {
          "function_name": "reg_may_point_to_spin_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/verifier.c",
          "lines": "445-449",
          "snippet": "static bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}",
          "includes": [
            "#include \"disasm.h\"",
            "#include <linux/btf_ids.h>",
            "#include <linux/bpf_lsm.h>",
            "#include <linux/error-injection.h>",
            "#include <linux/ctype.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sort.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/stringify.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <net/netlink.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf_verifier.h>",
            "#include <linux/btf.h>",
            "#include <linux/bpf.h>",
            "#include <linux/slab.h>",
            "#include <linux/types.h>",
            "#include <linux/kernel.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <uapi/linux/btf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"disasm.h\"\n#include <linux/btf_ids.h>\n#include <linux/bpf_lsm.h>\n#include <linux/error-injection.h>\n#include <linux/ctype.h>\n#include <linux/perf_event.h>\n#include <linux/sort.h>\n#include <linux/bsearch.h>\n#include <linux/stringify.h>\n#include <linux/vmalloc.h>\n#include <linux/file.h>\n#include <net/netlink.h>\n#include <linux/filter.h>\n#include <linux/bpf_verifier.h>\n#include <linux/btf.h>\n#include <linux/bpf.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/bpf-cgroup.h>\n#include <uapi/linux/btf.h>\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);\n\nstatic bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_hb_waiters_pending",
          "args": [
            "hb"
          ],
          "line": 161
        },
        "resolved": true,
        "details": {
          "function_name": "futex_hb_waiters_pending",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/futex.h",
          "lines": "205-216",
          "snippet": "static inline int futex_hb_waiters_pending(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * Full barrier (B), see the ordering comment above.\n\t */\n\tsmp_mb();\n\treturn atomic_read(&hb->waiters);\n#else\n\treturn 1;\n#endif\n}",
          "includes": [
            "#include <asm/futex.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/futex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/futex.h>\n#include <linux/rcuwait.h>\n#include <linux/sched/wake_q.h>\n#include <linux/futex.h>\n\nstatic inline int futex_hb_waiters_pending(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * Full barrier (B), see the ordering comment above.\n\t */\n\tsmp_mb();\n\treturn atomic_read(&hb->waiters);\n#else\n\treturn 1;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_hash",
          "args": [
            "&key"
          ],
          "line": 158
        },
        "resolved": true,
        "details": {
          "function_name": "futex_hash",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "115-121",
          "snippet": "struct futex_hash_bucket *futex_hash(union futex_key *key)\n{\n\tu32 hash = jhash2((u32 *)key, offsetof(typeof(*key), both.offset) / 4,\n\t\t\t  key->both.offset);\n\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [
            "#define futex_hashsize (__futex_data.hashsize)",
            "#define futex_queues   (__futex_data.queues)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n#define futex_hashsize (__futex_data.hashsize)\n#define futex_queues   (__futex_data.queues)\n\nstruct futex_hash_bucket *futex_hash(union futex_key *key)\n{\n\tu32 hash = jhash2((u32 *)key, offsetof(typeof(*key), both.offset) / 4,\n\t\t\t  key->both.offset);\n\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret != 0"
          ],
          "line": 155
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_futex_key",
          "args": [
            "uaddr",
            "flags & FLAGS_SHARED",
            "&key",
            "FUTEX_READ"
          ],
          "line": 154
        },
        "resolved": true,
        "details": {
          "function_name": "get_futex_key",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "220-395",
          "snippet": "int get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__read_mostly __aligned(2*sizeof(long));"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n__read_mostly __aligned(2*sizeof(long));\n\nint get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 149
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nint futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tint ret;\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, FUTEX_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\thb = futex_hash(&key);\n\n\t/* Make sure we really have tasks to wakeup */\n\tif (!futex_hb_waiters_pending(hb))\n\t\treturn ret;\n\n\tspin_lock(&hb->lock);\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (futex_match (&this->key, &key)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check if one of the bits is set in both bitsets */\n\t\t\tif (!(this->bitset & bitset))\n\t\t\t\tcontinue;\n\n\t\t\tfutex_wake_mark(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&hb->lock);\n\twake_up_q(&wake_q);\n\treturn ret;\n}"
  },
  {
    "function_name": "futex_wake_mark",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/waitwake.c",
    "lines": "115-138",
    "snippet": "void futex_wake_mark(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\tget_task_struct(p);\n\t__futex_unqueue(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __futex_unqueue().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock.\n\t */\n\twake_q_add_safe(wake_q, p);\n}",
    "includes": [
      "#include \"futex.h\"",
      "#include <linux/freezer.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/task.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "wake_q_add_safe",
          "args": [
            "wake_q",
            "p"
          ],
          "line": 137
        },
        "resolved": true,
        "details": {
          "function_name": "wake_q_add_safe",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "942-946",
          "snippet": "void wake_q_add_safe(struct wake_q_head *head, struct task_struct *task)\n{\n\tif (!__wake_q_add(head, task))\n\t\tput_task_struct(task);\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic __always_inline struct;\n\nvoid wake_q_add_safe(struct wake_q_head *head, struct task_struct *task)\n{\n\tif (!__wake_q_add(head, task))\n\t\tput_task_struct(task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_store_release",
          "args": [
            "&q->lock_ptr",
            "NULL"
          ],
          "line": 131
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__futex_unqueue",
          "args": [
            "q"
          ],
          "line": 123
        },
        "resolved": true,
        "details": {
          "function_name": "__futex_unqueue",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "499-510",
          "snippet": "void __futex_unqueue(struct futex_q *q)\n{\n\tstruct futex_hash_bucket *hb;\n\n\tif (WARN_ON_SMP(!q->lock_ptr) || WARN_ON(plist_node_empty(&q->list)))\n\t\treturn;\n\tlockdep_assert_held(q->lock_ptr);\n\n\thb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);\n\tplist_del(&q->list, &hb->chain);\n\tfutex_hb_waiters_dec(hb);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid __futex_unqueue(struct futex_q *q)\n{\n\tstruct futex_hash_bucket *hb;\n\n\tif (WARN_ON_SMP(!q->lock_ptr) || WARN_ON(plist_node_empty(&q->list)))\n\t\treturn;\n\tlockdep_assert_held(q->lock_ptr);\n\n\thb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);\n\tplist_del(&q->list, &hb->chain);\n\tfutex_hb_waiters_dec(hb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "p"
          ],
          "line": 122
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN",
          "args": [
            "q->pi_state || q->rt_waiter",
            "\"refusing to wake PI futex\\n\""
          ],
          "line": 119
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"futex.h\"\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n\nvoid futex_wake_mark(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\tget_task_struct(p);\n\t__futex_unqueue(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __futex_unqueue().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock.\n\t */\n\twake_q_add_safe(wake_q, p);\n}"
  }
]