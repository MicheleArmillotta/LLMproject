[
  {
    "function_name": "parse_nopvspin",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "585-589",
    "snippet": "static __init int parse_nopvspin(char *arg)\n{\n\tnopvspin = true;\n\treturn 0;\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __init int parse_nopvspin(char *arg)\n{\n\tnopvspin = true;\n\treturn 0;\n}"
  },
  {
    "function_name": "queued_spin_lock_slowpath",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "315-561",
    "snippet": "void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)\n{\n\tstruct mcs_spinlock *prev, *next, *node;\n\tu32 old, tail;\n\tint idx;\n\n\tBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));\n\n\tif (pv_enabled())\n\t\tgoto pv_queue;\n\n\tif (virt_spin_lock(lock))\n\t\treturn;\n\n\t/*\n\t * Wait for in-progress pending->locked hand-overs with a bounded\n\t * number of spins so that we guarantee forward progress.\n\t *\n\t * 0,1,0 -> 0,0,1\n\t */\n\tif (val == _Q_PENDING_VAL) {\n\t\tint cnt = _Q_PENDING_LOOPS;\n\t\tval = atomic_cond_read_relaxed(&lock->val,\n\t\t\t\t\t       (VAL != _Q_PENDING_VAL) || !cnt--);\n\t}\n\n\t/*\n\t * If we observe any contention; queue.\n\t */\n\tif (val & ~_Q_LOCKED_MASK)\n\t\tgoto queue;\n\n\t/*\n\t * trylock || pending\n\t *\n\t * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock\n\t */\n\tval = queued_fetch_set_pending_acquire(lock);\n\n\t/*\n\t * If we observe contention, there is a concurrent locker.\n\t *\n\t * Undo and queue; our setting of PENDING might have made the\n\t * n,0,0 -> 0,0,0 transition fail and it will now be waiting\n\t * on @next to become !NULL.\n\t */\n\tif (unlikely(val & ~_Q_LOCKED_MASK)) {\n\n\t\t/* Undo PENDING if we set it. */\n\t\tif (!(val & _Q_PENDING_MASK))\n\t\t\tclear_pending(lock);\n\n\t\tgoto queue;\n\t}\n\n\t/*\n\t * We're pending, wait for the owner to go away.\n\t *\n\t * 0,1,1 -> 0,1,0\n\t *\n\t * this wait loop must be a load-acquire such that we match the\n\t * store-release that clears the locked bit and create lock\n\t * sequentiality; this is because not all\n\t * clear_pending_set_locked() implementations imply full\n\t * barriers.\n\t */\n\tif (val & _Q_LOCKED_MASK)\n\t\tatomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_MASK));\n\n\t/*\n\t * take ownership and clear the pending bit.\n\t *\n\t * 0,1,0 -> 0,0,1\n\t */\n\tclear_pending_set_locked(lock);\n\tlockevent_inc(lock_pending);\n\treturn;\n\n\t/*\n\t * End of pending bit optimistic spinning and beginning of MCS\n\t * queuing.\n\t */\nqueue:\n\tlockevent_inc(lock_slowpath);\npv_queue:\n\tnode = this_cpu_ptr(&qnodes[0].mcs);\n\tidx = node->count++;\n\ttail = encode_tail(smp_processor_id(), idx);\n\n\t/*\n\t * 4 nodes are allocated based on the assumption that there will\n\t * not be nested NMIs taking spinlocks. That may not be true in\n\t * some architectures even though the chance of needing more than\n\t * 4 nodes will still be extremely unlikely. When that happens,\n\t * we fall back to spinning on the lock directly without using\n\t * any MCS node. This is not the most elegant solution, but is\n\t * simple enough.\n\t */\n\tif (unlikely(idx >= MAX_NODES)) {\n\t\tlockevent_inc(lock_no_node);\n\t\twhile (!queued_spin_trylock(lock))\n\t\t\tcpu_relax();\n\t\tgoto release;\n\t}\n\n\tnode = grab_mcs_node(node, idx);\n\n\t/*\n\t * Keep counts of non-zero index values:\n\t */\n\tlockevent_cond_inc(lock_use_node2 + idx - 1, idx);\n\n\t/*\n\t * Ensure that we increment the head node->count before initialising\n\t * the actual node. If the compiler is kind enough to reorder these\n\t * stores, then an IRQ could overwrite our assignments.\n\t */\n\tbarrier();\n\n\tnode->locked = 0;\n\tnode->next = NULL;\n\tpv_init_node(node);\n\n\t/*\n\t * We touched a (possibly) cold cacheline in the per-cpu queue node;\n\t * attempt the trylock once more in the hope someone let go while we\n\t * weren't watching.\n\t */\n\tif (queued_spin_trylock(lock))\n\t\tgoto release;\n\n\t/*\n\t * Ensure that the initialisation of @node is complete before we\n\t * publish the updated tail via xchg_tail() and potentially link\n\t * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.\n\t */\n\tsmp_wmb();\n\n\t/*\n\t * Publish the updated tail.\n\t * We have already touched the queueing cacheline; don't bother with\n\t * pending stuff.\n\t *\n\t * p,*,* -> n,*,*\n\t */\n\told = xchg_tail(lock, tail);\n\tnext = NULL;\n\n\t/*\n\t * if there was a previous node; link it and wait until reaching the\n\t * head of the waitqueue.\n\t */\n\tif (old & _Q_TAIL_MASK) {\n\t\tprev = decode_tail(old);\n\n\t\t/* Link @node into the waitqueue. */\n\t\tWRITE_ONCE(prev->next, node);\n\n\t\tpv_wait_node(node, prev);\n\t\tarch_mcs_spin_lock_contended(&node->locked);\n\n\t\t/*\n\t\t * While waiting for the MCS lock, the next pointer may have\n\t\t * been set by another lock waiter. We optimistically load\n\t\t * the next pointer & prefetch the cacheline for writing\n\t\t * to reduce latency in the upcoming MCS unlock operation.\n\t\t */\n\t\tnext = READ_ONCE(node->next);\n\t\tif (next)\n\t\t\tprefetchw(next);\n\t}\n\n\t/*\n\t * we're at the head of the waitqueue, wait for the owner & pending to\n\t * go away.\n\t *\n\t * *,x,y -> *,0,0\n\t *\n\t * this wait loop must use a load-acquire such that we match the\n\t * store-release that clears the locked bit and create lock\n\t * sequentiality; this is because the set_locked() function below\n\t * does not imply a full barrier.\n\t *\n\t * The PV pv_wait_head_or_lock function, if active, will acquire\n\t * the lock and return a non-zero value. So we have to skip the\n\t * atomic_cond_read_acquire() call. As the next PV queue head hasn't\n\t * been designated yet, there is no way for the locked value to become\n\t * _Q_SLOW_VAL. So both the set_locked() and the\n\t * atomic_cmpxchg_relaxed() calls will be safe.\n\t *\n\t * If PV isn't active, 0 will be returned instead.\n\t *\n\t */\n\tif ((val = pv_wait_head_or_lock(lock, node)))\n\t\tgoto locked;\n\n\tval = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));\n\nlocked:\n\t/*\n\t * claim the lock:\n\t *\n\t * n,0,0 -> 0,0,1 : lock, uncontended\n\t * *,*,0 -> *,*,1 : lock, contended\n\t *\n\t * If the queue head is the only one in the queue (lock value == tail)\n\t * and nobody is pending, clear the tail code and grab the lock.\n\t * Otherwise, we only need to grab the lock.\n\t */\n\n\t/*\n\t * In the PV case we might already have _Q_LOCKED_VAL set, because\n\t * of lock stealing; therefore we must also allow:\n\t *\n\t * n,0,1 -> 0,0,1\n\t *\n\t * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the\n\t *       above wait condition, therefore any concurrent setting of\n\t *       PENDING will make the uncontended transition fail.\n\t */\n\tif ((val & _Q_TAIL_MASK) == tail) {\n\t\tif (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))\n\t\t\tgoto release; /* No contention */\n\t}\n\n\t/*\n\t * Either somebody is queued behind us or _Q_PENDING_VAL got set\n\t * which will then detect the remaining tail and queue behind us\n\t * ensuring we'll see a @next.\n\t */\n\tset_locked(lock);\n\n\t/*\n\t * contended path; wait for next if not observed yet, release.\n\t */\n\tif (!next)\n\t\tnext = smp_cond_load_relaxed(&node->next, (VAL));\n\n\tarch_mcs_spin_unlock_contended(&next->locked);\n\tpv_kick_node(lock, next);\n\nrelease:\n\t/*\n\t * release the node\n\t */\n\t__this_cpu_dec(qnodes[0].mcs.count);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [
      "#define queued_spin_lock_slowpath\t__pv_queued_spin_lock_slowpath",
      "#define queued_spin_lock_slowpath\tnative_queued_spin_lock_slowpath",
      "#define pv_wait_head_or_lock\t__pv_wait_head_or_lock",
      "#define pv_kick_node\t\t__pv_kick_node",
      "#define pv_wait_node\t\t__pv_wait_node",
      "#define pv_init_node\t\t__pv_init_node",
      "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)",
      "#define _Q_PENDING_LOOPS\t1",
      "#define MAX_NODES\t4"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__this_cpu_dec",
          "args": [
            "qnodes[0].mcs.count"
          ],
          "line": 560
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_kick_node",
          "args": [
            "lock",
            "next"
          ],
          "line": 554
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_kick_node",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "275-276",
          "snippet": "static __always_inline void __pv_kick_node(struct qspinlock *lock,\n\t\t\t\t\t   struct mcs_spinlock *node) { }",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void __pv_kick_node(struct qspinlock *lock,\n\t\t\t\t\t   struct mcs_spinlock *node) { }"
        }
      },
      {
        "call_info": {
          "callee": "arch_mcs_spin_unlock_contended",
          "args": [
            "&next->locked"
          ],
          "line": 553
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_cond_load_relaxed",
          "args": [
            "&node->next",
            "(VAL)"
          ],
          "line": 551
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_locked",
          "args": [
            "lock"
          ],
          "line": 545
        },
        "resolved": true,
        "details": {
          "function_name": "set_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "261-264",
          "snippet": "static __always_inline void set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_try_cmpxchg_relaxed",
          "args": [
            "&lock->val",
            "&val",
            "_Q_LOCKED_VAL"
          ],
          "line": 536
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_cond_read_acquire",
          "args": [
            "&lock->val",
            "!(VAL & _Q_LOCKED_PENDING_MASK)"
          ],
          "line": 511
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_wait_head_or_lock",
          "args": [
            "lock",
            "node"
          ],
          "line": 508
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_wait_head_or_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "277-279",
          "snippet": "static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,\n\t\t\t\t\t\t   struct mcs_spinlock *node)\n\t\t\t\t\t\t   { return 0; }",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,\n\t\t\t\t\t\t   struct mcs_spinlock *node)\n\t\t\t\t\t\t   { return 0; }"
        }
      },
      {
        "call_info": {
          "callee": "prefetchw",
          "args": [
            "next"
          ],
          "line": 484
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "node->next"
          ],
          "line": 482
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "arch_mcs_spin_lock_contended",
          "args": [
            "&node->locked"
          ],
          "line": 474
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_wait_node",
          "args": [
            "node",
            "prev"
          ],
          "line": 473
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_wait_node",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "273-274",
          "snippet": "static __always_inline void __pv_wait_node(struct mcs_spinlock *node,\n\t\t\t\t\t   struct mcs_spinlock *prev) { }",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void __pv_wait_node(struct mcs_spinlock *node,\n\t\t\t\t\t   struct mcs_spinlock *prev) { }"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "prev->next",
            "node"
          ],
          "line": 471
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "decode_tail",
          "args": [
            "old"
          ],
          "line": 468
        },
        "resolved": true,
        "details": {
          "function_name": "decode_tail",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "125-131",
          "snippet": "mcs_spinlock *decode_tail(u32 tail)\n{\n\tint cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;\n\tint idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;\n\n\treturn per_cpu_ptr(&qnodes[idx].mcs, cpu);\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nmcs_spinlock *decode_tail(u32 tail)\n{\n\tint cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;\n\tint idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;\n\n\treturn per_cpu_ptr(&qnodes[idx].mcs, cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "xchg_tail",
          "args": [
            "lock",
            "tail"
          ],
          "line": 460
        },
        "resolved": true,
        "details": {
          "function_name": "xchg_tail",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "220-238",
          "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [
            "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_wmb",
          "args": [],
          "line": 451
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "queued_spin_trylock",
          "args": [
            "lock"
          ],
          "line": 443
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_init_node",
          "args": [
            "node"
          ],
          "line": 436
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_init_node",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "272-272",
          "snippet": "static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void __pv_init_node(struct mcs_spinlock *node) { }"
        }
      },
      {
        "call_info": {
          "callee": "barrier",
          "args": [],
          "line": 432
        },
        "resolved": true,
        "details": {
          "function_name": "membarrier_register_global_expedited",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/membarrier.c",
          "lines": "482-499",
          "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockevent_cond_inc",
          "args": [
            "lock_use_node2 + idx - 1",
            "idx"
          ],
          "line": 425
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "grab_mcs_node",
          "args": [
            "node",
            "idx"
          ],
          "line": 420
        },
        "resolved": true,
        "details": {
          "function_name": "grab_mcs_node",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "134-137",
          "snippet": "mcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)\n{\n\treturn &((struct qnode *)base + idx)->mcs;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nmcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)\n{\n\treturn &((struct qnode *)base + idx)->mcs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 416
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "queued_spin_trylock",
          "args": [
            "lock"
          ],
          "line": 415
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "lock_no_node"
          ],
          "line": 414
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "idx >= MAX_NODES"
          ],
          "line": 413
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "encode_tail",
          "args": [
            "smp_processor_id()",
            "idx"
          ],
          "line": 402
        },
        "resolved": true,
        "details": {
          "function_name": "encode_tail",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "115-123",
          "snippet": "static inline __pure u32 encode_tail(int cpu, int idx)\n{\n\tu32 tail;\n\n\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;\n\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */\n\n\treturn tail;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic inline __pure u32 encode_tail(int cpu, int idx)\n{\n\tu32 tail;\n\n\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;\n\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */\n\n\treturn tail;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 402
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "this_cpu_ptr",
          "args": [
            "&qnodes[0].mcs"
          ],
          "line": 400
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "lock_slowpath"
          ],
          "line": 398
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "lock_pending"
          ],
          "line": 390
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "clear_pending_set_locked",
          "args": [
            "lock"
          ],
          "line": 389
        },
        "resolved": true,
        "details": {
          "function_name": "clear_pending_set_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "205-208",
          "snippet": "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_cond_read_acquire",
          "args": [
            "&lock->val",
            "!(VAL & _Q_LOCKED_MASK)"
          ],
          "line": 382
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "val & ~_Q_LOCKED_MASK"
          ],
          "line": 361
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "queued_fetch_set_pending_acquire",
          "args": [
            "lock"
          ],
          "line": 352
        },
        "resolved": true,
        "details": {
          "function_name": "queued_fetch_set_pending_acquire",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "249-252",
          "snippet": "static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)\n{\n\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)\n{\n\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_cond_read_relaxed",
          "args": [
            "&lock->val",
            "(VAL != _Q_PENDING_VAL) || !cnt--"
          ],
          "line": 337
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "virt_spin_lock",
          "args": [
            "lock"
          ],
          "line": 326
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_enabled",
          "args": [],
          "line": 323
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUILD_BUG_ON",
          "args": [
            "CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS)"
          ],
          "line": 321
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define queued_spin_lock_slowpath\t__pv_queued_spin_lock_slowpath\n#define queued_spin_lock_slowpath\tnative_queued_spin_lock_slowpath\n#define pv_wait_head_or_lock\t__pv_wait_head_or_lock\n#define pv_kick_node\t\t__pv_kick_node\n#define pv_wait_node\t\t__pv_wait_node\n#define pv_init_node\t\t__pv_init_node\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n#define _Q_PENDING_LOOPS\t1\n#define MAX_NODES\t4\n\nvoid queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)\n{\n\tstruct mcs_spinlock *prev, *next, *node;\n\tu32 old, tail;\n\tint idx;\n\n\tBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));\n\n\tif (pv_enabled())\n\t\tgoto pv_queue;\n\n\tif (virt_spin_lock(lock))\n\t\treturn;\n\n\t/*\n\t * Wait for in-progress pending->locked hand-overs with a bounded\n\t * number of spins so that we guarantee forward progress.\n\t *\n\t * 0,1,0 -> 0,0,1\n\t */\n\tif (val == _Q_PENDING_VAL) {\n\t\tint cnt = _Q_PENDING_LOOPS;\n\t\tval = atomic_cond_read_relaxed(&lock->val,\n\t\t\t\t\t       (VAL != _Q_PENDING_VAL) || !cnt--);\n\t}\n\n\t/*\n\t * If we observe any contention; queue.\n\t */\n\tif (val & ~_Q_LOCKED_MASK)\n\t\tgoto queue;\n\n\t/*\n\t * trylock || pending\n\t *\n\t * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock\n\t */\n\tval = queued_fetch_set_pending_acquire(lock);\n\n\t/*\n\t * If we observe contention, there is a concurrent locker.\n\t *\n\t * Undo and queue; our setting of PENDING might have made the\n\t * n,0,0 -> 0,0,0 transition fail and it will now be waiting\n\t * on @next to become !NULL.\n\t */\n\tif (unlikely(val & ~_Q_LOCKED_MASK)) {\n\n\t\t/* Undo PENDING if we set it. */\n\t\tif (!(val & _Q_PENDING_MASK))\n\t\t\tclear_pending(lock);\n\n\t\tgoto queue;\n\t}\n\n\t/*\n\t * We're pending, wait for the owner to go away.\n\t *\n\t * 0,1,1 -> 0,1,0\n\t *\n\t * this wait loop must be a load-acquire such that we match the\n\t * store-release that clears the locked bit and create lock\n\t * sequentiality; this is because not all\n\t * clear_pending_set_locked() implementations imply full\n\t * barriers.\n\t */\n\tif (val & _Q_LOCKED_MASK)\n\t\tatomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_MASK));\n\n\t/*\n\t * take ownership and clear the pending bit.\n\t *\n\t * 0,1,0 -> 0,0,1\n\t */\n\tclear_pending_set_locked(lock);\n\tlockevent_inc(lock_pending);\n\treturn;\n\n\t/*\n\t * End of pending bit optimistic spinning and beginning of MCS\n\t * queuing.\n\t */\nqueue:\n\tlockevent_inc(lock_slowpath);\npv_queue:\n\tnode = this_cpu_ptr(&qnodes[0].mcs);\n\tidx = node->count++;\n\ttail = encode_tail(smp_processor_id(), idx);\n\n\t/*\n\t * 4 nodes are allocated based on the assumption that there will\n\t * not be nested NMIs taking spinlocks. That may not be true in\n\t * some architectures even though the chance of needing more than\n\t * 4 nodes will still be extremely unlikely. When that happens,\n\t * we fall back to spinning on the lock directly without using\n\t * any MCS node. This is not the most elegant solution, but is\n\t * simple enough.\n\t */\n\tif (unlikely(idx >= MAX_NODES)) {\n\t\tlockevent_inc(lock_no_node);\n\t\twhile (!queued_spin_trylock(lock))\n\t\t\tcpu_relax();\n\t\tgoto release;\n\t}\n\n\tnode = grab_mcs_node(node, idx);\n\n\t/*\n\t * Keep counts of non-zero index values:\n\t */\n\tlockevent_cond_inc(lock_use_node2 + idx - 1, idx);\n\n\t/*\n\t * Ensure that we increment the head node->count before initialising\n\t * the actual node. If the compiler is kind enough to reorder these\n\t * stores, then an IRQ could overwrite our assignments.\n\t */\n\tbarrier();\n\n\tnode->locked = 0;\n\tnode->next = NULL;\n\tpv_init_node(node);\n\n\t/*\n\t * We touched a (possibly) cold cacheline in the per-cpu queue node;\n\t * attempt the trylock once more in the hope someone let go while we\n\t * weren't watching.\n\t */\n\tif (queued_spin_trylock(lock))\n\t\tgoto release;\n\n\t/*\n\t * Ensure that the initialisation of @node is complete before we\n\t * publish the updated tail via xchg_tail() and potentially link\n\t * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.\n\t */\n\tsmp_wmb();\n\n\t/*\n\t * Publish the updated tail.\n\t * We have already touched the queueing cacheline; don't bother with\n\t * pending stuff.\n\t *\n\t * p,*,* -> n,*,*\n\t */\n\told = xchg_tail(lock, tail);\n\tnext = NULL;\n\n\t/*\n\t * if there was a previous node; link it and wait until reaching the\n\t * head of the waitqueue.\n\t */\n\tif (old & _Q_TAIL_MASK) {\n\t\tprev = decode_tail(old);\n\n\t\t/* Link @node into the waitqueue. */\n\t\tWRITE_ONCE(prev->next, node);\n\n\t\tpv_wait_node(node, prev);\n\t\tarch_mcs_spin_lock_contended(&node->locked);\n\n\t\t/*\n\t\t * While waiting for the MCS lock, the next pointer may have\n\t\t * been set by another lock waiter. We optimistically load\n\t\t * the next pointer & prefetch the cacheline for writing\n\t\t * to reduce latency in the upcoming MCS unlock operation.\n\t\t */\n\t\tnext = READ_ONCE(node->next);\n\t\tif (next)\n\t\t\tprefetchw(next);\n\t}\n\n\t/*\n\t * we're at the head of the waitqueue, wait for the owner & pending to\n\t * go away.\n\t *\n\t * *,x,y -> *,0,0\n\t *\n\t * this wait loop must use a load-acquire such that we match the\n\t * store-release that clears the locked bit and create lock\n\t * sequentiality; this is because the set_locked() function below\n\t * does not imply a full barrier.\n\t *\n\t * The PV pv_wait_head_or_lock function, if active, will acquire\n\t * the lock and return a non-zero value. So we have to skip the\n\t * atomic_cond_read_acquire() call. As the next PV queue head hasn't\n\t * been designated yet, there is no way for the locked value to become\n\t * _Q_SLOW_VAL. So both the set_locked() and the\n\t * atomic_cmpxchg_relaxed() calls will be safe.\n\t *\n\t * If PV isn't active, 0 will be returned instead.\n\t *\n\t */\n\tif ((val = pv_wait_head_or_lock(lock, node)))\n\t\tgoto locked;\n\n\tval = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));\n\nlocked:\n\t/*\n\t * claim the lock:\n\t *\n\t * n,0,0 -> 0,0,1 : lock, uncontended\n\t * *,*,0 -> *,*,1 : lock, contended\n\t *\n\t * If the queue head is the only one in the queue (lock value == tail)\n\t * and nobody is pending, clear the tail code and grab the lock.\n\t * Otherwise, we only need to grab the lock.\n\t */\n\n\t/*\n\t * In the PV case we might already have _Q_LOCKED_VAL set, because\n\t * of lock stealing; therefore we must also allow:\n\t *\n\t * n,0,1 -> 0,0,1\n\t *\n\t * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the\n\t *       above wait condition, therefore any concurrent setting of\n\t *       PENDING will make the uncontended transition fail.\n\t */\n\tif ((val & _Q_TAIL_MASK) == tail) {\n\t\tif (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))\n\t\t\tgoto release; /* No contention */\n\t}\n\n\t/*\n\t * Either somebody is queued behind us or _Q_PENDING_VAL got set\n\t * which will then detect the remaining tail and queue behind us\n\t * ensuring we'll see a @next.\n\t */\n\tset_locked(lock);\n\n\t/*\n\t * contended path; wait for next if not observed yet, release.\n\t */\n\tif (!next)\n\t\tnext = smp_cond_load_relaxed(&node->next, (VAL));\n\n\tarch_mcs_spin_unlock_contended(&next->locked);\n\tpv_kick_node(lock, next);\n\nrelease:\n\t/*\n\t * release the node\n\t */\n\t__this_cpu_dec(qnodes[0].mcs.count);\n}"
  },
  {
    "function_name": "__pv_wait_head_or_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "277-279",
    "snippet": "static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,\n\t\t\t\t\t\t   struct mcs_spinlock *node)\n\t\t\t\t\t\t   { return 0; }",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,\n\t\t\t\t\t\t   struct mcs_spinlock *node)\n\t\t\t\t\t\t   { return 0; }"
  },
  {
    "function_name": "__pv_kick_node",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "275-276",
    "snippet": "static __always_inline void __pv_kick_node(struct qspinlock *lock,\n\t\t\t\t\t   struct mcs_spinlock *node) { }",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void __pv_kick_node(struct qspinlock *lock,\n\t\t\t\t\t   struct mcs_spinlock *node) { }"
  },
  {
    "function_name": "__pv_wait_node",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "273-274",
    "snippet": "static __always_inline void __pv_wait_node(struct mcs_spinlock *node,\n\t\t\t\t\t   struct mcs_spinlock *prev) { }",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void __pv_wait_node(struct mcs_spinlock *node,\n\t\t\t\t\t   struct mcs_spinlock *prev) { }"
  },
  {
    "function_name": "__pv_init_node",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "272-272",
    "snippet": "static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void __pv_init_node(struct mcs_spinlock *node) { }"
  },
  {
    "function_name": "set_locked",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "261-264",
    "snippet": "static __always_inline void set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "lock->locked",
            "_Q_LOCKED_VAL"
          ],
          "line": 263
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n}"
  },
  {
    "function_name": "queued_fetch_set_pending_acquire",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "249-252",
    "snippet": "static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)\n{\n\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_fetch_or_acquire",
          "args": [
            "_Q_PENDING_VAL",
            "&lock->val"
          ],
          "line": 251
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)\n{\n\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);\n}"
  },
  {
    "function_name": "xchg_tail",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "220-238",
    "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [
      "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_cmpxchg_relaxed",
          "args": [
            "&lock->val",
            "val",
            "new"
          ],
          "line": 231
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&lock->val"
          ],
          "line": 222
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}"
  },
  {
    "function_name": "clear_pending_set_locked",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "205-208",
    "snippet": "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_add",
          "args": [
            "-_Q_PENDING_VAL + _Q_LOCKED_VAL",
            "&lock->val"
          ],
          "line": 207
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);\n}"
  },
  {
    "function_name": "clear_pending",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "194-197",
    "snippet": "static __always_inline void clear_pending(struct qspinlock *lock)\n{\n\tatomic_andnot(_Q_PENDING_VAL, &lock->val);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_andnot",
          "args": [
            "_Q_PENDING_VAL",
            "&lock->val"
          ],
          "line": 196
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void clear_pending(struct qspinlock *lock)\n{\n\tatomic_andnot(_Q_PENDING_VAL, &lock->val);\n}"
  },
  {
    "function_name": "xchg_tail",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "176-184",
    "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\t/*\n\t * We can use relaxed semantics since the caller ensures that the\n\t * MCS node is properly initialized before updating the tail.\n\t */\n\treturn (u32)xchg_relaxed(&lock->tail,\n\t\t\t\t tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "xchg_relaxed",
          "args": [
            "&lock->tail",
            "tail >> _Q_TAIL_OFFSET"
          ],
          "line": 182
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\t/*\n\t * We can use relaxed semantics since the caller ensures that the\n\t * MCS node is properly initialized before updating the tail.\n\t */\n\treturn (u32)xchg_relaxed(&lock->tail,\n\t\t\t\t tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;\n}"
  },
  {
    "function_name": "clear_pending_set_locked",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "161-164",
    "snippet": "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "lock->locked_pending",
            "_Q_LOCKED_VAL"
          ],
          "line": 163
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);\n}"
  },
  {
    "function_name": "clear_pending",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "148-151",
    "snippet": "static __always_inline void clear_pending(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->pending, 0);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "lock->pending",
            "0"
          ],
          "line": 150
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic __always_inline void clear_pending(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->pending, 0);\n}"
  },
  {
    "function_name": "grab_mcs_node",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "134-137",
    "snippet": "mcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)\n{\n\treturn &((struct qnode *)base + idx)->mcs;\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nmcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)\n{\n\treturn &((struct qnode *)base + idx)->mcs;\n}"
  },
  {
    "function_name": "decode_tail",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "125-131",
    "snippet": "mcs_spinlock *decode_tail(u32 tail)\n{\n\tint cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;\n\tint idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;\n\n\treturn per_cpu_ptr(&qnodes[idx].mcs, cpu);\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "per_cpu_ptr",
          "args": [
            "&qnodes[idx].mcs",
            "cpu"
          ],
          "line": 130
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nmcs_spinlock *decode_tail(u32 tail)\n{\n\tint cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;\n\tint idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;\n\n\treturn per_cpu_ptr(&qnodes[idx].mcs, cpu);\n}"
  },
  {
    "function_name": "encode_tail",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
    "lines": "115-123",
    "snippet": "static inline __pure u32 encode_tail(int cpu, int idx)\n{\n\tu32 tail;\n\n\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;\n\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */\n\n\treturn tail;\n}",
    "includes": [
      "#include \"qspinlock.c\"",
      "#include \"qspinlock_paravirt.h\"",
      "#include \"mcs_spinlock.h\"",
      "#include \"qspinlock_stat.h\"",
      "#include <asm/qspinlock.h>",
      "#include <asm/byteorder.h>",
      "#include <linux/prefetch.h>",
      "#include <linux/mutex.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nstatic inline __pure u32 encode_tail(int cpu, int idx)\n{\n\tu32 tail;\n\n\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;\n\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */\n\n\treturn tail;\n}"
  }
]