[
  {
    "function_name": "mutex_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "605-609",
    "snippet": "void __sched mutex_unlock(struct mutex *lock)\n{\n\tmutex_release(&lock->dep_map, _RET_IP_);\n\t__rt_mutex_unlock(&lock->rtmutex);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_unlock",
          "args": [
            "&lock->rtmutex"
          ],
          "line": 608
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1350-1356",
          "snippet": "static __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&lock->dep_map",
            "_RET_IP_"
          ],
          "line": 607
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched mutex_unlock(struct mutex *lock)\n{\n\tmutex_release(&lock->dep_map, _RET_IP_);\n\t__rt_mutex_unlock(&lock->rtmutex);\n}"
  },
  {
    "function_name": "mutex_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "590-602",
    "snippet": "int __sched mutex_trylock(struct mutex *lock)\n{\n\tint ret;\n\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES) && WARN_ON_ONCE(!in_task()))\n\t\treturn 0;\n\n\tret = __rt_mutex_trylock(&lock->rtmutex);\n\tif (ret)\n\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_acquire",
          "args": [
            "&lock->dep_map",
            "0",
            "1",
            "_RET_IP_"
          ],
          "line": 599
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_trylock",
          "args": [
            "&lock->rtmutex"
          ],
          "line": 597
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1278-1284",
          "snippet": "static __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "!in_task()"
          ],
          "line": 594
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "in_task",
          "args": [],
          "line": 594
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_DEBUG_RT_MUTEXES"
          ],
          "line": 594
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched mutex_trylock(struct mutex *lock)\n{\n\tint ret;\n\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES) && WARN_ON_ONCE(!in_task()))\n\t\treturn 0;\n\n\tret = __rt_mutex_trylock(&lock->rtmutex);\n\tif (ret)\n\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "mutex_lock_io",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "580-586",
    "snippet": "void __sched mutex_lock_io(struct mutex *lock)\n{\n\tint token = io_schedule_prepare();\n\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "io_schedule_finish",
          "args": [
            "token"
          ],
          "line": 585
        },
        "resolved": true,
        "details": {
          "function_name": "io_schedule_finish",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8387-8390",
          "snippet": "void io_schedule_finish(int token)\n{\n\tcurrent->in_iowait = token;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nvoid io_schedule_finish(int token)\n{\n\tcurrent->in_iowait = token;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "0",
            "NULL",
            "_RET_IP_"
          ],
          "line": 584
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "io_schedule_prepare",
          "args": [],
          "line": 582
        },
        "resolved": true,
        "details": {
          "function_name": "io_schedule_prepare",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8376-8385",
          "snippet": "int io_schedule_prepare(void)\n{\n\tint old_iowait = current->in_iowait;\n\n\tcurrent->in_iowait = 1;\n\tif (current->plug)\n\t\tblk_flush_plug(current->plug, true);\n\n\treturn old_iowait;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nint io_schedule_prepare(void)\n{\n\tint old_iowait = current->in_iowait;\n\n\tcurrent->in_iowait = 1;\n\tif (current->plug)\n\t\tblk_flush_plug(current->plug, true);\n\n\treturn old_iowait;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched mutex_lock_io(struct mutex *lock)\n{\n\tint token = io_schedule_prepare();\n\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}"
  },
  {
    "function_name": "mutex_lock_killable",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "574-577",
    "snippet": "int __sched mutex_lock_killable(struct mutex *lock)\n{\n\treturn __mutex_lock_common(lock, TASK_KILLABLE, 0, NULL, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_KILLABLE",
            "0",
            "NULL",
            "_RET_IP_"
          ],
          "line": 576
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched mutex_lock_killable(struct mutex *lock)\n{\n\treturn __mutex_lock_common(lock, TASK_KILLABLE, 0, NULL, _RET_IP_);\n}"
  },
  {
    "function_name": "mutex_lock_interruptible",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "568-571",
    "snippet": "int __sched mutex_lock_interruptible(struct mutex *lock)\n{\n\treturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE, 0, NULL, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_INTERRUPTIBLE",
            "0",
            "NULL",
            "_RET_IP_"
          ],
          "line": 570
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched mutex_lock_interruptible(struct mutex *lock)\n{\n\treturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE, 0, NULL, _RET_IP_);\n}"
  },
  {
    "function_name": "mutex_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "562-565",
    "snippet": "void __sched mutex_lock(struct mutex *lock)\n{\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "0",
            "NULL",
            "_RET_IP_"
          ],
          "line": 564
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched mutex_lock(struct mutex *lock)\n{\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n}"
  },
  {
    "function_name": "mutex_lock_io_nested",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "548-557",
    "snippet": "void __sched mutex_lock_io_nested(struct mutex *lock, unsigned int subclass)\n{\n\tint token;\n\n\tmight_sleep();\n\n\ttoken = io_schedule_prepare();\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, subclass, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "io_schedule_finish",
          "args": [
            "token"
          ],
          "line": 556
        },
        "resolved": true,
        "details": {
          "function_name": "io_schedule_finish",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8387-8390",
          "snippet": "void io_schedule_finish(int token)\n{\n\tcurrent->in_iowait = token;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nvoid io_schedule_finish(int token)\n{\n\tcurrent->in_iowait = token;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "subclass",
            "NULL",
            "_RET_IP_"
          ],
          "line": 555
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "io_schedule_prepare",
          "args": [],
          "line": 554
        },
        "resolved": true,
        "details": {
          "function_name": "io_schedule_prepare",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8376-8385",
          "snippet": "int io_schedule_prepare(void)\n{\n\tint old_iowait = current->in_iowait;\n\n\tcurrent->in_iowait = 1;\n\tif (current->plug)\n\t\tblk_flush_plug(current->plug, true);\n\n\treturn old_iowait;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nint io_schedule_prepare(void)\n{\n\tint old_iowait = current->in_iowait;\n\n\tcurrent->in_iowait = 1;\n\tif (current->plug)\n\t\tblk_flush_plug(current->plug, true);\n\n\treturn old_iowait;\n}"
        }
      },
      {
        "call_info": {
          "callee": "might_sleep",
          "args": [],
          "line": 552
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched mutex_lock_io_nested(struct mutex *lock, unsigned int subclass)\n{\n\tint token;\n\n\tmight_sleep();\n\n\ttoken = io_schedule_prepare();\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, subclass, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}"
  },
  {
    "function_name": "mutex_lock_killable_nested",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "541-545",
    "snippet": "int __sched mutex_lock_killable_nested(struct mutex *lock,\n\t\t\t\t\t    unsigned int subclass)\n{\n\treturn __mutex_lock_common(lock, TASK_KILLABLE, subclass, NULL, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_KILLABLE",
            "subclass",
            "NULL",
            "_RET_IP_"
          ],
          "line": 544
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched mutex_lock_killable_nested(struct mutex *lock,\n\t\t\t\t\t    unsigned int subclass)\n{\n\treturn __mutex_lock_common(lock, TASK_KILLABLE, subclass, NULL, _RET_IP_);\n}"
  },
  {
    "function_name": "mutex_lock_interruptible_nested",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "534-538",
    "snippet": "int __sched mutex_lock_interruptible_nested(struct mutex *lock,\n\t\t\t\t\t    unsigned int subclass)\n{\n\treturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE, subclass, NULL, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_INTERRUPTIBLE",
            "subclass",
            "NULL",
            "_RET_IP_"
          ],
          "line": 537
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched mutex_lock_interruptible_nested(struct mutex *lock,\n\t\t\t\t\t    unsigned int subclass)\n{\n\treturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE, subclass, NULL, _RET_IP_);\n}"
  },
  {
    "function_name": "_mutex_lock_nest_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "527-531",
    "snippet": "void __sched _mutex_lock_nest_lock(struct mutex *lock,\n\t\t\t\t   struct lockdep_map *nest_lock)\n{\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, nest_lock, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "0",
            "nest_lock",
            "_RET_IP_"
          ],
          "line": 530
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched _mutex_lock_nest_lock(struct mutex *lock,\n\t\t\t\t   struct lockdep_map *nest_lock)\n{\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, nest_lock, _RET_IP_);\n}"
  },
  {
    "function_name": "mutex_lock_nested",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "521-524",
    "snippet": "void __sched mutex_lock_nested(struct mutex *lock, unsigned int subclass)\n{\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, subclass, NULL, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "subclass",
            "NULL",
            "_RET_IP_"
          ],
          "line": 523
        },
        "resolved": true,
        "details": {
          "function_name": "__mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "502-518",
          "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched mutex_lock_nested(struct mutex *lock, unsigned int subclass)\n{\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, subclass, NULL, _RET_IP_);\n}"
  },
  {
    "function_name": "__mutex_lock_common",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "502-518",
    "snippet": "static __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "lock_acquired",
          "args": [
            "&lock->dep_map",
            "ip"
          ],
          "line": 516
        },
        "resolved": true,
        "details": {
          "function_name": "ww_mutex_lock_acquired",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "45-48",
          "snippet": "static inline void ww_mutex_lock_acquired(struct ww_mutex *lock,\n\t\t\t\t\t  struct ww_acquire_ctx *ww_ctx)\n{\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline void ww_mutex_lock_acquired(struct ww_mutex *lock,\n\t\t\t\t\t  struct ww_acquire_ctx *ww_ctx)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&lock->dep_map",
            "ip"
          ],
          "line": 514
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_lock",
          "args": [
            "&lock->rtmutex",
            "state"
          ],
          "line": 512
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1652-1659",
          "snippet": "static __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t   unsigned int state)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn rt_mutex_slowlock(lock, NULL, state);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t   unsigned int state)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn rt_mutex_slowlock(lock, NULL, state);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_acquire_nest",
          "args": [
            "&lock->dep_map",
            "subclass",
            "0",
            "nest_lock",
            "ip"
          ],
          "line": 511
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "might_sleep",
          "args": [],
          "line": 510
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __mutex_lock_common(struct mutex *lock,\n\t\t\t\t\t       unsigned int state,\n\t\t\t\t\t       unsigned int subclass,\n\t\t\t\t\t       struct lockdep_map *nest_lock,\n\t\t\t\t\t       unsigned long ip)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, ip);\n\telse\n\t\tlock_acquired(&lock->dep_map, ip);\n\treturn ret;\n}"
  },
  {
    "function_name": "__mutex_rt_init",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "494-499",
    "snippet": "void __mutex_rt_init(struct mutex *mutex, const char *name,\n\t\t     struct lock_class_key *key)\n{\n\tdebug_check_no_locks_freed((void *)mutex, sizeof(*mutex));\n\tlockdep_init_map_wait(&mutex->dep_map, name, key, 0, LD_WAIT_SLEEP);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "lockdep_init_map_wait",
          "args": [
            "&mutex->dep_map",
            "name",
            "key",
            "0",
            "LD_WAIT_SLEEP"
          ],
          "line": 498
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "debug_check_no_locks_freed",
          "args": [
            "(void *)mutex",
            "sizeof(*mutex)"
          ],
          "line": 497
        },
        "resolved": true,
        "details": {
          "function_name": "debug_check_no_locks_freed",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/lockdep.c",
          "lines": "6405-6427",
          "snippet": "void debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tunsigned long flags;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\n\t\tif (not_in_range(mem_from, mem_len, hlock->instance,\n\t\t\t\t\tsizeof(*hlock->instance)))\n\t\t\tcontinue;\n\n\t\tprint_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);\n\t\tbreak;\n\t}\n\traw_local_irq_restore(flags);\n}",
          "includes": [
            "#include \"lockdep_states.h\"",
            "#include <trace/events/lock.h>",
            "#include \"lockdep_internals.h\"",
            "#include <asm/sections.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/nmi.h>",
            "#include <linux/jhash.h>",
            "#include <linux/random.h>",
            "#include <linux/gfp.h>",
            "#include <linux/bitops.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/stringify.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/hash.h>",
            "#include <linux/utsname.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/module.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/mutex.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static noinstr struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"lockdep_states.h\"\n#include <trace/events/lock.h>\n#include \"lockdep_internals.h\"\n#include <asm/sections.h>\n#include <linux/lockdep.h>\n#include <linux/kprobes.h>\n#include <linux/rcupdate.h>\n#include <linux/nmi.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <linux/gfp.h>\n#include <linux/bitops.h>\n#include <linux/bitmap.h>\n#include <linux/stringify.h>\n#include <linux/ftrace.h>\n#include <linux/hash.h>\n#include <linux/utsname.h>\n#include <linux/irqflags.h>\n#include <linux/debug_locks.h>\n#include <linux/stacktrace.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n\nstatic noinstr struct;\n\nvoid debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tunsigned long flags;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\n\t\tif (not_in_range(mem_from, mem_len, hlock->instance,\n\t\t\t\t\tsizeof(*hlock->instance)))\n\t\t\tcontinue;\n\n\t\tprint_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);\n\t\tbreak;\n\t}\n\traw_local_irq_restore(flags);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __mutex_rt_init(struct mutex *mutex, const char *name,\n\t\t     struct lock_class_key *key)\n{\n\tdebug_check_no_locks_freed((void *)mutex, sizeof(*mutex));\n\tlockdep_init_map_wait(&mutex->dep_map, name, key, 0, LD_WAIT_SLEEP);\n}"
  },
  {
    "function_name": "rt_mutex_debug_task_free",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "485-489",
    "snippet": "void rt_mutex_debug_task_free(struct task_struct *task)\n{\n\tDEBUG_LOCKS_WARN_ON(!RB_EMPTY_ROOT(&task->pi_waiters.rb_root));\n\tDEBUG_LOCKS_WARN_ON(task->pi_blocked_on);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "DEBUG_LOCKS_WARN_ON",
          "args": [
            "task->pi_blocked_on"
          ],
          "line": 488
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DEBUG_LOCKS_WARN_ON",
          "args": [
            "!RB_EMPTY_ROOT(&task->pi_waiters.rb_root)"
          ],
          "line": 487
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RB_EMPTY_ROOT",
          "args": [
            "&task->pi_waiters.rb_root"
          ],
          "line": 487
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_debug_task_free(struct task_struct *task)\n{\n\tDEBUG_LOCKS_WARN_ON(!RB_EMPTY_ROOT(&task->pi_waiters.rb_root));\n\tDEBUG_LOCKS_WARN_ON(task->pi_blocked_on);\n}"
  },
  {
    "function_name": "rt_mutex_postunlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "479-482",
    "snippet": "void __sched rt_mutex_postunlock(struct rt_wake_q_head *wqh)\n{\n\trt_mutex_wake_up_q(wqh);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_wake_up_q",
          "args": [
            "wqh"
          ],
          "line": 481
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_up_q",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "469-482",
          "snippet": "static __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_postunlock(struct rt_wake_q_head *wqh)\n{\n\trt_mutex_wake_up_q(wqh);\n}"
  },
  {
    "function_name": "rt_mutex_adjust_pi",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "453-474",
    "snippet": "void __sched rt_mutex_adjust_pi(struct task_struct *task)\n{\n\tstruct rt_mutex_waiter *waiter;\n\tstruct rt_mutex_base *next_lock;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&task->pi_lock, flags);\n\n\twaiter = task->pi_blocked_on;\n\tif (!waiter || rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\t\treturn;\n\t}\n\tnext_lock = waiter->lock;\n\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(task);\n\n\trt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,\n\t\t\t\t   next_lock, NULL, task);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio_chain",
          "args": [
            "task",
            "RT_MUTEX_MIN_CHAINWALK",
            "NULL",
            "next_lock",
            "NULL",
            "task"
          ],
          "line": 472
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio_chain",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "574-956",
          "snippet": "static int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "task"
          ],
          "line": 470
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&task->pi_lock",
            "flags"
          ],
          "line": 467
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_equal",
          "args": [
            "waiter",
            "task_to_waiter(task)"
          ],
          "line": 462
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_equal",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "328-344",
          "snippet": "static __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_to_waiter",
          "args": [
            "task"
          ],
          "line": 462
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&task->pi_lock",
            "flags"
          ],
          "line": 459
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_adjust_pi(struct task_struct *task)\n{\n\tstruct rt_mutex_waiter *waiter;\n\tstruct rt_mutex_base *next_lock;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&task->pi_lock, flags);\n\n\twaiter = task->pi_blocked_on;\n\tif (!waiter || rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\t\treturn;\n\t}\n\tnext_lock = waiter->lock;\n\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(task);\n\n\trt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,\n\t\t\t\t   next_lock, NULL, task);\n}"
  },
  {
    "function_name": "rt_mutex_cleanup_proxy_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "411-446",
    "snippet": "bool __sched rt_mutex_cleanup_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tbool cleanup = false;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/*\n\t * Do an unconditional try-lock, this deals with the lock stealing\n\t * state where __rt_mutex_futex_unlock() -> mark_wakeup_next_waiter()\n\t * sets a NULL owner.\n\t *\n\t * We're not interested in the return value, because the subsequent\n\t * test on rt_mutex_owner() will infer that. If the trylock succeeded,\n\t * we will own the lock and it will have removed the waiter. If we\n\t * failed the trylock, we're still not owner and we need to remove\n\t * ourselves.\n\t */\n\ttry_to_take_rt_mutex(lock, current, waiter);\n\t/*\n\t * Unless we're the owner; we're still enqueued on the wait_list.\n\t * So check if we became owner, if not, take us off the wait_list.\n\t */\n\tif (rt_mutex_owner(lock) != current) {\n\t\tremove_waiter(lock, waiter);\n\t\tcleanup = true;\n\t}\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn cleanup;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 443
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 441
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "107-175",
          "snippet": "static __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "remove_waiter",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 434
        },
        "resolved": true,
        "details": {
          "function_name": "remove_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1417-1468",
          "snippet": "static void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 433
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "waiter"
          ],
          "line": 428
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "968-1076",
          "snippet": "static int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 416
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool __sched rt_mutex_cleanup_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tbool cleanup = false;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/*\n\t * Do an unconditional try-lock, this deals with the lock stealing\n\t * state where __rt_mutex_futex_unlock() -> mark_wakeup_next_waiter()\n\t * sets a NULL owner.\n\t *\n\t * We're not interested in the return value, because the subsequent\n\t * test on rt_mutex_owner() will infer that. If the trylock succeeded,\n\t * we will own the lock and it will have removed the waiter. If we\n\t * failed the trylock, we're still not owner and we need to remove\n\t * ourselves.\n\t */\n\ttry_to_take_rt_mutex(lock, current, waiter);\n\t/*\n\t * Unless we're the owner; we're still enqueued on the wait_list.\n\t * So check if we became owner, if not, take us off the wait_list.\n\t */\n\tif (rt_mutex_owner(lock) != current) {\n\t\tremove_waiter(lock, waiter);\n\t\tcleanup = true;\n\t}\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn cleanup;\n}"
  },
  {
    "function_name": "rt_mutex_wait_proxy_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "371-389",
    "snippet": "int __sched rt_mutex_wait_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t     struct hrtimer_sleeper *to,\n\t\t\t\t     struct rt_mutex_waiter *waiter)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/* sleep on the mutex */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tret = rt_mutex_slowlock_block(lock, NULL, TASK_INTERRUPTIBLE, to, waiter);\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 386
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 385
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "107-175",
          "snippet": "static __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_slowlock_block",
          "args": [
            "lock",
            "NULL",
            "TASK_INTERRUPTIBLE",
            "to",
            "waiter"
          ],
          "line": 380
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowlock_block",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1481-1526",
          "snippet": "static int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   unsigned int state,\n\t\t\t\t\t   struct hrtimer_sleeper *timeout,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\tif (timeout && !timeout->task) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(rtm, waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))\n\t\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   unsigned int state,\n\t\t\t\t\t   struct hrtimer_sleeper *timeout,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\tif (timeout && !timeout->task) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(rtm, waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))\n\t\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "TASK_INTERRUPTIBLE"
          ],
          "line": 379
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 377
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_wait_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t     struct hrtimer_sleeper *to,\n\t\t\t\t     struct rt_mutex_waiter *waiter)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/* sleep on the mutex */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tret = rt_mutex_slowlock_block(lock, NULL, TASK_INTERRUPTIBLE, to, waiter);\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_start_proxy_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "339-352",
    "snippet": "int __sched rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\tret = __rt_mutex_start_proxy_lock(lock, waiter, task);\n\tif (unlikely(ret))\n\t\tremove_waiter(lock, waiter);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 349
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "remove_waiter",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 348
        },
        "resolved": true,
        "details": {
          "function_name": "remove_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1417-1468",
          "snippet": "static void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret"
          ],
          "line": 347
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_start_proxy_lock",
          "args": [
            "lock",
            "waiter",
            "task"
          ],
          "line": 346
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_start_proxy_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "292-318",
          "snippet": "int __sched __rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task, NULL,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched __rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task, NULL,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 345
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\tret = __rt_mutex_start_proxy_lock(lock, waiter, task);\n\tif (unlikely(ret))\n\t\tremove_waiter(lock, waiter);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "__rt_mutex_start_proxy_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "292-318",
    "snippet": "int __sched __rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task, NULL,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 307
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocks_on_rt_mutex",
          "args": [
            "lock",
            "waiter",
            "task",
            "NULL",
            "RT_MUTEX_FULL_CHAINWALK"
          ],
          "line": 304
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocks_on_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1085-1184",
          "snippet": "static int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}"
        }
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "task",
            "NULL"
          ],
          "line": 300
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "968-1076",
          "snippet": "static int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 298
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched __rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task, NULL,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_proxy_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "267-271",
    "snippet": "void __sched rt_mutex_proxy_unlock(struct rt_mutex_base *lock)\n{\n\tdebug_rt_mutex_proxy_unlock(lock);\n\trt_mutex_set_owner(lock, NULL);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_set_owner",
          "args": [
            "lock",
            "NULL"
          ],
          "line": 270
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_set_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "90-99",
          "snippet": "static __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_proxy_unlock",
          "args": [
            "lock"
          ],
          "line": 269
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_proxy_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "172-176",
          "snippet": "static inline void debug_rt_mutex_proxy_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(!rt_mutex_owner(lock));\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void debug_rt_mutex_proxy_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(!rt_mutex_owner(lock));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_proxy_unlock(struct rt_mutex_base *lock)\n{\n\tdebug_rt_mutex_proxy_unlock(lock);\n\trt_mutex_set_owner(lock, NULL);\n}"
  },
  {
    "function_name": "rt_mutex_init_proxy_locked",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "236-253",
    "snippet": "void __sched rt_mutex_init_proxy_locked(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct task_struct *proxy_owner)\n{\n\tstatic struct lock_class_key pi_futex_key;\n\n\t__rt_mutex_base_init(lock);\n\t/*\n\t * On PREEMPT_RT the futex hashbucket spinlock becomes 'sleeping'\n\t * and rtmutex based. That causes a lockdep false positive, because\n\t * some of the futex functions invoke spin_unlock(&hb->lock) with\n\t * the wait_lock of the rtmutex associated to the pi_futex held.\n\t * spin_unlock() in turn takes wait_lock of the rtmutex on which\n\t * the spinlock is based, which makes lockdep notice a lock\n\t * recursion. Give the futex/rtmutex wait_lock a separate key.\n\t */\n\tlockdep_set_class(&lock->wait_lock, &pi_futex_key);\n\trt_mutex_set_owner(lock, proxy_owner);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_set_owner",
          "args": [
            "lock",
            "proxy_owner"
          ],
          "line": 252
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_set_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "90-99",
          "snippet": "static __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_set_class",
          "args": [
            "&lock->wait_lock",
            "&pi_futex_key"
          ],
          "line": 251
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_base_init",
          "args": [
            "lock"
          ],
          "line": 241
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_base_init",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "158-163",
          "snippet": "static inline void __rt_mutex_base_init(struct rt_mutex_base *lock)\n{\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\tlock->owner = NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void __rt_mutex_base_init(struct rt_mutex_base *lock)\n{\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\tlock->owner = NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_init_proxy_locked(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct task_struct *proxy_owner)\n{\n\tstatic struct lock_class_key pi_futex_key;\n\n\t__rt_mutex_base_init(lock);\n\t/*\n\t * On PREEMPT_RT the futex hashbucket spinlock becomes 'sleeping'\n\t * and rtmutex based. That causes a lockdep false positive, because\n\t * some of the futex functions invoke spin_unlock(&hb->lock) with\n\t * the wait_lock of the rtmutex associated to the pi_futex held.\n\t * spin_unlock() in turn takes wait_lock of the rtmutex on which\n\t * the spinlock is based, which makes lockdep notice a lock\n\t * recursion. Give the futex/rtmutex wait_lock a separate key.\n\t */\n\tlockdep_set_class(&lock->wait_lock, &pi_futex_key);\n\trt_mutex_set_owner(lock, proxy_owner);\n}"
  },
  {
    "function_name": "__rt_mutex_init",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "213-219",
    "snippet": "void __sched __rt_mutex_init(struct rt_mutex *lock, const char *name,\n\t\t\t     struct lock_class_key *key)\n{\n\tdebug_check_no_locks_freed((void *)lock, sizeof(*lock));\n\t__rt_mutex_base_init(&lock->rtmutex);\n\tlockdep_init_map_wait(&lock->dep_map, name, key, 0, LD_WAIT_SLEEP);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "lockdep_init_map_wait",
          "args": [
            "&lock->dep_map",
            "name",
            "key",
            "0",
            "LD_WAIT_SLEEP"
          ],
          "line": 218
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_base_init",
          "args": [
            "&lock->rtmutex"
          ],
          "line": 217
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_base_init",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "158-163",
          "snippet": "static inline void __rt_mutex_base_init(struct rt_mutex_base *lock)\n{\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\tlock->owner = NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void __rt_mutex_base_init(struct rt_mutex_base *lock)\n{\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\tlock->owner = NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_check_no_locks_freed",
          "args": [
            "(void *)lock",
            "sizeof(*lock)"
          ],
          "line": 216
        },
        "resolved": true,
        "details": {
          "function_name": "debug_check_no_locks_freed",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/lockdep.c",
          "lines": "6405-6427",
          "snippet": "void debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tunsigned long flags;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\n\t\tif (not_in_range(mem_from, mem_len, hlock->instance,\n\t\t\t\t\tsizeof(*hlock->instance)))\n\t\t\tcontinue;\n\n\t\tprint_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);\n\t\tbreak;\n\t}\n\traw_local_irq_restore(flags);\n}",
          "includes": [
            "#include \"lockdep_states.h\"",
            "#include <trace/events/lock.h>",
            "#include \"lockdep_internals.h\"",
            "#include <asm/sections.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/nmi.h>",
            "#include <linux/jhash.h>",
            "#include <linux/random.h>",
            "#include <linux/gfp.h>",
            "#include <linux/bitops.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/stringify.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/hash.h>",
            "#include <linux/utsname.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/module.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/mutex.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static noinstr struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"lockdep_states.h\"\n#include <trace/events/lock.h>\n#include \"lockdep_internals.h\"\n#include <asm/sections.h>\n#include <linux/lockdep.h>\n#include <linux/kprobes.h>\n#include <linux/rcupdate.h>\n#include <linux/nmi.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <linux/gfp.h>\n#include <linux/bitops.h>\n#include <linux/bitmap.h>\n#include <linux/stringify.h>\n#include <linux/ftrace.h>\n#include <linux/hash.h>\n#include <linux/utsname.h>\n#include <linux/irqflags.h>\n#include <linux/debug_locks.h>\n#include <linux/stacktrace.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n\nstatic noinstr struct;\n\nvoid debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tunsigned long flags;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\n\t\tif (not_in_range(mem_from, mem_len, hlock->instance,\n\t\t\t\t\tsizeof(*hlock->instance)))\n\t\t\tcontinue;\n\n\t\tprint_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);\n\t\tbreak;\n\t}\n\traw_local_irq_restore(flags);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched __rt_mutex_init(struct rt_mutex *lock, const char *name,\n\t\t\t     struct lock_class_key *key)\n{\n\tdebug_check_no_locks_freed((void *)lock, sizeof(*lock));\n\t__rt_mutex_base_init(&lock->rtmutex);\n\tlockdep_init_map_wait(&lock->dep_map, name, key, 0, LD_WAIT_SLEEP);\n}"
  },
  {
    "function_name": "rt_mutex_futex_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "188-200",
    "snippet": "void __sched rt_mutex_futex_unlock(struct rt_mutex_base *lock)\n{\n\tDEFINE_RT_WAKE_Q(wqh);\n\tunsigned long flags;\n\tbool postunlock;\n\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tpostunlock = __rt_mutex_futex_unlock(lock, &wqh);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wqh);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_postunlock",
          "args": [
            "&wqh"
          ],
          "line": 199
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_postunlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "479-482",
          "snippet": "void __sched rt_mutex_postunlock(struct rt_wake_q_head *wqh)\n{\n\trt_mutex_wake_up_q(wqh);\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_postunlock(struct rt_wake_q_head *wqh)\n{\n\trt_mutex_wake_up_q(wqh);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 196
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_futex_unlock",
          "args": [
            "lock",
            "&wqh"
          ],
          "line": 195
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_futex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "165-186",
          "snippet": "bool __sched __rt_mutex_futex_unlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct rt_wake_q_head *wqh)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wqh, lock);\n\n\treturn true; /* call postunlock() */\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool __sched __rt_mutex_futex_unlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct rt_wake_q_head *wqh)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wqh, lock);\n\n\treturn true; /* call postunlock() */\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 194
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_RT_WAKE_Q",
          "args": [
            "wqh"
          ],
          "line": 190
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_futex_unlock(struct rt_mutex_base *lock)\n{\n\tDEFINE_RT_WAKE_Q(wqh);\n\tunsigned long flags;\n\tbool postunlock;\n\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tpostunlock = __rt_mutex_futex_unlock(lock, &wqh);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wqh);\n}"
  },
  {
    "function_name": "__rt_mutex_futex_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "165-186",
    "snippet": "bool __sched __rt_mutex_futex_unlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct rt_wake_q_head *wqh)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wqh, lock);\n\n\treturn true; /* call postunlock() */\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mark_wakeup_next_waiter",
          "args": [
            "wqh",
            "lock"
          ],
          "line": 183
        },
        "resolved": true,
        "details": {
          "function_name": "mark_wakeup_next_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1192-1234",
          "snippet": "static void __sched mark_wakeup_next_waiter(struct rt_wake_q_head *wqh,\n\t\t\t\t\t    struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_wake_up_q();\n\t */\n\tpreempt_disable();\n\trt_mutex_wake_q_add(wqh, waiter);\n\traw_spin_unlock(&current->pi_lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched mark_wakeup_next_waiter(struct rt_wake_q_head *wqh,\n\t\t\t\t\t    struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_wake_up_q();\n\t */\n\tpreempt_disable();\n\trt_mutex_wake_q_add(wqh, waiter);\n\traw_spin_unlock(&current->pi_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 172
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "93-96",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_unlock",
          "args": [
            "lock"
          ],
          "line": 170
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "166-170",
          "snippet": "static inline void debug_rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void debug_rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 168
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool __sched __rt_mutex_futex_unlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct rt_wake_q_head *wqh)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wqh, lock);\n\n\treturn true; /* call postunlock() */\n}"
  },
  {
    "function_name": "__rt_mutex_futex_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "153-156",
    "snippet": "int __sched __rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_slowtrylock",
          "args": [
            "lock"
          ],
          "line": 155
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowtrylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1236-1247",
          "snippet": "static int __sched __rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched __rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched __rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}"
  },
  {
    "function_name": "rt_mutex_futex_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "148-151",
    "snippet": "int __sched rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn rt_mutex_slowtrylock(lock);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_slowtrylock",
          "args": [
            "lock"
          ],
          "line": 150
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowtrylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1252-1276",
          "snippet": "static int __sched rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn rt_mutex_slowtrylock(lock);\n}"
  },
  {
    "function_name": "rt_mutex_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "138-142",
    "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, _RET_IP_);\n\t__rt_mutex_unlock(&lock->rtmutex);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_unlock",
          "args": [
            "&lock->rtmutex"
          ],
          "line": 141
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1350-1356",
          "snippet": "static __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&lock->dep_map",
            "_RET_IP_"
          ],
          "line": 140
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, _RET_IP_);\n\t__rt_mutex_unlock(&lock->rtmutex);\n}"
  },
  {
    "function_name": "rt_mutex_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "118-130",
    "snippet": "int __sched rt_mutex_trylock(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES) && WARN_ON_ONCE(!in_task()))\n\t\treturn 0;\n\n\tret = __rt_mutex_trylock(&lock->rtmutex);\n\tif (ret)\n\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_acquire",
          "args": [
            "&lock->dep_map",
            "0",
            "1",
            "_RET_IP_"
          ],
          "line": 127
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_trylock",
          "args": [
            "&lock->rtmutex"
          ],
          "line": 125
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1278-1284",
          "snippet": "static __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "!in_task()"
          ],
          "line": 122
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "in_task",
          "args": [],
          "line": 122
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_DEBUG_RT_MUTEXES"
          ],
          "line": 122
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_trylock(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES) && WARN_ON_ONCE(!in_task()))\n\t\treturn 0;\n\n\tret = __rt_mutex_trylock(&lock->rtmutex);\n\tif (ret)\n\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_lock_killable",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "100-103",
    "snippet": "int __sched rt_mutex_lock_killable(struct rt_mutex *lock)\n{\n\treturn __rt_mutex_lock_common(lock, TASK_KILLABLE, NULL, 0);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_lock_common",
          "args": [
            "lock",
            "TASK_KILLABLE",
            "NULL",
            "0"
          ],
          "line": 102
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "22-35",
          "snippet": "static __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_killable(struct rt_mutex *lock)\n{\n\treturn __rt_mutex_lock_common(lock, TASK_KILLABLE, NULL, 0);\n}"
  },
  {
    "function_name": "rt_mutex_lock_interruptible",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "85-88",
    "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\treturn __rt_mutex_lock_common(lock, TASK_INTERRUPTIBLE, NULL, 0);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_lock_common",
          "args": [
            "lock",
            "TASK_INTERRUPTIBLE",
            "NULL",
            "0"
          ],
          "line": 87
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "22-35",
          "snippet": "static __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\treturn __rt_mutex_lock_common(lock, TASK_INTERRUPTIBLE, NULL, 0);\n}"
  },
  {
    "function_name": "rt_mutex_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "69-72",
    "snippet": "void __sched rt_mutex_lock(struct rt_mutex *lock)\n{\n\t__rt_mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, NULL, 0);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "NULL",
            "0"
          ],
          "line": 71
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "22-35",
          "snippet": "static __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_lock(struct rt_mutex *lock)\n{\n\t__rt_mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, NULL, 0);\n}"
  },
  {
    "function_name": "_rt_mutex_lock_nest_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "56-59",
    "snippet": "void __sched _rt_mutex_lock_nest_lock(struct rt_mutex *lock, struct lockdep_map *nest_lock)\n{\n\t__rt_mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, nest_lock, 0);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "nest_lock",
            "0"
          ],
          "line": 58
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "22-35",
          "snippet": "static __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched _rt_mutex_lock_nest_lock(struct rt_mutex *lock, struct lockdep_map *nest_lock)\n{\n\t__rt_mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, nest_lock, 0);\n}"
  },
  {
    "function_name": "rt_mutex_lock_nested",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "50-53",
    "snippet": "void __sched rt_mutex_lock_nested(struct rt_mutex *lock, unsigned int subclass)\n{\n\t__rt_mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, NULL, subclass);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_lock_common",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "NULL",
            "subclass"
          ],
          "line": 52
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock_common",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "22-35",
          "snippet": "static __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_lock_nested(struct rt_mutex *lock, unsigned int subclass)\n{\n\t__rt_mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, NULL, subclass);\n}"
  },
  {
    "function_name": "rt_mutex_base_init",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "37-40",
    "snippet": "void rt_mutex_base_init(struct rt_mutex_base *rtb)\n{\n\t__rt_mutex_base_init(rtb);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_base_init",
          "args": [
            "rtb"
          ],
          "line": 39
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_base_init",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "158-163",
          "snippet": "static inline void __rt_mutex_base_init(struct rt_mutex_base *lock)\n{\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\tlock->owner = NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void __rt_mutex_base_init(struct rt_mutex_base *lock)\n{\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\tlock->owner = NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_base_init(struct rt_mutex_base *rtb)\n{\n\t__rt_mutex_base_init(rtb);\n}"
  },
  {
    "function_name": "__rt_mutex_lock_common",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
    "lines": "22-35",
    "snippet": "static __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&lock->dep_map",
            "_RET_IP_"
          ],
          "line": 33
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_lock",
          "args": [
            "&lock->rtmutex",
            "state"
          ],
          "line": 31
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1652-1659",
          "snippet": "static __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t   unsigned int state)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn rt_mutex_slowlock(lock, NULL, state);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t   unsigned int state)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn rt_mutex_slowlock(lock, NULL, state);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_acquire_nest",
          "args": [
            "&lock->dep_map",
            "subclass",
            "0",
            "nest_lock",
            "_RET_IP_"
          ],
          "line": 30
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "might_sleep",
          "args": [],
          "line": 29
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int __rt_mutex_lock_common(struct rt_mutex *lock,\n\t\t\t\t\t\t  unsigned int state,\n\t\t\t\t\t\t  struct lockdep_map *nest_lock,\n\t\t\t\t\t\t  unsigned int subclass)\n{\n\tint ret;\n\n\tmight_sleep();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);\n\tret = __rt_mutex_lock(&lock->rtmutex, state);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, _RET_IP_);\n\treturn ret;\n}"
  }
]