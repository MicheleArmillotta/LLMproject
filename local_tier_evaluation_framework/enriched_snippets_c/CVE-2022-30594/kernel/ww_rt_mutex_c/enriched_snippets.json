[
  {
    "function_name": "ww_mutex_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_rt_mutex.c",
    "lines": "92-100",
    "snippet": "void __sched ww_mutex_unlock(struct ww_mutex *lock)\n{\n\tstruct rt_mutex *rtm = &lock->base;\n\n\t__ww_mutex_unlock(lock);\n\n\tmutex_release(&rtm->dep_map, _RET_IP_);\n\t__rt_mutex_unlock(&rtm->rtmutex);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_unlock",
          "args": [
            "&rtm->rtmutex"
          ],
          "line": 99
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1350-1356",
          "snippet": "static __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&rtm->dep_map",
            "_RET_IP_"
          ],
          "line": 98
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__ww_mutex_unlock",
          "args": [
            "lock"
          ],
          "line": 96
        },
        "resolved": true,
        "details": {
          "function_name": "__ww_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_mutex.h",
          "lines": "559-569",
          "snippet": "static inline void __ww_mutex_unlock(struct ww_mutex *lock)\n{\n\tif (lock->ctx) {\n#ifdef DEBUG_WW_MUTEXES\n\t\tDEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);\n#endif\n\t\tif (lock->ctx->acquired > 0)\n\t\t\tlock->ctx->acquired--;\n\t\tlock->ctx = NULL;\n\t}\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void __ww_mutex_unlock(struct ww_mutex *lock)\n{\n\tif (lock->ctx) {\n#ifdef DEBUG_WW_MUTEXES\n\t\tDEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);\n#endif\n\t\tif (lock->ctx->acquired > 0)\n\t\t\tlock->ctx->acquired--;\n\t\tlock->ctx = NULL;\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched ww_mutex_unlock(struct ww_mutex *lock)\n{\n\tstruct rt_mutex *rtm = &lock->base;\n\n\t__ww_mutex_unlock(lock);\n\n\tmutex_release(&rtm->dep_map, _RET_IP_);\n\t__rt_mutex_unlock(&rtm->rtmutex);\n}"
  },
  {
    "function_name": "ww_mutex_lock_interruptible",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_rt_mutex.c",
    "lines": "85-89",
    "snippet": "int __sched\nww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\treturn __ww_rt_mutex_lock(lock, ctx, TASK_INTERRUPTIBLE, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__ww_rt_mutex_lock",
          "args": [
            "lock",
            "ctx",
            "TASK_INTERRUPTIBLE",
            "_RET_IP_"
          ],
          "line": 88
        },
        "resolved": true,
        "details": {
          "function_name": "__ww_rt_mutex_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_rt_mutex.c",
          "lines": "37-76",
          "snippet": "static int __sched\n__ww_rt_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t   unsigned int state, unsigned long ip)\n{\n\tstruct lockdep_map __maybe_unused *nest_lock = NULL;\n\tstruct rt_mutex *rtm = &lock->base;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (ww_ctx) {\n\t\tif (unlikely(ww_ctx == READ_ONCE(lock->ctx)))\n\t\t\treturn -EALREADY;\n\n\t\t/*\n\t\t * Reset the wounded flag after a kill. No other process can\n\t\t * race and wound us here, since they can't have a valid owner\n\t\t * pointer if we don't have any locks held.\n\t\t */\n\t\tif (ww_ctx->acquired == 0)\n\t\t\tww_ctx->wounded = 0;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t\tnest_lock = &ww_ctx->dep_map;\n#endif\n\t}\n\tmutex_acquire_nest(&rtm->dep_map, 0, 0, nest_lock, ip);\n\n\tif (likely(rt_mutex_cmpxchg_acquire(&rtm->rtmutex, NULL, current))) {\n\t\tif (ww_ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\treturn 0;\n\t}\n\n\tret = rt_mutex_slowlock(&rtm->rtmutex, ww_ctx, state);\n\n\tif (ret)\n\t\tmutex_release(&rtm->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int __sched\n__ww_rt_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t   unsigned int state, unsigned long ip)\n{\n\tstruct lockdep_map __maybe_unused *nest_lock = NULL;\n\tstruct rt_mutex *rtm = &lock->base;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (ww_ctx) {\n\t\tif (unlikely(ww_ctx == READ_ONCE(lock->ctx)))\n\t\t\treturn -EALREADY;\n\n\t\t/*\n\t\t * Reset the wounded flag after a kill. No other process can\n\t\t * race and wound us here, since they can't have a valid owner\n\t\t * pointer if we don't have any locks held.\n\t\t */\n\t\tif (ww_ctx->acquired == 0)\n\t\t\tww_ctx->wounded = 0;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t\tnest_lock = &ww_ctx->dep_map;\n#endif\n\t}\n\tmutex_acquire_nest(&rtm->dep_map, 0, 0, nest_lock, ip);\n\n\tif (likely(rt_mutex_cmpxchg_acquire(&rtm->rtmutex, NULL, current))) {\n\t\tif (ww_ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\treturn 0;\n\t}\n\n\tret = rt_mutex_slowlock(&rtm->rtmutex, ww_ctx, state);\n\n\tif (ret)\n\t\tmutex_release(&rtm->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched\nww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\treturn __ww_rt_mutex_lock(lock, ctx, TASK_INTERRUPTIBLE, _RET_IP_);\n}"
  },
  {
    "function_name": "ww_mutex_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_rt_mutex.c",
    "lines": "78-82",
    "snippet": "int __sched\nww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\treturn __ww_rt_mutex_lock(lock, ctx, TASK_UNINTERRUPTIBLE, _RET_IP_);\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__ww_rt_mutex_lock",
          "args": [
            "lock",
            "ctx",
            "TASK_UNINTERRUPTIBLE",
            "_RET_IP_"
          ],
          "line": 81
        },
        "resolved": true,
        "details": {
          "function_name": "__ww_rt_mutex_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_rt_mutex.c",
          "lines": "37-76",
          "snippet": "static int __sched\n__ww_rt_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t   unsigned int state, unsigned long ip)\n{\n\tstruct lockdep_map __maybe_unused *nest_lock = NULL;\n\tstruct rt_mutex *rtm = &lock->base;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (ww_ctx) {\n\t\tif (unlikely(ww_ctx == READ_ONCE(lock->ctx)))\n\t\t\treturn -EALREADY;\n\n\t\t/*\n\t\t * Reset the wounded flag after a kill. No other process can\n\t\t * race and wound us here, since they can't have a valid owner\n\t\t * pointer if we don't have any locks held.\n\t\t */\n\t\tif (ww_ctx->acquired == 0)\n\t\t\tww_ctx->wounded = 0;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t\tnest_lock = &ww_ctx->dep_map;\n#endif\n\t}\n\tmutex_acquire_nest(&rtm->dep_map, 0, 0, nest_lock, ip);\n\n\tif (likely(rt_mutex_cmpxchg_acquire(&rtm->rtmutex, NULL, current))) {\n\t\tif (ww_ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\treturn 0;\n\t}\n\n\tret = rt_mutex_slowlock(&rtm->rtmutex, ww_ctx, state);\n\n\tif (ret)\n\t\tmutex_release(&rtm->dep_map, ip);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int __sched\n__ww_rt_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t   unsigned int state, unsigned long ip)\n{\n\tstruct lockdep_map __maybe_unused *nest_lock = NULL;\n\tstruct rt_mutex *rtm = &lock->base;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (ww_ctx) {\n\t\tif (unlikely(ww_ctx == READ_ONCE(lock->ctx)))\n\t\t\treturn -EALREADY;\n\n\t\t/*\n\t\t * Reset the wounded flag after a kill. No other process can\n\t\t * race and wound us here, since they can't have a valid owner\n\t\t * pointer if we don't have any locks held.\n\t\t */\n\t\tif (ww_ctx->acquired == 0)\n\t\t\tww_ctx->wounded = 0;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t\tnest_lock = &ww_ctx->dep_map;\n#endif\n\t}\n\tmutex_acquire_nest(&rtm->dep_map, 0, 0, nest_lock, ip);\n\n\tif (likely(rt_mutex_cmpxchg_acquire(&rtm->rtmutex, NULL, current))) {\n\t\tif (ww_ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\treturn 0;\n\t}\n\n\tret = rt_mutex_slowlock(&rtm->rtmutex, ww_ctx, state);\n\n\tif (ret)\n\t\tmutex_release(&rtm->dep_map, ip);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched\nww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\treturn __ww_rt_mutex_lock(lock, ctx, TASK_UNINTERRUPTIBLE, _RET_IP_);\n}"
  },
  {
    "function_name": "__ww_rt_mutex_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_rt_mutex.c",
    "lines": "37-76",
    "snippet": "static int __sched\n__ww_rt_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t   unsigned int state, unsigned long ip)\n{\n\tstruct lockdep_map __maybe_unused *nest_lock = NULL;\n\tstruct rt_mutex *rtm = &lock->base;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (ww_ctx) {\n\t\tif (unlikely(ww_ctx == READ_ONCE(lock->ctx)))\n\t\t\treturn -EALREADY;\n\n\t\t/*\n\t\t * Reset the wounded flag after a kill. No other process can\n\t\t * race and wound us here, since they can't have a valid owner\n\t\t * pointer if we don't have any locks held.\n\t\t */\n\t\tif (ww_ctx->acquired == 0)\n\t\t\tww_ctx->wounded = 0;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t\tnest_lock = &ww_ctx->dep_map;\n#endif\n\t}\n\tmutex_acquire_nest(&rtm->dep_map, 0, 0, nest_lock, ip);\n\n\tif (likely(rt_mutex_cmpxchg_acquire(&rtm->rtmutex, NULL, current))) {\n\t\tif (ww_ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\treturn 0;\n\t}\n\n\tret = rt_mutex_slowlock(&rtm->rtmutex, ww_ctx, state);\n\n\tif (ret)\n\t\tmutex_release(&rtm->dep_map, ip);\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&rtm->dep_map",
            "ip"
          ],
          "line": 74
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_slowlock",
          "args": [
            "&rtm->rtmutex",
            "ww_ctx",
            "state"
          ],
          "line": 71
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1630-1650",
          "snippet": "static int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t     unsigned int state)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t     unsigned int state)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "ww_mutex_set_context_fastpath",
          "args": [
            "lock",
            "ww_ctx"
          ],
          "line": 67
        },
        "resolved": true,
        "details": {
          "function_name": "ww_mutex_set_context_fastpath",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_mutex.h",
          "lines": "377-410",
          "snippet": "static __always_inline void\nww_mutex_set_context_fastpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tww_mutex_lock_acquired(lock, ctx);\n\n\t/*\n\t * The lock->ctx update should be visible on all cores before\n\t * the WAITERS check is done, otherwise contended waiters might be\n\t * missed. The contended waiters will either see ww_ctx == NULL\n\t * and keep spinning, or it will acquire wait_lock, add itself\n\t * to waiter list and sleep.\n\t */\n\tsmp_mb(); /* See comments above and below. */\n\n\t/*\n\t * [W] ww->ctx = ctx\t    [W] MUTEX_FLAG_WAITERS\n\t *     MB\t\t        MB\n\t * [R] MUTEX_FLAG_WAITERS   [R] ww->ctx\n\t *\n\t * The memory barrier above pairs with the memory barrier in\n\t * __ww_mutex_add_waiter() and makes sure we either observe ww->ctx\n\t * and/or !empty list.\n\t */\n\tif (likely(!__ww_mutex_has_waiters(&lock->base)))\n\t\treturn;\n\n\t/*\n\t * Uh oh, we raced in fastpath, check if any of the waiters need to\n\t * die or wound us.\n\t */\n\tlock_wait_lock(&lock->base);\n\t__ww_mutex_check_waiters(&lock->base, ctx);\n\tunlock_wait_lock(&lock->base);\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static __always_inline void\nww_mutex_set_context_fastpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tww_mutex_lock_acquired(lock, ctx);\n\n\t/*\n\t * The lock->ctx update should be visible on all cores before\n\t * the WAITERS check is done, otherwise contended waiters might be\n\t * missed. The contended waiters will either see ww_ctx == NULL\n\t * and keep spinning, or it will acquire wait_lock, add itself\n\t * to waiter list and sleep.\n\t */\n\tsmp_mb(); /* See comments above and below. */\n\n\t/*\n\t * [W] ww->ctx = ctx\t    [W] MUTEX_FLAG_WAITERS\n\t *     MB\t\t        MB\n\t * [R] MUTEX_FLAG_WAITERS   [R] ww->ctx\n\t *\n\t * The memory barrier above pairs with the memory barrier in\n\t * __ww_mutex_add_waiter() and makes sure we either observe ww->ctx\n\t * and/or !empty list.\n\t */\n\tif (likely(!__ww_mutex_has_waiters(&lock->base)))\n\t\treturn;\n\n\t/*\n\t * Uh oh, we raced in fastpath, check if any of the waiters need to\n\t * die or wound us.\n\t */\n\tlock_wait_lock(&lock->base);\n\t__ww_mutex_check_waiters(&lock->base, ctx);\n\tunlock_wait_lock(&lock->base);\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_acquire(&rtm->rtmutex, NULL, current)"
          ],
          "line": 65
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_acquire",
          "args": [
            "&rtm->rtmutex",
            "NULL",
            "current"
          ],
          "line": 65
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_acquire",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "253-259",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_acquire_nest",
          "args": [
            "&rtm->dep_map",
            "0",
            "0",
            "nest_lock",
            "ip"
          ],
          "line": 63
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ww_ctx == READ_ONCE(lock->ctx)"
          ],
          "line": 48
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "lock->ctx"
          ],
          "line": 48
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "might_sleep",
          "args": [],
          "line": 45
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int __sched\n__ww_rt_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t   unsigned int state, unsigned long ip)\n{\n\tstruct lockdep_map __maybe_unused *nest_lock = NULL;\n\tstruct rt_mutex *rtm = &lock->base;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (ww_ctx) {\n\t\tif (unlikely(ww_ctx == READ_ONCE(lock->ctx)))\n\t\t\treturn -EALREADY;\n\n\t\t/*\n\t\t * Reset the wounded flag after a kill. No other process can\n\t\t * race and wound us here, since they can't have a valid owner\n\t\t * pointer if we don't have any locks held.\n\t\t */\n\t\tif (ww_ctx->acquired == 0)\n\t\t\tww_ctx->wounded = 0;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t\tnest_lock = &ww_ctx->dep_map;\n#endif\n\t}\n\tmutex_acquire_nest(&rtm->dep_map, 0, 0, nest_lock, ip);\n\n\tif (likely(rt_mutex_cmpxchg_acquire(&rtm->rtmutex, NULL, current))) {\n\t\tif (ww_ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\treturn 0;\n\t}\n\n\tret = rt_mutex_slowlock(&rtm->rtmutex, ww_ctx, state);\n\n\tif (ret)\n\t\tmutex_release(&rtm->dep_map, ip);\n\treturn ret;\n}"
  },
  {
    "function_name": "ww_mutex_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_rt_mutex.c",
    "lines": "12-34",
    "snippet": "int ww_mutex_trylock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx)\n{\n\tstruct rt_mutex *rtm = &lock->base;\n\n\tif (!ww_ctx)\n\t\treturn rt_mutex_trylock(rtm);\n\n\t/*\n\t * Reset the wounded flag after a kill. No other process can\n\t * race and wound us here, since they can't have a valid owner\n\t * pointer if we don't have any locks held.\n\t */\n\tif (ww_ctx->acquired == 0)\n\t\tww_ctx->wounded = 0;\n\n\tif (__rt_mutex_trylock(&rtm->rtmutex)) {\n\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\tmutex_acquire_nest(&rtm->dep_map, 0, 1, &ww_ctx->dep_map, _RET_IP_);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
    "includes": [
      "#include \"rtmutex.c\"",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_acquire_nest",
          "args": [
            "&rtm->dep_map",
            "0",
            "1",
            "&ww_ctx->dep_map",
            "_RET_IP_"
          ],
          "line": 29
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ww_mutex_set_context_fastpath",
          "args": [
            "lock",
            "ww_ctx"
          ],
          "line": 28
        },
        "resolved": true,
        "details": {
          "function_name": "ww_mutex_set_context_fastpath",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/ww_mutex.h",
          "lines": "377-410",
          "snippet": "static __always_inline void\nww_mutex_set_context_fastpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tww_mutex_lock_acquired(lock, ctx);\n\n\t/*\n\t * The lock->ctx update should be visible on all cores before\n\t * the WAITERS check is done, otherwise contended waiters might be\n\t * missed. The contended waiters will either see ww_ctx == NULL\n\t * and keep spinning, or it will acquire wait_lock, add itself\n\t * to waiter list and sleep.\n\t */\n\tsmp_mb(); /* See comments above and below. */\n\n\t/*\n\t * [W] ww->ctx = ctx\t    [W] MUTEX_FLAG_WAITERS\n\t *     MB\t\t        MB\n\t * [R] MUTEX_FLAG_WAITERS   [R] ww->ctx\n\t *\n\t * The memory barrier above pairs with the memory barrier in\n\t * __ww_mutex_add_waiter() and makes sure we either observe ww->ctx\n\t * and/or !empty list.\n\t */\n\tif (likely(!__ww_mutex_has_waiters(&lock->base)))\n\t\treturn;\n\n\t/*\n\t * Uh oh, we raced in fastpath, check if any of the waiters need to\n\t * die or wound us.\n\t */\n\tlock_wait_lock(&lock->base);\n\t__ww_mutex_check_waiters(&lock->base, ctx);\n\tunlock_wait_lock(&lock->base);\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static __always_inline void\nww_mutex_set_context_fastpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tww_mutex_lock_acquired(lock, ctx);\n\n\t/*\n\t * The lock->ctx update should be visible on all cores before\n\t * the WAITERS check is done, otherwise contended waiters might be\n\t * missed. The contended waiters will either see ww_ctx == NULL\n\t * and keep spinning, or it will acquire wait_lock, add itself\n\t * to waiter list and sleep.\n\t */\n\tsmp_mb(); /* See comments above and below. */\n\n\t/*\n\t * [W] ww->ctx = ctx\t    [W] MUTEX_FLAG_WAITERS\n\t *     MB\t\t        MB\n\t * [R] MUTEX_FLAG_WAITERS   [R] ww->ctx\n\t *\n\t * The memory barrier above pairs with the memory barrier in\n\t * __ww_mutex_add_waiter() and makes sure we either observe ww->ctx\n\t * and/or !empty list.\n\t */\n\tif (likely(!__ww_mutex_has_waiters(&lock->base)))\n\t\treturn;\n\n\t/*\n\t * Uh oh, we raced in fastpath, check if any of the waiters need to\n\t * die or wound us.\n\t */\n\tlock_wait_lock(&lock->base);\n\t__ww_mutex_check_waiters(&lock->base, ctx);\n\tunlock_wait_lock(&lock->base);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_trylock",
          "args": [
            "&rtm->rtmutex"
          ],
          "line": 27
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1278-1284",
          "snippet": "static __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint ww_mutex_trylock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx)\n{\n\tstruct rt_mutex *rtm = &lock->base;\n\n\tif (!ww_ctx)\n\t\treturn rt_mutex_trylock(rtm);\n\n\t/*\n\t * Reset the wounded flag after a kill. No other process can\n\t * race and wound us here, since they can't have a valid owner\n\t * pointer if we don't have any locks held.\n\t */\n\tif (ww_ctx->acquired == 0)\n\t\tww_ctx->wounded = 0;\n\n\tif (__rt_mutex_trylock(&rtm->rtmutex)) {\n\t\tww_mutex_set_context_fastpath(lock, ww_ctx);\n\t\tmutex_acquire_nest(&rtm->dep_map, 0, 1, &ww_ctx->dep_map, _RET_IP_);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
  }
]