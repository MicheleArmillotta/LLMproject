[
  {
    "function_name": "mcs_spin_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/mcs_spinlock.h",
    "lines": "101-119",
    "snippet": "static inline\nvoid mcs_spin_unlock(struct mcs_spinlock **lock, struct mcs_spinlock *node)\n{\n\tstruct mcs_spinlock *next = READ_ONCE(node->next);\n\n\tif (likely(!next)) {\n\t\t/*\n\t\t * Release the lock by setting it to NULL\n\t\t */\n\t\tif (likely(cmpxchg_release(lock, node, NULL) == node))\n\t\t\treturn;\n\t\t/* Wait until the next pointer is set */\n\t\twhile (!(next = READ_ONCE(node->next)))\n\t\t\tcpu_relax();\n\t}\n\n\t/* Pass lock to next waiter. */\n\tarch_mcs_spin_unlock_contended(&next->locked);\n}",
    "includes": [
      "#include <asm/mcs_spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "arch_mcs_spin_unlock_contended",
          "args": [
            "&next->locked"
          ],
          "line": 118
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 114
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "node->next"
          ],
          "line": 113
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "cmpxchg_release(lock, node, NULL) == node"
          ],
          "line": 110
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cmpxchg_release",
          "args": [
            "lock",
            "node",
            "NULL"
          ],
          "line": 110
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_release",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "261-266",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!next"
          ],
          "line": 106
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "node->next"
          ],
          "line": 104
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/mcs_spinlock.h>\n\nstatic inline\nvoid mcs_spin_unlock(struct mcs_spinlock **lock, struct mcs_spinlock *node)\n{\n\tstruct mcs_spinlock *next = READ_ONCE(node->next);\n\n\tif (likely(!next)) {\n\t\t/*\n\t\t * Release the lock by setting it to NULL\n\t\t */\n\t\tif (likely(cmpxchg_release(lock, node, NULL) == node))\n\t\t\treturn;\n\t\t/* Wait until the next pointer is set */\n\t\twhile (!(next = READ_ONCE(node->next)))\n\t\t\tcpu_relax();\n\t}\n\n\t/* Pass lock to next waiter. */\n\tarch_mcs_spin_unlock_contended(&next->locked);\n}"
  },
  {
    "function_name": "mcs_spin_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/mcs_spinlock.h",
    "lines": "64-95",
    "snippet": "static inline\nvoid mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node)\n{\n\tstruct mcs_spinlock *prev;\n\n\t/* Init node */\n\tnode->locked = 0;\n\tnode->next   = NULL;\n\n\t/*\n\t * We rely on the full barrier with global transitivity implied by the\n\t * below xchg() to order the initialization stores above against any\n\t * observation of @node. And to provide the ACQUIRE ordering associated\n\t * with a LOCK primitive.\n\t */\n\tprev = xchg(lock, node);\n\tif (likely(prev == NULL)) {\n\t\t/*\n\t\t * Lock acquired, don't need to set node->locked to 1. Threads\n\t\t * only spin on its own node->locked value for lock acquisition.\n\t\t * However, since this thread can immediately acquire the lock\n\t\t * and does not proceed to spin on its own node->locked, this\n\t\t * value won't be used. If a debug mode is needed to\n\t\t * audit lock status, then set node->locked value here.\n\t\t */\n\t\treturn;\n\t}\n\tWRITE_ONCE(prev->next, node);\n\n\t/* Wait until the lock holder passes the lock down. */\n\tarch_mcs_spin_lock_contended(&node->locked);\n}",
    "includes": [
      "#include <asm/mcs_spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "arch_mcs_spin_lock_contended",
          "args": [
            "&node->locked"
          ],
          "line": 94
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "prev->next",
            "node"
          ],
          "line": 91
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "prev == NULL"
          ],
          "line": 80
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "xchg",
          "args": [
            "lock",
            "node"
          ],
          "line": 79
        },
        "resolved": true,
        "details": {
          "function_name": "xchg_tail",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "220-238",
          "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [
            "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <asm/mcs_spinlock.h>\n\nstatic inline\nvoid mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node)\n{\n\tstruct mcs_spinlock *prev;\n\n\t/* Init node */\n\tnode->locked = 0;\n\tnode->next   = NULL;\n\n\t/*\n\t * We rely on the full barrier with global transitivity implied by the\n\t * below xchg() to order the initialization stores above against any\n\t * observation of @node. And to provide the ACQUIRE ordering associated\n\t * with a LOCK primitive.\n\t */\n\tprev = xchg(lock, node);\n\tif (likely(prev == NULL)) {\n\t\t/*\n\t\t * Lock acquired, don't need to set node->locked to 1. Threads\n\t\t * only spin on its own node->locked value for lock acquisition.\n\t\t * However, since this thread can immediately acquire the lock\n\t\t * and does not proceed to spin on its own node->locked, this\n\t\t * value won't be used. If a debug mode is needed to\n\t\t * audit lock status, then set node->locked value here.\n\t\t */\n\t\treturn;\n\t}\n\tWRITE_ONCE(prev->next, node);\n\n\t/* Wait until the lock holder passes the lock down. */\n\tarch_mcs_spin_lock_contended(&node->locked);\n}"
  }
]