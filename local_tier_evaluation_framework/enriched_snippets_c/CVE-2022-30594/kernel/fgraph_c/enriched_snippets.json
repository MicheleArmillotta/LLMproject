[
  {
    "function_name": "unregister_ftrace_graph",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "636-653",
    "snippet": "void unregister_ftrace_graph(struct fgraph_ops *gops)\n{\n\tmutex_lock(&ftrace_lock);\n\n\tif (unlikely(!ftrace_graph_active))\n\t\tgoto out;\n\n\tftrace_graph_active--;\n\tftrace_graph_return = ftrace_stub_graph;\n\tftrace_graph_entry = ftrace_graph_entry_stub;\n\t__ftrace_graph_entry = ftrace_graph_entry_stub;\n\tftrace_shutdown(&graph_ops, FTRACE_STOP_FUNC_RET);\n\tunregister_pm_notifier(&ftrace_suspend_notifier);\n\tunregister_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\n out:\n\tmutex_unlock(&ftrace_lock);\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "int ftrace_graph_active;",
      "static struct notifier_block ftrace_suspend_notifier = {\n\t.notifier_call = ftrace_suspend_notifier_call,\n};",
      "static struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};",
      "trace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;",
      "trace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;",
      "static trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&ftrace_lock"
          ],
          "line": 652
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1350-1356",
          "snippet": "static __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unregister_trace_sched_switch",
          "args": [
            "ftrace_graph_probe_sched_switch",
            "NULL"
          ],
          "line": 649
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unregister_pm_notifier",
          "args": [
            "&ftrace_suspend_notifier"
          ],
          "line": 648
        },
        "resolved": true,
        "details": {
          "function_name": "unregister_pm_notifier",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/power/main.c",
          "lines": "77-80",
          "snippet": "int unregister_pm_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&pm_chain_head, nb);\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <linux/pm_runtime.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/suspend.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/pm-trace.h>",
            "#include <linux/string.h>",
            "#include <linux/kobject.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <linux/pm_runtime.h>\n#include <linux/syscalls.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/debugfs.h>\n#include <linux/workqueue.h>\n#include <linux/pm-trace.h>\n#include <linux/string.h>\n#include <linux/kobject.h>\n#include <linux/export.h>\n\nint unregister_pm_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&pm_chain_head, nb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "ftrace_shutdown",
          "args": [
            "&graph_ops",
            "FTRACE_STOP_FUNC_RET"
          ],
          "line": 647
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_shutdown",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/ftrace.c",
          "lines": "2937-3049",
          "snippet": "int ftrace_shutdown(struct ftrace_ops *ops, int command)\n{\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tret = __unregister_ftrace_function(ops);\n\tif (ret)\n\t\treturn ret;\n\n\tftrace_start_up--;\n\t/*\n\t * Just warn in case of unbalance, no need to kill ftrace, it's not\n\t * critical but the ftrace_call callers may be never nopped again after\n\t * further ftrace uses.\n\t */\n\tWARN_ON_ONCE(ftrace_start_up < 0);\n\n\t/* Disabling ipmodify never fails */\n\tftrace_hash_ipmodify_disable(ops);\n\n\tif (ftrace_hash_rec_disable(ops, 1))\n\t\tcommand |= FTRACE_UPDATE_CALLS;\n\n\tops->flags &= ~FTRACE_OPS_FL_ENABLED;\n\n\tif (saved_ftrace_func != ftrace_trace_function) {\n\t\tsaved_ftrace_func = ftrace_trace_function;\n\t\tcommand |= FTRACE_UPDATE_TRACE_FUNC;\n\t}\n\n\tif (!command || !ftrace_enabled) {\n\t\t/*\n\t\t * If these are dynamic or per_cpu ops, they still\n\t\t * need their data freed. Since, function tracing is\n\t\t * not currently active, we can just free them\n\t\t * without synchronizing all CPUs.\n\t\t */\n\t\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC)\n\t\t\tgoto free_ops;\n\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the ops uses a trampoline, then it needs to be\n\t * tested first on update.\n\t */\n\tops->flags |= FTRACE_OPS_FL_REMOVING;\n\tremoved_ops = ops;\n\n\t/* The trampoline logic checks the old hashes */\n\tops->old_hash.filter_hash = ops->func_hash->filter_hash;\n\tops->old_hash.notrace_hash = ops->func_hash->notrace_hash;\n\n\tftrace_run_update_code(command);\n\n\t/*\n\t * If there's no more ops registered with ftrace, run a\n\t * sanity check to make sure all rec flags are cleared.\n\t */\n\tif (rcu_dereference_protected(ftrace_ops_list,\n\t\t\tlockdep_is_held(&ftrace_lock)) == &ftrace_list_end) {\n\t\tstruct ftrace_page *pg;\n\t\tstruct dyn_ftrace *rec;\n\n\t\tdo_for_each_ftrace_rec(pg, rec) {\n\t\t\tif (FTRACE_WARN_ON_ONCE(rec->flags & ~FTRACE_FL_DISABLED))\n\t\t\t\tpr_warn(\"  %pS flags:%lx\\n\",\n\t\t\t\t\t(void *)rec->ip, rec->flags);\n\t\t} while_for_each_ftrace_rec();\n\t}\n\n\tops->old_hash.filter_hash = NULL;\n\tops->old_hash.notrace_hash = NULL;\n\n\tremoved_ops = NULL;\n\tops->flags &= ~FTRACE_OPS_FL_REMOVING;\n\n\t/*\n\t * Dynamic ops may be freed, we must make sure that all\n\t * callers are done before leaving this function.\n\t * The same goes for freeing the per_cpu data of the per_cpu\n\t * ops.\n\t */\n\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC) {\n\t\t/*\n\t\t * We need to do a hard force of sched synchronization.\n\t\t * This is because we use preempt_disable() to do RCU, but\n\t\t * the function tracers can be called where RCU is not watching\n\t\t * (like before user_exit()). We can not rely on the RCU\n\t\t * infrastructure to do the synchronization, thus we must do it\n\t\t * ourselves.\n\t\t */\n\t\tsynchronize_rcu_tasks_rude();\n\n\t\t/*\n\t\t * When the kernel is preemptive, tasks can be preempted\n\t\t * while on a ftrace trampoline. Just scheduling a task on\n\t\t * a CPU is not good enough to flush them. Calling\n\t\t * synchronize_rcu_tasks() will wait for those tasks to\n\t\t * execute and either schedule voluntarily or enter user space.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPTION))\n\t\t\tsynchronize_rcu_tasks();\n\n free_ops:\n\t\tftrace_trampoline_free(ops);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include \"ftrace_internal.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/security.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "int ftrace_enabled",
            "static void ftrace_update_trampoline(struct ftrace_ops *ops);",
            "static int ftrace_disabled",
            "struct ftrace_ops __rcu *ftrace_ops_list",
            "ftrace_func_t ftrace_trace_function",
            "static void ftrace_update_trampoline(struct ftrace_ops *ops);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include \"ftrace_internal.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/kprobes.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/seq_file.h>\n#include <linux/security.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nint ftrace_enabled;\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\nstatic int ftrace_disabled;\nstruct ftrace_ops __rcu *ftrace_ops_list;\nftrace_func_t ftrace_trace_function;\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\n\nint ftrace_shutdown(struct ftrace_ops *ops, int command)\n{\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tret = __unregister_ftrace_function(ops);\n\tif (ret)\n\t\treturn ret;\n\n\tftrace_start_up--;\n\t/*\n\t * Just warn in case of unbalance, no need to kill ftrace, it's not\n\t * critical but the ftrace_call callers may be never nopped again after\n\t * further ftrace uses.\n\t */\n\tWARN_ON_ONCE(ftrace_start_up < 0);\n\n\t/* Disabling ipmodify never fails */\n\tftrace_hash_ipmodify_disable(ops);\n\n\tif (ftrace_hash_rec_disable(ops, 1))\n\t\tcommand |= FTRACE_UPDATE_CALLS;\n\n\tops->flags &= ~FTRACE_OPS_FL_ENABLED;\n\n\tif (saved_ftrace_func != ftrace_trace_function) {\n\t\tsaved_ftrace_func = ftrace_trace_function;\n\t\tcommand |= FTRACE_UPDATE_TRACE_FUNC;\n\t}\n\n\tif (!command || !ftrace_enabled) {\n\t\t/*\n\t\t * If these are dynamic or per_cpu ops, they still\n\t\t * need their data freed. Since, function tracing is\n\t\t * not currently active, we can just free them\n\t\t * without synchronizing all CPUs.\n\t\t */\n\t\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC)\n\t\t\tgoto free_ops;\n\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the ops uses a trampoline, then it needs to be\n\t * tested first on update.\n\t */\n\tops->flags |= FTRACE_OPS_FL_REMOVING;\n\tremoved_ops = ops;\n\n\t/* The trampoline logic checks the old hashes */\n\tops->old_hash.filter_hash = ops->func_hash->filter_hash;\n\tops->old_hash.notrace_hash = ops->func_hash->notrace_hash;\n\n\tftrace_run_update_code(command);\n\n\t/*\n\t * If there's no more ops registered with ftrace, run a\n\t * sanity check to make sure all rec flags are cleared.\n\t */\n\tif (rcu_dereference_protected(ftrace_ops_list,\n\t\t\tlockdep_is_held(&ftrace_lock)) == &ftrace_list_end) {\n\t\tstruct ftrace_page *pg;\n\t\tstruct dyn_ftrace *rec;\n\n\t\tdo_for_each_ftrace_rec(pg, rec) {\n\t\t\tif (FTRACE_WARN_ON_ONCE(rec->flags & ~FTRACE_FL_DISABLED))\n\t\t\t\tpr_warn(\"  %pS flags:%lx\\n\",\n\t\t\t\t\t(void *)rec->ip, rec->flags);\n\t\t} while_for_each_ftrace_rec();\n\t}\n\n\tops->old_hash.filter_hash = NULL;\n\tops->old_hash.notrace_hash = NULL;\n\n\tremoved_ops = NULL;\n\tops->flags &= ~FTRACE_OPS_FL_REMOVING;\n\n\t/*\n\t * Dynamic ops may be freed, we must make sure that all\n\t * callers are done before leaving this function.\n\t * The same goes for freeing the per_cpu data of the per_cpu\n\t * ops.\n\t */\n\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC) {\n\t\t/*\n\t\t * We need to do a hard force of sched synchronization.\n\t\t * This is because we use preempt_disable() to do RCU, but\n\t\t * the function tracers can be called where RCU is not watching\n\t\t * (like before user_exit()). We can not rely on the RCU\n\t\t * infrastructure to do the synchronization, thus we must do it\n\t\t * ourselves.\n\t\t */\n\t\tsynchronize_rcu_tasks_rude();\n\n\t\t/*\n\t\t * When the kernel is preemptive, tasks can be preempted\n\t\t * while on a ftrace trampoline. Just scheduling a task on\n\t\t * a CPU is not good enough to flush them. Calling\n\t\t * synchronize_rcu_tasks() will wait for those tasks to\n\t\t * execute and either schedule voluntarily or enter user space.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPTION))\n\t\t\tsynchronize_rcu_tasks();\n\n free_ops:\n\t\tftrace_trampoline_free(ops);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!ftrace_graph_active"
          ],
          "line": 640
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&ftrace_lock"
          ],
          "line": 638
        },
        "resolved": true,
        "details": {
          "function_name": "mutex_lock_io",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "580-586",
          "snippet": "void __sched mutex_lock_io(struct mutex *lock)\n{\n\tint token = io_schedule_prepare();\n\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched mutex_lock_io(struct mutex *lock)\n{\n\tint token = io_schedule_prepare();\n\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nint ftrace_graph_active;\nstatic struct notifier_block ftrace_suspend_notifier = {\n\t.notifier_call = ftrace_suspend_notifier_call,\n};\nstatic struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};\ntrace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;\ntrace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;\nstatic trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;\n\nvoid unregister_ftrace_graph(struct fgraph_ops *gops)\n{\n\tmutex_lock(&ftrace_lock);\n\n\tif (unlikely(!ftrace_graph_active))\n\t\tgoto out;\n\n\tftrace_graph_active--;\n\tftrace_graph_return = ftrace_stub_graph;\n\tftrace_graph_entry = ftrace_graph_entry_stub;\n\t__ftrace_graph_entry = ftrace_graph_entry_stub;\n\tftrace_shutdown(&graph_ops, FTRACE_STOP_FUNC_RET);\n\tunregister_pm_notifier(&ftrace_suspend_notifier);\n\tunregister_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\n out:\n\tmutex_unlock(&ftrace_lock);\n}"
  },
  {
    "function_name": "register_ftrace_graph",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "597-634",
    "snippet": "int register_ftrace_graph(struct fgraph_ops *gops)\n{\n\tint ret = 0;\n\n\tmutex_lock(&ftrace_lock);\n\n\t/* we currently allow only one tracer registered at a time */\n\tif (ftrace_graph_active) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tregister_pm_notifier(&ftrace_suspend_notifier);\n\n\tftrace_graph_active++;\n\tret = start_graph_tracing();\n\tif (ret) {\n\t\tftrace_graph_active--;\n\t\tgoto out;\n\t}\n\n\tftrace_graph_return = gops->retfunc;\n\n\t/*\n\t * Update the indirect function to the entryfunc, and the\n\t * function that gets called to the entry_test first. Then\n\t * call the update fgraph entry function to determine if\n\t * the entryfunc should be called directly or not.\n\t */\n\t__ftrace_graph_entry = gops->entryfunc;\n\tftrace_graph_entry = ftrace_graph_entry_test;\n\tupdate_function_graph_func();\n\n\tret = ftrace_startup(&graph_ops, FTRACE_START_FUNC_RET);\nout:\n\tmutex_unlock(&ftrace_lock);\n\treturn ret;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "int ftrace_graph_active;",
      "static struct notifier_block ftrace_suspend_notifier = {\n\t.notifier_call = ftrace_suspend_notifier_call,\n};",
      "static struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};",
      "trace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;",
      "trace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;",
      "static trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&ftrace_lock"
          ],
          "line": 632
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1350-1356",
          "snippet": "static __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "ftrace_startup",
          "args": [
            "&graph_ops",
            "FTRACE_START_FUNC_RET"
          ],
          "line": 630
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_startup",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/ftrace.c",
          "lines": "2893-2935",
          "snippet": "int ftrace_startup(struct ftrace_ops *ops, int command)\n{\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tret = __register_ftrace_function(ops);\n\tif (ret)\n\t\treturn ret;\n\n\tftrace_start_up++;\n\n\t/*\n\t * Note that ftrace probes uses this to start up\n\t * and modify functions it will probe. But we still\n\t * set the ADDING flag for modification, as probes\n\t * do not have trampolines. If they add them in the\n\t * future, then the probes will need to distinguish\n\t * between adding and updating probes.\n\t */\n\tops->flags |= FTRACE_OPS_FL_ENABLED | FTRACE_OPS_FL_ADDING;\n\n\tret = ftrace_hash_ipmodify_enable(ops);\n\tif (ret < 0) {\n\t\t/* Rollback registration process */\n\t\t__unregister_ftrace_function(ops);\n\t\tftrace_start_up--;\n\t\tops->flags &= ~FTRACE_OPS_FL_ENABLED;\n\t\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC)\n\t\t\tftrace_trampoline_free(ops);\n\t\treturn ret;\n\t}\n\n\tif (ftrace_hash_rec_enable(ops, 1))\n\t\tcommand |= FTRACE_UPDATE_CALLS;\n\n\tftrace_startup_enable(command);\n\n\tops->flags &= ~FTRACE_OPS_FL_ADDING;\n\n\treturn 0;\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include \"ftrace_internal.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/security.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void ftrace_update_trampoline(struct ftrace_ops *ops);",
            "static int ftrace_disabled",
            "static void ftrace_update_trampoline(struct ftrace_ops *ops);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include \"ftrace_internal.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/kprobes.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/seq_file.h>\n#include <linux/security.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\nstatic int ftrace_disabled;\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\n\nint ftrace_startup(struct ftrace_ops *ops, int command)\n{\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tret = __register_ftrace_function(ops);\n\tif (ret)\n\t\treturn ret;\n\n\tftrace_start_up++;\n\n\t/*\n\t * Note that ftrace probes uses this to start up\n\t * and modify functions it will probe. But we still\n\t * set the ADDING flag for modification, as probes\n\t * do not have trampolines. If they add them in the\n\t * future, then the probes will need to distinguish\n\t * between adding and updating probes.\n\t */\n\tops->flags |= FTRACE_OPS_FL_ENABLED | FTRACE_OPS_FL_ADDING;\n\n\tret = ftrace_hash_ipmodify_enable(ops);\n\tif (ret < 0) {\n\t\t/* Rollback registration process */\n\t\t__unregister_ftrace_function(ops);\n\t\tftrace_start_up--;\n\t\tops->flags &= ~FTRACE_OPS_FL_ENABLED;\n\t\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC)\n\t\t\tftrace_trampoline_free(ops);\n\t\treturn ret;\n\t}\n\n\tif (ftrace_hash_rec_enable(ops, 1))\n\t\tcommand |= FTRACE_UPDATE_CALLS;\n\n\tftrace_startup_enable(command);\n\n\tops->flags &= ~FTRACE_OPS_FL_ADDING;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_function_graph_func",
          "args": [],
          "line": 628
        },
        "resolved": true,
        "details": {
          "function_name": "update_function_graph_func",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "462-486",
          "snippet": "void update_function_graph_func(void)\n{\n\tstruct ftrace_ops *op;\n\tbool do_test = false;\n\n\t/*\n\t * The graph and global ops share the same set of functions\n\t * to test. If any other ops is on the list, then\n\t * the graph tracing needs to test if its the function\n\t * it should call.\n\t */\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op != &global_ops && op != &graph_ops &&\n\t\t    op != &ftrace_list_end) {\n\t\t\tdo_test = true;\n\t\t\t/* in double loop, break out with goto */\n\t\t\tgoto out;\n\t\t}\n\t} while_for_each_ftrace_op(op);\n out:\n\tif (do_test)\n\t\tftrace_graph_entry = ftrace_graph_entry_test;\n\telse\n\t\tftrace_graph_entry = __ftrace_graph_entry;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};",
            "trace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;",
            "static trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};\ntrace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;\nstatic trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;\n\nvoid update_function_graph_func(void)\n{\n\tstruct ftrace_ops *op;\n\tbool do_test = false;\n\n\t/*\n\t * The graph and global ops share the same set of functions\n\t * to test. If any other ops is on the list, then\n\t * the graph tracing needs to test if its the function\n\t * it should call.\n\t */\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op != &global_ops && op != &graph_ops &&\n\t\t    op != &ftrace_list_end) {\n\t\t\tdo_test = true;\n\t\t\t/* in double loop, break out with goto */\n\t\t\tgoto out;\n\t\t}\n\t} while_for_each_ftrace_op(op);\n out:\n\tif (do_test)\n\t\tftrace_graph_entry = ftrace_graph_entry_test;\n\telse\n\t\tftrace_graph_entry = __ftrace_graph_entry;\n}"
        }
      },
      {
        "call_info": {
          "callee": "start_graph_tracing",
          "args": [],
          "line": 612
        },
        "resolved": true,
        "details": {
          "function_name": "start_graph_tracing",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "564-595",
          "snippet": "static int start_graph_tracing(void)\n{\n\tstruct ftrace_ret_stack **ret_stack_list;\n\tint ret, cpu;\n\n\tret_stack_list = kmalloc_array(FTRACE_RETSTACK_ALLOC_SIZE,\n\t\t\t\t       sizeof(struct ftrace_ret_stack *),\n\t\t\t\t       GFP_KERNEL);\n\n\tif (!ret_stack_list)\n\t\treturn -ENOMEM;\n\n\t/* The cpu_boot init_task->ret_stack will never be freed */\n\tfor_each_online_cpu(cpu) {\n\t\tif (!idle_task(cpu)->ret_stack)\n\t\t\tftrace_graph_init_idle_task(idle_task(cpu), cpu);\n\t}\n\n\tdo {\n\t\tret = alloc_retstack_tasklist(ret_stack_list);\n\t} while (ret == -EAGAIN);\n\n\tif (!ret) {\n\t\tret = register_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\t\tif (ret)\n\t\t\tpr_info(\"ftrace_graph: Couldn't activate tracepoint\"\n\t\t\t\t\" probe to kernel_sched_switch\\n\");\n\t}\n\n\tkfree(ret_stack_list);\n\treturn ret;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic int start_graph_tracing(void)\n{\n\tstruct ftrace_ret_stack **ret_stack_list;\n\tint ret, cpu;\n\n\tret_stack_list = kmalloc_array(FTRACE_RETSTACK_ALLOC_SIZE,\n\t\t\t\t       sizeof(struct ftrace_ret_stack *),\n\t\t\t\t       GFP_KERNEL);\n\n\tif (!ret_stack_list)\n\t\treturn -ENOMEM;\n\n\t/* The cpu_boot init_task->ret_stack will never be freed */\n\tfor_each_online_cpu(cpu) {\n\t\tif (!idle_task(cpu)->ret_stack)\n\t\t\tftrace_graph_init_idle_task(idle_task(cpu), cpu);\n\t}\n\n\tdo {\n\t\tret = alloc_retstack_tasklist(ret_stack_list);\n\t} while (ret == -EAGAIN);\n\n\tif (!ret) {\n\t\tret = register_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\t\tif (ret)\n\t\t\tpr_info(\"ftrace_graph: Couldn't activate tracepoint\"\n\t\t\t\t\" probe to kernel_sched_switch\\n\");\n\t}\n\n\tkfree(ret_stack_list);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "register_pm_notifier",
          "args": [
            "&ftrace_suspend_notifier"
          ],
          "line": 609
        },
        "resolved": true,
        "details": {
          "function_name": "unregister_pm_notifier",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/power/main.c",
          "lines": "77-80",
          "snippet": "int unregister_pm_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&pm_chain_head, nb);\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <linux/pm_runtime.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/suspend.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/pm-trace.h>",
            "#include <linux/string.h>",
            "#include <linux/kobject.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <linux/pm_runtime.h>\n#include <linux/syscalls.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/debugfs.h>\n#include <linux/workqueue.h>\n#include <linux/pm-trace.h>\n#include <linux/string.h>\n#include <linux/kobject.h>\n#include <linux/export.h>\n\nint unregister_pm_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&pm_chain_head, nb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&ftrace_lock"
          ],
          "line": 601
        },
        "resolved": true,
        "details": {
          "function_name": "mutex_lock_io",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "580-586",
          "snippet": "void __sched mutex_lock_io(struct mutex *lock)\n{\n\tint token = io_schedule_prepare();\n\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched mutex_lock_io(struct mutex *lock)\n{\n\tint token = io_schedule_prepare();\n\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n\tio_schedule_finish(token);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nint ftrace_graph_active;\nstatic struct notifier_block ftrace_suspend_notifier = {\n\t.notifier_call = ftrace_suspend_notifier_call,\n};\nstatic struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};\ntrace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;\ntrace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;\nstatic trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;\n\nint register_ftrace_graph(struct fgraph_ops *gops)\n{\n\tint ret = 0;\n\n\tmutex_lock(&ftrace_lock);\n\n\t/* we currently allow only one tracer registered at a time */\n\tif (ftrace_graph_active) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tregister_pm_notifier(&ftrace_suspend_notifier);\n\n\tftrace_graph_active++;\n\tret = start_graph_tracing();\n\tif (ret) {\n\t\tftrace_graph_active--;\n\t\tgoto out;\n\t}\n\n\tftrace_graph_return = gops->retfunc;\n\n\t/*\n\t * Update the indirect function to the entryfunc, and the\n\t * function that gets called to the entry_test first. Then\n\t * call the update fgraph entry function to determine if\n\t * the entryfunc should be called directly or not.\n\t */\n\t__ftrace_graph_entry = gops->entryfunc;\n\tftrace_graph_entry = ftrace_graph_entry_test;\n\tupdate_function_graph_func();\n\n\tret = ftrace_startup(&graph_ops, FTRACE_START_FUNC_RET);\nout:\n\tmutex_unlock(&ftrace_lock);\n\treturn ret;\n}"
  },
  {
    "function_name": "start_graph_tracing",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "564-595",
    "snippet": "static int start_graph_tracing(void)\n{\n\tstruct ftrace_ret_stack **ret_stack_list;\n\tint ret, cpu;\n\n\tret_stack_list = kmalloc_array(FTRACE_RETSTACK_ALLOC_SIZE,\n\t\t\t\t       sizeof(struct ftrace_ret_stack *),\n\t\t\t\t       GFP_KERNEL);\n\n\tif (!ret_stack_list)\n\t\treturn -ENOMEM;\n\n\t/* The cpu_boot init_task->ret_stack will never be freed */\n\tfor_each_online_cpu(cpu) {\n\t\tif (!idle_task(cpu)->ret_stack)\n\t\t\tftrace_graph_init_idle_task(idle_task(cpu), cpu);\n\t}\n\n\tdo {\n\t\tret = alloc_retstack_tasklist(ret_stack_list);\n\t} while (ret == -EAGAIN);\n\n\tif (!ret) {\n\t\tret = register_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\t\tif (ret)\n\t\t\tpr_info(\"ftrace_graph: Couldn't activate tracepoint\"\n\t\t\t\t\" probe to kernel_sched_switch\\n\");\n\t}\n\n\tkfree(ret_stack_list);\n\treturn ret;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "ret_stack_list"
          ],
          "line": 593
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/params.c",
          "lines": "62-75",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/security.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/security.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "pr_info",
          "args": [
            "\"ftrace_graph: Couldn't activate tracepoint\"\n\t\t\t\t\" probe to kernel_sched_switch\\n\""
          ],
          "line": 589
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "register_trace_sched_switch",
          "args": [
            "ftrace_graph_probe_sched_switch",
            "NULL"
          ],
          "line": 587
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "alloc_retstack_tasklist",
          "args": [
            "ret_stack_list"
          ],
          "line": 583
        },
        "resolved": true,
        "details": {
          "function_name": "alloc_retstack_tasklist",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "371-414",
          "snippet": "static int alloc_retstack_tasklist(struct ftrace_ret_stack **ret_stack_list)\n{\n\tint i;\n\tint ret = 0;\n\tint start = 0, end = FTRACE_RETSTACK_ALLOC_SIZE;\n\tstruct task_struct *g, *t;\n\n\tfor (i = 0; i < FTRACE_RETSTACK_ALLOC_SIZE; i++) {\n\t\tret_stack_list[i] =\n\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!ret_stack_list[i]) {\n\t\t\tstart = 0;\n\t\t\tend = i;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, t) {\n\t\tif (start == end) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (t->ret_stack == NULL) {\n\t\t\tatomic_set(&t->trace_overrun, 0);\n\t\t\tt->curr_ret_stack = -1;\n\t\t\tt->curr_ret_depth = -1;\n\t\t\t/* Make sure the tasks see the -1 first: */\n\t\t\tsmp_wmb();\n\t\t\tt->ret_stack = ret_stack_list[start++];\n\t\t}\n\t}\n\nunlock:\n\trcu_read_unlock();\nfree:\n\tfor (i = start; i < end; i++)\n\t\tkfree(ret_stack_list[i]);\n\treturn ret;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic int alloc_retstack_tasklist(struct ftrace_ret_stack **ret_stack_list)\n{\n\tint i;\n\tint ret = 0;\n\tint start = 0, end = FTRACE_RETSTACK_ALLOC_SIZE;\n\tstruct task_struct *g, *t;\n\n\tfor (i = 0; i < FTRACE_RETSTACK_ALLOC_SIZE; i++) {\n\t\tret_stack_list[i] =\n\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!ret_stack_list[i]) {\n\t\t\tstart = 0;\n\t\t\tend = i;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, t) {\n\t\tif (start == end) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (t->ret_stack == NULL) {\n\t\t\tatomic_set(&t->trace_overrun, 0);\n\t\t\tt->curr_ret_stack = -1;\n\t\t\tt->curr_ret_depth = -1;\n\t\t\t/* Make sure the tasks see the -1 first: */\n\t\t\tsmp_wmb();\n\t\t\tt->ret_stack = ret_stack_list[start++];\n\t\t}\n\t}\n\nunlock:\n\trcu_read_unlock();\nfree:\n\tfor (i = start; i < end; i++)\n\t\tkfree(ret_stack_list[i]);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "ftrace_graph_init_idle_task",
          "args": [
            "idle_task(cpu)",
            "cpu"
          ],
          "line": 579
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_graph_init_idle_task",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "504-530",
          "snippet": "void ftrace_graph_init_idle_task(struct task_struct *t, int cpu)\n{\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\t/*\n\t * The idle task has no parent, it either has its own\n\t * stack or no stack at all.\n\t */\n\tif (t->ret_stack)\n\t\tWARN_ON(t->ret_stack != per_cpu(idle_ret_stack, cpu));\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = per_cpu(idle_ret_stack, cpu);\n\t\tif (!ret_stack) {\n\t\t\tret_stack =\n\t\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!ret_stack)\n\t\t\t\treturn;\n\t\t\tper_cpu(idle_ret_stack, cpu) = ret_stack;\n\t\t}\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "int ftrace_graph_active;",
            "static DEFINE_PER_CPU(struct ftrace_ret_stack *, idle_ret_stack);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nint ftrace_graph_active;\nstatic DEFINE_PER_CPU(struct ftrace_ret_stack *, idle_ret_stack);\n\nvoid ftrace_graph_init_idle_task(struct task_struct *t, int cpu)\n{\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\t/*\n\t * The idle task has no parent, it either has its own\n\t * stack or no stack at all.\n\t */\n\tif (t->ret_stack)\n\t\tWARN_ON(t->ret_stack != per_cpu(idle_ret_stack, cpu));\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = per_cpu(idle_ret_stack, cpu);\n\t\tif (!ret_stack) {\n\t\t\tret_stack =\n\t\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!ret_stack)\n\t\t\t\treturn;\n\t\t\tper_cpu(idle_ret_stack, cpu) = ret_stack;\n\t\t}\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "idle_task",
          "args": [
            "cpu"
          ],
          "line": 579
        },
        "resolved": true,
        "details": {
          "function_name": "idle_task",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "7058-7061",
          "snippet": "struct task_struct *idle_task(int cpu)\n{\n\treturn cpu_rq(cpu)->idle;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic __always_inline struct;\n\nstruct task_struct *idle_task(int cpu)\n{\n\treturn cpu_rq(cpu)->idle;\n}"
        }
      },
      {
        "call_info": {
          "callee": "kmalloc_array",
          "args": [
            "FTRACE_RETSTACK_ALLOC_SIZE",
            "sizeof(struct ftrace_ret_stack *)",
            "GFP_KERNEL"
          ],
          "line": 569
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic int start_graph_tracing(void)\n{\n\tstruct ftrace_ret_stack **ret_stack_list;\n\tint ret, cpu;\n\n\tret_stack_list = kmalloc_array(FTRACE_RETSTACK_ALLOC_SIZE,\n\t\t\t\t       sizeof(struct ftrace_ret_stack *),\n\t\t\t\t       GFP_KERNEL);\n\n\tif (!ret_stack_list)\n\t\treturn -ENOMEM;\n\n\t/* The cpu_boot init_task->ret_stack will never be freed */\n\tfor_each_online_cpu(cpu) {\n\t\tif (!idle_task(cpu)->ret_stack)\n\t\t\tftrace_graph_init_idle_task(idle_task(cpu), cpu);\n\t}\n\n\tdo {\n\t\tret = alloc_retstack_tasklist(ret_stack_list);\n\t} while (ret == -EAGAIN);\n\n\tif (!ret) {\n\t\tret = register_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\t\tif (ret)\n\t\t\tpr_info(\"ftrace_graph: Couldn't activate tracepoint\"\n\t\t\t\t\" probe to kernel_sched_switch\\n\");\n\t}\n\n\tkfree(ret_stack_list);\n\treturn ret;\n}"
  },
  {
    "function_name": "ftrace_graph_exit_task",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "552-561",
    "snippet": "void ftrace_graph_exit_task(struct task_struct *t)\n{\n\tstruct ftrace_ret_stack\t*ret_stack = t->ret_stack;\n\n\tt->ret_stack = NULL;\n\t/* NULL must become visible to IRQs before we free it: */\n\tbarrier();\n\n\tkfree(ret_stack);\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "ret_stack"
          ],
          "line": 560
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/params.c",
          "lines": "62-75",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/security.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/security.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "barrier",
          "args": [],
          "line": 558
        },
        "resolved": true,
        "details": {
          "function_name": "membarrier_register_global_expedited",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/membarrier.c",
          "lines": "482-499",
          "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nvoid ftrace_graph_exit_task(struct task_struct *t)\n{\n\tstruct ftrace_ret_stack\t*ret_stack = t->ret_stack;\n\n\tt->ret_stack = NULL;\n\t/* NULL must become visible to IRQs before we free it: */\n\tbarrier();\n\n\tkfree(ret_stack);\n}"
  },
  {
    "function_name": "ftrace_graph_init_task",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "533-550",
    "snippet": "void ftrace_graph_init_task(struct task_struct *t)\n{\n\t/* Make sure we do not use the parent ret_stack */\n\tt->ret_stack = NULL;\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = kmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t  sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!ret_stack)\n\t\t\treturn;\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "int ftrace_graph_active;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "graph_init_task",
          "args": [
            "t",
            "ret_stack"
          ],
          "line": 548
        },
        "resolved": true,
        "details": {
          "function_name": "graph_init_task",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "490-498",
          "snippet": "static void\ngraph_init_task(struct task_struct *t, struct ftrace_ret_stack *ret_stack)\n{\n\tatomic_set(&t->trace_overrun, 0);\n\tt->ftrace_timestamp = 0;\n\t/* make curr_ret_stack visible before we add the ret_stack */\n\tsmp_wmb();\n\tt->ret_stack = ret_stack;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic void\ngraph_init_task(struct task_struct *t, struct ftrace_ret_stack *ret_stack)\n{\n\tatomic_set(&t->trace_overrun, 0);\n\tt->ftrace_timestamp = 0;\n\t/* make curr_ret_stack visible before we add the ret_stack */\n\tsmp_wmb();\n\tt->ret_stack = ret_stack;\n}"
        }
      },
      {
        "call_info": {
          "callee": "kmalloc_array",
          "args": [
            "FTRACE_RETFUNC_DEPTH",
            "sizeof(struct ftrace_ret_stack)",
            "GFP_KERNEL"
          ],
          "line": 543
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nint ftrace_graph_active;\n\nvoid ftrace_graph_init_task(struct task_struct *t)\n{\n\t/* Make sure we do not use the parent ret_stack */\n\tt->ret_stack = NULL;\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = kmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t  sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!ret_stack)\n\t\t\treturn;\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}"
  },
  {
    "function_name": "ftrace_graph_init_idle_task",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "504-530",
    "snippet": "void ftrace_graph_init_idle_task(struct task_struct *t, int cpu)\n{\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\t/*\n\t * The idle task has no parent, it either has its own\n\t * stack or no stack at all.\n\t */\n\tif (t->ret_stack)\n\t\tWARN_ON(t->ret_stack != per_cpu(idle_ret_stack, cpu));\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = per_cpu(idle_ret_stack, cpu);\n\t\tif (!ret_stack) {\n\t\t\tret_stack =\n\t\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!ret_stack)\n\t\t\t\treturn;\n\t\t\tper_cpu(idle_ret_stack, cpu) = ret_stack;\n\t\t}\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "int ftrace_graph_active;",
      "static DEFINE_PER_CPU(struct ftrace_ret_stack *, idle_ret_stack);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "graph_init_task",
          "args": [
            "t",
            "ret_stack"
          ],
          "line": 528
        },
        "resolved": true,
        "details": {
          "function_name": "graph_init_task",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "490-498",
          "snippet": "static void\ngraph_init_task(struct task_struct *t, struct ftrace_ret_stack *ret_stack)\n{\n\tatomic_set(&t->trace_overrun, 0);\n\tt->ftrace_timestamp = 0;\n\t/* make curr_ret_stack visible before we add the ret_stack */\n\tsmp_wmb();\n\tt->ret_stack = ret_stack;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic void\ngraph_init_task(struct task_struct *t, struct ftrace_ret_stack *ret_stack)\n{\n\tatomic_set(&t->trace_overrun, 0);\n\tt->ftrace_timestamp = 0;\n\t/* make curr_ret_stack visible before we add the ret_stack */\n\tsmp_wmb();\n\tt->ret_stack = ret_stack;\n}"
        }
      },
      {
        "call_info": {
          "callee": "per_cpu",
          "args": [
            "idle_ret_stack",
            "cpu"
          ],
          "line": 526
        },
        "resolved": true,
        "details": {
          "function_name": "kthread_set_per_cpu",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/kthread.c",
          "lines": "588-603",
          "snippet": "void kthread_set_per_cpu(struct task_struct *k, int cpu)\n{\n\tstruct kthread *kthread = to_kthread(k);\n\tif (!kthread)\n\t\treturn;\n\n\tWARN_ON_ONCE(!(k->flags & PF_NO_SETAFFINITY));\n\n\tif (cpu < 0) {\n\t\tclear_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\n\t\treturn;\n\t}\n\n\tkthread->cpu = cpu;\n\tset_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\n}",
          "includes": [
            "#include <trace/events/sched.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/numa.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/freezer.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/export.h>",
            "#include <linux/file.h>",
            "#include <linux/unistd.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/err.h>",
            "#include <linux/completion.h>",
            "#include <linux/kthread.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/mm.h>",
            "#include <uapi/linux/sched/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/sched.h>\n#include <linux/sched/isolation.h>\n#include <linux/numa.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/freezer.h>\n#include <linux/slab.h>\n#include <linux/mutex.h>\n#include <linux/export.h>\n#include <linux/file.h>\n#include <linux/unistd.h>\n#include <linux/cpuset.h>\n#include <linux/cgroup.h>\n#include <linux/err.h>\n#include <linux/completion.h>\n#include <linux/kthread.h>\n#include <linux/sched/task.h>\n#include <linux/sched/mm.h>\n#include <linux/sched.h>\n#include <linux/mmu_context.h>\n#include <linux/mm.h>\n#include <uapi/linux/sched/types.h>\n\nvoid kthread_set_per_cpu(struct task_struct *k, int cpu)\n{\n\tstruct kthread *kthread = to_kthread(k);\n\tif (!kthread)\n\t\treturn;\n\n\tWARN_ON_ONCE(!(k->flags & PF_NO_SETAFFINITY));\n\n\tif (cpu < 0) {\n\t\tclear_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\n\t\treturn;\n\t}\n\n\tkthread->cpu = cpu;\n\tset_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "kmalloc_array",
          "args": [
            "FTRACE_RETFUNC_DEPTH",
            "sizeof(struct ftrace_ret_stack)",
            "GFP_KERNEL"
          ],
          "line": 521
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "t->ret_stack != per_cpu(idle_ret_stack, cpu)"
          ],
          "line": 513
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nint ftrace_graph_active;\nstatic DEFINE_PER_CPU(struct ftrace_ret_stack *, idle_ret_stack);\n\nvoid ftrace_graph_init_idle_task(struct task_struct *t, int cpu)\n{\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\t/*\n\t * The idle task has no parent, it either has its own\n\t * stack or no stack at all.\n\t */\n\tif (t->ret_stack)\n\t\tWARN_ON(t->ret_stack != per_cpu(idle_ret_stack, cpu));\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = per_cpu(idle_ret_stack, cpu);\n\t\tif (!ret_stack) {\n\t\t\tret_stack =\n\t\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!ret_stack)\n\t\t\t\treturn;\n\t\t\tper_cpu(idle_ret_stack, cpu) = ret_stack;\n\t\t}\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}"
  },
  {
    "function_name": "graph_init_task",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "490-498",
    "snippet": "static void\ngraph_init_task(struct task_struct *t, struct ftrace_ret_stack *ret_stack)\n{\n\tatomic_set(&t->trace_overrun, 0);\n\tt->ftrace_timestamp = 0;\n\t/* make curr_ret_stack visible before we add the ret_stack */\n\tsmp_wmb();\n\tt->ret_stack = ret_stack;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_wmb",
          "args": [],
          "line": 496
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_set",
          "args": [
            "&t->trace_overrun",
            "0"
          ],
          "line": 493
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic void\ngraph_init_task(struct task_struct *t, struct ftrace_ret_stack *ret_stack)\n{\n\tatomic_set(&t->trace_overrun, 0);\n\tt->ftrace_timestamp = 0;\n\t/* make curr_ret_stack visible before we add the ret_stack */\n\tsmp_wmb();\n\tt->ret_stack = ret_stack;\n}"
  },
  {
    "function_name": "update_function_graph_func",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "462-486",
    "snippet": "void update_function_graph_func(void)\n{\n\tstruct ftrace_ops *op;\n\tbool do_test = false;\n\n\t/*\n\t * The graph and global ops share the same set of functions\n\t * to test. If any other ops is on the list, then\n\t * the graph tracing needs to test if its the function\n\t * it should call.\n\t */\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op != &global_ops && op != &graph_ops &&\n\t\t    op != &ftrace_list_end) {\n\t\t\tdo_test = true;\n\t\t\t/* in double loop, break out with goto */\n\t\t\tgoto out;\n\t\t}\n\t} while_for_each_ftrace_op(op);\n out:\n\tif (do_test)\n\t\tftrace_graph_entry = ftrace_graph_entry_test;\n\telse\n\t\tftrace_graph_entry = __ftrace_graph_entry;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};",
      "trace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;",
      "static trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "while_for_each_ftrace_op",
          "args": [
            "op"
          ],
          "line": 480
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "do_for_each_ftrace_op",
          "args": [
            "op",
            "ftrace_ops_list"
          ],
          "line": 473
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t/* trampoline_size is only needed for dynamically allocated tramps */\n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};\ntrace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;\nstatic trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;\n\nvoid update_function_graph_func(void)\n{\n\tstruct ftrace_ops *op;\n\tbool do_test = false;\n\n\t/*\n\t * The graph and global ops share the same set of functions\n\t * to test. If any other ops is on the list, then\n\t * the graph tracing needs to test if its the function\n\t * it should call.\n\t */\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op != &global_ops && op != &graph_ops &&\n\t\t    op != &ftrace_list_end) {\n\t\t\tdo_test = true;\n\t\t\t/* in double loop, break out with goto */\n\t\t\tgoto out;\n\t\t}\n\t} while_for_each_ftrace_op(op);\n out:\n\tif (do_test)\n\t\tftrace_graph_entry = ftrace_graph_entry_test;\n\telse\n\t\tftrace_graph_entry = __ftrace_graph_entry;\n}"
  },
  {
    "function_name": "ftrace_graph_entry_test",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "448-453",
    "snippet": "static int ftrace_graph_entry_test(struct ftrace_graph_ent *trace)\n{\n\tif (!ftrace_ops_test(&global_ops, trace->func, NULL))\n\t\treturn 0;\n\treturn __ftrace_graph_entry(trace);\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__ftrace_graph_entry",
          "args": [
            "trace"
          ],
          "line": 452
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ftrace_ops_test",
          "args": [
            "&global_ops",
            "trace->func",
            "NULL"
          ],
          "line": 450
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_ops_test",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/ftrace.c",
          "lines": "1471-1496",
          "snippet": "int\nftrace_ops_test(struct ftrace_ops *ops, unsigned long ip, void *regs)\n{\n\tstruct ftrace_ops_hash hash;\n\tint ret;\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS\n\t/*\n\t * There's a small race when adding ops that the ftrace handler\n\t * that wants regs, may be called without them. We can not\n\t * allow that handler to be called if regs is NULL.\n\t */\n\tif (regs == NULL && (ops->flags & FTRACE_OPS_FL_SAVE_REGS))\n\t\treturn 0;\n#endif\n\n\trcu_assign_pointer(hash.filter_hash, ops->func_hash->filter_hash);\n\trcu_assign_pointer(hash.notrace_hash, ops->func_hash->notrace_hash);\n\n\tif (hash_contains_ip(ip, &hash))\n\t\tret = 1;\n\telse\n\t\tret = 0;\n\n\treturn ret;\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include \"ftrace_internal.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/security.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void ftrace_update_trampoline(struct ftrace_ops *ops);",
            "static void ftrace_update_trampoline(struct ftrace_ops *ops);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include \"ftrace_internal.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/kprobes.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/seq_file.h>\n#include <linux/security.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\n\nint\nftrace_ops_test(struct ftrace_ops *ops, unsigned long ip, void *regs)\n{\n\tstruct ftrace_ops_hash hash;\n\tint ret;\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS\n\t/*\n\t * There's a small race when adding ops that the ftrace handler\n\t * that wants regs, may be called without them. We can not\n\t * allow that handler to be called if regs is NULL.\n\t */\n\tif (regs == NULL && (ops->flags & FTRACE_OPS_FL_SAVE_REGS))\n\t\treturn 0;\n#endif\n\n\trcu_assign_pointer(hash.filter_hash, ops->func_hash->filter_hash);\n\trcu_assign_pointer(hash.notrace_hash, ops->func_hash->notrace_hash);\n\n\tif (hash_contains_ip(ip, &hash))\n\t\tret = 1;\n\telse\n\t\tret = 0;\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;\n\nstatic int ftrace_graph_entry_test(struct ftrace_graph_ent *trace)\n{\n\tif (!ftrace_ops_test(&global_ops, trace->func, NULL))\n\t\treturn 0;\n\treturn __ftrace_graph_entry(trace);\n}"
  },
  {
    "function_name": "ftrace_graph_probe_sched_switch",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "416-446",
    "snippet": "static void\nftrace_graph_probe_sched_switch(void *ignore, bool preempt,\n\t\t\tstruct task_struct *prev, struct task_struct *next)\n{\n\tunsigned long long timestamp;\n\tint index;\n\n\t/*\n\t * Does the user want to count the time a function was asleep.\n\t * If so, do not update the time stamps.\n\t */\n\tif (fgraph_sleep_time)\n\t\treturn;\n\n\ttimestamp = trace_clock_local();\n\n\tprev->ftrace_timestamp = timestamp;\n\n\t/* only process tasks that we timestamped */\n\tif (!next->ftrace_timestamp)\n\t\treturn;\n\n\t/*\n\t * Update all the counters in next to make up for the\n\t * time next was sleeping.\n\t */\n\ttimestamp -= next->ftrace_timestamp;\n\n\tfor (index = next->curr_ret_stack; index >= 0; index--)\n\t\tnext->ret_stack[index].calltime += timestamp;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static bool fgraph_sleep_time = true;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "trace_clock_local",
          "args": [],
          "line": 430
        },
        "resolved": true,
        "details": {
          "function_name": "trace_clock_local",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/trace_clock.c",
          "lines": "32-46",
          "snippet": "u64 notrace trace_clock_local(void)\n{\n\tu64 clock;\n\n\t/*\n\t * sched_clock() is an architecture implemented, fast, scalable,\n\t * lockless clock. It is not guaranteed to be coherent across\n\t * CPUs, nor across CPU idle events.\n\t */\n\tpreempt_disable_notrace();\n\tclock = sched_clock();\n\tpreempt_enable_notrace();\n\n\treturn clock;\n}",
          "includes": [
            "#include <linux/trace_clock.h>",
            "#include <linux/ktime.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>",
            "#include <linux/module.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_clock.h>\n#include <linux/ktime.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n#include <linux/module.h>\n#include <linux/hardirq.h>\n#include <linux/irqflags.h>\n#include <linux/spinlock.h>\n\nu64 notrace trace_clock_local(void)\n{\n\tu64 clock;\n\n\t/*\n\t * sched_clock() is an architecture implemented, fast, scalable,\n\t * lockless clock. It is not guaranteed to be coherent across\n\t * CPUs, nor across CPU idle events.\n\t */\n\tpreempt_disable_notrace();\n\tclock = sched_clock();\n\tpreempt_enable_notrace();\n\n\treturn clock;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic bool fgraph_sleep_time = true;\n\nstatic void\nftrace_graph_probe_sched_switch(void *ignore, bool preempt,\n\t\t\tstruct task_struct *prev, struct task_struct *next)\n{\n\tunsigned long long timestamp;\n\tint index;\n\n\t/*\n\t * Does the user want to count the time a function was asleep.\n\t * If so, do not update the time stamps.\n\t */\n\tif (fgraph_sleep_time)\n\t\treturn;\n\n\ttimestamp = trace_clock_local();\n\n\tprev->ftrace_timestamp = timestamp;\n\n\t/* only process tasks that we timestamped */\n\tif (!next->ftrace_timestamp)\n\t\treturn;\n\n\t/*\n\t * Update all the counters in next to make up for the\n\t * time next was sleeping.\n\t */\n\ttimestamp -= next->ftrace_timestamp;\n\n\tfor (index = next->curr_ret_stack; index >= 0; index--)\n\t\tnext->ret_stack[index].calltime += timestamp;\n}"
  },
  {
    "function_name": "alloc_retstack_tasklist",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "371-414",
    "snippet": "static int alloc_retstack_tasklist(struct ftrace_ret_stack **ret_stack_list)\n{\n\tint i;\n\tint ret = 0;\n\tint start = 0, end = FTRACE_RETSTACK_ALLOC_SIZE;\n\tstruct task_struct *g, *t;\n\n\tfor (i = 0; i < FTRACE_RETSTACK_ALLOC_SIZE; i++) {\n\t\tret_stack_list[i] =\n\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!ret_stack_list[i]) {\n\t\t\tstart = 0;\n\t\t\tend = i;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, t) {\n\t\tif (start == end) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (t->ret_stack == NULL) {\n\t\t\tatomic_set(&t->trace_overrun, 0);\n\t\t\tt->curr_ret_stack = -1;\n\t\t\tt->curr_ret_depth = -1;\n\t\t\t/* Make sure the tasks see the -1 first: */\n\t\t\tsmp_wmb();\n\t\t\tt->ret_stack = ret_stack_list[start++];\n\t\t}\n\t}\n\nunlock:\n\trcu_read_unlock();\nfree:\n\tfor (i = start; i < end; i++)\n\t\tkfree(ret_stack_list[i]);\n\treturn ret;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "ret_stack_list[i]"
          ],
          "line": 412
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/params.c",
          "lines": "62-75",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/security.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/security.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 409
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_unlock_strict",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/rcu/tree_plugin.h",
          "lines": "815-824",
          "snippet": "void rcu_read_unlock_strict(void)\n{\n\tstruct rcu_data *rdp;\n\n\tif (irqs_disabled() || preempt_count() || !rcu_state.gp_kthread)\n\t\treturn;\n\trdp = this_cpu_ptr(&rcu_data);\n\trcu_report_qs_rdp(rdp);\n\tudelay(rcu_unlock_delay);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n\nvoid rcu_read_unlock_strict(void)\n{\n\tstruct rcu_data *rdp;\n\n\tif (irqs_disabled() || preempt_count() || !rcu_state.gp_kthread)\n\t\treturn;\n\trdp = this_cpu_ptr(&rcu_data);\n\trcu_report_qs_rdp(rdp);\n\tudelay(rcu_unlock_delay);\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_wmb",
          "args": [],
          "line": 403
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_set",
          "args": [
            "&t->trace_overrun",
            "0"
          ],
          "line": 399
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_process_thread",
          "args": [
            "g",
            "t"
          ],
          "line": 392
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 391
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_any_held",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/rcu/update.c",
          "lines": "340-351",
          "snippet": "int rcu_read_lock_any_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\tif (lock_is_held(&rcu_lock_map) ||\n\t    lock_is_held(&rcu_bh_lock_map) ||\n\t    lock_is_held(&rcu_sched_lock_map))\n\t\treturn 1;\n\treturn !preemptible();\n}",
          "includes": [
            "#include \"tasks.h\"",
            "#include \"rcu.h\"",
            "#include <linux/rcupdate_trace.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/slab.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tasks.h\"\n#include \"rcu.h\"\n#include <linux/rcupdate_trace.h>\n#include <linux/irq_work.h>\n#include <linux/slab.h>\n#include <linux/kprobes.h>\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_any_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\tif (lock_is_held(&rcu_lock_map) ||\n\t    lock_is_held(&rcu_bh_lock_map) ||\n\t    lock_is_held(&rcu_sched_lock_map))\n\t\treturn 1;\n\treturn !preemptible();\n}"
        }
      },
      {
        "call_info": {
          "callee": "kmalloc_array",
          "args": [
            "FTRACE_RETFUNC_DEPTH",
            "sizeof(struct ftrace_ret_stack)",
            "GFP_KERNEL"
          ],
          "line": 380
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic int alloc_retstack_tasklist(struct ftrace_ret_stack **ret_stack_list)\n{\n\tint i;\n\tint ret = 0;\n\tint start = 0, end = FTRACE_RETSTACK_ALLOC_SIZE;\n\tstruct task_struct *g, *t;\n\n\tfor (i = 0; i < FTRACE_RETSTACK_ALLOC_SIZE; i++) {\n\t\tret_stack_list[i] =\n\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!ret_stack_list[i]) {\n\t\t\tstart = 0;\n\t\t\tend = i;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, t) {\n\t\tif (start == end) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (t->ret_stack == NULL) {\n\t\t\tatomic_set(&t->trace_overrun, 0);\n\t\t\tt->curr_ret_stack = -1;\n\t\t\tt->curr_ret_depth = -1;\n\t\t\t/* Make sure the tasks see the -1 first: */\n\t\t\tsmp_wmb();\n\t\t\tt->ret_stack = ret_stack_list[start++];\n\t\t}\n\t}\n\nunlock:\n\trcu_read_unlock();\nfree:\n\tfor (i = start; i < end; i++)\n\t\tkfree(ret_stack_list[i]);\n\treturn ret;\n}"
  },
  {
    "function_name": "ftrace_graph_entry_stub",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "354-357",
    "snippet": "int ftrace_graph_entry_stub(struct ftrace_graph_ent *trace)\n{\n\treturn 0;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nint ftrace_graph_entry_stub(struct ftrace_graph_ent *trace)\n{\n\treturn 0;\n}"
  },
  {
    "function_name": "ftrace_graph_sleep_time_control",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "349-352",
    "snippet": "void ftrace_graph_sleep_time_control(bool enable)\n{\n\tfgraph_sleep_time = enable;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static bool fgraph_sleep_time = true;"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic bool fgraph_sleep_time = true;\n\nvoid ftrace_graph_sleep_time_control(bool enable)\n{\n\tfgraph_sleep_time = enable;\n}"
  },
  {
    "function_name": "ftrace_graph_ret_addr",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "317-334",
    "snippet": "unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,\n\t\t\t\t    unsigned long ret, unsigned long *retp)\n{\n\tint task_idx;\n\n\tif (ret != (unsigned long)dereference_kernel_function_descriptor(return_to_handler))\n\t\treturn ret;\n\n\ttask_idx = task->curr_ret_stack;\n\n\tif (!task->ret_stack || task_idx < *idx)\n\t\treturn ret;\n\n\ttask_idx -= *idx;\n\t(*idx)++;\n\n\treturn task->ret_stack[task_idx].ret;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "dereference_kernel_function_descriptor",
          "args": [
            "return_to_handler"
          ],
          "line": 322
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nunsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,\n\t\t\t\t    unsigned long ret, unsigned long *retp)\n{\n\tint task_idx;\n\n\tif (ret != (unsigned long)dereference_kernel_function_descriptor(return_to_handler))\n\t\treturn ret;\n\n\ttask_idx = task->curr_ret_stack;\n\n\tif (!task->ret_stack || task_idx < *idx)\n\t\treturn ret;\n\n\ttask_idx -= *idx;\n\t(*idx)++;\n\n\treturn task->ret_stack[task_idx].ret;\n}"
  },
  {
    "function_name": "ftrace_graph_ret_addr",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "298-315",
    "snippet": "unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,\n\t\t\t\t    unsigned long ret, unsigned long *retp)\n{\n\tint index = task->curr_ret_stack;\n\tint i;\n\n\tif (ret != (unsigned long)dereference_kernel_function_descriptor(return_to_handler))\n\t\treturn ret;\n\n\tif (index < 0)\n\t\treturn ret;\n\n\tfor (i = 0; i <= index; i++)\n\t\tif (task->ret_stack[i].retp == retp)\n\t\t\treturn task->ret_stack[i].ret;\n\n\treturn ret;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "dereference_kernel_function_descriptor",
          "args": [
            "return_to_handler"
          ],
          "line": 304
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nunsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,\n\t\t\t\t    unsigned long ret, unsigned long *retp)\n{\n\tint index = task->curr_ret_stack;\n\tint i;\n\n\tif (ret != (unsigned long)dereference_kernel_function_descriptor(return_to_handler))\n\t\treturn ret;\n\n\tif (index < 0)\n\t\treturn ret;\n\n\tfor (i = 0; i <= index; i++)\n\t\tif (task->ret_stack[i].retp == retp)\n\t\t\treturn task->ret_stack[i].ret;\n\n\treturn ret;\n}"
  },
  {
    "function_name": "ftrace_graph_get_ret_stack",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "271-280",
    "snippet": "struct ftrace_ret_stack *\nftrace_graph_get_ret_stack(struct task_struct *task, int idx)\n{\n\tidx = task->curr_ret_stack - idx;\n\n\tif (idx >= 0 && idx <= task->curr_ret_stack)\n\t\treturn &task->ret_stack[idx];\n\n\treturn NULL;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstruct ftrace_ret_stack *\nftrace_graph_get_ret_stack(struct task_struct *task, int idx)\n{\n\tidx = task->curr_ret_stack - idx;\n\n\tif (idx >= 0 && idx <= task->curr_ret_stack)\n\t\treturn &task->ret_stack[idx];\n\n\treturn NULL;\n}"
  },
  {
    "function_name": "ftrace_return_to_handler",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "234-258",
    "snippet": "unsigned long ftrace_return_to_handler(unsigned long frame_pointer)\n{\n\tstruct ftrace_graph_ret trace;\n\tunsigned long ret;\n\n\tftrace_pop_return_trace(&trace, &ret, frame_pointer);\n\ttrace.rettime = trace_clock_local();\n\tftrace_graph_return(&trace);\n\t/*\n\t * The ftrace_graph_return() may still access the current\n\t * ret_stack structure, we need to make sure the update of\n\t * curr_ret_stack is after that.\n\t */\n\tbarrier();\n\tcurrent->curr_ret_stack--;\n\n\tif (unlikely(!ret)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t/* Might as well panic. What else to do? */\n\t\tret = (unsigned long)panic;\n\t}\n\n\treturn ret;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "extern void ftrace_stub_graph(struct ftrace_graph_ret *);",
      "trace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "1"
          ],
          "line": 252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ftrace_graph_stop",
          "args": [],
          "line": 251
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_graph_stop",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "52-55",
          "snippet": "void ftrace_graph_stop(void)\n{\n\tkill_ftrace_graph = true;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static bool kill_ftrace_graph;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic bool kill_ftrace_graph;\n\nvoid ftrace_graph_stop(void)\n{\n\tkill_ftrace_graph = true;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!ret"
          ],
          "line": 250
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "barrier",
          "args": [],
          "line": 247
        },
        "resolved": true,
        "details": {
          "function_name": "membarrier_register_global_expedited",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/membarrier.c",
          "lines": "482-499",
          "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "ftrace_graph_return",
          "args": [
            "&trace"
          ],
          "line": 241
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_clock_local",
          "args": [],
          "line": 240
        },
        "resolved": true,
        "details": {
          "function_name": "trace_clock_local",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/trace_clock.c",
          "lines": "32-46",
          "snippet": "u64 notrace trace_clock_local(void)\n{\n\tu64 clock;\n\n\t/*\n\t * sched_clock() is an architecture implemented, fast, scalable,\n\t * lockless clock. It is not guaranteed to be coherent across\n\t * CPUs, nor across CPU idle events.\n\t */\n\tpreempt_disable_notrace();\n\tclock = sched_clock();\n\tpreempt_enable_notrace();\n\n\treturn clock;\n}",
          "includes": [
            "#include <linux/trace_clock.h>",
            "#include <linux/ktime.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>",
            "#include <linux/module.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_clock.h>\n#include <linux/ktime.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n#include <linux/module.h>\n#include <linux/hardirq.h>\n#include <linux/irqflags.h>\n#include <linux/spinlock.h>\n\nu64 notrace trace_clock_local(void)\n{\n\tu64 clock;\n\n\t/*\n\t * sched_clock() is an architecture implemented, fast, scalable,\n\t * lockless clock. It is not guaranteed to be coherent across\n\t * CPUs, nor across CPU idle events.\n\t */\n\tpreempt_disable_notrace();\n\tclock = sched_clock();\n\tpreempt_enable_notrace();\n\n\treturn clock;\n}"
        }
      },
      {
        "call_info": {
          "callee": "ftrace_pop_return_trace",
          "args": [
            "&trace",
            "&ret",
            "frame_pointer"
          ],
          "line": 239
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_pop_return_trace",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "148-203",
          "snippet": "static void\nftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret,\n\t\t\tunsigned long frame_pointer)\n{\n\tint index;\n\n\tindex = current->curr_ret_stack;\n\n\tif (unlikely(index < 0 || index >= FTRACE_RETFUNC_DEPTH)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t/* Might as well panic, otherwise we have no where to go */\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\t/*\n\t * The arch may choose to record the frame pointer used\n\t * and check it here to make sure that it is what we expect it\n\t * to be. If gcc does not set the place holder of the return\n\t * address in the frame pointer, and does a copy instead, then\n\t * the function graph trace will fail. This test detects this\n\t * case.\n\t *\n\t * Currently, x86_32 with optimize for size (-Os) makes the latest\n\t * gcc do the above.\n\t *\n\t * Note, -mfentry does not use frame pointers, and this test\n\t *  is not needed if CC_USING_FENTRY is set.\n\t */\n\tif (unlikely(current->ret_stack[index].fp != frame_pointer)) {\n\t\tftrace_graph_stop();\n\t\tWARN(1, \"Bad frame pointer: expected %lx, received %lx\\n\"\n\t\t     \"  from func %ps return to %lx\\n\",\n\t\t     current->ret_stack[index].fp,\n\t\t     frame_pointer,\n\t\t     (void *)current->ret_stack[index].func,\n\t\t     current->ret_stack[index].ret);\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n#endif\n\n\t*ret = current->ret_stack[index].ret;\n\ttrace->func = current->ret_stack[index].func;\n\ttrace->calltime = current->ret_stack[index].calltime;\n\ttrace->overrun = atomic_read(&current->trace_overrun);\n\ttrace->depth = current->curr_ret_depth--;\n\t/*\n\t * We still want to trace interrupts coming in if\n\t * max_depth is set to 1. Make sure the decrement is\n\t * seen before ftrace_graph_return.\n\t */\n\tbarrier();\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void ftrace_stub_graph(struct ftrace_graph_ret *);",
            "trace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nextern void ftrace_stub_graph(struct ftrace_graph_ret *);\ntrace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;\n\nstatic void\nftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret,\n\t\t\tunsigned long frame_pointer)\n{\n\tint index;\n\n\tindex = current->curr_ret_stack;\n\n\tif (unlikely(index < 0 || index >= FTRACE_RETFUNC_DEPTH)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t/* Might as well panic, otherwise we have no where to go */\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\t/*\n\t * The arch may choose to record the frame pointer used\n\t * and check it here to make sure that it is what we expect it\n\t * to be. If gcc does not set the place holder of the return\n\t * address in the frame pointer, and does a copy instead, then\n\t * the function graph trace will fail. This test detects this\n\t * case.\n\t *\n\t * Currently, x86_32 with optimize for size (-Os) makes the latest\n\t * gcc do the above.\n\t *\n\t * Note, -mfentry does not use frame pointers, and this test\n\t *  is not needed if CC_USING_FENTRY is set.\n\t */\n\tif (unlikely(current->ret_stack[index].fp != frame_pointer)) {\n\t\tftrace_graph_stop();\n\t\tWARN(1, \"Bad frame pointer: expected %lx, received %lx\\n\"\n\t\t     \"  from func %ps return to %lx\\n\",\n\t\t     current->ret_stack[index].fp,\n\t\t     frame_pointer,\n\t\t     (void *)current->ret_stack[index].func,\n\t\t     current->ret_stack[index].ret);\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n#endif\n\n\t*ret = current->ret_stack[index].ret;\n\ttrace->func = current->ret_stack[index].func;\n\ttrace->calltime = current->ret_stack[index].calltime;\n\ttrace->overrun = atomic_read(&current->trace_overrun);\n\ttrace->depth = current->curr_ret_depth--;\n\t/*\n\t * We still want to trace interrupts coming in if\n\t * max_depth is set to 1. Make sure the decrement is\n\t * seen before ftrace_graph_return.\n\t */\n\tbarrier();\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nextern void ftrace_stub_graph(struct ftrace_graph_ret *);\ntrace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;\n\nunsigned long ftrace_return_to_handler(unsigned long frame_pointer)\n{\n\tstruct ftrace_graph_ret trace;\n\tunsigned long ret;\n\n\tftrace_pop_return_trace(&trace, &ret, frame_pointer);\n\ttrace.rettime = trace_clock_local();\n\tftrace_graph_return(&trace);\n\t/*\n\t * The ftrace_graph_return() may still access the current\n\t * ret_stack structure, we need to make sure the update of\n\t * curr_ret_stack is after that.\n\t */\n\tbarrier();\n\tcurrent->curr_ret_stack--;\n\n\tif (unlikely(!ret)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t/* Might as well panic. What else to do? */\n\t\tret = (unsigned long)panic;\n\t}\n\n\treturn ret;\n}"
  },
  {
    "function_name": "ftrace_suspend_notifier_call",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "210-224",
    "snippet": "static int\nftrace_suspend_notifier_call(struct notifier_block *bl, unsigned long state,\n\t\t\t\t\t\t\tvoid *unused)\n{\n\tswitch (state) {\n\tcase PM_HIBERNATION_PREPARE:\n\t\tpause_graph_tracing();\n\t\tbreak;\n\n\tcase PM_POST_HIBERNATION:\n\t\tunpause_graph_tracing();\n\t\tbreak;\n\t}\n\treturn NOTIFY_DONE;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "unpause_graph_tracing",
          "args": [],
          "line": 220
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pause_graph_tracing",
          "args": [],
          "line": 216
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic int\nftrace_suspend_notifier_call(struct notifier_block *bl, unsigned long state,\n\t\t\t\t\t\t\tvoid *unused)\n{\n\tswitch (state) {\n\tcase PM_HIBERNATION_PREPARE:\n\t\tpause_graph_tracing();\n\t\tbreak;\n\n\tcase PM_POST_HIBERNATION:\n\t\tunpause_graph_tracing();\n\t\tbreak;\n\t}\n\treturn NOTIFY_DONE;\n}"
  },
  {
    "function_name": "ftrace_pop_return_trace",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "148-203",
    "snippet": "static void\nftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret,\n\t\t\tunsigned long frame_pointer)\n{\n\tint index;\n\n\tindex = current->curr_ret_stack;\n\n\tif (unlikely(index < 0 || index >= FTRACE_RETFUNC_DEPTH)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t/* Might as well panic, otherwise we have no where to go */\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\t/*\n\t * The arch may choose to record the frame pointer used\n\t * and check it here to make sure that it is what we expect it\n\t * to be. If gcc does not set the place holder of the return\n\t * address in the frame pointer, and does a copy instead, then\n\t * the function graph trace will fail. This test detects this\n\t * case.\n\t *\n\t * Currently, x86_32 with optimize for size (-Os) makes the latest\n\t * gcc do the above.\n\t *\n\t * Note, -mfentry does not use frame pointers, and this test\n\t *  is not needed if CC_USING_FENTRY is set.\n\t */\n\tif (unlikely(current->ret_stack[index].fp != frame_pointer)) {\n\t\tftrace_graph_stop();\n\t\tWARN(1, \"Bad frame pointer: expected %lx, received %lx\\n\"\n\t\t     \"  from func %ps return to %lx\\n\",\n\t\t     current->ret_stack[index].fp,\n\t\t     frame_pointer,\n\t\t     (void *)current->ret_stack[index].func,\n\t\t     current->ret_stack[index].ret);\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n#endif\n\n\t*ret = current->ret_stack[index].ret;\n\ttrace->func = current->ret_stack[index].func;\n\ttrace->calltime = current->ret_stack[index].calltime;\n\ttrace->overrun = atomic_read(&current->trace_overrun);\n\ttrace->depth = current->curr_ret_depth--;\n\t/*\n\t * We still want to trace interrupts coming in if\n\t * max_depth is set to 1. Make sure the decrement is\n\t * seen before ftrace_graph_return.\n\t */\n\tbarrier();\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "extern void ftrace_stub_graph(struct ftrace_graph_ret *);",
      "trace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "barrier",
          "args": [],
          "line": 202
        },
        "resolved": true,
        "details": {
          "function_name": "membarrier_register_global_expedited",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/membarrier.c",
          "lines": "482-499",
          "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&current->trace_overrun"
          ],
          "line": 195
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN",
          "args": [
            "1",
            "\"Bad frame pointer: expected %lx, received %lx\\n\"\n\t\t     \"  from func %ps return to %lx\\n\"",
            "current->ret_stack[index].fp",
            "frame_pointer",
            "(void *)current->ret_stack[index].func",
            "current->ret_stack[index].ret"
          ],
          "line": 181
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ftrace_graph_stop",
          "args": [],
          "line": 180
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_graph_stop",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "52-55",
          "snippet": "void ftrace_graph_stop(void)\n{\n\tkill_ftrace_graph = true;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static bool kill_ftrace_graph;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic bool kill_ftrace_graph;\n\nvoid ftrace_graph_stop(void)\n{\n\tkill_ftrace_graph = true;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "current->ret_stack[index].fp != frame_pointer"
          ],
          "line": 179
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "1"
          ],
          "line": 158
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "index < 0 || index >= FTRACE_RETFUNC_DEPTH"
          ],
          "line": 156
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nextern void ftrace_stub_graph(struct ftrace_graph_ret *);\ntrace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;\n\nstatic void\nftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret,\n\t\t\tunsigned long frame_pointer)\n{\n\tint index;\n\n\tindex = current->curr_ret_stack;\n\n\tif (unlikely(index < 0 || index >= FTRACE_RETFUNC_DEPTH)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t/* Might as well panic, otherwise we have no where to go */\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\t/*\n\t * The arch may choose to record the frame pointer used\n\t * and check it here to make sure that it is what we expect it\n\t * to be. If gcc does not set the place holder of the return\n\t * address in the frame pointer, and does a copy instead, then\n\t * the function graph trace will fail. This test detects this\n\t * case.\n\t *\n\t * Currently, x86_32 with optimize for size (-Os) makes the latest\n\t * gcc do the above.\n\t *\n\t * Note, -mfentry does not use frame pointers, and this test\n\t *  is not needed if CC_USING_FENTRY is set.\n\t */\n\tif (unlikely(current->ret_stack[index].fp != frame_pointer)) {\n\t\tftrace_graph_stop();\n\t\tWARN(1, \"Bad frame pointer: expected %lx, received %lx\\n\"\n\t\t     \"  from func %ps return to %lx\\n\",\n\t\t     current->ret_stack[index].fp,\n\t\t     frame_pointer,\n\t\t     (void *)current->ret_stack[index].func,\n\t\t     current->ret_stack[index].ret);\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n#endif\n\n\t*ret = current->ret_stack[index].ret;\n\ttrace->func = current->ret_stack[index].func;\n\ttrace->calltime = current->ret_stack[index].calltime;\n\ttrace->overrun = atomic_read(&current->trace_overrun);\n\ttrace->depth = current->curr_ret_depth--;\n\t/*\n\t * We still want to trace interrupts coming in if\n\t * max_depth is set to 1. Make sure the decrement is\n\t * seen before ftrace_graph_return.\n\t */\n\tbarrier();\n}"
  },
  {
    "function_name": "function_graph_enter",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "113-145",
    "snippet": "int function_graph_enter(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tstruct ftrace_graph_ent trace;\n\n#ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_ARGS\n\t/*\n\t * Skip graph tracing if the return location is served by direct trampoline,\n\t * since call sequence and return addresses are unpredictable anyway.\n\t * Ex: BPF trampoline may call original function and may skip frame\n\t * depending on type of BPF programs attached.\n\t */\n\tif (ftrace_direct_func_count &&\n\t    ftrace_find_rec_direct(ret - MCOUNT_INSN_SIZE))\n\t\treturn -EBUSY;\n#endif\n\ttrace.func = func;\n\ttrace.depth = ++current->curr_ret_depth;\n\n\tif (ftrace_push_return_trace(ret, func, frame_pointer, retp))\n\t\tgoto out;\n\n\t/* Only trace if the calling function expects to */\n\tif (!ftrace_graph_entry(&trace))\n\t\tgoto out_ret;\n\n\treturn 0;\n out_ret:\n\tcurrent->curr_ret_stack--;\n out:\n\tcurrent->curr_ret_depth--;\n\treturn -EBUSY;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "trace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "ftrace_graph_entry",
          "args": [
            "&trace"
          ],
          "line": 136
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_graph_entry_test",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "448-453",
          "snippet": "static int ftrace_graph_entry_test(struct ftrace_graph_ent *trace)\n{\n\tif (!ftrace_ops_test(&global_ops, trace->func, NULL))\n\t\treturn 0;\n\treturn __ftrace_graph_entry(trace);\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;\n\nstatic int ftrace_graph_entry_test(struct ftrace_graph_ent *trace)\n{\n\tif (!ftrace_ops_test(&global_ops, trace->func, NULL))\n\t\treturn 0;\n\treturn __ftrace_graph_entry(trace);\n}"
        }
      },
      {
        "call_info": {
          "callee": "ftrace_push_return_trace",
          "args": [
            "ret",
            "func",
            "frame_pointer",
            "retp"
          ],
          "line": 132
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_push_return_trace",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "58-97",
          "snippet": "static int\nftrace_push_return_trace(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tunsigned long long calltime;\n\tint index;\n\n\tif (unlikely(ftrace_graph_is_dead()))\n\t\treturn -EBUSY;\n\n\tif (!current->ret_stack)\n\t\treturn -EBUSY;\n\n\t/*\n\t * We must make sure the ret_stack is tested before we read\n\t * anything else.\n\t */\n\tsmp_rmb();\n\n\t/* The return trace stack is full */\n\tif (current->curr_ret_stack == FTRACE_RETFUNC_DEPTH - 1) {\n\t\tatomic_inc(&current->trace_overrun);\n\t\treturn -EBUSY;\n\t}\n\n\tcalltime = trace_clock_local();\n\n\tindex = ++current->curr_ret_stack;\n\tbarrier();\n\tcurrent->ret_stack[index].ret = ret;\n\tcurrent->ret_stack[index].func = func;\n\tcurrent->ret_stack[index].calltime = calltime;\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\tcurrent->ret_stack[index].fp = frame_pointer;\n#endif\n#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR\n\tcurrent->ret_stack[index].retp = retp;\n#endif\n\treturn 0;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic int\nftrace_push_return_trace(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tunsigned long long calltime;\n\tint index;\n\n\tif (unlikely(ftrace_graph_is_dead()))\n\t\treturn -EBUSY;\n\n\tif (!current->ret_stack)\n\t\treturn -EBUSY;\n\n\t/*\n\t * We must make sure the ret_stack is tested before we read\n\t * anything else.\n\t */\n\tsmp_rmb();\n\n\t/* The return trace stack is full */\n\tif (current->curr_ret_stack == FTRACE_RETFUNC_DEPTH - 1) {\n\t\tatomic_inc(&current->trace_overrun);\n\t\treturn -EBUSY;\n\t}\n\n\tcalltime = trace_clock_local();\n\n\tindex = ++current->curr_ret_stack;\n\tbarrier();\n\tcurrent->ret_stack[index].ret = ret;\n\tcurrent->ret_stack[index].func = func;\n\tcurrent->ret_stack[index].calltime = calltime;\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\tcurrent->ret_stack[index].fp = frame_pointer;\n#endif\n#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR\n\tcurrent->ret_stack[index].retp = retp;\n#endif\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "ftrace_find_rec_direct",
          "args": [
            "ret - MCOUNT_INSN_SIZE"
          ],
          "line": 126
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_find_rec_direct",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/ftrace.c",
          "lines": "2382-2391",
          "snippet": "unsigned long ftrace_find_rec_direct(unsigned long ip)\n{\n\tstruct ftrace_func_entry *entry;\n\n\tentry = __ftrace_lookup_ip(direct_functions, ip);\n\tif (!entry)\n\t\treturn 0;\n\n\treturn entry->direct;\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include \"ftrace_internal.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/security.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include \"ftrace_internal.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/kprobes.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/seq_file.h>\n#include <linux/security.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nunsigned long ftrace_find_rec_direct(unsigned long ip)\n{\n\tstruct ftrace_func_entry *entry;\n\n\tentry = __ftrace_lookup_ip(direct_functions, ip);\n\tif (!entry)\n\t\treturn 0;\n\n\treturn entry->direct;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\ntrace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;\n\nint function_graph_enter(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tstruct ftrace_graph_ent trace;\n\n#ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_ARGS\n\t/*\n\t * Skip graph tracing if the return location is served by direct trampoline,\n\t * since call sequence and return addresses are unpredictable anyway.\n\t * Ex: BPF trampoline may call original function and may skip frame\n\t * depending on type of BPF programs attached.\n\t */\n\tif (ftrace_direct_func_count &&\n\t    ftrace_find_rec_direct(ret - MCOUNT_INSN_SIZE))\n\t\treturn -EBUSY;\n#endif\n\ttrace.func = func;\n\ttrace.depth = ++current->curr_ret_depth;\n\n\tif (ftrace_push_return_trace(ret, func, frame_pointer, retp))\n\t\tgoto out;\n\n\t/* Only trace if the calling function expects to */\n\tif (!ftrace_graph_entry(&trace))\n\t\tgoto out_ret;\n\n\treturn 0;\n out_ret:\n\tcurrent->curr_ret_stack--;\n out:\n\tcurrent->curr_ret_depth--;\n\treturn -EBUSY;\n}"
  },
  {
    "function_name": "ftrace_push_return_trace",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "58-97",
    "snippet": "static int\nftrace_push_return_trace(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tunsigned long long calltime;\n\tint index;\n\n\tif (unlikely(ftrace_graph_is_dead()))\n\t\treturn -EBUSY;\n\n\tif (!current->ret_stack)\n\t\treturn -EBUSY;\n\n\t/*\n\t * We must make sure the ret_stack is tested before we read\n\t * anything else.\n\t */\n\tsmp_rmb();\n\n\t/* The return trace stack is full */\n\tif (current->curr_ret_stack == FTRACE_RETFUNC_DEPTH - 1) {\n\t\tatomic_inc(&current->trace_overrun);\n\t\treturn -EBUSY;\n\t}\n\n\tcalltime = trace_clock_local();\n\n\tindex = ++current->curr_ret_stack;\n\tbarrier();\n\tcurrent->ret_stack[index].ret = ret;\n\tcurrent->ret_stack[index].func = func;\n\tcurrent->ret_stack[index].calltime = calltime;\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\tcurrent->ret_stack[index].fp = frame_pointer;\n#endif\n#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR\n\tcurrent->ret_stack[index].retp = retp;\n#endif\n\treturn 0;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "barrier",
          "args": [],
          "line": 86
        },
        "resolved": true,
        "details": {
          "function_name": "membarrier_register_global_expedited",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/membarrier.c",
          "lines": "482-499",
          "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "trace_clock_local",
          "args": [],
          "line": 83
        },
        "resolved": true,
        "details": {
          "function_name": "trace_clock_local",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/trace_clock.c",
          "lines": "32-46",
          "snippet": "u64 notrace trace_clock_local(void)\n{\n\tu64 clock;\n\n\t/*\n\t * sched_clock() is an architecture implemented, fast, scalable,\n\t * lockless clock. It is not guaranteed to be coherent across\n\t * CPUs, nor across CPU idle events.\n\t */\n\tpreempt_disable_notrace();\n\tclock = sched_clock();\n\tpreempt_enable_notrace();\n\n\treturn clock;\n}",
          "includes": [
            "#include <linux/trace_clock.h>",
            "#include <linux/ktime.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>",
            "#include <linux/module.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_clock.h>\n#include <linux/ktime.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n#include <linux/module.h>\n#include <linux/hardirq.h>\n#include <linux/irqflags.h>\n#include <linux/spinlock.h>\n\nu64 notrace trace_clock_local(void)\n{\n\tu64 clock;\n\n\t/*\n\t * sched_clock() is an architecture implemented, fast, scalable,\n\t * lockless clock. It is not guaranteed to be coherent across\n\t * CPUs, nor across CPU idle events.\n\t */\n\tpreempt_disable_notrace();\n\tclock = sched_clock();\n\tpreempt_enable_notrace();\n\n\treturn clock;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_inc",
          "args": [
            "&current->trace_overrun"
          ],
          "line": 79
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 75
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ftrace_graph_is_dead()"
          ],
          "line": 65
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ftrace_graph_is_dead",
          "args": [],
          "line": 65
        },
        "resolved": true,
        "details": {
          "function_name": "ftrace_graph_is_dead",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
          "lines": "39-42",
          "snippet": "bool ftrace_graph_is_dead(void)\n{\n\treturn kill_ftrace_graph;\n}",
          "includes": [
            "#include \"ftrace_internal.h\"",
            "#include <trace/events/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/suspend.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static bool kill_ftrace_graph;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic bool kill_ftrace_graph;\n\nbool ftrace_graph_is_dead(void)\n{\n\treturn kill_ftrace_graph;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic int\nftrace_push_return_trace(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tunsigned long long calltime;\n\tint index;\n\n\tif (unlikely(ftrace_graph_is_dead()))\n\t\treturn -EBUSY;\n\n\tif (!current->ret_stack)\n\t\treturn -EBUSY;\n\n\t/*\n\t * We must make sure the ret_stack is tested before we read\n\t * anything else.\n\t */\n\tsmp_rmb();\n\n\t/* The return trace stack is full */\n\tif (current->curr_ret_stack == FTRACE_RETFUNC_DEPTH - 1) {\n\t\tatomic_inc(&current->trace_overrun);\n\t\treturn -EBUSY;\n\t}\n\n\tcalltime = trace_clock_local();\n\n\tindex = ++current->curr_ret_stack;\n\tbarrier();\n\tcurrent->ret_stack[index].ret = ret;\n\tcurrent->ret_stack[index].func = func;\n\tcurrent->ret_stack[index].calltime = calltime;\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\tcurrent->ret_stack[index].fp = frame_pointer;\n#endif\n#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR\n\tcurrent->ret_stack[index].retp = retp;\n#endif\n\treturn 0;\n}"
  },
  {
    "function_name": "ftrace_graph_stop",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "52-55",
    "snippet": "void ftrace_graph_stop(void)\n{\n\tkill_ftrace_graph = true;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static bool kill_ftrace_graph;"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic bool kill_ftrace_graph;\n\nvoid ftrace_graph_stop(void)\n{\n\tkill_ftrace_graph = true;\n}"
  },
  {
    "function_name": "ftrace_graph_is_dead",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/trace/fgraph.c",
    "lines": "39-42",
    "snippet": "bool ftrace_graph_is_dead(void)\n{\n\treturn kill_ftrace_graph;\n}",
    "includes": [
      "#include \"ftrace_internal.h\"",
      "#include <trace/events/sched.h>",
      "#include <linux/slab.h>",
      "#include <linux/ftrace.h>",
      "#include <linux/suspend.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static bool kill_ftrace_graph;"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"ftrace_internal.h\"\n#include <trace/events/sched.h>\n#include <linux/slab.h>\n#include <linux/ftrace.h>\n#include <linux/suspend.h>\n\nstatic bool kill_ftrace_graph;\n\nbool ftrace_graph_is_dead(void)\n{\n\treturn kill_ftrace_graph;\n}"
  }
]