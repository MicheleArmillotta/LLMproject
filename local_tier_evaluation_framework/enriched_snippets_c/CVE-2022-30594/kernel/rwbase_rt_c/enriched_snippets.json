[
  {
    "function_name": "rwbase_write_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "274-291",
    "snippet": "static inline int rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\tif (!rwbase_rtmutex_trylock(rtm))\n\t\treturn 0;\n\n\tatomic_sub(READER_BIAS, &rwb->readers);\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\tif (__rwbase_write_trylock(rwb)) {\n\t\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\t\treturn 1;\n\t}\n\t__rwbase_write_unlock(rwb, 0, flags);\n\treturn 0;\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rwbase_write_unlock",
          "args": [
            "rwb",
            "0",
            "flags"
          ],
          "line": 289
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_write_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "182-194",
          "snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&rtm->wait_lock",
            "flags"
          ],
          "line": 286
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rwbase_write_trylock",
          "args": [
            "rwb"
          ],
          "line": 285
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_write_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "215-230",
          "snippet": "static inline bool __rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\t/* Can do without CAS because we're serialized by wait_lock. */\n\tlockdep_assert_held(&rwb->rtmutex.wait_lock);\n\n\t/*\n\t * _acquire is needed in case the reader is in the fast path, pairing\n\t * with rwbase_read_unlock(), provides ACQUIRE.\n\t */\n\tif (!atomic_read_acquire(&rwb->readers)) {\n\t\tatomic_set(&rwb->readers, WRITER_BIAS);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline bool __rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\t/* Can do without CAS because we're serialized by wait_lock. */\n\tlockdep_assert_held(&rwb->rtmutex.wait_lock);\n\n\t/*\n\t * _acquire is needed in case the reader is in the fast path, pairing\n\t * with rwbase_read_unlock(), provides ACQUIRE.\n\t */\n\tif (!atomic_read_acquire(&rwb->readers)) {\n\t\tatomic_set(&rwb->readers, WRITER_BIAS);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&rtm->wait_lock",
            "flags"
          ],
          "line": 284
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_sub",
          "args": [
            "READER_BIAS",
            "&rwb->readers"
          ],
          "line": 282
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwbase_rtmutex_trylock",
          "args": [
            "rtm"
          ],
          "line": 279
        },
        "resolved": true,
        "details": {
          "function_name": "rwbase_rtmutex_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock_rt.c",
          "lines": "177-183",
          "snippet": "static __always_inline int  rwbase_rtmutex_trylock(struct rt_mutex_base *rtm)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(rtm, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(rtm);\n}",
          "includes": [
            "#include \"rwbase_rt.c\"",
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rwbase_rt.c\"\n#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int  rwbase_rtmutex_trylock(struct rt_mutex_base *rtm)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(rtm, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(rtm);\n}"
        }
      }
    ],
    "contextual_snippet": "static inline int rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\tif (!rwbase_rtmutex_trylock(rtm))\n\t\treturn 0;\n\n\tatomic_sub(READER_BIAS, &rwb->readers);\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\tif (__rwbase_write_trylock(rwb)) {\n\t\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\t\treturn 1;\n\t}\n\t__rwbase_write_unlock(rwb, 0, flags);\n\treturn 0;\n}"
  },
  {
    "function_name": "rwbase_write_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "232-272",
    "snippet": "static int __sched rwbase_write_lock(struct rwbase_rt *rwb,\n\t\t\t\t     unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\t/* Take the rtmutex as a first step */\n\tif (rwbase_rtmutex_lock_state(rtm, state))\n\t\treturn -EINTR;\n\n\t/* Force readers into slow path */\n\tatomic_sub(READER_BIAS, &rwb->readers);\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\tif (__rwbase_write_trylock(rwb))\n\t\tgoto out_unlock;\n\n\trwbase_set_and_save_current_state(state);\n\tfor (;;) {\n\t\t/* Optimized out for rwlocks */\n\t\tif (rwbase_signal_pending_state(state, current)) {\n\t\t\trwbase_restore_current_state();\n\t\t\t__rwbase_write_unlock(rwb, 0, flags);\n\t\t\treturn -EINTR;\n\t\t}\n\n\t\tif (__rwbase_write_trylock(rwb))\n\t\t\tbreak;\n\n\t\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\t\trwbase_schedule();\n\t\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\n\t\tset_current_state(state);\n\t}\n\trwbase_restore_current_state();\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\treturn 0;\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&rtm->wait_lock",
            "flags"
          ],
          "line": 270
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rwbase_restore_current_state",
          "args": [],
          "line": 267
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 265
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&rtm->wait_lock",
            "flags"
          ],
          "line": 263
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rwbase_schedule",
          "args": [],
          "line": 262
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rwbase_write_trylock",
          "args": [
            "rwb"
          ],
          "line": 258
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_write_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "215-230",
          "snippet": "static inline bool __rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\t/* Can do without CAS because we're serialized by wait_lock. */\n\tlockdep_assert_held(&rwb->rtmutex.wait_lock);\n\n\t/*\n\t * _acquire is needed in case the reader is in the fast path, pairing\n\t * with rwbase_read_unlock(), provides ACQUIRE.\n\t */\n\tif (!atomic_read_acquire(&rwb->readers)) {\n\t\tatomic_set(&rwb->readers, WRITER_BIAS);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline bool __rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\t/* Can do without CAS because we're serialized by wait_lock. */\n\tlockdep_assert_held(&rwb->rtmutex.wait_lock);\n\n\t/*\n\t * _acquire is needed in case the reader is in the fast path, pairing\n\t * with rwbase_read_unlock(), provides ACQUIRE.\n\t */\n\tif (!atomic_read_acquire(&rwb->readers)) {\n\t\tatomic_set(&rwb->readers, WRITER_BIAS);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rwbase_write_unlock",
          "args": [
            "rwb",
            "0",
            "flags"
          ],
          "line": 254
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_write_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "182-194",
          "snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rwbase_restore_current_state",
          "args": [],
          "line": 253
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwbase_signal_pending_state",
          "args": [
            "state",
            "current"
          ],
          "line": 252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwbase_set_and_save_current_state",
          "args": [
            "state"
          ],
          "line": 249
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_sub",
          "args": [
            "READER_BIAS",
            "&rwb->readers"
          ],
          "line": 243
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwbase_rtmutex_lock_state",
          "args": [
            "rtm",
            "state"
          ],
          "line": 239
        },
        "resolved": true,
        "details": {
          "function_name": "rwbase_rtmutex_lock_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock_rt.c",
          "lines": "154-160",
          "snippet": "static __always_inline int\nrwbase_rtmutex_lock_state(struct rt_mutex_base *rtm, unsigned int state)\n{\n\tif (unlikely(!rt_mutex_cmpxchg_acquire(rtm, NULL, current)))\n\t\trtlock_slowlock(rtm);\n\treturn 0;\n}",
          "includes": [
            "#include \"rwbase_rt.c\"",
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rwbase_rt.c\"\n#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int\nrwbase_rtmutex_lock_state(struct rt_mutex_base *rtm, unsigned int state)\n{\n\tif (unlikely(!rt_mutex_cmpxchg_acquire(rtm, NULL, current)))\n\t\trtlock_slowlock(rtm);\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "static int __sched rwbase_write_lock(struct rwbase_rt *rwb,\n\t\t\t\t     unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\t/* Take the rtmutex as a first step */\n\tif (rwbase_rtmutex_lock_state(rtm, state))\n\t\treturn -EINTR;\n\n\t/* Force readers into slow path */\n\tatomic_sub(READER_BIAS, &rwb->readers);\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\tif (__rwbase_write_trylock(rwb))\n\t\tgoto out_unlock;\n\n\trwbase_set_and_save_current_state(state);\n\tfor (;;) {\n\t\t/* Optimized out for rwlocks */\n\t\tif (rwbase_signal_pending_state(state, current)) {\n\t\t\trwbase_restore_current_state();\n\t\t\t__rwbase_write_unlock(rwb, 0, flags);\n\t\t\treturn -EINTR;\n\t\t}\n\n\t\tif (__rwbase_write_trylock(rwb))\n\t\t\tbreak;\n\n\t\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\t\trwbase_schedule();\n\t\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\n\t\tset_current_state(state);\n\t}\n\trwbase_restore_current_state();\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\treturn 0;\n}"
  },
  {
    "function_name": "__rwbase_write_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "215-230",
    "snippet": "static inline bool __rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\t/* Can do without CAS because we're serialized by wait_lock. */\n\tlockdep_assert_held(&rwb->rtmutex.wait_lock);\n\n\t/*\n\t * _acquire is needed in case the reader is in the fast path, pairing\n\t * with rwbase_read_unlock(), provides ACQUIRE.\n\t */\n\tif (!atomic_read_acquire(&rwb->readers)) {\n\t\tatomic_set(&rwb->readers, WRITER_BIAS);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_set",
          "args": [
            "&rwb->readers",
            "WRITER_BIAS"
          ],
          "line": 225
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read_acquire",
          "args": [
            "&rwb->readers"
          ],
          "line": 224
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&rwb->rtmutex.wait_lock"
          ],
          "line": 218
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "static inline bool __rwbase_write_trylock(struct rwbase_rt *rwb)\n{\n\t/* Can do without CAS because we're serialized by wait_lock. */\n\tlockdep_assert_held(&rwb->rtmutex.wait_lock);\n\n\t/*\n\t * _acquire is needed in case the reader is in the fast path, pairing\n\t * with rwbase_read_unlock(), provides ACQUIRE.\n\t */\n\tif (!atomic_read_acquire(&rwb->readers)) {\n\t\tatomic_set(&rwb->readers, WRITER_BIAS);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
  },
  {
    "function_name": "rwbase_write_downgrade",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "205-213",
    "snippet": "static inline void rwbase_write_downgrade(struct rwbase_rt *rwb)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\t/* Release it and account current as reader */\n\t__rwbase_write_unlock(rwb, WRITER_BIAS - 1, flags);\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rwbase_write_unlock",
          "args": [
            "rwb",
            "WRITER_BIAS - 1",
            "flags"
          ],
          "line": 212
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_write_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "182-194",
          "snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&rtm->wait_lock",
            "flags"
          ],
          "line": 210
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      }
    ],
    "contextual_snippet": "static inline void rwbase_write_downgrade(struct rwbase_rt *rwb)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\t/* Release it and account current as reader */\n\t__rwbase_write_unlock(rwb, WRITER_BIAS - 1, flags);\n}"
  },
  {
    "function_name": "rwbase_write_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "196-203",
    "snippet": "static inline void rwbase_write_unlock(struct rwbase_rt *rwb)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\t__rwbase_write_unlock(rwb, WRITER_BIAS, flags);\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rwbase_write_unlock",
          "args": [
            "rwb",
            "WRITER_BIAS",
            "flags"
          ],
          "line": 202
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_write_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "182-194",
          "snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&rtm->wait_lock",
            "flags"
          ],
          "line": 201
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      }
    ],
    "contextual_snippet": "static inline void rwbase_write_unlock(struct rwbase_rt *rwb)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rtm->wait_lock, flags);\n\t__rwbase_write_unlock(rwb, WRITER_BIAS, flags);\n}"
  },
  {
    "function_name": "__rwbase_write_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "182-194",
    "snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rwbase_rtmutex_unlock",
          "args": [
            "rtm"
          ],
          "line": 193
        },
        "resolved": true,
        "details": {
          "function_name": "rwbase_rtmutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock_rt.c",
          "lines": "169-175",
          "snippet": "static __always_inline void rwbase_rtmutex_unlock(struct rt_mutex_base *rtm)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(rtm, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(rtm);\n}",
          "includes": [
            "#include \"rwbase_rt.c\"",
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rwbase_rt.c\"\n#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline void rwbase_rtmutex_unlock(struct rt_mutex_base *rtm)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(rtm, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(rtm);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&rtm->wait_lock",
            "flags"
          ],
          "line": 192
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_add_return_release",
          "args": [
            "READER_BIAS - bias",
            "&rwb->readers"
          ],
          "line": 191
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "static inline void __rwbase_write_unlock(struct rwbase_rt *rwb, int bias,\n\t\t\t\t\t unsigned long flags)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\n\t/*\n\t * _release() is needed in case that reader is in fast path, pairing\n\t * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().\n\t */\n\t(void)atomic_add_return_release(READER_BIAS - bias, &rwb->readers);\n\traw_spin_unlock_irqrestore(&rtm->wait_lock, flags);\n\trwbase_rtmutex_unlock(rtm);\n}"
  },
  {
    "function_name": "rwbase_read_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "169-180",
    "snippet": "static __always_inline void rwbase_read_unlock(struct rwbase_rt *rwb,\n\t\t\t\t\t       unsigned int state)\n{\n\t/*\n\t * rwb->readers can only hit 0 when a writer is waiting for the\n\t * active readers to leave the critical section.\n\t *\n\t * dec_and_test() is fully ordered, provides RELEASE.\n\t */\n\tif (unlikely(atomic_dec_and_test(&rwb->readers)))\n\t\t__rwbase_read_unlock(rwb, state);\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rwbase_read_unlock",
          "args": [
            "rwb",
            "state"
          ],
          "line": 179
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_read_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "145-167",
          "snippet": "static void __sched __rwbase_read_unlock(struct rwbase_rt *rwb,\n\t\t\t\t\t unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tstruct task_struct *owner;\n\tDEFINE_RT_WAKE_Q(wqh);\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Wake the writer, i.e. the rtmutex owner. It might release the\n\t * rtmutex concurrently in the fast path (due to a signal), but to\n\t * clean up rwb->readers it needs to acquire rtm->wait_lock. The\n\t * worst case which can happen is a spurious wakeup.\n\t */\n\towner = rt_mutex_owner(rtm);\n\tif (owner)\n\t\trt_mutex_wake_q_add_task(&wqh, owner, state);\n\n\t/* Pairs with the preempt_enable in rt_mutex_wake_up_q() */\n\tpreempt_disable();\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\trt_mutex_wake_up_q(&wqh);\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static void __sched __rwbase_read_unlock(struct rwbase_rt *rwb,\n\t\t\t\t\t unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tstruct task_struct *owner;\n\tDEFINE_RT_WAKE_Q(wqh);\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Wake the writer, i.e. the rtmutex owner. It might release the\n\t * rtmutex concurrently in the fast path (due to a signal), but to\n\t * clean up rwb->readers it needs to acquire rtm->wait_lock. The\n\t * worst case which can happen is a spurious wakeup.\n\t */\n\towner = rt_mutex_owner(rtm);\n\tif (owner)\n\t\trt_mutex_wake_q_add_task(&wqh, owner, state);\n\n\t/* Pairs with the preempt_enable in rt_mutex_wake_up_q() */\n\tpreempt_disable();\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\trt_mutex_wake_up_q(&wqh);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "atomic_dec_and_test(&rwb->readers)"
          ],
          "line": 178
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_dec_and_test",
          "args": [
            "&rwb->readers"
          ],
          "line": 178
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "static __always_inline void rwbase_read_unlock(struct rwbase_rt *rwb,\n\t\t\t\t\t       unsigned int state)\n{\n\t/*\n\t * rwb->readers can only hit 0 when a writer is waiting for the\n\t * active readers to leave the critical section.\n\t *\n\t * dec_and_test() is fully ordered, provides RELEASE.\n\t */\n\tif (unlikely(atomic_dec_and_test(&rwb->readers)))\n\t\t__rwbase_read_unlock(rwb, state);\n}"
  },
  {
    "function_name": "__rwbase_read_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "145-167",
    "snippet": "static void __sched __rwbase_read_unlock(struct rwbase_rt *rwb,\n\t\t\t\t\t unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tstruct task_struct *owner;\n\tDEFINE_RT_WAKE_Q(wqh);\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Wake the writer, i.e. the rtmutex owner. It might release the\n\t * rtmutex concurrently in the fast path (due to a signal), but to\n\t * clean up rwb->readers it needs to acquire rtm->wait_lock. The\n\t * worst case which can happen is a spurious wakeup.\n\t */\n\towner = rt_mutex_owner(rtm);\n\tif (owner)\n\t\trt_mutex_wake_q_add_task(&wqh, owner, state);\n\n\t/* Pairs with the preempt_enable in rt_mutex_wake_up_q() */\n\tpreempt_disable();\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\trt_mutex_wake_up_q(&wqh);\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_wake_up_q",
          "args": [
            "&wqh"
          ],
          "line": 166
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_up_q",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "469-482",
          "snippet": "static __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&rtm->wait_lock"
          ],
          "line": 165
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "preempt_disable",
          "args": [],
          "line": 164
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_preempt_disabled",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "6425-6430",
          "snippet": "void __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic void __sched;\nstatic void __sched;\n\nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_wake_q_add_task",
          "args": [
            "&wqh",
            "owner",
            "state"
          ],
          "line": 161
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_q_add_task",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "449-461",
          "snippet": "static __always_inline void rt_mutex_wake_q_add_task(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\t     struct task_struct *task,\n\t\t\t\t\t\t     unsigned int wake_state)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wake_state == TASK_RTLOCK_WAIT) {\n\t\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\t\tWARN_ON_ONCE(wqh->rtlock_task);\n\t\tget_task_struct(task);\n\t\twqh->rtlock_task = task;\n\t} else {\n\t\twake_q_add(&wqh->head, task);\n\t}\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_q_add_task(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\t     struct task_struct *task,\n\t\t\t\t\t\t     unsigned int wake_state)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wake_state == TASK_RTLOCK_WAIT) {\n\t\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\t\tWARN_ON_ONCE(wqh->rtlock_task);\n\t\tget_task_struct(task);\n\t\twqh->rtlock_task = task;\n\t} else {\n\t\twake_q_add(&wqh->head, task);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "rtm"
          ],
          "line": 159
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&rtm->wait_lock"
          ],
          "line": 152
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_RT_WAKE_Q",
          "args": [
            "wqh"
          ],
          "line": 150
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "static void __sched __rwbase_read_unlock(struct rwbase_rt *rwb,\n\t\t\t\t\t unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tstruct task_struct *owner;\n\tDEFINE_RT_WAKE_Q(wqh);\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Wake the writer, i.e. the rtmutex owner. It might release the\n\t * rtmutex concurrently in the fast path (due to a signal), but to\n\t * clean up rwb->readers it needs to acquire rtm->wait_lock. The\n\t * worst case which can happen is a spurious wakeup.\n\t */\n\towner = rt_mutex_owner(rtm);\n\tif (owner)\n\t\trt_mutex_wake_q_add_task(&wqh, owner, state);\n\n\t/* Pairs with the preempt_enable in rt_mutex_wake_up_q() */\n\tpreempt_disable();\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\trt_mutex_wake_up_q(&wqh);\n}"
  },
  {
    "function_name": "rwbase_read_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "136-143",
    "snippet": "static __always_inline int rwbase_read_lock(struct rwbase_rt *rwb,\n\t\t\t\t\t    unsigned int state)\n{\n\tif (rwbase_read_trylock(rwb))\n\t\treturn 0;\n\n\treturn __rwbase_read_lock(rwb, state);\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rwbase_read_lock",
          "args": [
            "rwb",
            "state"
          ],
          "line": 142
        },
        "resolved": true,
        "details": {
          "function_name": "__rwbase_read_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "68-134",
          "snippet": "static int __sched __rwbase_read_lock(struct rwbase_rt *rwb,\n\t\t\t\t      unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tint ret;\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Allow readers, as long as the writer has not completely\n\t * acquired the semaphore for write.\n\t */\n\tif (atomic_read(&rwb->readers) != WRITER_BIAS) {\n\t\tatomic_inc(&rwb->readers);\n\t\traw_spin_unlock_irq(&rtm->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Call into the slow lock path with the rtmutex->wait_lock\n\t * held, so this can't result in the following race:\n\t *\n\t * Reader1\t\tReader2\t\tWriter\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * down_read()\n\t * unlock(m->wait_lock)\n\t *\t\t\tup_read()\n\t *\t\t\twake(Writer)\n\t *\t\t\t\t\tlock(m->wait_lock)\n\t *\t\t\t\t\tsem->writelocked=true\n\t *\t\t\t\t\tunlock(m->wait_lock)\n\t *\n\t *\t\t\t\t\tup_write()\n\t *\t\t\t\t\tsem->writelocked=false\n\t *\t\t\t\t\trtmutex_unlock(m)\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * rtmutex_lock(m)\n\t *\n\t * That would put Reader1 behind the writer waiting on\n\t * Reader2 to call up_read(), which might be unbound.\n\t */\n\n\t/*\n\t * For rwlocks this returns 0 unconditionally, so the below\n\t * !ret conditionals are optimized out.\n\t */\n\tret = rwbase_rtmutex_slowlock_locked(rtm, state);\n\n\t/*\n\t * On success the rtmutex is held, so there can't be a writer\n\t * active. Increment the reader count and immediately drop the\n\t * rtmutex again.\n\t *\n\t * rtmutex->wait_lock has to be unlocked in any case of course.\n\t */\n\tif (!ret)\n\t\tatomic_inc(&rwb->readers);\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\tif (!ret)\n\t\trwbase_rtmutex_unlock(rtm);\n\treturn ret;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static int __sched __rwbase_read_lock(struct rwbase_rt *rwb,\n\t\t\t\t      unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tint ret;\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Allow readers, as long as the writer has not completely\n\t * acquired the semaphore for write.\n\t */\n\tif (atomic_read(&rwb->readers) != WRITER_BIAS) {\n\t\tatomic_inc(&rwb->readers);\n\t\traw_spin_unlock_irq(&rtm->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Call into the slow lock path with the rtmutex->wait_lock\n\t * held, so this can't result in the following race:\n\t *\n\t * Reader1\t\tReader2\t\tWriter\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * down_read()\n\t * unlock(m->wait_lock)\n\t *\t\t\tup_read()\n\t *\t\t\twake(Writer)\n\t *\t\t\t\t\tlock(m->wait_lock)\n\t *\t\t\t\t\tsem->writelocked=true\n\t *\t\t\t\t\tunlock(m->wait_lock)\n\t *\n\t *\t\t\t\t\tup_write()\n\t *\t\t\t\t\tsem->writelocked=false\n\t *\t\t\t\t\trtmutex_unlock(m)\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * rtmutex_lock(m)\n\t *\n\t * That would put Reader1 behind the writer waiting on\n\t * Reader2 to call up_read(), which might be unbound.\n\t */\n\n\t/*\n\t * For rwlocks this returns 0 unconditionally, so the below\n\t * !ret conditionals are optimized out.\n\t */\n\tret = rwbase_rtmutex_slowlock_locked(rtm, state);\n\n\t/*\n\t * On success the rtmutex is held, so there can't be a writer\n\t * active. Increment the reader count and immediately drop the\n\t * rtmutex again.\n\t *\n\t * rtmutex->wait_lock has to be unlocked in any case of course.\n\t */\n\tif (!ret)\n\t\tatomic_inc(&rwb->readers);\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\tif (!ret)\n\t\trwbase_rtmutex_unlock(rtm);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rwbase_read_trylock",
          "args": [
            "rwb"
          ],
          "line": 139
        },
        "resolved": true,
        "details": {
          "function_name": "rwbase_read_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
          "lines": "53-66",
          "snippet": "static __always_inline int rwbase_read_trylock(struct rwbase_rt *rwb)\n{\n\tint r;\n\n\t/*\n\t * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is\n\t * set.\n\t */\n\tfor (r = atomic_read(&rwb->readers); r < 0;) {\n\t\tif (likely(atomic_try_cmpxchg_acquire(&rwb->readers, &r, r + 1)))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static __always_inline int rwbase_read_trylock(struct rwbase_rt *rwb)\n{\n\tint r;\n\n\t/*\n\t * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is\n\t * set.\n\t */\n\tfor (r = atomic_read(&rwb->readers); r < 0;) {\n\t\tif (likely(atomic_try_cmpxchg_acquire(&rwb->readers, &r, r + 1)))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "static __always_inline int rwbase_read_lock(struct rwbase_rt *rwb,\n\t\t\t\t\t    unsigned int state)\n{\n\tif (rwbase_read_trylock(rwb))\n\t\treturn 0;\n\n\treturn __rwbase_read_lock(rwb, state);\n}"
  },
  {
    "function_name": "__rwbase_read_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "68-134",
    "snippet": "static int __sched __rwbase_read_lock(struct rwbase_rt *rwb,\n\t\t\t\t      unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tint ret;\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Allow readers, as long as the writer has not completely\n\t * acquired the semaphore for write.\n\t */\n\tif (atomic_read(&rwb->readers) != WRITER_BIAS) {\n\t\tatomic_inc(&rwb->readers);\n\t\traw_spin_unlock_irq(&rtm->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Call into the slow lock path with the rtmutex->wait_lock\n\t * held, so this can't result in the following race:\n\t *\n\t * Reader1\t\tReader2\t\tWriter\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * down_read()\n\t * unlock(m->wait_lock)\n\t *\t\t\tup_read()\n\t *\t\t\twake(Writer)\n\t *\t\t\t\t\tlock(m->wait_lock)\n\t *\t\t\t\t\tsem->writelocked=true\n\t *\t\t\t\t\tunlock(m->wait_lock)\n\t *\n\t *\t\t\t\t\tup_write()\n\t *\t\t\t\t\tsem->writelocked=false\n\t *\t\t\t\t\trtmutex_unlock(m)\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * rtmutex_lock(m)\n\t *\n\t * That would put Reader1 behind the writer waiting on\n\t * Reader2 to call up_read(), which might be unbound.\n\t */\n\n\t/*\n\t * For rwlocks this returns 0 unconditionally, so the below\n\t * !ret conditionals are optimized out.\n\t */\n\tret = rwbase_rtmutex_slowlock_locked(rtm, state);\n\n\t/*\n\t * On success the rtmutex is held, so there can't be a writer\n\t * active. Increment the reader count and immediately drop the\n\t * rtmutex again.\n\t *\n\t * rtmutex->wait_lock has to be unlocked in any case of course.\n\t */\n\tif (!ret)\n\t\tatomic_inc(&rwb->readers);\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\tif (!ret)\n\t\trwbase_rtmutex_unlock(rtm);\n\treturn ret;\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rwbase_rtmutex_unlock",
          "args": [
            "rtm"
          ],
          "line": 132
        },
        "resolved": true,
        "details": {
          "function_name": "rwbase_rtmutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock_rt.c",
          "lines": "169-175",
          "snippet": "static __always_inline void rwbase_rtmutex_unlock(struct rt_mutex_base *rtm)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(rtm, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(rtm);\n}",
          "includes": [
            "#include \"rwbase_rt.c\"",
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rwbase_rt.c\"\n#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline void rwbase_rtmutex_unlock(struct rt_mutex_base *rtm)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(rtm, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(rtm);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&rtm->wait_lock"
          ],
          "line": 130
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_inc",
          "args": [
            "&rwb->readers"
          ],
          "line": 129
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwbase_rtmutex_slowlock_locked",
          "args": [
            "rtm",
            "state"
          ],
          "line": 119
        },
        "resolved": true,
        "details": {
          "function_name": "rwbase_rtmutex_slowlock_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock_rt.c",
          "lines": "162-167",
          "snippet": "static __always_inline int\nrwbase_rtmutex_slowlock_locked(struct rt_mutex_base *rtm, unsigned int state)\n{\n\trtlock_slowlock_locked(rtm);\n\treturn 0;\n}",
          "includes": [
            "#include \"rwbase_rt.c\"",
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rwbase_rt.c\"\n#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic __always_inline int\nrwbase_rtmutex_slowlock_locked(struct rt_mutex_base *rtm, unsigned int state)\n{\n\trtlock_slowlock_locked(rtm);\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_inc",
          "args": [
            "&rwb->readers"
          ],
          "line": 80
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&rwb->readers"
          ],
          "line": 79
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&rtm->wait_lock"
          ],
          "line": 74
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "static int __sched __rwbase_read_lock(struct rwbase_rt *rwb,\n\t\t\t\t      unsigned int state)\n{\n\tstruct rt_mutex_base *rtm = &rwb->rtmutex;\n\tint ret;\n\n\traw_spin_lock_irq(&rtm->wait_lock);\n\t/*\n\t * Allow readers, as long as the writer has not completely\n\t * acquired the semaphore for write.\n\t */\n\tif (atomic_read(&rwb->readers) != WRITER_BIAS) {\n\t\tatomic_inc(&rwb->readers);\n\t\traw_spin_unlock_irq(&rtm->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Call into the slow lock path with the rtmutex->wait_lock\n\t * held, so this can't result in the following race:\n\t *\n\t * Reader1\t\tReader2\t\tWriter\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * down_read()\n\t * unlock(m->wait_lock)\n\t *\t\t\tup_read()\n\t *\t\t\twake(Writer)\n\t *\t\t\t\t\tlock(m->wait_lock)\n\t *\t\t\t\t\tsem->writelocked=true\n\t *\t\t\t\t\tunlock(m->wait_lock)\n\t *\n\t *\t\t\t\t\tup_write()\n\t *\t\t\t\t\tsem->writelocked=false\n\t *\t\t\t\t\trtmutex_unlock(m)\n\t *\t\t\tdown_read()\n\t *\t\t\t\t\tdown_write()\n\t *\t\t\t\t\trtmutex_lock(m)\n\t *\t\t\t\t\twait()\n\t * rtmutex_lock(m)\n\t *\n\t * That would put Reader1 behind the writer waiting on\n\t * Reader2 to call up_read(), which might be unbound.\n\t */\n\n\t/*\n\t * For rwlocks this returns 0 unconditionally, so the below\n\t * !ret conditionals are optimized out.\n\t */\n\tret = rwbase_rtmutex_slowlock_locked(rtm, state);\n\n\t/*\n\t * On success the rtmutex is held, so there can't be a writer\n\t * active. Increment the reader count and immediately drop the\n\t * rtmutex again.\n\t *\n\t * rtmutex->wait_lock has to be unlocked in any case of course.\n\t */\n\tif (!ret)\n\t\tatomic_inc(&rwb->readers);\n\traw_spin_unlock_irq(&rtm->wait_lock);\n\tif (!ret)\n\t\trwbase_rtmutex_unlock(rtm);\n\treturn ret;\n}"
  },
  {
    "function_name": "rwbase_read_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rwbase_rt.c",
    "lines": "53-66",
    "snippet": "static __always_inline int rwbase_read_trylock(struct rwbase_rt *rwb)\n{\n\tint r;\n\n\t/*\n\t * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is\n\t * set.\n\t */\n\tfor (r = atomic_read(&rwb->readers); r < 0;) {\n\t\tif (likely(atomic_try_cmpxchg_acquire(&rwb->readers, &r, r + 1)))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}",
    "includes": [],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "atomic_try_cmpxchg_acquire(&rwb->readers, &r, r + 1)"
          ],
          "line": 62
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_try_cmpxchg_acquire",
          "args": [
            "&rwb->readers",
            "&r",
            "r + 1"
          ],
          "line": 62
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&rwb->readers"
          ],
          "line": 61
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "static __always_inline int rwbase_read_trylock(struct rwbase_rt *rwb)\n{\n\tint r;\n\n\t/*\n\t * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is\n\t * set.\n\t */\n\tfor (r = atomic_read(&rwb->readers); r < 0;) {\n\t\tif (likely(atomic_try_cmpxchg_acquire(&rwb->readers, &r, r + 1)))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}"
  }
]