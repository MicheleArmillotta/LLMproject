[
  {
    "function_name": "osq_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
    "lines": "207-232",
    "snippet": "void osq_unlock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\n\t/*\n\t * Fast path for the uncontended case.\n\t */\n\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,\n\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))\n\t\treturn;\n\n\t/*\n\t * Second most likely case.\n\t */\n\tnode = this_cpu_ptr(&osq_node);\n\tnext = xchg(&node->next, NULL);\n\tif (next) {\n\t\tWRITE_ONCE(next->locked, 1);\n\t\treturn;\n\t}\n\n\tnext = osq_wait_next(lock, node, NULL);\n\tif (next)\n\t\tWRITE_ONCE(next->locked, 1);\n}",
    "includes": [
      "#include <linux/osq_lock.h>",
      "#include <linux/sched.h>",
      "#include <linux/percpu.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "next->locked",
            "1"
          ],
          "line": 231
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "osq_wait_next",
          "args": [
            "lock",
            "node",
            "NULL"
          ],
          "line": 229
        },
        "resolved": true,
        "details": {
          "function_name": "osq_wait_next",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
          "lines": "41-88",
          "snippet": "static inline struct optimistic_spin_node *\nosq_wait_next(struct optimistic_spin_queue *lock,\n\t      struct optimistic_spin_node *node,\n\t      struct optimistic_spin_node *prev)\n{\n\tstruct optimistic_spin_node *next = NULL;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\t/*\n\t * If there is a prev node in queue, then the 'old' value will be\n\t * the prev node's CPU #, else it's set to OSQ_UNLOCKED_VAL since if\n\t * we're currently last in queue, then the queue will then become empty.\n\t */\n\told = prev ? prev->cpu : OSQ_UNLOCKED_VAL;\n\n\tfor (;;) {\n\t\tif (atomic_read(&lock->tail) == curr &&\n\t\t    atomic_cmpxchg_acquire(&lock->tail, curr, old) == curr) {\n\t\t\t/*\n\t\t\t * We were the last queued, we moved @lock back. @prev\n\t\t\t * will now observe @lock and will complete its\n\t\t\t * unlock()/unqueue().\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We must xchg() the @node->next value, because if we were to\n\t\t * leave it in, a concurrent unlock()/unqueue() from\n\t\t * @node->next might complete Step-A and think its @prev is\n\t\t * still valid.\n\t\t *\n\t\t * If the concurrent unlock()/unqueue() wins the race, we'll\n\t\t * wait for either @lock to point to us, through its Step-B, or\n\t\t * wait for a new @node->next from its Step-C.\n\t\t */\n\t\tif (node->next) {\n\t\t\tnext = xchg(&node->next, NULL);\n\t\t\tif (next)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn next;\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline struct optimistic_spin_node *\nosq_wait_next(struct optimistic_spin_queue *lock,\n\t      struct optimistic_spin_node *node,\n\t      struct optimistic_spin_node *prev)\n{\n\tstruct optimistic_spin_node *next = NULL;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\t/*\n\t * If there is a prev node in queue, then the 'old' value will be\n\t * the prev node's CPU #, else it's set to OSQ_UNLOCKED_VAL since if\n\t * we're currently last in queue, then the queue will then become empty.\n\t */\n\told = prev ? prev->cpu : OSQ_UNLOCKED_VAL;\n\n\tfor (;;) {\n\t\tif (atomic_read(&lock->tail) == curr &&\n\t\t    atomic_cmpxchg_acquire(&lock->tail, curr, old) == curr) {\n\t\t\t/*\n\t\t\t * We were the last queued, we moved @lock back. @prev\n\t\t\t * will now observe @lock and will complete its\n\t\t\t * unlock()/unqueue().\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We must xchg() the @node->next value, because if we were to\n\t\t * leave it in, a concurrent unlock()/unqueue() from\n\t\t * @node->next might complete Step-A and think its @prev is\n\t\t * still valid.\n\t\t *\n\t\t * If the concurrent unlock()/unqueue() wins the race, we'll\n\t\t * wait for either @lock to point to us, through its Step-B, or\n\t\t * wait for a new @node->next from its Step-C.\n\t\t */\n\t\tif (node->next) {\n\t\t\tnext = xchg(&node->next, NULL);\n\t\t\tif (next)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn next;\n}"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "next->locked",
            "1"
          ],
          "line": 225
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "xchg",
          "args": [
            "&node->next",
            "NULL"
          ],
          "line": 223
        },
        "resolved": true,
        "details": {
          "function_name": "xchg_tail",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "220-238",
          "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [
            "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_cpu_ptr",
          "args": [
            "&osq_node"
          ],
          "line": 222
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "atomic_cmpxchg_release(&lock->tail, curr,\n\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr"
          ],
          "line": 215
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_cmpxchg_release",
          "args": [
            "&lock->tail",
            "curr",
            "OSQ_UNLOCKED_VAL"
          ],
          "line": 215
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "encode_cpu",
          "args": [
            "smp_processor_id()"
          ],
          "line": 210
        },
        "resolved": true,
        "details": {
          "function_name": "encode_cpu",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
          "lines": "20-23",
          "snippet": "static inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 210
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);\n\nvoid osq_unlock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\n\t/*\n\t * Fast path for the uncontended case.\n\t */\n\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,\n\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))\n\t\treturn;\n\n\t/*\n\t * Second most likely case.\n\t */\n\tnode = this_cpu_ptr(&osq_node);\n\tnext = xchg(&node->next, NULL);\n\tif (next) {\n\t\tWRITE_ONCE(next->locked, 1);\n\t\treturn;\n\t}\n\n\tnext = osq_wait_next(lock, node, NULL);\n\tif (next)\n\t\tWRITE_ONCE(next->locked, 1);\n}"
  },
  {
    "function_name": "osq_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
    "lines": "90-205",
    "snippet": "bool osq_lock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);\n\tstruct optimistic_spin_node *prev, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\tnode->locked = 0;\n\tnode->next = NULL;\n\tnode->cpu = curr;\n\n\t/*\n\t * We need both ACQUIRE (pairs with corresponding RELEASE in\n\t * unlock() uncontended, or fastpath) and RELEASE (to publish\n\t * the node fields we just initialised) semantics when updating\n\t * the lock tail.\n\t */\n\told = atomic_xchg(&lock->tail, curr);\n\tif (old == OSQ_UNLOCKED_VAL)\n\t\treturn true;\n\n\tprev = decode_cpu(old);\n\tnode->prev = prev;\n\n\t/*\n\t * osq_lock()\t\t\tunqueue\n\t *\n\t * node->prev = prev\t\tosq_wait_next()\n\t * WMB\t\t\t\tMB\n\t * prev->next = node\t\tnext->prev = prev // unqueue-C\n\t *\n\t * Here 'node->prev' and 'next->prev' are the same variable and we need\n\t * to ensure these stores happen in-order to avoid corrupting the list.\n\t */\n\tsmp_wmb();\n\n\tWRITE_ONCE(prev->next, node);\n\n\t/*\n\t * Normally @prev is untouchable after the above store; because at that\n\t * moment unlock can proceed and wipe the node element from stack.\n\t *\n\t * However, since our nodes are static per-cpu storage, we're\n\t * guaranteed their existence -- this allows us to apply\n\t * cmpxchg in an attempt to undo our queueing.\n\t */\n\n\t/*\n\t * Wait to acquire the lock or cancellation. Note that need_resched()\n\t * will come with an IPI, which will wake smp_cond_load_relaxed() if it\n\t * is implemented with a monitor-wait. vcpu_is_preempted() relies on\n\t * polling, be careful.\n\t */\n\tif (smp_cond_load_relaxed(&node->locked, VAL || need_resched() ||\n\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))))\n\t\treturn true;\n\n\t/* unqueue */\n\t/*\n\t * Step - A  -- stabilize @prev\n\t *\n\t * Undo our @prev->next assignment; this will make @prev's\n\t * unlock()/unqueue() wait for a next pointer since @lock points to us\n\t * (or later).\n\t */\n\n\tfor (;;) {\n\t\t/*\n\t\t * cpu_relax() below implies a compiler barrier which would\n\t\t * prevent this comparison being optimized away.\n\t\t */\n\t\tif (data_race(prev->next) == node &&\n\t\t    cmpxchg(&prev->next, node, NULL) == node)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We can only fail the cmpxchg() racing against an unlock(),\n\t\t * in which case we should observe @node->locked becoming\n\t\t * true.\n\t\t */\n\t\tif (smp_load_acquire(&node->locked))\n\t\t\treturn true;\n\n\t\tcpu_relax();\n\n\t\t/*\n\t\t * Or we race against a concurrent unqueue()'s step-B, in which\n\t\t * case its step-C will write us a new @node->prev pointer.\n\t\t */\n\t\tprev = READ_ONCE(node->prev);\n\t}\n\n\t/*\n\t * Step - B -- stabilize @next\n\t *\n\t * Similar to unlock(), wait for @node->next or move @lock from @node\n\t * back to @prev.\n\t */\n\n\tnext = osq_wait_next(lock, node, prev);\n\tif (!next)\n\t\treturn false;\n\n\t/*\n\t * Step - C -- unlink\n\t *\n\t * @prev is stable because its still waiting for a new @prev->next\n\t * pointer, @next is stable because our @node->next pointer is NULL and\n\t * it will wait in Step-A.\n\t */\n\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n\n\treturn false;\n}",
    "includes": [
      "#include <linux/osq_lock.h>",
      "#include <linux/sched.h>",
      "#include <linux/percpu.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "prev->next",
            "next"
          ],
          "line": 202
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "next->prev",
            "prev"
          ],
          "line": 201
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "osq_wait_next",
          "args": [
            "lock",
            "node",
            "prev"
          ],
          "line": 189
        },
        "resolved": true,
        "details": {
          "function_name": "osq_wait_next",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
          "lines": "41-88",
          "snippet": "static inline struct optimistic_spin_node *\nosq_wait_next(struct optimistic_spin_queue *lock,\n\t      struct optimistic_spin_node *node,\n\t      struct optimistic_spin_node *prev)\n{\n\tstruct optimistic_spin_node *next = NULL;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\t/*\n\t * If there is a prev node in queue, then the 'old' value will be\n\t * the prev node's CPU #, else it's set to OSQ_UNLOCKED_VAL since if\n\t * we're currently last in queue, then the queue will then become empty.\n\t */\n\told = prev ? prev->cpu : OSQ_UNLOCKED_VAL;\n\n\tfor (;;) {\n\t\tif (atomic_read(&lock->tail) == curr &&\n\t\t    atomic_cmpxchg_acquire(&lock->tail, curr, old) == curr) {\n\t\t\t/*\n\t\t\t * We were the last queued, we moved @lock back. @prev\n\t\t\t * will now observe @lock and will complete its\n\t\t\t * unlock()/unqueue().\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We must xchg() the @node->next value, because if we were to\n\t\t * leave it in, a concurrent unlock()/unqueue() from\n\t\t * @node->next might complete Step-A and think its @prev is\n\t\t * still valid.\n\t\t *\n\t\t * If the concurrent unlock()/unqueue() wins the race, we'll\n\t\t * wait for either @lock to point to us, through its Step-B, or\n\t\t * wait for a new @node->next from its Step-C.\n\t\t */\n\t\tif (node->next) {\n\t\t\tnext = xchg(&node->next, NULL);\n\t\t\tif (next)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn next;\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline struct optimistic_spin_node *\nosq_wait_next(struct optimistic_spin_queue *lock,\n\t      struct optimistic_spin_node *node,\n\t      struct optimistic_spin_node *prev)\n{\n\tstruct optimistic_spin_node *next = NULL;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\t/*\n\t * If there is a prev node in queue, then the 'old' value will be\n\t * the prev node's CPU #, else it's set to OSQ_UNLOCKED_VAL since if\n\t * we're currently last in queue, then the queue will then become empty.\n\t */\n\told = prev ? prev->cpu : OSQ_UNLOCKED_VAL;\n\n\tfor (;;) {\n\t\tif (atomic_read(&lock->tail) == curr &&\n\t\t    atomic_cmpxchg_acquire(&lock->tail, curr, old) == curr) {\n\t\t\t/*\n\t\t\t * We were the last queued, we moved @lock back. @prev\n\t\t\t * will now observe @lock and will complete its\n\t\t\t * unlock()/unqueue().\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We must xchg() the @node->next value, because if we were to\n\t\t * leave it in, a concurrent unlock()/unqueue() from\n\t\t * @node->next might complete Step-A and think its @prev is\n\t\t * still valid.\n\t\t *\n\t\t * If the concurrent unlock()/unqueue() wins the race, we'll\n\t\t * wait for either @lock to point to us, through its Step-B, or\n\t\t * wait for a new @node->next from its Step-C.\n\t\t */\n\t\tif (node->next) {\n\t\t\tnext = xchg(&node->next, NULL);\n\t\t\tif (next)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn next;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "node->prev"
          ],
          "line": 179
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 173
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_load_acquire",
          "args": [
            "&node->locked"
          ],
          "line": 170
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cmpxchg",
          "args": [
            "&prev->next",
            "node",
            "NULL"
          ],
          "line": 162
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_release",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "261-266",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "data_race",
          "args": [
            "prev->next"
          ],
          "line": 161
        },
        "resolved": true,
        "details": {
          "function_name": "test_data_race",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/kcsan/kcsan_test.c",
          "lines": "998-1008",
          "snippet": "__no_kcsan\nstatic void test_data_race(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_data_race, test_kernel_data_race);\n\tdo {\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}",
          "includes": [
            "#include <trace/events/printk.h>",
            "#include <linux/types.h>",
            "#include <linux/tracepoint.h>",
            "#include <linux/torture.h>",
            "#include <linux/timer.h>",
            "#include <linux/string.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/seqlock.h>",
            "#include <linux/sched.h>",
            "#include <linux/mutex.h>",
            "#include <linux/kernel.h>",
            "#include <linux/kcsan-checks.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <kunit/test.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline const struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/printk.h>\n#include <linux/types.h>\n#include <linux/tracepoint.h>\n#include <linux/torture.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n#include <linux/kernel.h>\n#include <linux/kcsan-checks.h>\n#include <linux/jiffies.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <kunit/test.h>\n\nstatic __always_inline const struct;\n\n__no_kcsan\nstatic void test_data_race(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_data_race, test_kernel_data_race);\n\tdo {\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_cond_load_relaxed",
          "args": [
            "&node->locked",
            "VAL || need_resched() ||\n\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))"
          ],
          "line": 143
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vcpu_is_preempted",
          "args": [
            "node_cpu(node->prev)"
          ],
          "line": 144
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "node_cpu",
          "args": [
            "node->prev"
          ],
          "line": 144
        },
        "resolved": true,
        "details": {
          "function_name": "node_cpu",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
          "lines": "25-28",
          "snippet": "static inline int node_cpu(struct optimistic_spin_node *node)\n{\n\treturn node->cpu - 1;\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline int node_cpu(struct optimistic_spin_node *node)\n{\n\treturn node->cpu - 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "need_resched",
          "args": [],
          "line": 143
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "prev->next",
            "node"
          ],
          "line": 126
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_wmb",
          "args": [],
          "line": 124
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "decode_cpu",
          "args": [
            "old"
          ],
          "line": 111
        },
        "resolved": true,
        "details": {
          "function_name": "decode_cpu",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
          "lines": "30-35",
          "snippet": "static inline struct optimistic_spin_node *decode_cpu(int encoded_cpu_val)\n{\n\tint cpu_nr = encoded_cpu_val - 1;\n\n\treturn per_cpu_ptr(&osq_node, cpu_nr);\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);\n\nstatic inline struct optimistic_spin_node *decode_cpu(int encoded_cpu_val)\n{\n\tint cpu_nr = encoded_cpu_val - 1;\n\n\treturn per_cpu_ptr(&osq_node, cpu_nr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_xchg",
          "args": [
            "&lock->tail",
            "curr"
          ],
          "line": 107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "encode_cpu",
          "args": [
            "smp_processor_id()"
          ],
          "line": 94
        },
        "resolved": true,
        "details": {
          "function_name": "encode_cpu",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
          "lines": "20-23",
          "snippet": "static inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 94
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "this_cpu_ptr",
          "args": [
            "&osq_node"
          ],
          "line": 92
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);\n\nbool osq_lock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);\n\tstruct optimistic_spin_node *prev, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\tnode->locked = 0;\n\tnode->next = NULL;\n\tnode->cpu = curr;\n\n\t/*\n\t * We need both ACQUIRE (pairs with corresponding RELEASE in\n\t * unlock() uncontended, or fastpath) and RELEASE (to publish\n\t * the node fields we just initialised) semantics when updating\n\t * the lock tail.\n\t */\n\told = atomic_xchg(&lock->tail, curr);\n\tif (old == OSQ_UNLOCKED_VAL)\n\t\treturn true;\n\n\tprev = decode_cpu(old);\n\tnode->prev = prev;\n\n\t/*\n\t * osq_lock()\t\t\tunqueue\n\t *\n\t * node->prev = prev\t\tosq_wait_next()\n\t * WMB\t\t\t\tMB\n\t * prev->next = node\t\tnext->prev = prev // unqueue-C\n\t *\n\t * Here 'node->prev' and 'next->prev' are the same variable and we need\n\t * to ensure these stores happen in-order to avoid corrupting the list.\n\t */\n\tsmp_wmb();\n\n\tWRITE_ONCE(prev->next, node);\n\n\t/*\n\t * Normally @prev is untouchable after the above store; because at that\n\t * moment unlock can proceed and wipe the node element from stack.\n\t *\n\t * However, since our nodes are static per-cpu storage, we're\n\t * guaranteed their existence -- this allows us to apply\n\t * cmpxchg in an attempt to undo our queueing.\n\t */\n\n\t/*\n\t * Wait to acquire the lock or cancellation. Note that need_resched()\n\t * will come with an IPI, which will wake smp_cond_load_relaxed() if it\n\t * is implemented with a monitor-wait. vcpu_is_preempted() relies on\n\t * polling, be careful.\n\t */\n\tif (smp_cond_load_relaxed(&node->locked, VAL || need_resched() ||\n\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))))\n\t\treturn true;\n\n\t/* unqueue */\n\t/*\n\t * Step - A  -- stabilize @prev\n\t *\n\t * Undo our @prev->next assignment; this will make @prev's\n\t * unlock()/unqueue() wait for a next pointer since @lock points to us\n\t * (or later).\n\t */\n\n\tfor (;;) {\n\t\t/*\n\t\t * cpu_relax() below implies a compiler barrier which would\n\t\t * prevent this comparison being optimized away.\n\t\t */\n\t\tif (data_race(prev->next) == node &&\n\t\t    cmpxchg(&prev->next, node, NULL) == node)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We can only fail the cmpxchg() racing against an unlock(),\n\t\t * in which case we should observe @node->locked becoming\n\t\t * true.\n\t\t */\n\t\tif (smp_load_acquire(&node->locked))\n\t\t\treturn true;\n\n\t\tcpu_relax();\n\n\t\t/*\n\t\t * Or we race against a concurrent unqueue()'s step-B, in which\n\t\t * case its step-C will write us a new @node->prev pointer.\n\t\t */\n\t\tprev = READ_ONCE(node->prev);\n\t}\n\n\t/*\n\t * Step - B -- stabilize @next\n\t *\n\t * Similar to unlock(), wait for @node->next or move @lock from @node\n\t * back to @prev.\n\t */\n\n\tnext = osq_wait_next(lock, node, prev);\n\tif (!next)\n\t\treturn false;\n\n\t/*\n\t * Step - C -- unlink\n\t *\n\t * @prev is stable because its still waiting for a new @prev->next\n\t * pointer, @next is stable because our @node->next pointer is NULL and\n\t * it will wait in Step-A.\n\t */\n\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n\n\treturn false;\n}"
  },
  {
    "function_name": "osq_wait_next",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
    "lines": "41-88",
    "snippet": "static inline struct optimistic_spin_node *\nosq_wait_next(struct optimistic_spin_queue *lock,\n\t      struct optimistic_spin_node *node,\n\t      struct optimistic_spin_node *prev)\n{\n\tstruct optimistic_spin_node *next = NULL;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\t/*\n\t * If there is a prev node in queue, then the 'old' value will be\n\t * the prev node's CPU #, else it's set to OSQ_UNLOCKED_VAL since if\n\t * we're currently last in queue, then the queue will then become empty.\n\t */\n\told = prev ? prev->cpu : OSQ_UNLOCKED_VAL;\n\n\tfor (;;) {\n\t\tif (atomic_read(&lock->tail) == curr &&\n\t\t    atomic_cmpxchg_acquire(&lock->tail, curr, old) == curr) {\n\t\t\t/*\n\t\t\t * We were the last queued, we moved @lock back. @prev\n\t\t\t * will now observe @lock and will complete its\n\t\t\t * unlock()/unqueue().\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We must xchg() the @node->next value, because if we were to\n\t\t * leave it in, a concurrent unlock()/unqueue() from\n\t\t * @node->next might complete Step-A and think its @prev is\n\t\t * still valid.\n\t\t *\n\t\t * If the concurrent unlock()/unqueue() wins the race, we'll\n\t\t * wait for either @lock to point to us, through its Step-B, or\n\t\t * wait for a new @node->next from its Step-C.\n\t\t */\n\t\tif (node->next) {\n\t\t\tnext = xchg(&node->next, NULL);\n\t\t\tif (next)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn next;\n}",
    "includes": [
      "#include <linux/osq_lock.h>",
      "#include <linux/sched.h>",
      "#include <linux/percpu.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 84
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "xchg",
          "args": [
            "&node->next",
            "NULL"
          ],
          "line": 79
        },
        "resolved": true,
        "details": {
          "function_name": "xchg_tail",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "220-238",
          "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [
            "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_cmpxchg_acquire",
          "args": [
            "&lock->tail",
            "curr",
            "old"
          ],
          "line": 59
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&lock->tail"
          ],
          "line": 58
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "encode_cpu",
          "args": [
            "smp_processor_id()"
          ],
          "line": 47
        },
        "resolved": true,
        "details": {
          "function_name": "encode_cpu",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
          "lines": "20-23",
          "snippet": "static inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 47
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline struct optimistic_spin_node *\nosq_wait_next(struct optimistic_spin_queue *lock,\n\t      struct optimistic_spin_node *node,\n\t      struct optimistic_spin_node *prev)\n{\n\tstruct optimistic_spin_node *next = NULL;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\t/*\n\t * If there is a prev node in queue, then the 'old' value will be\n\t * the prev node's CPU #, else it's set to OSQ_UNLOCKED_VAL since if\n\t * we're currently last in queue, then the queue will then become empty.\n\t */\n\told = prev ? prev->cpu : OSQ_UNLOCKED_VAL;\n\n\tfor (;;) {\n\t\tif (atomic_read(&lock->tail) == curr &&\n\t\t    atomic_cmpxchg_acquire(&lock->tail, curr, old) == curr) {\n\t\t\t/*\n\t\t\t * We were the last queued, we moved @lock back. @prev\n\t\t\t * will now observe @lock and will complete its\n\t\t\t * unlock()/unqueue().\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We must xchg() the @node->next value, because if we were to\n\t\t * leave it in, a concurrent unlock()/unqueue() from\n\t\t * @node->next might complete Step-A and think its @prev is\n\t\t * still valid.\n\t\t *\n\t\t * If the concurrent unlock()/unqueue() wins the race, we'll\n\t\t * wait for either @lock to point to us, through its Step-B, or\n\t\t * wait for a new @node->next from its Step-C.\n\t\t */\n\t\tif (node->next) {\n\t\t\tnext = xchg(&node->next, NULL);\n\t\t\tif (next)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn next;\n}"
  },
  {
    "function_name": "decode_cpu",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
    "lines": "30-35",
    "snippet": "static inline struct optimistic_spin_node *decode_cpu(int encoded_cpu_val)\n{\n\tint cpu_nr = encoded_cpu_val - 1;\n\n\treturn per_cpu_ptr(&osq_node, cpu_nr);\n}",
    "includes": [
      "#include <linux/osq_lock.h>",
      "#include <linux/sched.h>",
      "#include <linux/percpu.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "per_cpu_ptr",
          "args": [
            "&osq_node",
            "cpu_nr"
          ],
          "line": 34
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);\n\nstatic inline struct optimistic_spin_node *decode_cpu(int encoded_cpu_val)\n{\n\tint cpu_nr = encoded_cpu_val - 1;\n\n\treturn per_cpu_ptr(&osq_node, cpu_nr);\n}"
  },
  {
    "function_name": "node_cpu",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
    "lines": "25-28",
    "snippet": "static inline int node_cpu(struct optimistic_spin_node *node)\n{\n\treturn node->cpu - 1;\n}",
    "includes": [
      "#include <linux/osq_lock.h>",
      "#include <linux/sched.h>",
      "#include <linux/percpu.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline int node_cpu(struct optimistic_spin_node *node)\n{\n\treturn node->cpu - 1;\n}"
  },
  {
    "function_name": "encode_cpu",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/osq_lock.c",
    "lines": "20-23",
    "snippet": "static inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}",
    "includes": [
      "#include <linux/osq_lock.h>",
      "#include <linux/sched.h>",
      "#include <linux/percpu.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic inline int encode_cpu(int cpu_nr)\n{\n\treturn cpu_nr + 1;\n}"
  }
]