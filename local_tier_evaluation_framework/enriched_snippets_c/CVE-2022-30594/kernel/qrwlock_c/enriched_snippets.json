[
  {
    "function_name": "queued_write_lock_slowpath",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qrwlock.c",
    "lines": "61-82",
    "snippet": "void queued_write_lock_slowpath(struct qrwlock *lock)\n{\n\tint cnts;\n\n\t/* Put the writer into the wait queue */\n\tarch_spin_lock(&lock->wait_lock);\n\n\t/* Try to acquire the lock directly if no reader is present */\n\tif (!(cnts = atomic_read(&lock->cnts)) &&\n\t    atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, _QW_LOCKED))\n\t\tgoto unlock;\n\n\t/* Set the waiting flag to notify readers that a writer is pending */\n\tatomic_or(_QW_WAITING, &lock->cnts);\n\n\t/* When no more readers or writers, set the locked flag */\n\tdo {\n\t\tcnts = atomic_cond_read_relaxed(&lock->cnts, VAL == _QW_WAITING);\n\t} while (!atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, _QW_LOCKED));\nunlock:\n\tarch_spin_unlock(&lock->wait_lock);\n}",
    "includes": [
      "#include <linux/spinlock.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "arch_spin_unlock",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 81
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_try_cmpxchg_acquire",
          "args": [
            "&lock->cnts",
            "&cnts",
            "_QW_LOCKED"
          ],
          "line": 79
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_cond_read_relaxed",
          "args": [
            "&lock->cnts",
            "VAL == _QW_WAITING"
          ],
          "line": 78
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_or",
          "args": [
            "_QW_WAITING",
            "&lock->cnts"
          ],
          "line": 74
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_try_cmpxchg_acquire",
          "args": [
            "&lock->cnts",
            "&cnts",
            "_QW_LOCKED"
          ],
          "line": 70
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&lock->cnts"
          ],
          "line": 69
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "arch_spin_lock",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 66
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/spinlock.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nvoid queued_write_lock_slowpath(struct qrwlock *lock)\n{\n\tint cnts;\n\n\t/* Put the writer into the wait queue */\n\tarch_spin_lock(&lock->wait_lock);\n\n\t/* Try to acquire the lock directly if no reader is present */\n\tif (!(cnts = atomic_read(&lock->cnts)) &&\n\t    atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, _QW_LOCKED))\n\t\tgoto unlock;\n\n\t/* Set the waiting flag to notify readers that a writer is pending */\n\tatomic_or(_QW_WAITING, &lock->cnts);\n\n\t/* When no more readers or writers, set the locked flag */\n\tdo {\n\t\tcnts = atomic_cond_read_relaxed(&lock->cnts, VAL == _QW_WAITING);\n\t} while (!atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, _QW_LOCKED));\nunlock:\n\tarch_spin_unlock(&lock->wait_lock);\n}"
  },
  {
    "function_name": "queued_read_lock_slowpath",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qrwlock.c",
    "lines": "20-54",
    "snippet": "void queued_read_lock_slowpath(struct qrwlock *lock)\n{\n\t/*\n\t * Readers come here when they cannot get the lock without waiting\n\t */\n\tif (unlikely(in_interrupt())) {\n\t\t/*\n\t\t * Readers in interrupt context will get the lock immediately\n\t\t * if the writer is just waiting (not holding the lock yet),\n\t\t * so spin with ACQUIRE semantics until the lock is available\n\t\t * without waiting in the queue.\n\t\t */\n\t\tatomic_cond_read_acquire(&lock->cnts, !(VAL & _QW_LOCKED));\n\t\treturn;\n\t}\n\tatomic_sub(_QR_BIAS, &lock->cnts);\n\n\t/*\n\t * Put the reader into the wait queue\n\t */\n\tarch_spin_lock(&lock->wait_lock);\n\tatomic_add(_QR_BIAS, &lock->cnts);\n\n\t/*\n\t * The ACQUIRE semantics of the following spinning code ensure\n\t * that accesses can't leak upwards out of our subsequent critical\n\t * section in the case that the lock is currently held for write.\n\t */\n\tatomic_cond_read_acquire(&lock->cnts, !(VAL & _QW_LOCKED));\n\n\t/*\n\t * Signal the next one in queue to become queue head\n\t */\n\tarch_spin_unlock(&lock->wait_lock);\n}",
    "includes": [
      "#include <linux/spinlock.h>",
      "#include <linux/hardirq.h>",
      "#include <linux/percpu.h>",
      "#include <linux/cpumask.h>",
      "#include <linux/bug.h>",
      "#include <linux/smp.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "arch_spin_unlock",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 53
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_cond_read_acquire",
          "args": [
            "&lock->cnts",
            "!(VAL & _QW_LOCKED)"
          ],
          "line": 48
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_add",
          "args": [
            "_QR_BIAS",
            "&lock->cnts"
          ],
          "line": 41
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "arch_spin_lock",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 40
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_sub",
          "args": [
            "_QR_BIAS",
            "&lock->cnts"
          ],
          "line": 35
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_cond_read_acquire",
          "args": [
            "&lock->cnts",
            "!(VAL & _QW_LOCKED)"
          ],
          "line": 32
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "in_interrupt()"
          ],
          "line": 25
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "in_interrupt",
          "args": [],
          "line": 25
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/spinlock.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\nvoid queued_read_lock_slowpath(struct qrwlock *lock)\n{\n\t/*\n\t * Readers come here when they cannot get the lock without waiting\n\t */\n\tif (unlikely(in_interrupt())) {\n\t\t/*\n\t\t * Readers in interrupt context will get the lock immediately\n\t\t * if the writer is just waiting (not holding the lock yet),\n\t\t * so spin with ACQUIRE semantics until the lock is available\n\t\t * without waiting in the queue.\n\t\t */\n\t\tatomic_cond_read_acquire(&lock->cnts, !(VAL & _QW_LOCKED));\n\t\treturn;\n\t}\n\tatomic_sub(_QR_BIAS, &lock->cnts);\n\n\t/*\n\t * Put the reader into the wait queue\n\t */\n\tarch_spin_lock(&lock->wait_lock);\n\tatomic_add(_QR_BIAS, &lock->cnts);\n\n\t/*\n\t * The ACQUIRE semantics of the following spinning code ensure\n\t * that accesses can't leak upwards out of our subsequent critical\n\t * section in the case that the lock is currently held for write.\n\t */\n\tatomic_cond_read_acquire(&lock->cnts, !(VAL & _QW_LOCKED));\n\n\t/*\n\t * Signal the next one in queue to become queue head\n\t */\n\tarch_spin_unlock(&lock->wait_lock);\n}"
  }
]