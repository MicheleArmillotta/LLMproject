[
  {
    "function_name": "futex_unlock_pi",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "1100-1232",
    "snippet": "int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)\n{\n\tu32 curval, uval, vpid = task_pid_vnr(current);\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -EFAULT;\n\t/*\n\t * We release only a lock we actually own:\n\t */\n\tif ((uval & FUTEX_TID_MASK) != vpid)\n\t\treturn -EPERM;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, FUTEX_WRITE);\n\tif (ret)\n\t\treturn ret;\n\n\thb = futex_hash(&key);\n\tspin_lock(&hb->lock);\n\n\t/*\n\t * Check waiters first. We do not trust user space values at\n\t * all and we at least want to know if user space fiddled\n\t * with the futex value instead of blindly unlocking.\n\t */\n\ttop_waiter = futex_top_waiter(hb, &key);\n\tif (top_waiter) {\n\t\tstruct futex_pi_state *pi_state = top_waiter->pi_state;\n\n\t\tret = -EINVAL;\n\t\tif (!pi_state)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If current does not own the pi_state then the futex is\n\t\t * inconsistent and user space fiddled with the futex value.\n\t\t */\n\t\tif (pi_state->owner != current)\n\t\t\tgoto out_unlock;\n\n\t\tget_pi_state(pi_state);\n\t\t/*\n\t\t * By taking wait_lock while still holding hb->lock, we ensure\n\t\t * there is no point where we hold neither; and therefore\n\t\t * wake_futex_p() must observe a state consistent with what we\n\t\t * observed.\n\t\t *\n\t\t * In particular; this forces __rt_mutex_start_proxy() to\n\t\t * complete such that we're guaranteed to observe the\n\t\t * rt_waiter. Also see the WARN in wake_futex_pi().\n\t\t */\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\tspin_unlock(&hb->lock);\n\n\t\t/* drops pi_state->pi_mutex.wait_lock */\n\t\tret = wake_futex_pi(uaddr, uval, pi_state);\n\n\t\tput_pi_state(pi_state);\n\n\t\t/*\n\t\t * Success, we're done! No tricky corner cases.\n\t\t */\n\t\tif (!ret)\n\t\t\treturn ret;\n\t\t/*\n\t\t * The atomic access to the futex value generated a\n\t\t * pagefault, so retry the user-access and the wakeup:\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t\t/*\n\t\t * A unconditional UNLOCK_PI op raced against a waiter\n\t\t * setting the FUTEX_WAITERS bit. Try again.\n\t\t */\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto pi_retry;\n\t\t/*\n\t\t * wake_futex_pi has detected invalid state. Tell user\n\t\t * space.\n\t\t */\n\t\treturn ret;\n\t}\n\n\t/*\n\t * We have no kernel internal state, i.e. no waiters in the\n\t * kernel. Waiters which are about to queue themselves are stuck\n\t * on hb->lock. So we can safely ignore them. We do neither\n\t * preserve the WAITERS bit not the OWNER_DIED one. We are the\n\t * owner.\n\t */\n\tif ((ret = futex_cmpxchg_value_locked(&curval, uaddr, uval, 0))) {\n\t\tspin_unlock(&hb->lock);\n\t\tswitch (ret) {\n\t\tcase -EFAULT:\n\t\t\tgoto pi_faulted;\n\n\t\tcase -EAGAIN:\n\t\t\tgoto pi_retry;\n\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * If uval has changed, let user space handle it.\n\t */\n\tret = (curval == uval) ? 0 : -EAGAIN;\n\nout_unlock:\n\tspin_unlock(&hb->lock);\n\treturn ret;\n\npi_retry:\n\tcond_resched();\n\tgoto retry;\n\npi_faulted:\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (!ret)\n\t\tgoto retry;\n\n\treturn ret;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "fault_in_user_writeable",
          "args": [
            "uaddr"
          ],
          "line": 1227
        },
        "resolved": true,
        "details": {
          "function_name": "fault_in_user_writeable",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "409-420",
          "snippet": "int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cond_resched",
          "args": [],
          "line": 1222
        },
        "resolved": true,
        "details": {
          "function_name": "__cond_resched",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8172-8193",
          "snippet": "int __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic void __sched;\nstatic void __sched;\n\nint __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&hb->lock"
          ],
          "line": 1218
        },
        "resolved": true,
        "details": {
          "function_name": "__bpf_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/helpers.c",
          "lines": "315-322",
          "snippet": "static inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}",
          "includes": [
            "#include \"../../lib/kstrtox.h\"",
            "#include <linux/security.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/ctype.h>",
            "#include <linux/filter.h>",
            "#include <linux/uidgid.h>",
            "#include <linux/sched.h>",
            "#include <linux/ktime.h>",
            "#include <linux/topology.h>",
            "#include <linux/smp.h>",
            "#include <linux/random.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <linux/bpf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU(unsigned long, irqsave_flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../../lib/kstrtox.h\"\n#include <linux/security.h>\n#include <linux/proc_ns.h>\n#include <linux/pid_namespace.h>\n#include <linux/jiffies.h>\n#include <linux/ctype.h>\n#include <linux/filter.h>\n#include <linux/uidgid.h>\n#include <linux/sched.h>\n#include <linux/ktime.h>\n#include <linux/topology.h>\n#include <linux/smp.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/bpf.h>\n\nstatic DEFINE_PER_CPU(unsigned long, irqsave_flags);\n\nstatic inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "1"
          ],
          "line": 1207
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_cmpxchg_value_locked",
          "args": [
            "&curval",
            "uaddr",
            "uval",
            "0"
          ],
          "line": 1197
        },
        "resolved": true,
        "details": {
          "function_name": "futex_cmpxchg_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "440-449",
          "snippet": "int futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "put_pi_state",
          "args": [
            "pi_state"
          ],
          "line": 1164
        },
        "resolved": true,
        "details": {
          "function_name": "put_pi_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "77-110",
          "snippet": "void put_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!pi_state)\n\t\treturn;\n\n\tif (!refcount_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&pi_state->pi_mutex.wait_lock, flags);\n\t\tpi_state_update_owner(pi_state, NULL);\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex);\n\t\traw_spin_unlock_irqrestore(&pi_state->pi_mutex.wait_lock, flags);\n\t}\n\n\tif (current->pi_state_cache) {\n\t\tkfree(pi_state);\n\t} else {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\trefcount_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nvoid put_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!pi_state)\n\t\treturn;\n\n\tif (!refcount_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&pi_state->pi_mutex.wait_lock, flags);\n\t\tpi_state_update_owner(pi_state, NULL);\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex);\n\t\traw_spin_unlock_irqrestore(&pi_state->pi_mutex.wait_lock, flags);\n\t}\n\n\tif (current->pi_state_cache) {\n\t\tkfree(pi_state);\n\t} else {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\trefcount_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_futex_pi",
          "args": [
            "uaddr",
            "uval",
            "pi_state"
          ],
          "line": 1162
        },
        "resolved": true,
        "details": {
          "function_name": "wake_futex_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "613-681",
          "snippet": "static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_state)\n{\n\tstruct rt_mutex_waiter *top_waiter;\n\tstruct task_struct *new_owner;\n\tbool postunlock = false;\n\tDEFINE_RT_WAKE_Q(wqh);\n\tu32 curval, newval;\n\tint ret = 0;\n\n\ttop_waiter = rt_mutex_top_waiter(&pi_state->pi_mutex);\n\tif (WARN_ON_ONCE(!top_waiter)) {\n\t\t/*\n\t\t * As per the comment in futex_unlock_pi() this should not happen.\n\t\t *\n\t\t * When this happens, give up our locks and try again, giving\n\t\t * the futex_lock_pi() instance time to complete, either by\n\t\t * waiting on the rtmutex or removing itself from the futex\n\t\t * queue.\n\t\t */\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tnew_owner = top_waiter->task;\n\n\t/*\n\t * We pass it to the next owner. The WAITERS bit is always kept\n\t * enabled while there is PI state around. We cleanup the owner\n\t * died bit, because we are the owner.\n\t */\n\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\tif (unlikely(should_fail_futex(true))) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tret = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (!ret && (curval != uval)) {\n\t\t/*\n\t\t * If a unconditional UNLOCK_PI operation (user space did not\n\t\t * try the TID->0 transition) raced with a waiter setting the\n\t\t * FUTEX_WAITERS flag between get_user() and locking the hash\n\t\t * bucket lock, retry the operation.\n\t\t */\n\t\tif ((FUTEX_TID_MASK & curval) == uval)\n\t\t\tret = -EAGAIN;\n\t\telse\n\t\t\tret = -EINVAL;\n\t}\n\n\tif (!ret) {\n\t\t/*\n\t\t * This is a point of no return; once we modified the uval\n\t\t * there is no going back and subsequent operations must\n\t\t * not fail.\n\t\t */\n\t\tpi_state_update_owner(pi_state, new_owner);\n\t\tpostunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wqh);\n\t}\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wqh);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_state)\n{\n\tstruct rt_mutex_waiter *top_waiter;\n\tstruct task_struct *new_owner;\n\tbool postunlock = false;\n\tDEFINE_RT_WAKE_Q(wqh);\n\tu32 curval, newval;\n\tint ret = 0;\n\n\ttop_waiter = rt_mutex_top_waiter(&pi_state->pi_mutex);\n\tif (WARN_ON_ONCE(!top_waiter)) {\n\t\t/*\n\t\t * As per the comment in futex_unlock_pi() this should not happen.\n\t\t *\n\t\t * When this happens, give up our locks and try again, giving\n\t\t * the futex_lock_pi() instance time to complete, either by\n\t\t * waiting on the rtmutex or removing itself from the futex\n\t\t * queue.\n\t\t */\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tnew_owner = top_waiter->task;\n\n\t/*\n\t * We pass it to the next owner. The WAITERS bit is always kept\n\t * enabled while there is PI state around. We cleanup the owner\n\t * died bit, because we are the owner.\n\t */\n\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\tif (unlikely(should_fail_futex(true))) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tret = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (!ret && (curval != uval)) {\n\t\t/*\n\t\t * If a unconditional UNLOCK_PI operation (user space did not\n\t\t * try the TID->0 transition) raced with a waiter setting the\n\t\t * FUTEX_WAITERS flag between get_user() and locking the hash\n\t\t * bucket lock, retry the operation.\n\t\t */\n\t\tif ((FUTEX_TID_MASK & curval) == uval)\n\t\t\tret = -EAGAIN;\n\t\telse\n\t\t\tret = -EINVAL;\n\t}\n\n\tif (!ret) {\n\t\t/*\n\t\t * This is a point of no return; once we modified the uval\n\t\t * there is no going back and subsequent operations must\n\t\t * not fail.\n\t\t */\n\t\tpi_state_update_owner(pi_state, new_owner);\n\t\tpostunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wqh);\n\t}\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wqh);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 1158
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_pi_state",
          "args": [
            "pi_state"
          ],
          "line": 1147
        },
        "resolved": true,
        "details": {
          "function_name": "get_pi_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "68-71",
          "snippet": "void get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!refcount_inc_not_zero(&pi_state->refcount));\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nvoid get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!refcount_inc_not_zero(&pi_state->refcount));\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_top_waiter",
          "args": [
            "hb",
            "&key"
          ],
          "line": 1132
        },
        "resolved": true,
        "details": {
          "function_name": "futex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "429-438",
          "snippet": "struct futex_q *futex_top_waiter(struct futex_hash_bucket *hb, union futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (futex_match(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nstruct futex_q *futex_top_waiter(struct futex_hash_bucket *hb, union futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (futex_match(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "&hb->lock"
          ],
          "line": 1125
        },
        "resolved": true,
        "details": {
          "function_name": "reg_may_point_to_spin_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/verifier.c",
          "lines": "445-449",
          "snippet": "static bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}",
          "includes": [
            "#include \"disasm.h\"",
            "#include <linux/btf_ids.h>",
            "#include <linux/bpf_lsm.h>",
            "#include <linux/error-injection.h>",
            "#include <linux/ctype.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sort.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/stringify.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <net/netlink.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf_verifier.h>",
            "#include <linux/btf.h>",
            "#include <linux/bpf.h>",
            "#include <linux/slab.h>",
            "#include <linux/types.h>",
            "#include <linux/kernel.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <uapi/linux/btf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"disasm.h\"\n#include <linux/btf_ids.h>\n#include <linux/bpf_lsm.h>\n#include <linux/error-injection.h>\n#include <linux/ctype.h>\n#include <linux/perf_event.h>\n#include <linux/sort.h>\n#include <linux/bsearch.h>\n#include <linux/stringify.h>\n#include <linux/vmalloc.h>\n#include <linux/file.h>\n#include <net/netlink.h>\n#include <linux/filter.h>\n#include <linux/bpf_verifier.h>\n#include <linux/btf.h>\n#include <linux/bpf.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/bpf-cgroup.h>\n#include <uapi/linux/btf.h>\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);\n\nstatic bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_hash",
          "args": [
            "&key"
          ],
          "line": 1124
        },
        "resolved": true,
        "details": {
          "function_name": "futex_hash",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "115-121",
          "snippet": "struct futex_hash_bucket *futex_hash(union futex_key *key)\n{\n\tu32 hash = jhash2((u32 *)key, offsetof(typeof(*key), both.offset) / 4,\n\t\t\t  key->both.offset);\n\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [
            "#define futex_hashsize (__futex_data.hashsize)",
            "#define futex_queues   (__futex_data.queues)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n#define futex_hashsize (__futex_data.hashsize)\n#define futex_queues   (__futex_data.queues)\n\nstruct futex_hash_bucket *futex_hash(union futex_key *key)\n{\n\tu32 hash = jhash2((u32 *)key, offsetof(typeof(*key), both.offset) / 4,\n\t\t\t  key->both.offset);\n\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_futex_key",
          "args": [
            "uaddr",
            "flags & FLAGS_SHARED",
            "&key",
            "FUTEX_WRITE"
          ],
          "line": 1120
        },
        "resolved": true,
        "details": {
          "function_name": "get_futex_key",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "220-395",
          "snippet": "int get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__read_mostly __aligned(2*sizeof(long));"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n__read_mostly __aligned(2*sizeof(long));\n\nint get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_user",
          "args": [
            "uval",
            "uaddr"
          ],
          "line": 1112
        },
        "resolved": true,
        "details": {
          "function_name": "bpf_obj_get_user",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/inode.c",
          "lines": "530-557",
          "snippet": "int bpf_obj_get_user(const char __user *pathname, int flags)\n{\n\tenum bpf_type type = BPF_TYPE_UNSPEC;\n\tint f_flags;\n\tvoid *raw;\n\tint ret;\n\n\tf_flags = bpf_get_file_flag(flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\traw = bpf_obj_do_get(pathname, &type, f_flags);\n\tif (IS_ERR(raw))\n\t\treturn PTR_ERR(raw);\n\n\tif (type == BPF_TYPE_PROG)\n\t\tret = bpf_prog_new_fd(raw);\n\telse if (type == BPF_TYPE_MAP)\n\t\tret = bpf_map_new_fd(raw, f_flags);\n\telse if (type == BPF_TYPE_LINK)\n\t\tret = (f_flags != O_RDWR) ? -EINVAL : bpf_link_new_fd(raw);\n\telse\n\t\treturn -ENOENT;\n\n\tif (ret < 0)\n\t\tbpf_any_put(raw, type);\n\treturn ret;\n}",
          "includes": [
            "#include \"preload/bpf_preload.h\"",
            "#include <linux/bpf_trace.h>",
            "#include <linux/bpf.h>",
            "#include <linux/filter.h>",
            "#include <linux/kdev_t.h>",
            "#include <linux/fs_parser.h>",
            "#include <linux/fs_context.h>",
            "#include <linux/fs.h>",
            "#include <linux/namei.h>",
            "#include <linux/mount.h>",
            "#include <linux/major.h>",
            "#include <linux/magic.h>",
            "#include <linux/init.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"preload/bpf_preload.h\"\n#include <linux/bpf_trace.h>\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <linux/kdev_t.h>\n#include <linux/fs_parser.h>\n#include <linux/fs_context.h>\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/major.h>\n#include <linux/magic.h>\n#include <linux/init.h>\n\nint bpf_obj_get_user(const char __user *pathname, int flags)\n{\n\tenum bpf_type type = BPF_TYPE_UNSPEC;\n\tint f_flags;\n\tvoid *raw;\n\tint ret;\n\n\tf_flags = bpf_get_file_flag(flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\traw = bpf_obj_do_get(pathname, &type, f_flags);\n\tif (IS_ERR(raw))\n\t\treturn PTR_ERR(raw);\n\n\tif (type == BPF_TYPE_PROG)\n\t\tret = bpf_prog_new_fd(raw);\n\telse if (type == BPF_TYPE_MAP)\n\t\tret = bpf_map_new_fd(raw, f_flags);\n\telse if (type == BPF_TYPE_LINK)\n\t\tret = (f_flags != O_RDWR) ? -EINVAL : bpf_link_new_fd(raw);\n\telse\n\t\treturn -ENOENT;\n\n\tif (ret < 0)\n\t\tbpf_any_put(raw, type);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_FUTEX_PI"
          ],
          "line": 1108
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_pid_vnr",
          "args": [
            "current"
          ],
          "line": 1102
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint futex_unlock_pi(u32 __user *uaddr, unsigned int flags)\n{\n\tu32 curval, uval, vpid = task_pid_vnr(current);\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -EFAULT;\n\t/*\n\t * We release only a lock we actually own:\n\t */\n\tif ((uval & FUTEX_TID_MASK) != vpid)\n\t\treturn -EPERM;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, FUTEX_WRITE);\n\tif (ret)\n\t\treturn ret;\n\n\thb = futex_hash(&key);\n\tspin_lock(&hb->lock);\n\n\t/*\n\t * Check waiters first. We do not trust user space values at\n\t * all and we at least want to know if user space fiddled\n\t * with the futex value instead of blindly unlocking.\n\t */\n\ttop_waiter = futex_top_waiter(hb, &key);\n\tif (top_waiter) {\n\t\tstruct futex_pi_state *pi_state = top_waiter->pi_state;\n\n\t\tret = -EINVAL;\n\t\tif (!pi_state)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If current does not own the pi_state then the futex is\n\t\t * inconsistent and user space fiddled with the futex value.\n\t\t */\n\t\tif (pi_state->owner != current)\n\t\t\tgoto out_unlock;\n\n\t\tget_pi_state(pi_state);\n\t\t/*\n\t\t * By taking wait_lock while still holding hb->lock, we ensure\n\t\t * there is no point where we hold neither; and therefore\n\t\t * wake_futex_p() must observe a state consistent with what we\n\t\t * observed.\n\t\t *\n\t\t * In particular; this forces __rt_mutex_start_proxy() to\n\t\t * complete such that we're guaranteed to observe the\n\t\t * rt_waiter. Also see the WARN in wake_futex_pi().\n\t\t */\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\tspin_unlock(&hb->lock);\n\n\t\t/* drops pi_state->pi_mutex.wait_lock */\n\t\tret = wake_futex_pi(uaddr, uval, pi_state);\n\n\t\tput_pi_state(pi_state);\n\n\t\t/*\n\t\t * Success, we're done! No tricky corner cases.\n\t\t */\n\t\tif (!ret)\n\t\t\treturn ret;\n\t\t/*\n\t\t * The atomic access to the futex value generated a\n\t\t * pagefault, so retry the user-access and the wakeup:\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t\t/*\n\t\t * A unconditional UNLOCK_PI op raced against a waiter\n\t\t * setting the FUTEX_WAITERS bit. Try again.\n\t\t */\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto pi_retry;\n\t\t/*\n\t\t * wake_futex_pi has detected invalid state. Tell user\n\t\t * space.\n\t\t */\n\t\treturn ret;\n\t}\n\n\t/*\n\t * We have no kernel internal state, i.e. no waiters in the\n\t * kernel. Waiters which are about to queue themselves are stuck\n\t * on hb->lock. So we can safely ignore them. We do neither\n\t * preserve the WAITERS bit not the OWNER_DIED one. We are the\n\t * owner.\n\t */\n\tif ((ret = futex_cmpxchg_value_locked(&curval, uaddr, uval, 0))) {\n\t\tspin_unlock(&hb->lock);\n\t\tswitch (ret) {\n\t\tcase -EFAULT:\n\t\t\tgoto pi_faulted;\n\n\t\tcase -EAGAIN:\n\t\t\tgoto pi_retry;\n\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * If uval has changed, let user space handle it.\n\t */\n\tret = (curval == uval) ? 0 : -EAGAIN;\n\nout_unlock:\n\tspin_unlock(&hb->lock);\n\treturn ret;\n\npi_retry:\n\tcond_resched();\n\tgoto retry;\n\npi_faulted:\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (!ret)\n\t\tgoto retry;\n\n\treturn ret;\n}"
  },
  {
    "function_name": "futex_lock_pi",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "930-1093",
    "snippet": "int futex_lock_pi(u32 __user *uaddr, unsigned int flags, ktime_t *time, int trylock)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct task_struct *exiting = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (refill_pi_state_cache())\n\t\treturn -ENOMEM;\n\n\tto = futex_setup_timer(time, &timeout, flags, 0);\n\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, FUTEX_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\nretry_private:\n\thb = futex_q_lock(&q);\n\n\tret = futex_lock_pi_atomic(uaddr, hb, &q.key, &q.pi_state, current,\n\t\t\t\t   &exiting, 0);\n\tif (unlikely(ret)) {\n\t\t/*\n\t\t * Atomic work succeeded and we got the lock,\n\t\t * or failed. Either way, we do _not_ block.\n\t\t */\n\t\tswitch (ret) {\n\t\tcase 1:\n\t\t\t/* We got the lock. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_put_key;\n\t\tcase -EFAULT:\n\t\t\tgoto uaddr_faulted;\n\t\tcase -EBUSY:\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Two reasons for this:\n\t\t\t * - EBUSY: Task is exiting and we just wait for the\n\t\t\t *   exit to complete.\n\t\t\t * - EAGAIN: The user space value changed.\n\t\t\t */\n\t\t\tfutex_q_unlock(hb);\n\t\t\t/*\n\t\t\t * Handle the case where the owner is in the middle of\n\t\t\t * exiting. Wait for the exit to complete otherwise\n\t\t\t * this task might loop forever, aka. live lock.\n\t\t\t */\n\t\t\twait_for_owner_exiting(ret, exiting);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock_put_key;\n\t\t}\n\t}\n\n\tWARN_ON(!q.pi_state);\n\n\t/*\n\t * Only actually queue now that the atomic ops are done:\n\t */\n\t__futex_queue(&q, hb);\n\n\tif (trylock) {\n\t\tret = rt_mutex_futex_trylock(&q.pi_state->pi_mutex);\n\t\t/* Fixup the trylock return value: */\n\t\tret = ret ? 0 : -EWOULDBLOCK;\n\t\tgoto no_block;\n\t}\n\n\trt_mutex_init_waiter(&rt_waiter);\n\n\t/*\n\t * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not\n\t * hold it while doing rt_mutex_start_proxy(), because then it will\n\t * include hb->lock in the blocking chain, even through we'll not in\n\t * fact hold it while blocking. This will lead it to report -EDEADLK\n\t * and BUG when futex_unlock_pi() interleaves with this.\n\t *\n\t * Therefore acquire wait_lock while holding hb->lock, but drop the\n\t * latter before calling __rt_mutex_start_proxy_lock(). This\n\t * interleaves with futex_unlock_pi() -- which does a similar lock\n\t * handoff -- such that the latter can observe the futex_q::pi_state\n\t * before __rt_mutex_start_proxy_lock() is done.\n\t */\n\traw_spin_lock_irq(&q.pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q.lock_ptr);\n\t/*\n\t * __rt_mutex_start_proxy_lock() unconditionally enqueues the @rt_waiter\n\t * such that futex_unlock_pi() is guaranteed to observe the waiter when\n\t * it sees the futex_q::pi_state.\n\t */\n\tret = __rt_mutex_start_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter, current);\n\traw_spin_unlock_irq(&q.pi_state->pi_mutex.wait_lock);\n\n\tif (ret) {\n\t\tif (ret == 1)\n\t\t\tret = 0;\n\t\tgoto cleanup;\n\t}\n\n\tif (unlikely(to))\n\t\thrtimer_sleeper_start_expires(to, HRTIMER_MODE_ABS);\n\n\tret = rt_mutex_wait_proxy_lock(&q.pi_state->pi_mutex, to, &rt_waiter);\n\ncleanup:\n\tspin_lock(q.lock_ptr);\n\t/*\n\t * If we failed to acquire the lock (deadlock/signal/timeout), we must\n\t * first acquire the hb->lock before removing the lock from the\n\t * rt_mutex waitqueue, such that we can keep the hb and rt_mutex wait\n\t * lists consistent.\n\t *\n\t * In particular; it is important that futex_unlock_pi() can not\n\t * observe this inconsistency.\n\t */\n\tif (ret && !rt_mutex_cleanup_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter))\n\t\tret = 0;\n\nno_block:\n\t/*\n\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t * haven't already.\n\t */\n\tres = fixup_pi_owner(uaddr, &q, !ret);\n\t/*\n\t * If fixup_pi_owner() returned an error, propagate that.  If it acquired\n\t * the lock, clear our -ETIMEDOUT or -EINTR.\n\t */\n\tif (res)\n\t\tret = (res < 0) ? res : 0;\n\n\tfutex_unqueue_pi(&q);\n\tspin_unlock(q.lock_ptr);\n\tgoto out;\n\nout_unlock_put_key:\n\tfutex_q_unlock(hb);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret != -EINTR ? ret : -ERESTARTNOINTR;\n\nuaddr_faulted:\n\tfutex_q_unlock(hb);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (!(flags & FLAGS_SHARED))\n\t\tgoto retry_private;\n\n\tgoto retry;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "fault_in_user_writeable",
          "args": [
            "uaddr"
          ],
          "line": 1085
        },
        "resolved": true,
        "details": {
          "function_name": "fault_in_user_writeable",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "409-420",
          "snippet": "int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_q_unlock",
          "args": [
            "hb"
          ],
          "line": 1083
        },
        "resolved": true,
        "details": {
          "function_name": "futex_q_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "536-541",
          "snippet": "void futex_q_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\tfutex_hb_waiters_dec(hb);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid futex_q_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\tfutex_hb_waiters_dec(hb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "destroy_hrtimer_on_stack",
          "args": [
            "&to->timer"
          ],
          "line": 1078
        },
        "resolved": true,
        "details": {
          "function_name": "destroy_hrtimer_on_stack",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/time/hrtimer.c",
          "lines": "450-453",
          "snippet": "void destroy_hrtimer_on_stack(struct hrtimer *timer)\n{\n\tdebug_object_free(timer, &hrtimer_debug_descr);\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nvoid destroy_hrtimer_on_stack(struct hrtimer *timer)\n{\n\tdebug_object_free(timer, &hrtimer_debug_descr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_cancel",
          "args": [
            "&to->timer"
          ],
          "line": 1077
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_cancel",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/time/hrtimer.c",
          "lines": "1438-1449",
          "snippet": "int hrtimer_cancel(struct hrtimer *timer)\n{\n\tint ret;\n\n\tdo {\n\t\tret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret < 0)\n\t\t\thrtimer_cancel_wait_running(timer);\n\t} while (ret < 0);\n\treturn ret;\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nint hrtimer_cancel(struct hrtimer *timer)\n{\n\tint ret;\n\n\tdo {\n\t\tret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret < 0)\n\t\t\thrtimer_cancel_wait_running(timer);\n\t} while (ret < 0);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "q.lock_ptr"
          ],
          "line": 1069
        },
        "resolved": true,
        "details": {
          "function_name": "__bpf_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/helpers.c",
          "lines": "315-322",
          "snippet": "static inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}",
          "includes": [
            "#include \"../../lib/kstrtox.h\"",
            "#include <linux/security.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/ctype.h>",
            "#include <linux/filter.h>",
            "#include <linux/uidgid.h>",
            "#include <linux/sched.h>",
            "#include <linux/ktime.h>",
            "#include <linux/topology.h>",
            "#include <linux/smp.h>",
            "#include <linux/random.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <linux/bpf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU(unsigned long, irqsave_flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../../lib/kstrtox.h\"\n#include <linux/security.h>\n#include <linux/proc_ns.h>\n#include <linux/pid_namespace.h>\n#include <linux/jiffies.h>\n#include <linux/ctype.h>\n#include <linux/filter.h>\n#include <linux/uidgid.h>\n#include <linux/sched.h>\n#include <linux/ktime.h>\n#include <linux/topology.h>\n#include <linux/smp.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/bpf.h>\n\nstatic DEFINE_PER_CPU(unsigned long, irqsave_flags);\n\nstatic inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_unqueue_pi",
          "args": [
            "&q"
          ],
          "line": 1068
        },
        "resolved": true,
        "details": {
          "function_name": "futex_unqueue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "620-627",
          "snippet": "void futex_unqueue_pi(struct futex_q *q)\n{\n\t__futex_unqueue(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid futex_unqueue_pi(struct futex_q *q)\n{\n\t__futex_unqueue(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "fixup_pi_owner",
          "args": [
            "uaddr",
            "&q",
            "!ret"
          ],
          "line": 1060
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_pi_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "884-919",
          "snippet": "int fixup_pi_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current)\n\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex. Warn and establish consistent state.\n\t */\n\tif (WARN_ON_ONCE(rt_mutex_owner(&q->pi_state->pi_mutex) == current))\n\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint fixup_pi_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current)\n\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex. Warn and establish consistent state.\n\t */\n\tif (WARN_ON_ONCE(rt_mutex_owner(&q->pi_state->pi_mutex) == current))\n\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_cleanup_proxy_lock",
          "args": [
            "&q.pi_state->pi_mutex",
            "&rt_waiter"
          ],
          "line": 1052
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cleanup_proxy_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "411-446",
          "snippet": "bool __sched rt_mutex_cleanup_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tbool cleanup = false;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/*\n\t * Do an unconditional try-lock, this deals with the lock stealing\n\t * state where __rt_mutex_futex_unlock() -> mark_wakeup_next_waiter()\n\t * sets a NULL owner.\n\t *\n\t * We're not interested in the return value, because the subsequent\n\t * test on rt_mutex_owner() will infer that. If the trylock succeeded,\n\t * we will own the lock and it will have removed the waiter. If we\n\t * failed the trylock, we're still not owner and we need to remove\n\t * ourselves.\n\t */\n\ttry_to_take_rt_mutex(lock, current, waiter);\n\t/*\n\t * Unless we're the owner; we're still enqueued on the wait_list.\n\t * So check if we became owner, if not, take us off the wait_list.\n\t */\n\tif (rt_mutex_owner(lock) != current) {\n\t\tremove_waiter(lock, waiter);\n\t\tcleanup = true;\n\t}\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn cleanup;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool __sched rt_mutex_cleanup_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tbool cleanup = false;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/*\n\t * Do an unconditional try-lock, this deals with the lock stealing\n\t * state where __rt_mutex_futex_unlock() -> mark_wakeup_next_waiter()\n\t * sets a NULL owner.\n\t *\n\t * We're not interested in the return value, because the subsequent\n\t * test on rt_mutex_owner() will infer that. If the trylock succeeded,\n\t * we will own the lock and it will have removed the waiter. If we\n\t * failed the trylock, we're still not owner and we need to remove\n\t * ourselves.\n\t */\n\ttry_to_take_rt_mutex(lock, current, waiter);\n\t/*\n\t * Unless we're the owner; we're still enqueued on the wait_list.\n\t * So check if we became owner, if not, take us off the wait_list.\n\t */\n\tif (rt_mutex_owner(lock) != current) {\n\t\tremove_waiter(lock, waiter);\n\t\tcleanup = true;\n\t}\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn cleanup;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "q.lock_ptr"
          ],
          "line": 1042
        },
        "resolved": true,
        "details": {
          "function_name": "reg_may_point_to_spin_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/verifier.c",
          "lines": "445-449",
          "snippet": "static bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}",
          "includes": [
            "#include \"disasm.h\"",
            "#include <linux/btf_ids.h>",
            "#include <linux/bpf_lsm.h>",
            "#include <linux/error-injection.h>",
            "#include <linux/ctype.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sort.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/stringify.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <net/netlink.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf_verifier.h>",
            "#include <linux/btf.h>",
            "#include <linux/bpf.h>",
            "#include <linux/slab.h>",
            "#include <linux/types.h>",
            "#include <linux/kernel.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <uapi/linux/btf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"disasm.h\"\n#include <linux/btf_ids.h>\n#include <linux/bpf_lsm.h>\n#include <linux/error-injection.h>\n#include <linux/ctype.h>\n#include <linux/perf_event.h>\n#include <linux/sort.h>\n#include <linux/bsearch.h>\n#include <linux/stringify.h>\n#include <linux/vmalloc.h>\n#include <linux/file.h>\n#include <net/netlink.h>\n#include <linux/filter.h>\n#include <linux/bpf_verifier.h>\n#include <linux/btf.h>\n#include <linux/bpf.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/bpf-cgroup.h>\n#include <uapi/linux/btf.h>\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);\n\nstatic bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_wait_proxy_lock",
          "args": [
            "&q.pi_state->pi_mutex",
            "to",
            "&rt_waiter"
          ],
          "line": 1039
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wait_proxy_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "371-389",
          "snippet": "int __sched rt_mutex_wait_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t     struct hrtimer_sleeper *to,\n\t\t\t\t     struct rt_mutex_waiter *waiter)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/* sleep on the mutex */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tret = rt_mutex_slowlock_block(lock, NULL, TASK_INTERRUPTIBLE, to, waiter);\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_wait_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t     struct hrtimer_sleeper *to,\n\t\t\t\t     struct rt_mutex_waiter *waiter)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/* sleep on the mutex */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tret = rt_mutex_slowlock_block(lock, NULL, TASK_INTERRUPTIBLE, to, waiter);\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_sleeper_start_expires",
          "args": [
            "to",
            "HRTIMER_MODE_ABS"
          ],
          "line": 1037
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_sleeper_start_expires",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/time/hrtimer.c",
          "lines": "1952-1966",
          "snippet": "void hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,\n\t\t\t\t   enum hrtimer_mode mode)\n{\n\t/*\n\t * Make the enqueue delivery mode check work on RT. If the sleeper\n\t * was initialized for hard interrupt delivery, force the mode bit.\n\t * This is a special case for hrtimer_sleepers because\n\t * hrtimer_init_sleeper() determines the delivery mode on RT so the\n\t * fiddling with this decision is avoided at the call sites.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && sl->timer.is_hard)\n\t\tmode |= HRTIMER_MODE_HARD;\n\n\thrtimer_start_expires(&sl->timer, mode);\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nvoid hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,\n\t\t\t\t   enum hrtimer_mode mode)\n{\n\t/*\n\t * Make the enqueue delivery mode check work on RT. If the sleeper\n\t * was initialized for hard interrupt delivery, force the mode bit.\n\t * This is a special case for hrtimer_sleepers because\n\t * hrtimer_init_sleeper() determines the delivery mode on RT so the\n\t * fiddling with this decision is avoided at the call sites.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && sl->timer.is_hard)\n\t\tmode |= HRTIMER_MODE_HARD;\n\n\thrtimer_start_expires(&sl->timer, mode);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "to"
          ],
          "line": 1036
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&q.pi_state->pi_mutex.wait_lock"
          ],
          "line": 1028
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_start_proxy_lock",
          "args": [
            "&q.pi_state->pi_mutex",
            "&rt_waiter",
            "current"
          ],
          "line": 1027
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_start_proxy_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "292-318",
          "snippet": "int __sched __rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task, NULL,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched __rt_mutex_start_proxy_lock(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task, NULL,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&q.pi_state->pi_mutex.wait_lock"
          ],
          "line": 1020
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_init_waiter",
          "args": [
            "&rt_waiter"
          ],
          "line": 1005
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_init_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "190-197",
          "snippet": "static inline void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->wake_state = TASK_NORMAL;\n\twaiter->task = NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->wake_state = TASK_NORMAL;\n\twaiter->task = NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_futex_trylock",
          "args": [
            "&q.pi_state->pi_mutex"
          ],
          "line": 999
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_futex_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "153-156",
          "snippet": "int __sched __rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched __rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__futex_queue",
          "args": [
            "&q",
            "hb"
          ],
          "line": 996
        },
        "resolved": true,
        "details": {
          "function_name": "__futex_queue",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "543-560",
          "snippet": "void __futex_queue(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid __futex_queue(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "!q.pi_state"
          ],
          "line": 991
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cond_resched",
          "args": [],
          "line": 984
        },
        "resolved": true,
        "details": {
          "function_name": "__cond_resched",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8172-8193",
          "snippet": "int __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic void __sched;\nstatic void __sched;\n\nint __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "wait_for_owner_exiting",
          "args": [
            "ret",
            "exiting"
          ],
          "line": 983
        },
        "resolved": true,
        "details": {
          "function_name": "wait_for_owner_exiting",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "469-491",
          "snippet": "void wait_for_owner_exiting(int ret, struct task_struct *exiting)\n{\n\tif (ret != -EBUSY) {\n\t\tWARN_ON_ONCE(exiting);\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(ret == -EBUSY && !exiting))\n\t\treturn;\n\n\tmutex_lock(&exiting->futex_exit_mutex);\n\t/*\n\t * No point in doing state checking here. If the waiter got here\n\t * while the task was in exec()->exec_futex_release() then it can\n\t * have any FUTEX_STATE_* value when the waiter has acquired the\n\t * mutex. OK, if running, EXITING or DEAD if it reached exit()\n\t * already. Highly unlikely and not a problem. Just one more round\n\t * through the futex maze.\n\t */\n\tmutex_unlock(&exiting->futex_exit_mutex);\n\n\tput_task_struct(exiting);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nvoid wait_for_owner_exiting(int ret, struct task_struct *exiting)\n{\n\tif (ret != -EBUSY) {\n\t\tWARN_ON_ONCE(exiting);\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(ret == -EBUSY && !exiting))\n\t\treturn;\n\n\tmutex_lock(&exiting->futex_exit_mutex);\n\t/*\n\t * No point in doing state checking here. If the waiter got here\n\t * while the task was in exec()->exec_futex_release() then it can\n\t * have any FUTEX_STATE_* value when the waiter has acquired the\n\t * mutex. OK, if running, EXITING or DEAD if it reached exit()\n\t * already. Highly unlikely and not a problem. Just one more round\n\t * through the futex maze.\n\t */\n\tmutex_unlock(&exiting->futex_exit_mutex);\n\n\tput_task_struct(exiting);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret"
          ],
          "line": 957
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_lock_pi_atomic",
          "args": [
            "uaddr",
            "hb",
            "&q.key",
            "&q.pi_state",
            "current",
            "&exiting",
            "0"
          ],
          "line": 955
        },
        "resolved": true,
        "details": {
          "function_name": "futex_lock_pi_atomic",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "514-608",
          "snippet": "int futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t union futex_key *key,\n\t\t\t struct futex_pi_state **ps,\n\t\t\t struct task_struct *task,\n\t\t\t struct task_struct **exiting,\n\t\t\t int set_waiters)\n{\n\tu32 uval, newval, vpid = task_pid_vnr(task);\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\t/*\n\t * Read the user space value first so we can validate a few\n\t * things before proceeding further.\n\t */\n\tif (futex_get_value_locked(&uval, uaddr))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((uval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\tif ((unlikely(should_fail_futex(true))))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Lookup existing state first. If it exists, try to attach to\n\t * its pi_state.\n\t */\n\ttop_waiter = futex_top_waiter(hb, key);\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * No waiter and user TID is 0. We are here because the\n\t * waiters or the owner died bit is set or called from\n\t * requeue_cmp_pi or for whatever reason something took the\n\t * syscall.\n\t */\n\tif (!(uval & FUTEX_TID_MASK)) {\n\t\t/*\n\t\t * We take over the futex. No other waiters and the user space\n\t\t * TID is 0. We preserve the owner died bit.\n\t\t */\n\t\tnewval = uval & FUTEX_OWNER_DIED;\n\t\tnewval |= vpid;\n\n\t\t/* The futex requeue_pi code can enforce the waiters bit */\n\t\tif (set_waiters)\n\t\t\tnewval |= FUTEX_WAITERS;\n\n\t\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * If the waiter bit was requested the caller also needs PI\n\t\t * state attached to the new owner of the user space futex.\n\t\t *\n\t\t * @task is guaranteed to be alive and it cannot be exiting\n\t\t * because it is either sleeping or waiting in\n\t\t * futex_requeue_pi_wakeup_sync().\n\t\t *\n\t\t * No need to do the full attach_to_pi_owner() exercise\n\t\t * because @task is known and valid.\n\t\t */\n\t\tif (set_waiters) {\n\t\t\traw_spin_lock_irq(&task->pi_lock);\n\t\t\t__attach_to_pi_owner(task, key, ps);\n\t\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\t}\n\t\treturn 1;\n\t}\n\n\t/*\n\t * First waiter. Set the waiters bit before attaching ourself to\n\t * the owner. If owner tries to unlock, it will be forced into\n\t * the kernel and blocked on hb->lock.\n\t */\n\tnewval = uval | FUTEX_WAITERS;\n\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\tif (ret)\n\t\treturn ret;\n\t/*\n\t * If the update of the user space value succeeded, we try to\n\t * attach to the owner. If that fails, no harm done, we only\n\t * set the FUTEX_WAITERS bit in the user space variable.\n\t */\n\treturn attach_to_pi_owner(uaddr, newval, key, ps, exiting);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t union futex_key *key,\n\t\t\t struct futex_pi_state **ps,\n\t\t\t struct task_struct *task,\n\t\t\t struct task_struct **exiting,\n\t\t\t int set_waiters)\n{\n\tu32 uval, newval, vpid = task_pid_vnr(task);\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\t/*\n\t * Read the user space value first so we can validate a few\n\t * things before proceeding further.\n\t */\n\tif (futex_get_value_locked(&uval, uaddr))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((uval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\tif ((unlikely(should_fail_futex(true))))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Lookup existing state first. If it exists, try to attach to\n\t * its pi_state.\n\t */\n\ttop_waiter = futex_top_waiter(hb, key);\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * No waiter and user TID is 0. We are here because the\n\t * waiters or the owner died bit is set or called from\n\t * requeue_cmp_pi or for whatever reason something took the\n\t * syscall.\n\t */\n\tif (!(uval & FUTEX_TID_MASK)) {\n\t\t/*\n\t\t * We take over the futex. No other waiters and the user space\n\t\t * TID is 0. We preserve the owner died bit.\n\t\t */\n\t\tnewval = uval & FUTEX_OWNER_DIED;\n\t\tnewval |= vpid;\n\n\t\t/* The futex requeue_pi code can enforce the waiters bit */\n\t\tif (set_waiters)\n\t\t\tnewval |= FUTEX_WAITERS;\n\n\t\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * If the waiter bit was requested the caller also needs PI\n\t\t * state attached to the new owner of the user space futex.\n\t\t *\n\t\t * @task is guaranteed to be alive and it cannot be exiting\n\t\t * because it is either sleeping or waiting in\n\t\t * futex_requeue_pi_wakeup_sync().\n\t\t *\n\t\t * No need to do the full attach_to_pi_owner() exercise\n\t\t * because @task is known and valid.\n\t\t */\n\t\tif (set_waiters) {\n\t\t\traw_spin_lock_irq(&task->pi_lock);\n\t\t\t__attach_to_pi_owner(task, key, ps);\n\t\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\t}\n\t\treturn 1;\n\t}\n\n\t/*\n\t * First waiter. Set the waiters bit before attaching ourself to\n\t * the owner. If owner tries to unlock, it will be forced into\n\t * the kernel and blocked on hb->lock.\n\t */\n\tnewval = uval | FUTEX_WAITERS;\n\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\tif (ret)\n\t\treturn ret;\n\t/*\n\t * If the update of the user space value succeeded, we try to\n\t * attach to the owner. If that fails, no harm done, we only\n\t * set the FUTEX_WAITERS bit in the user space variable.\n\t */\n\treturn attach_to_pi_owner(uaddr, newval, key, ps, exiting);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_q_lock",
          "args": [
            "&q"
          ],
          "line": 953
        },
        "resolved": true,
        "details": {
          "function_name": "futex_q_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "513-534",
          "snippet": "struct futex_hash_bucket *futex_q_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = futex_hash(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all futex_q_lock()\n\t * users end up calling futex_queue(). Similarly, for housekeeping,\n\t * decrement the counter at futex_q_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\tfutex_hb_waiters_inc(hb); /* implies smp_mb(); (A) */\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock);\n\treturn hb;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nstruct futex_hash_bucket *futex_q_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = futex_hash(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all futex_q_lock()\n\t * users end up calling futex_queue(). Similarly, for housekeeping,\n\t * decrement the counter at futex_q_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\tfutex_hb_waiters_inc(hb); /* implies smp_mb(); (A) */\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock);\n\treturn hb;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret != 0"
          ],
          "line": 949
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_futex_key",
          "args": [
            "uaddr",
            "flags & FLAGS_SHARED",
            "&q.key",
            "FUTEX_WRITE"
          ],
          "line": 948
        },
        "resolved": true,
        "details": {
          "function_name": "get_futex_key",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "220-395",
          "snippet": "int get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__read_mostly __aligned(2*sizeof(long));"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\n__read_mostly __aligned(2*sizeof(long));\n\nint get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_setup_timer",
          "args": [
            "time",
            "&timeout",
            "flags",
            "0"
          ],
          "line": 945
        },
        "resolved": true,
        "details": {
          "function_name": "futex_setup_timer",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "134-151",
          "snippet": "struct hrtimer_sleeper *\nfutex_setup_timer(ktime_t *time, struct hrtimer_sleeper *timeout,\n\t\t  int flags, u64 range_ns)\n{\n\tif (!time)\n\t\treturn NULL;\n\n\thrtimer_init_sleeper_on_stack(timeout, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t/*\n\t * If range_ns is 0, calling hrtimer_set_expires_range_ns() is\n\t * effectively the same as calling hrtimer_set_expires().\n\t */\n\thrtimer_set_expires_range_ns(&timeout->timer, *time, range_ns);\n\n\treturn timeout;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nstruct hrtimer_sleeper *\nfutex_setup_timer(ktime_t *time, struct hrtimer_sleeper *timeout,\n\t\t  int flags, u64 range_ns)\n{\n\tif (!time)\n\t\treturn NULL;\n\n\thrtimer_init_sleeper_on_stack(timeout, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t/*\n\t * If range_ns is 0, calling hrtimer_set_expires_range_ns() is\n\t * effectively the same as calling hrtimer_set_expires().\n\t */\n\thrtimer_set_expires_range_ns(&timeout->timer, *time, range_ns);\n\n\treturn timeout;\n}"
        }
      },
      {
        "call_info": {
          "callee": "refill_pi_state_cache",
          "args": [],
          "line": 942
        },
        "resolved": true,
        "details": {
          "function_name": "refill_pi_state_cache",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "12-33",
          "snippet": "int refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\trefcount_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\trefcount_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_FUTEX_PI"
          ],
          "line": 939
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint futex_lock_pi(u32 __user *uaddr, unsigned int flags, ktime_t *time, int trylock)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct task_struct *exiting = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (refill_pi_state_cache())\n\t\treturn -ENOMEM;\n\n\tto = futex_setup_timer(time, &timeout, flags, 0);\n\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, FUTEX_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\nretry_private:\n\thb = futex_q_lock(&q);\n\n\tret = futex_lock_pi_atomic(uaddr, hb, &q.key, &q.pi_state, current,\n\t\t\t\t   &exiting, 0);\n\tif (unlikely(ret)) {\n\t\t/*\n\t\t * Atomic work succeeded and we got the lock,\n\t\t * or failed. Either way, we do _not_ block.\n\t\t */\n\t\tswitch (ret) {\n\t\tcase 1:\n\t\t\t/* We got the lock. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_put_key;\n\t\tcase -EFAULT:\n\t\t\tgoto uaddr_faulted;\n\t\tcase -EBUSY:\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Two reasons for this:\n\t\t\t * - EBUSY: Task is exiting and we just wait for the\n\t\t\t *   exit to complete.\n\t\t\t * - EAGAIN: The user space value changed.\n\t\t\t */\n\t\t\tfutex_q_unlock(hb);\n\t\t\t/*\n\t\t\t * Handle the case where the owner is in the middle of\n\t\t\t * exiting. Wait for the exit to complete otherwise\n\t\t\t * this task might loop forever, aka. live lock.\n\t\t\t */\n\t\t\twait_for_owner_exiting(ret, exiting);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock_put_key;\n\t\t}\n\t}\n\n\tWARN_ON(!q.pi_state);\n\n\t/*\n\t * Only actually queue now that the atomic ops are done:\n\t */\n\t__futex_queue(&q, hb);\n\n\tif (trylock) {\n\t\tret = rt_mutex_futex_trylock(&q.pi_state->pi_mutex);\n\t\t/* Fixup the trylock return value: */\n\t\tret = ret ? 0 : -EWOULDBLOCK;\n\t\tgoto no_block;\n\t}\n\n\trt_mutex_init_waiter(&rt_waiter);\n\n\t/*\n\t * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not\n\t * hold it while doing rt_mutex_start_proxy(), because then it will\n\t * include hb->lock in the blocking chain, even through we'll not in\n\t * fact hold it while blocking. This will lead it to report -EDEADLK\n\t * and BUG when futex_unlock_pi() interleaves with this.\n\t *\n\t * Therefore acquire wait_lock while holding hb->lock, but drop the\n\t * latter before calling __rt_mutex_start_proxy_lock(). This\n\t * interleaves with futex_unlock_pi() -- which does a similar lock\n\t * handoff -- such that the latter can observe the futex_q::pi_state\n\t * before __rt_mutex_start_proxy_lock() is done.\n\t */\n\traw_spin_lock_irq(&q.pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q.lock_ptr);\n\t/*\n\t * __rt_mutex_start_proxy_lock() unconditionally enqueues the @rt_waiter\n\t * such that futex_unlock_pi() is guaranteed to observe the waiter when\n\t * it sees the futex_q::pi_state.\n\t */\n\tret = __rt_mutex_start_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter, current);\n\traw_spin_unlock_irq(&q.pi_state->pi_mutex.wait_lock);\n\n\tif (ret) {\n\t\tif (ret == 1)\n\t\t\tret = 0;\n\t\tgoto cleanup;\n\t}\n\n\tif (unlikely(to))\n\t\thrtimer_sleeper_start_expires(to, HRTIMER_MODE_ABS);\n\n\tret = rt_mutex_wait_proxy_lock(&q.pi_state->pi_mutex, to, &rt_waiter);\n\ncleanup:\n\tspin_lock(q.lock_ptr);\n\t/*\n\t * If we failed to acquire the lock (deadlock/signal/timeout), we must\n\t * first acquire the hb->lock before removing the lock from the\n\t * rt_mutex waitqueue, such that we can keep the hb and rt_mutex wait\n\t * lists consistent.\n\t *\n\t * In particular; it is important that futex_unlock_pi() can not\n\t * observe this inconsistency.\n\t */\n\tif (ret && !rt_mutex_cleanup_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter))\n\t\tret = 0;\n\nno_block:\n\t/*\n\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t * haven't already.\n\t */\n\tres = fixup_pi_owner(uaddr, &q, !ret);\n\t/*\n\t * If fixup_pi_owner() returned an error, propagate that.  If it acquired\n\t * the lock, clear our -ETIMEDOUT or -EINTR.\n\t */\n\tif (res)\n\t\tret = (res < 0) ? res : 0;\n\n\tfutex_unqueue_pi(&q);\n\tspin_unlock(q.lock_ptr);\n\tgoto out;\n\nout_unlock_put_key:\n\tfutex_q_unlock(hb);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret != -EINTR ? ret : -ERESTARTNOINTR;\n\nuaddr_faulted:\n\tfutex_q_unlock(hb);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (!(flags & FLAGS_SHARED))\n\t\tgoto retry_private;\n\n\tgoto retry;\n}"
  },
  {
    "function_name": "fixup_pi_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "884-919",
    "snippet": "int fixup_pi_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current)\n\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex. Warn and establish consistent state.\n\t */\n\tif (WARN_ON_ONCE(rt_mutex_owner(&q->pi_state->pi_mutex) == current))\n\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\n\treturn 0;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "fixup_pi_state_owner",
          "args": [
            "uaddr",
            "q",
            "current"
          ],
          "line": 916
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_pi_state_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "855-867",
          "snippet": "static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tint ret;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\tret = __fixup_pi_state_owner(uaddr, q, argowner);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tint ret;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\tret = __fixup_pi_state_owner(uaddr, q, argowner);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "rt_mutex_owner(&q->pi_state->pi_mutex) == current"
          ],
          "line": 915
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "&q->pi_state->pi_mutex"
          ],
          "line": 915
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint fixup_pi_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current)\n\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex. Warn and establish consistent state.\n\t */\n\tif (WARN_ON_ONCE(rt_mutex_owner(&q->pi_state->pi_mutex) == current))\n\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "fixup_pi_state_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "855-867",
    "snippet": "static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tint ret;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\tret = __fixup_pi_state_owner(uaddr, q, argowner);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 865
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__fixup_pi_state_owner",
          "args": [
            "uaddr",
            "q",
            "argowner"
          ],
          "line": 864
        },
        "resolved": true,
        "details": {
          "function_name": "__fixup_pi_state_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "683-853",
          "snippet": "static int __fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\t  struct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 uval, curval, newval, newtid;\n\tint err = 0;\n\n\toldowner = pi_state->owner;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID checks when attaching to PI state .\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock. pi_state is correct. Tell caller. */\n\t\t\treturn 1;\n\t\t}\n\n\t\t/*\n\t\t * The trylock just failed, so either there is an owner or\n\t\t * there is a higher priority waiter than this one.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\t/*\n\t\t * If the higher priority waiter has not yet taken over the\n\t\t * rtmutex then newowner is NULL. We can't return here with\n\t\t * that state because it's inconsistent vs. the user space\n\t\t * state. So drop the locks and try again. It's a valid\n\t\t * situation and not any different from the other retry\n\t\t * conditions.\n\t\t */\n\t\tif (unlikely(!newowner)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto handle_err;\n\t\t}\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 1;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\terr = futex_get_value_locked(&uval, uaddr);\n\tif (err)\n\t\tgoto handle_err;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\t\tif (err)\n\t\t\tgoto handle_err;\n\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tpi_state_update_owner(pi_state, newowner);\n\n\treturn argowner == current;\n\n\t/*\n\t * In order to reschedule or handle a page fault, we need to drop the\n\t * locks here. In the case of a fault, this gives the other task\n\t * (either the highest priority waiter itself or the task which stole\n\t * the rtmutex) the chance to try the fixup of the pi_state. So once we\n\t * are back from handling the fault we need to check the pi_state after\n\t * reacquiring the locks and before trying to do another fixup. When\n\t * the fixup has been done already we simply return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_err:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tswitch (err) {\n\tcase -EFAULT:\n\t\terr = fault_in_user_writeable(uaddr);\n\t\tbreak;\n\n\tcase -EAGAIN:\n\t\tcond_resched();\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner)\n\t\treturn argowner == current;\n\n\t/* Retry if err was -EAGAIN or the fault in succeeded */\n\tif (!err)\n\t\tgoto retry;\n\n\t/*\n\t * fault_in_user_writeable() failed so user state is immutable. At\n\t * best we can make the kernel state consistent but user state will\n\t * be most likely hosed and any subsequent unlock operation will be\n\t * rejected due to PI futex rule [10].\n\t *\n\t * Ensure that the rtmutex owner is also the pi_state owner despite\n\t * the user space value claiming something different. There is no\n\t * point in unlocking the rtmutex if current is the owner as it\n\t * would need to wait until the next waiter has taken the rtmutex\n\t * to guarantee consistent state. Keep it simple. Userspace asked\n\t * for this wreckaged state.\n\t *\n\t * The rtmutex has an owner - either current or some other\n\t * task. See the EAGAIN loop above.\n\t */\n\tpi_state_update_owner(pi_state, rt_mutex_owner(&pi_state->pi_mutex));\n\n\treturn err;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int __fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\t  struct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 uval, curval, newval, newtid;\n\tint err = 0;\n\n\toldowner = pi_state->owner;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID checks when attaching to PI state .\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock. pi_state is correct. Tell caller. */\n\t\t\treturn 1;\n\t\t}\n\n\t\t/*\n\t\t * The trylock just failed, so either there is an owner or\n\t\t * there is a higher priority waiter than this one.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\t/*\n\t\t * If the higher priority waiter has not yet taken over the\n\t\t * rtmutex then newowner is NULL. We can't return here with\n\t\t * that state because it's inconsistent vs. the user space\n\t\t * state. So drop the locks and try again. It's a valid\n\t\t * situation and not any different from the other retry\n\t\t * conditions.\n\t\t */\n\t\tif (unlikely(!newowner)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto handle_err;\n\t\t}\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 1;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\terr = futex_get_value_locked(&uval, uaddr);\n\tif (err)\n\t\tgoto handle_err;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\t\tif (err)\n\t\t\tgoto handle_err;\n\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tpi_state_update_owner(pi_state, newowner);\n\n\treturn argowner == current;\n\n\t/*\n\t * In order to reschedule or handle a page fault, we need to drop the\n\t * locks here. In the case of a fault, this gives the other task\n\t * (either the highest priority waiter itself or the task which stole\n\t * the rtmutex) the chance to try the fixup of the pi_state. So once we\n\t * are back from handling the fault we need to check the pi_state after\n\t * reacquiring the locks and before trying to do another fixup. When\n\t * the fixup has been done already we simply return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_err:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tswitch (err) {\n\tcase -EFAULT:\n\t\terr = fault_in_user_writeable(uaddr);\n\t\tbreak;\n\n\tcase -EAGAIN:\n\t\tcond_resched();\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner)\n\t\treturn argowner == current;\n\n\t/* Retry if err was -EAGAIN or the fault in succeeded */\n\tif (!err)\n\t\tgoto retry;\n\n\t/*\n\t * fault_in_user_writeable() failed so user state is immutable. At\n\t * best we can make the kernel state consistent but user state will\n\t * be most likely hosed and any subsequent unlock operation will be\n\t * rejected due to PI futex rule [10].\n\t *\n\t * Ensure that the rtmutex owner is also the pi_state owner despite\n\t * the user space value claiming something different. There is no\n\t * point in unlocking the rtmutex if current is the owner as it\n\t * would need to wait until the next waiter has taken the rtmutex\n\t * to guarantee consistent state. Keep it simple. Userspace asked\n\t * for this wreckaged state.\n\t *\n\t * The rtmutex has an owner - either current or some other\n\t * task. See the EAGAIN loop above.\n\t */\n\tpi_state_update_owner(pi_state, rt_mutex_owner(&pi_state->pi_mutex));\n\n\treturn err;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 863
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "q->lock_ptr"
          ],
          "line": 861
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tint ret;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\tret = __fixup_pi_state_owner(uaddr, q, argowner);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}"
  },
  {
    "function_name": "__fixup_pi_state_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "683-853",
    "snippet": "static int __fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\t  struct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 uval, curval, newval, newtid;\n\tint err = 0;\n\n\toldowner = pi_state->owner;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID checks when attaching to PI state .\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock. pi_state is correct. Tell caller. */\n\t\t\treturn 1;\n\t\t}\n\n\t\t/*\n\t\t * The trylock just failed, so either there is an owner or\n\t\t * there is a higher priority waiter than this one.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\t/*\n\t\t * If the higher priority waiter has not yet taken over the\n\t\t * rtmutex then newowner is NULL. We can't return here with\n\t\t * that state because it's inconsistent vs. the user space\n\t\t * state. So drop the locks and try again. It's a valid\n\t\t * situation and not any different from the other retry\n\t\t * conditions.\n\t\t */\n\t\tif (unlikely(!newowner)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto handle_err;\n\t\t}\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 1;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\terr = futex_get_value_locked(&uval, uaddr);\n\tif (err)\n\t\tgoto handle_err;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\t\tif (err)\n\t\t\tgoto handle_err;\n\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tpi_state_update_owner(pi_state, newowner);\n\n\treturn argowner == current;\n\n\t/*\n\t * In order to reschedule or handle a page fault, we need to drop the\n\t * locks here. In the case of a fault, this gives the other task\n\t * (either the highest priority waiter itself or the task which stole\n\t * the rtmutex) the chance to try the fixup of the pi_state. So once we\n\t * are back from handling the fault we need to check the pi_state after\n\t * reacquiring the locks and before trying to do another fixup. When\n\t * the fixup has been done already we simply return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_err:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tswitch (err) {\n\tcase -EFAULT:\n\t\terr = fault_in_user_writeable(uaddr);\n\t\tbreak;\n\n\tcase -EAGAIN:\n\t\tcond_resched();\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner)\n\t\treturn argowner == current;\n\n\t/* Retry if err was -EAGAIN or the fault in succeeded */\n\tif (!err)\n\t\tgoto retry;\n\n\t/*\n\t * fault_in_user_writeable() failed so user state is immutable. At\n\t * best we can make the kernel state consistent but user state will\n\t * be most likely hosed and any subsequent unlock operation will be\n\t * rejected due to PI futex rule [10].\n\t *\n\t * Ensure that the rtmutex owner is also the pi_state owner despite\n\t * the user space value claiming something different. There is no\n\t * point in unlocking the rtmutex if current is the owner as it\n\t * would need to wait until the next waiter has taken the rtmutex\n\t * to guarantee consistent state. Keep it simple. Userspace asked\n\t * for this wreckaged state.\n\t *\n\t * The rtmutex has an owner - either current or some other\n\t * task. See the EAGAIN loop above.\n\t */\n\tpi_state_update_owner(pi_state, rt_mutex_owner(&pi_state->pi_mutex));\n\n\treturn err;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "pi_state_update_owner",
          "args": [
            "pi_state",
            "rt_mutex_owner(&pi_state->pi_mutex)"
          ],
          "line": 850
        },
        "resolved": true,
        "details": {
          "function_name": "pi_state_update_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "45-66",
          "snippet": "static void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "&pi_state->pi_mutex"
          ],
          "line": 850
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 822
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "q->lock_ptr"
          ],
          "line": 821
        },
        "resolved": true,
        "details": {
          "function_name": "reg_may_point_to_spin_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/verifier.c",
          "lines": "445-449",
          "snippet": "static bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}",
          "includes": [
            "#include \"disasm.h\"",
            "#include <linux/btf_ids.h>",
            "#include <linux/bpf_lsm.h>",
            "#include <linux/error-injection.h>",
            "#include <linux/ctype.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sort.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/stringify.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <net/netlink.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf_verifier.h>",
            "#include <linux/btf.h>",
            "#include <linux/bpf.h>",
            "#include <linux/slab.h>",
            "#include <linux/types.h>",
            "#include <linux/kernel.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <uapi/linux/btf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"disasm.h\"\n#include <linux/btf_ids.h>\n#include <linux/bpf_lsm.h>\n#include <linux/error-injection.h>\n#include <linux/ctype.h>\n#include <linux/perf_event.h>\n#include <linux/sort.h>\n#include <linux/bsearch.h>\n#include <linux/stringify.h>\n#include <linux/vmalloc.h>\n#include <linux/file.h>\n#include <net/netlink.h>\n#include <linux/filter.h>\n#include <linux/bpf_verifier.h>\n#include <linux/btf.h>\n#include <linux/bpf.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/bpf-cgroup.h>\n#include <uapi/linux/btf.h>\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);\n\nstatic bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "1"
          ],
          "line": 817
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cond_resched",
          "args": [],
          "line": 812
        },
        "resolved": true,
        "details": {
          "function_name": "__cond_resched",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "8172-8193",
          "snippet": "int __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic void __sched;\nstatic void __sched;\n\nint __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t/*\n\t * In preemptible kernels, ->rcu_read_lock_nesting tells the tick\n\t * whether the current CPU is in an RCU read-side critical section,\n\t * so the tick can report quiescent states even for CPUs looping\n\t * in kernel context.  In contrast, in non-preemptible kernels,\n\t * RCU readers leave no in-memory hints, which means that CPU-bound\n\t * processes executing in kernel context might never report an\n\t * RCU quiescent state.  Therefore, the following code causes\n\t * cond_resched() to report a quiescent state, but only when RCU\n\t * is in urgent need of one.\n\t */\n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "fault_in_user_writeable",
          "args": [
            "uaddr"
          ],
          "line": 808
        },
        "resolved": true,
        "details": {
          "function_name": "fault_in_user_writeable",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "409-420",
          "snippet": "int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "q->lock_ptr"
          ],
          "line": 804
        },
        "resolved": true,
        "details": {
          "function_name": "__bpf_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/bpf/helpers.c",
          "lines": "315-322",
          "snippet": "static inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}",
          "includes": [
            "#include \"../../lib/kstrtox.h\"",
            "#include <linux/security.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/ctype.h>",
            "#include <linux/filter.h>",
            "#include <linux/uidgid.h>",
            "#include <linux/sched.h>",
            "#include <linux/ktime.h>",
            "#include <linux/topology.h>",
            "#include <linux/smp.h>",
            "#include <linux/random.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/bpf-cgroup.h>",
            "#include <linux/bpf.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU(unsigned long, irqsave_flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../../lib/kstrtox.h\"\n#include <linux/security.h>\n#include <linux/proc_ns.h>\n#include <linux/pid_namespace.h>\n#include <linux/jiffies.h>\n#include <linux/ctype.h>\n#include <linux/filter.h>\n#include <linux/uidgid.h>\n#include <linux/sched.h>\n#include <linux/ktime.h>\n#include <linux/topology.h>\n#include <linux/smp.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/bpf.h>\n\nstatic DEFINE_PER_CPU(unsigned long, irqsave_flags);\n\nstatic inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 803
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_cmpxchg_value_locked",
          "args": [
            "&curval",
            "uaddr",
            "uval",
            "newval"
          ],
          "line": 772
        },
        "resolved": true,
        "details": {
          "function_name": "futex_cmpxchg_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "440-449",
          "snippet": "int futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_get_value_locked",
          "args": [
            "&uval",
            "uaddr"
          ],
          "line": 765
        },
        "resolved": true,
        "details": {
          "function_name": "futex_get_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "451-460",
          "snippet": "int futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_pid_vnr",
          "args": [
            "newowner"
          ],
          "line": 760
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "argowner != current"
          ],
          "line": 749
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!newowner"
          ],
          "line": 744
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_futex_trylock",
          "args": [
            "&pi_state->pi_mutex"
          ],
          "line": 726
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_futex_trylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "153-156",
          "snippet": "int __sched __rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched __rt_mutex_futex_trylock(struct rt_mutex_base *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int __fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\t  struct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 uval, curval, newval, newtid;\n\tint err = 0;\n\n\toldowner = pi_state->owner;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID checks when attaching to PI state .\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock. pi_state is correct. Tell caller. */\n\t\t\treturn 1;\n\t\t}\n\n\t\t/*\n\t\t * The trylock just failed, so either there is an owner or\n\t\t * there is a higher priority waiter than this one.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\t/*\n\t\t * If the higher priority waiter has not yet taken over the\n\t\t * rtmutex then newowner is NULL. We can't return here with\n\t\t * that state because it's inconsistent vs. the user space\n\t\t * state. So drop the locks and try again. It's a valid\n\t\t * situation and not any different from the other retry\n\t\t * conditions.\n\t\t */\n\t\tif (unlikely(!newowner)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto handle_err;\n\t\t}\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\treturn 1;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\terr = futex_get_value_locked(&uval, uaddr);\n\tif (err)\n\t\tgoto handle_err;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\t\tif (err)\n\t\t\tgoto handle_err;\n\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tpi_state_update_owner(pi_state, newowner);\n\n\treturn argowner == current;\n\n\t/*\n\t * In order to reschedule or handle a page fault, we need to drop the\n\t * locks here. In the case of a fault, this gives the other task\n\t * (either the highest priority waiter itself or the task which stole\n\t * the rtmutex) the chance to try the fixup of the pi_state. So once we\n\t * are back from handling the fault we need to check the pi_state after\n\t * reacquiring the locks and before trying to do another fixup. When\n\t * the fixup has been done already we simply return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_err:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tswitch (err) {\n\tcase -EFAULT:\n\t\terr = fault_in_user_writeable(uaddr);\n\t\tbreak;\n\n\tcase -EAGAIN:\n\t\tcond_resched();\n\t\terr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner)\n\t\treturn argowner == current;\n\n\t/* Retry if err was -EAGAIN or the fault in succeeded */\n\tif (!err)\n\t\tgoto retry;\n\n\t/*\n\t * fault_in_user_writeable() failed so user state is immutable. At\n\t * best we can make the kernel state consistent but user state will\n\t * be most likely hosed and any subsequent unlock operation will be\n\t * rejected due to PI futex rule [10].\n\t *\n\t * Ensure that the rtmutex owner is also the pi_state owner despite\n\t * the user space value claiming something different. There is no\n\t * point in unlocking the rtmutex if current is the owner as it\n\t * would need to wait until the next waiter has taken the rtmutex\n\t * to guarantee consistent state. Keep it simple. Userspace asked\n\t * for this wreckaged state.\n\t *\n\t * The rtmutex has an owner - either current or some other\n\t * task. See the EAGAIN loop above.\n\t */\n\tpi_state_update_owner(pi_state, rt_mutex_owner(&pi_state->pi_mutex));\n\n\treturn err;\n}"
  },
  {
    "function_name": "wake_futex_pi",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "613-681",
    "snippet": "static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_state)\n{\n\tstruct rt_mutex_waiter *top_waiter;\n\tstruct task_struct *new_owner;\n\tbool postunlock = false;\n\tDEFINE_RT_WAKE_Q(wqh);\n\tu32 curval, newval;\n\tint ret = 0;\n\n\ttop_waiter = rt_mutex_top_waiter(&pi_state->pi_mutex);\n\tif (WARN_ON_ONCE(!top_waiter)) {\n\t\t/*\n\t\t * As per the comment in futex_unlock_pi() this should not happen.\n\t\t *\n\t\t * When this happens, give up our locks and try again, giving\n\t\t * the futex_lock_pi() instance time to complete, either by\n\t\t * waiting on the rtmutex or removing itself from the futex\n\t\t * queue.\n\t\t */\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tnew_owner = top_waiter->task;\n\n\t/*\n\t * We pass it to the next owner. The WAITERS bit is always kept\n\t * enabled while there is PI state around. We cleanup the owner\n\t * died bit, because we are the owner.\n\t */\n\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\tif (unlikely(should_fail_futex(true))) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tret = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (!ret && (curval != uval)) {\n\t\t/*\n\t\t * If a unconditional UNLOCK_PI operation (user space did not\n\t\t * try the TID->0 transition) raced with a waiter setting the\n\t\t * FUTEX_WAITERS flag between get_user() and locking the hash\n\t\t * bucket lock, retry the operation.\n\t\t */\n\t\tif ((FUTEX_TID_MASK & curval) == uval)\n\t\t\tret = -EAGAIN;\n\t\telse\n\t\t\tret = -EINVAL;\n\t}\n\n\tif (!ret) {\n\t\t/*\n\t\t * This is a point of no return; once we modified the uval\n\t\t * there is no going back and subsequent operations must\n\t\t * not fail.\n\t\t */\n\t\tpi_state_update_owner(pi_state, new_owner);\n\t\tpostunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wqh);\n\t}\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wqh);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_postunlock",
          "args": [
            "&wqh"
          ],
          "line": 678
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_postunlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "479-482",
          "snippet": "void __sched rt_mutex_postunlock(struct rt_wake_q_head *wqh)\n{\n\trt_mutex_wake_up_q(wqh);\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_postunlock(struct rt_wake_q_head *wqh)\n{\n\trt_mutex_wake_up_q(wqh);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 675
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_futex_unlock",
          "args": [
            "&pi_state->pi_mutex",
            "&wqh"
          ],
          "line": 671
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_futex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "165-186",
          "snippet": "bool __sched __rt_mutex_futex_unlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct rt_wake_q_head *wqh)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wqh, lock);\n\n\treturn true; /* call postunlock() */\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool __sched __rt_mutex_futex_unlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct rt_wake_q_head *wqh)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wqh, lock);\n\n\treturn true; /* call postunlock() */\n}"
        }
      },
      {
        "call_info": {
          "callee": "pi_state_update_owner",
          "args": [
            "pi_state",
            "new_owner"
          ],
          "line": 670
        },
        "resolved": true,
        "details": {
          "function_name": "pi_state_update_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "45-66",
          "snippet": "static void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_cmpxchg_value_locked",
          "args": [
            "&curval",
            "uaddr",
            "uval",
            "newval"
          ],
          "line": 650
        },
        "resolved": true,
        "details": {
          "function_name": "futex_cmpxchg_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "440-449",
          "snippet": "int futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "should_fail_futex(true)"
          ],
          "line": 645
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "should_fail_futex",
          "args": [
            "true"
          ],
          "line": 645
        },
        "resolved": true,
        "details": {
          "function_name": "should_fail_futex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "77-83",
          "snippet": "bool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nbool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_pid_vnr",
          "args": [
            "new_owner"
          ],
          "line": 643
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "!top_waiter"
          ],
          "line": 623
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "&pi_state->pi_mutex"
          ],
          "line": 622
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_RT_WAKE_Q",
          "args": [
            "wqh"
          ],
          "line": 618
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_state)\n{\n\tstruct rt_mutex_waiter *top_waiter;\n\tstruct task_struct *new_owner;\n\tbool postunlock = false;\n\tDEFINE_RT_WAKE_Q(wqh);\n\tu32 curval, newval;\n\tint ret = 0;\n\n\ttop_waiter = rt_mutex_top_waiter(&pi_state->pi_mutex);\n\tif (WARN_ON_ONCE(!top_waiter)) {\n\t\t/*\n\t\t * As per the comment in futex_unlock_pi() this should not happen.\n\t\t *\n\t\t * When this happens, give up our locks and try again, giving\n\t\t * the futex_lock_pi() instance time to complete, either by\n\t\t * waiting on the rtmutex or removing itself from the futex\n\t\t * queue.\n\t\t */\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tnew_owner = top_waiter->task;\n\n\t/*\n\t * We pass it to the next owner. The WAITERS bit is always kept\n\t * enabled while there is PI state around. We cleanup the owner\n\t * died bit, because we are the owner.\n\t */\n\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\tif (unlikely(should_fail_futex(true))) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tret = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (!ret && (curval != uval)) {\n\t\t/*\n\t\t * If a unconditional UNLOCK_PI operation (user space did not\n\t\t * try the TID->0 transition) raced with a waiter setting the\n\t\t * FUTEX_WAITERS flag between get_user() and locking the hash\n\t\t * bucket lock, retry the operation.\n\t\t */\n\t\tif ((FUTEX_TID_MASK & curval) == uval)\n\t\t\tret = -EAGAIN;\n\t\telse\n\t\t\tret = -EINVAL;\n\t}\n\n\tif (!ret) {\n\t\t/*\n\t\t * This is a point of no return; once we modified the uval\n\t\t * there is no going back and subsequent operations must\n\t\t * not fail.\n\t\t */\n\t\tpi_state_update_owner(pi_state, new_owner);\n\t\tpostunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wqh);\n\t}\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wqh);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "futex_lock_pi_atomic",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "514-608",
    "snippet": "int futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t union futex_key *key,\n\t\t\t struct futex_pi_state **ps,\n\t\t\t struct task_struct *task,\n\t\t\t struct task_struct **exiting,\n\t\t\t int set_waiters)\n{\n\tu32 uval, newval, vpid = task_pid_vnr(task);\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\t/*\n\t * Read the user space value first so we can validate a few\n\t * things before proceeding further.\n\t */\n\tif (futex_get_value_locked(&uval, uaddr))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((uval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\tif ((unlikely(should_fail_futex(true))))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Lookup existing state first. If it exists, try to attach to\n\t * its pi_state.\n\t */\n\ttop_waiter = futex_top_waiter(hb, key);\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * No waiter and user TID is 0. We are here because the\n\t * waiters or the owner died bit is set or called from\n\t * requeue_cmp_pi or for whatever reason something took the\n\t * syscall.\n\t */\n\tif (!(uval & FUTEX_TID_MASK)) {\n\t\t/*\n\t\t * We take over the futex. No other waiters and the user space\n\t\t * TID is 0. We preserve the owner died bit.\n\t\t */\n\t\tnewval = uval & FUTEX_OWNER_DIED;\n\t\tnewval |= vpid;\n\n\t\t/* The futex requeue_pi code can enforce the waiters bit */\n\t\tif (set_waiters)\n\t\t\tnewval |= FUTEX_WAITERS;\n\n\t\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * If the waiter bit was requested the caller also needs PI\n\t\t * state attached to the new owner of the user space futex.\n\t\t *\n\t\t * @task is guaranteed to be alive and it cannot be exiting\n\t\t * because it is either sleeping or waiting in\n\t\t * futex_requeue_pi_wakeup_sync().\n\t\t *\n\t\t * No need to do the full attach_to_pi_owner() exercise\n\t\t * because @task is known and valid.\n\t\t */\n\t\tif (set_waiters) {\n\t\t\traw_spin_lock_irq(&task->pi_lock);\n\t\t\t__attach_to_pi_owner(task, key, ps);\n\t\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\t}\n\t\treturn 1;\n\t}\n\n\t/*\n\t * First waiter. Set the waiters bit before attaching ourself to\n\t * the owner. If owner tries to unlock, it will be forced into\n\t * the kernel and blocked on hb->lock.\n\t */\n\tnewval = uval | FUTEX_WAITERS;\n\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\tif (ret)\n\t\treturn ret;\n\t/*\n\t * If the update of the user space value succeeded, we try to\n\t * attach to the owner. If that fails, no harm done, we only\n\t * set the FUTEX_WAITERS bit in the user space variable.\n\t */\n\treturn attach_to_pi_owner(uaddr, newval, key, ps, exiting);\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "attach_to_pi_owner",
          "args": [
            "uaddr",
            "newval",
            "key",
            "ps",
            "exiting"
          ],
          "line": 607
        },
        "resolved": true,
        "details": {
          "function_name": "attach_to_pi_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "410-472",
          "snippet": "static int attach_to_pi_owner(u32 __user *uaddr, u32 uval, union futex_key *key,\n\t\t\t      struct futex_pi_state **ps,\n\t\t\t      struct task_struct **exiting)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tstruct task_struct *p;\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0 [1]\n\t *\n\t * The !pid check is paranoid. None of the call sites should end up\n\t * with pid == 0, but better safe than sorry. Let the caller retry\n\t */\n\tif (!pid)\n\t\treturn -EAGAIN;\n\tp = find_get_task_by_vpid(pid);\n\tif (!p)\n\t\treturn handle_exit_race(uaddr, uval, NULL);\n\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state to figure out, whether the\n\t * task is exiting. To protect against the change of the task state\n\t * in futex_exit_release(), we do this protected by p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->futex_state != FUTEX_STATE_OK)) {\n\t\t/*\n\t\t * The task is on the way out. When the futex state is\n\t\t * FUTEX_STATE_DEAD, we know that the task has finished\n\t\t * the cleanup:\n\t\t */\n\t\tint ret = handle_exit_race(uaddr, uval, p);\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\t/*\n\t\t * If the owner task is between FUTEX_STATE_EXITING and\n\t\t * FUTEX_STATE_DEAD then store the task pointer and keep\n\t\t * the reference on the task struct. The calling code will\n\t\t * drop all locks, wait for the task to reach\n\t\t * FUTEX_STATE_DEAD and then drop the refcount. This is\n\t\t * required to prevent a live lock when the current task\n\t\t * preempted the exiting task between the two states.\n\t\t */\n\t\tif (ret == -EBUSY)\n\t\t\t*exiting = p;\n\t\telse\n\t\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\t__attach_to_pi_owner(p, key, ps);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int attach_to_pi_owner(u32 __user *uaddr, u32 uval, union futex_key *key,\n\t\t\t      struct futex_pi_state **ps,\n\t\t\t      struct task_struct **exiting)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tstruct task_struct *p;\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0 [1]\n\t *\n\t * The !pid check is paranoid. None of the call sites should end up\n\t * with pid == 0, but better safe than sorry. Let the caller retry\n\t */\n\tif (!pid)\n\t\treturn -EAGAIN;\n\tp = find_get_task_by_vpid(pid);\n\tif (!p)\n\t\treturn handle_exit_race(uaddr, uval, NULL);\n\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state to figure out, whether the\n\t * task is exiting. To protect against the change of the task state\n\t * in futex_exit_release(), we do this protected by p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->futex_state != FUTEX_STATE_OK)) {\n\t\t/*\n\t\t * The task is on the way out. When the futex state is\n\t\t * FUTEX_STATE_DEAD, we know that the task has finished\n\t\t * the cleanup:\n\t\t */\n\t\tint ret = handle_exit_race(uaddr, uval, p);\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\t/*\n\t\t * If the owner task is between FUTEX_STATE_EXITING and\n\t\t * FUTEX_STATE_DEAD then store the task pointer and keep\n\t\t * the reference on the task struct. The calling code will\n\t\t * drop all locks, wait for the task to reach\n\t\t * FUTEX_STATE_DEAD and then drop the refcount. This is\n\t\t * required to prevent a live lock when the current task\n\t\t * preempted the exiting task between the two states.\n\t\t */\n\t\tif (ret == -EBUSY)\n\t\t\t*exiting = p;\n\t\telse\n\t\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\t__attach_to_pi_owner(p, key, ps);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lock_pi_update_atomic",
          "args": [
            "uaddr",
            "uval",
            "newval"
          ],
          "line": 599
        },
        "resolved": true,
        "details": {
          "function_name": "lock_pi_update_atomic",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "474-488",
          "snippet": "static int lock_pi_update_atomic(u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint err;\n\tu32 curval;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t/* If user space value changed, let the caller retry */\n\treturn curval != uval ? -EAGAIN : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int lock_pi_update_atomic(u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint err;\n\tu32 curval;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t/* If user space value changed, let the caller retry */\n\treturn curval != uval ? -EAGAIN : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&task->pi_lock"
          ],
          "line": 588
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__attach_to_pi_owner",
          "args": [
            "task",
            "key",
            "ps"
          ],
          "line": 587
        },
        "resolved": true,
        "details": {
          "function_name": "__attach_to_pi_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "376-405",
          "snippet": "static void __attach_to_pi_owner(struct task_struct *p, union futex_key *key,\n\t\t\t\t struct futex_pi_state **ps)\n{\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tstruct futex_pi_state *pi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\n\t*ps = pi_state;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic void __attach_to_pi_owner(struct task_struct *p, union futex_key *key,\n\t\t\t\t struct futex_pi_state **ps)\n{\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tstruct futex_pi_state *pi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\n\t*ps = pi_state;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&task->pi_lock"
          ],
          "line": 586
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "attach_to_pi_state",
          "args": [
            "uaddr",
            "uval",
            "top_waiter->pi_state",
            "ps"
          ],
          "line": 550
        },
        "resolved": true,
        "details": {
          "function_name": "attach_to_pi_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "202-318",
          "snippet": "static int attach_to_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t      struct futex_pi_state *pi_state,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tu32 uval2;\n\tint ret;\n\n\t/*\n\t * Userspace might have messed up non-PI and PI futexes [3]\n\t */\n\tif (unlikely(!pi_state))\n\t\treturn -EINVAL;\n\n\t/*\n\t * We get here with hb->lock held, and having found a\n\t * futex_top_waiter(). This means that futex_lock_pi() of said futex_q\n\t * has dropped the hb->lock in between futex_queue() and futex_unqueue_pi(),\n\t * which in turn means that futex_lock_pi() still has a reference on\n\t * our pi_state.\n\t *\n\t * The waiter holding a reference on @pi_state also protects against\n\t * the unlocked put_pi_state() in futex_unlock_pi(), futex_lock_pi()\n\t * and futex_wait_requeue_pi() as it cannot go to 0 and consequently\n\t * free pi_state before we can take a reference ourselves.\n\t */\n\tWARN_ON(!refcount_read(&pi_state->refcount));\n\n\t/*\n\t * Now that we have a pi_state, we can acquire wait_lock\n\t * and do the state validation.\n\t */\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Since {uval, pi_state} is serialized by wait_lock, and our current\n\t * uval was read without holding it, it can have changed. Verify it\n\t * still is what we expect it to be, otherwise retry the entire\n\t * operation.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\tgoto out_efault;\n\n\tif (uval != uval2)\n\t\tgoto out_eagain;\n\n\t/*\n\t * Handle the owner died case:\n\t */\n\tif (uval & FUTEX_OWNER_DIED) {\n\t\t/*\n\t\t * exit_pi_state_list sets owner to NULL and wakes the\n\t\t * topmost waiter. The task which acquires the\n\t\t * pi_state->rt_mutex will fixup owner.\n\t\t */\n\t\tif (!pi_state->owner) {\n\t\t\t/*\n\t\t\t * No pi state owner, but the user space TID\n\t\t\t * is not 0. Inconsistent state. [5]\n\t\t\t */\n\t\t\tif (pid)\n\t\t\t\tgoto out_einval;\n\t\t\t/*\n\t\t\t * Take a ref on the state and return success. [4]\n\t\t\t */\n\t\t\tgoto out_attach;\n\t\t}\n\n\t\t/*\n\t\t * If TID is 0, then either the dying owner has not\n\t\t * yet executed exit_pi_state_list() or some waiter\n\t\t * acquired the rtmutex in the pi state, but did not\n\t\t * yet fixup the TID in user space.\n\t\t *\n\t\t * Take a ref on the state and return success. [6]\n\t\t */\n\t\tif (!pid)\n\t\t\tgoto out_attach;\n\t} else {\n\t\t/*\n\t\t * If the owner died bit is not set, then the pi_state\n\t\t * must have an owner. [7]\n\t\t */\n\t\tif (!pi_state->owner)\n\t\t\tgoto out_einval;\n\t}\n\n\t/*\n\t * Bail out if user space manipulated the futex value. If pi\n\t * state exists then the owner TID must be the same as the\n\t * user space TID. [9/10]\n\t */\n\tif (pid != task_pid_vnr(pi_state->owner))\n\t\tgoto out_einval;\n\nout_attach:\n\tget_pi_state(pi_state);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t*ps = pi_state;\n\treturn 0;\n\nout_einval:\n\tret = -EINVAL;\n\tgoto out_error;\n\nout_eagain:\n\tret = -EAGAIN;\n\tgoto out_error;\n\nout_efault:\n\tret = -EFAULT;\n\tgoto out_error;\n\nout_error:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int attach_to_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t      struct futex_pi_state *pi_state,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tu32 uval2;\n\tint ret;\n\n\t/*\n\t * Userspace might have messed up non-PI and PI futexes [3]\n\t */\n\tif (unlikely(!pi_state))\n\t\treturn -EINVAL;\n\n\t/*\n\t * We get here with hb->lock held, and having found a\n\t * futex_top_waiter(). This means that futex_lock_pi() of said futex_q\n\t * has dropped the hb->lock in between futex_queue() and futex_unqueue_pi(),\n\t * which in turn means that futex_lock_pi() still has a reference on\n\t * our pi_state.\n\t *\n\t * The waiter holding a reference on @pi_state also protects against\n\t * the unlocked put_pi_state() in futex_unlock_pi(), futex_lock_pi()\n\t * and futex_wait_requeue_pi() as it cannot go to 0 and consequently\n\t * free pi_state before we can take a reference ourselves.\n\t */\n\tWARN_ON(!refcount_read(&pi_state->refcount));\n\n\t/*\n\t * Now that we have a pi_state, we can acquire wait_lock\n\t * and do the state validation.\n\t */\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Since {uval, pi_state} is serialized by wait_lock, and our current\n\t * uval was read without holding it, it can have changed. Verify it\n\t * still is what we expect it to be, otherwise retry the entire\n\t * operation.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\tgoto out_efault;\n\n\tif (uval != uval2)\n\t\tgoto out_eagain;\n\n\t/*\n\t * Handle the owner died case:\n\t */\n\tif (uval & FUTEX_OWNER_DIED) {\n\t\t/*\n\t\t * exit_pi_state_list sets owner to NULL and wakes the\n\t\t * topmost waiter. The task which acquires the\n\t\t * pi_state->rt_mutex will fixup owner.\n\t\t */\n\t\tif (!pi_state->owner) {\n\t\t\t/*\n\t\t\t * No pi state owner, but the user space TID\n\t\t\t * is not 0. Inconsistent state. [5]\n\t\t\t */\n\t\t\tif (pid)\n\t\t\t\tgoto out_einval;\n\t\t\t/*\n\t\t\t * Take a ref on the state and return success. [4]\n\t\t\t */\n\t\t\tgoto out_attach;\n\t\t}\n\n\t\t/*\n\t\t * If TID is 0, then either the dying owner has not\n\t\t * yet executed exit_pi_state_list() or some waiter\n\t\t * acquired the rtmutex in the pi state, but did not\n\t\t * yet fixup the TID in user space.\n\t\t *\n\t\t * Take a ref on the state and return success. [6]\n\t\t */\n\t\tif (!pid)\n\t\t\tgoto out_attach;\n\t} else {\n\t\t/*\n\t\t * If the owner died bit is not set, then the pi_state\n\t\t * must have an owner. [7]\n\t\t */\n\t\tif (!pi_state->owner)\n\t\t\tgoto out_einval;\n\t}\n\n\t/*\n\t * Bail out if user space manipulated the futex value. If pi\n\t * state exists then the owner TID must be the same as the\n\t * user space TID. [9/10]\n\t */\n\tif (pid != task_pid_vnr(pi_state->owner))\n\t\tgoto out_einval;\n\nout_attach:\n\tget_pi_state(pi_state);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t*ps = pi_state;\n\treturn 0;\n\nout_einval:\n\tret = -EINVAL;\n\tgoto out_error;\n\nout_eagain:\n\tret = -EAGAIN;\n\tgoto out_error;\n\nout_efault:\n\tret = -EFAULT;\n\tgoto out_error;\n\nout_error:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "futex_top_waiter",
          "args": [
            "hb",
            "key"
          ],
          "line": 548
        },
        "resolved": true,
        "details": {
          "function_name": "futex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "429-438",
          "snippet": "struct futex_q *futex_top_waiter(struct futex_hash_bucket *hb, union futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (futex_match(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nstruct futex_q *futex_top_waiter(struct futex_hash_bucket *hb, union futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (futex_match(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "should_fail_futex(true)"
          ],
          "line": 541
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "should_fail_futex",
          "args": [
            "true"
          ],
          "line": 541
        },
        "resolved": true,
        "details": {
          "function_name": "should_fail_futex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "77-83",
          "snippet": "bool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nbool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "(uval & FUTEX_TID_MASK) == vpid"
          ],
          "line": 538
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "should_fail_futex(true)"
          ],
          "line": 532
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_get_value_locked",
          "args": [
            "&uval",
            "uaddr"
          ],
          "line": 529
        },
        "resolved": true,
        "details": {
          "function_name": "futex_get_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "451-460",
          "snippet": "int futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_pid_vnr",
          "args": [
            "task"
          ],
          "line": 521
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t union futex_key *key,\n\t\t\t struct futex_pi_state **ps,\n\t\t\t struct task_struct *task,\n\t\t\t struct task_struct **exiting,\n\t\t\t int set_waiters)\n{\n\tu32 uval, newval, vpid = task_pid_vnr(task);\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\t/*\n\t * Read the user space value first so we can validate a few\n\t * things before proceeding further.\n\t */\n\tif (futex_get_value_locked(&uval, uaddr))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((uval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\tif ((unlikely(should_fail_futex(true))))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Lookup existing state first. If it exists, try to attach to\n\t * its pi_state.\n\t */\n\ttop_waiter = futex_top_waiter(hb, key);\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * No waiter and user TID is 0. We are here because the\n\t * waiters or the owner died bit is set or called from\n\t * requeue_cmp_pi or for whatever reason something took the\n\t * syscall.\n\t */\n\tif (!(uval & FUTEX_TID_MASK)) {\n\t\t/*\n\t\t * We take over the futex. No other waiters and the user space\n\t\t * TID is 0. We preserve the owner died bit.\n\t\t */\n\t\tnewval = uval & FUTEX_OWNER_DIED;\n\t\tnewval |= vpid;\n\n\t\t/* The futex requeue_pi code can enforce the waiters bit */\n\t\tif (set_waiters)\n\t\t\tnewval |= FUTEX_WAITERS;\n\n\t\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * If the waiter bit was requested the caller also needs PI\n\t\t * state attached to the new owner of the user space futex.\n\t\t *\n\t\t * @task is guaranteed to be alive and it cannot be exiting\n\t\t * because it is either sleeping or waiting in\n\t\t * futex_requeue_pi_wakeup_sync().\n\t\t *\n\t\t * No need to do the full attach_to_pi_owner() exercise\n\t\t * because @task is known and valid.\n\t\t */\n\t\tif (set_waiters) {\n\t\t\traw_spin_lock_irq(&task->pi_lock);\n\t\t\t__attach_to_pi_owner(task, key, ps);\n\t\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\t}\n\t\treturn 1;\n\t}\n\n\t/*\n\t * First waiter. Set the waiters bit before attaching ourself to\n\t * the owner. If owner tries to unlock, it will be forced into\n\t * the kernel and blocked on hb->lock.\n\t */\n\tnewval = uval | FUTEX_WAITERS;\n\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\tif (ret)\n\t\treturn ret;\n\t/*\n\t * If the update of the user space value succeeded, we try to\n\t * attach to the owner. If that fails, no harm done, we only\n\t * set the FUTEX_WAITERS bit in the user space variable.\n\t */\n\treturn attach_to_pi_owner(uaddr, newval, key, ps, exiting);\n}"
  },
  {
    "function_name": "lock_pi_update_atomic",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "474-488",
    "snippet": "static int lock_pi_update_atomic(u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint err;\n\tu32 curval;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t/* If user space value changed, let the caller retry */\n\treturn curval != uval ? -EAGAIN : 0;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "err"
          ],
          "line": 483
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_cmpxchg_value_locked",
          "args": [
            "&curval",
            "uaddr",
            "uval",
            "newval"
          ],
          "line": 482
        },
        "resolved": true,
        "details": {
          "function_name": "futex_cmpxchg_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "440-449",
          "snippet": "int futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "should_fail_futex(true)"
          ],
          "line": 479
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "should_fail_futex",
          "args": [
            "true"
          ],
          "line": 479
        },
        "resolved": true,
        "details": {
          "function_name": "should_fail_futex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "77-83",
          "snippet": "bool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nbool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int lock_pi_update_atomic(u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint err;\n\tu32 curval;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = futex_cmpxchg_value_locked(&curval, uaddr, uval, newval);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t/* If user space value changed, let the caller retry */\n\treturn curval != uval ? -EAGAIN : 0;\n}"
  },
  {
    "function_name": "attach_to_pi_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "410-472",
    "snippet": "static int attach_to_pi_owner(u32 __user *uaddr, u32 uval, union futex_key *key,\n\t\t\t      struct futex_pi_state **ps,\n\t\t\t      struct task_struct **exiting)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tstruct task_struct *p;\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0 [1]\n\t *\n\t * The !pid check is paranoid. None of the call sites should end up\n\t * with pid == 0, but better safe than sorry. Let the caller retry\n\t */\n\tif (!pid)\n\t\treturn -EAGAIN;\n\tp = find_get_task_by_vpid(pid);\n\tif (!p)\n\t\treturn handle_exit_race(uaddr, uval, NULL);\n\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state to figure out, whether the\n\t * task is exiting. To protect against the change of the task state\n\t * in futex_exit_release(), we do this protected by p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->futex_state != FUTEX_STATE_OK)) {\n\t\t/*\n\t\t * The task is on the way out. When the futex state is\n\t\t * FUTEX_STATE_DEAD, we know that the task has finished\n\t\t * the cleanup:\n\t\t */\n\t\tint ret = handle_exit_race(uaddr, uval, p);\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\t/*\n\t\t * If the owner task is between FUTEX_STATE_EXITING and\n\t\t * FUTEX_STATE_DEAD then store the task pointer and keep\n\t\t * the reference on the task struct. The calling code will\n\t\t * drop all locks, wait for the task to reach\n\t\t * FUTEX_STATE_DEAD and then drop the refcount. This is\n\t\t * required to prevent a live lock when the current task\n\t\t * preempted the exiting task between the two states.\n\t\t */\n\t\tif (ret == -EBUSY)\n\t\t\t*exiting = p;\n\t\telse\n\t\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\t__attach_to_pi_owner(p, key, ps);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\treturn 0;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "put_task_struct",
          "args": [
            "p"
          ],
          "line": 469
        },
        "resolved": true,
        "details": {
          "function_name": "__put_task_struct",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/fork.c",
          "lines": "745-761",
          "snippet": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tio_uring_free(tsk);\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\tbpf_task_storage_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\tsched_core_free(tsk);\n\tfree_task(tsk);\n}",
          "includes": [
            "#include <linux/init_task.h>",
            "#include <trace/events/task.h>",
            "#include <trace/events/sched.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/pgalloc.h>",
            "#include <linux/bpf.h>",
            "#include <linux/io_uring.h>",
            "#include <linux/scs.h>",
            "#include <linux/kasan.h>",
            "#include <linux/stackleak.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/kcov.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/compiler.h>",
            "#include <linux/aio.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/oom.h>",
            "#include <linux/user-return-notifier.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/magic.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tty.h>",
            "#include <linux/random.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/acct.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/profile.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/audit.h>",
            "#include <linux/mount.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/kthread.h>",
            "#include <linux/compat.h>",
            "#include <linux/futex.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/seccomp.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/cpu.h>",
            "#include <linux/capability.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/mman.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/sem.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/module.h>",
            "#include <linux/unistd.h>",
            "#include <linux/init.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/anon_inodes.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __latent_entropy struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/init_task.h>\n#include <trace/events/task.h>\n#include <trace/events/sched.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <linux/bpf.h>\n#include <linux/io_uring.h>\n#include <linux/scs.h>\n#include <linux/kasan.h>\n#include <linux/stackleak.h>\n#include <linux/thread_info.h>\n#include <linux/livepatch.h>\n#include <linux/kcov.h>\n#include <linux/sysctl.h>\n#include <linux/compiler.h>\n#include <linux/aio.h>\n#include <linux/uprobes.h>\n#include <linux/signalfd.h>\n#include <linux/khugepaged.h>\n#include <linux/oom.h>\n#include <linux/user-return-notifier.h>\n#include <linux/posix-timers.h>\n#include <linux/perf_event.h>\n#include <linux/magic.h>\n#include <linux/fs_struct.h>\n#include <linux/tty.h>\n#include <linux/random.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/freezer.h>\n#include <linux/cn_proc.h>\n#include <linux/tsacct_kern.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/acct.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/profile.h>\n#include <linux/proc_fs.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/audit.h>\n#include <linux/mount.h>\n#include <linux/ptrace.h>\n#include <linux/rcupdate.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/kthread.h>\n#include <linux/compat.h>\n#include <linux/futex.h>\n#include <linux/jiffies.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/seccomp.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/cgroup.h>\n#include <linux/cpu.h>\n#include <linux/capability.h>\n#include <linux/nsproxy.h>\n#include <linux/vmacache.h>\n#include <linux/mm_inline.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mman.h>\n#include <linux/binfmts.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/sem.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/vmalloc.h>\n#include <linux/module.h>\n#include <linux/unistd.h>\n#include <linux/init.h>\n#include <linux/rtmutex.h>\n#include <linux/seq_file.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/user.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/anon_inodes.h>\n\nstatic __latent_entropy struct;\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tio_uring_free(tsk);\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\tbpf_task_storage_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\tsched_core_free(tsk);\n\tfree_task(tsk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&p->pi_lock"
          ],
          "line": 467
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__attach_to_pi_owner",
          "args": [
            "p",
            "key",
            "ps"
          ],
          "line": 466
        },
        "resolved": true,
        "details": {
          "function_name": "__attach_to_pi_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "376-405",
          "snippet": "static void __attach_to_pi_owner(struct task_struct *p, union futex_key *key,\n\t\t\t\t struct futex_pi_state **ps)\n{\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tstruct futex_pi_state *pi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\n\t*ps = pi_state;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic void __attach_to_pi_owner(struct task_struct *p, union futex_key *key,\n\t\t\t\t struct futex_pi_state **ps)\n{\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tstruct futex_pi_state *pi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\n\t*ps = pi_state;\n}"
        }
      },
      {
        "call_info": {
          "callee": "handle_exit_race",
          "args": [
            "uaddr",
            "uval",
            "p"
          ],
          "line": 447
        },
        "resolved": true,
        "details": {
          "function_name": "handle_exit_race",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "320-374",
          "snippet": "static int handle_exit_race(u32 __user *uaddr, u32 uval,\n\t\t\t    struct task_struct *tsk)\n{\n\tu32 uval2;\n\n\t/*\n\t * If the futex exit state is not yet FUTEX_STATE_DEAD, tell the\n\t * caller that the alleged owner is busy.\n\t */\n\tif (tsk && tsk->futex_state != FUTEX_STATE_DEAD)\n\t\treturn -EBUSY;\n\n\t/*\n\t * Reread the user space value to handle the following situation:\n\t *\n\t * CPU0\t\t\t\tCPU1\n\t *\n\t * sys_exit()\t\t\tsys_futex()\n\t *  do_exit()\t\t\t futex_lock_pi()\n\t *                                futex_lock_pi_atomic()\n\t *   exit_signals(tsk)\t\t    No waiters:\n\t *    tsk->flags |= PF_EXITING;\t    *uaddr == 0x00000PID\n\t *  mm_release(tsk)\t\t    Set waiter bit\n\t *   exit_robust_list(tsk) {\t    *uaddr = 0x80000PID;\n\t *      Set owner died\t\t    attach_to_pi_owner() {\n\t *    *uaddr = 0xC0000000;\t     tsk = get_task(PID);\n\t *   }\t\t\t\t     if (!tsk->flags & PF_EXITING) {\n\t *  ...\t\t\t\t       attach();\n\t *  tsk->futex_state =               } else {\n\t *\tFUTEX_STATE_DEAD;              if (tsk->futex_state !=\n\t *\t\t\t\t\t  FUTEX_STATE_DEAD)\n\t *\t\t\t\t         return -EAGAIN;\n\t *\t\t\t\t       return -ESRCH; <--- FAIL\n\t *\t\t\t\t     }\n\t *\n\t * Returning ESRCH unconditionally is wrong here because the\n\t * user space value has been changed by the exiting task.\n\t *\n\t * The same logic applies to the case where the exiting task is\n\t * already gone.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\treturn -EFAULT;\n\n\t/* If the user space value has changed, try again. */\n\tif (uval2 != uval)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * The exiting task did not have a robust list, the robust list was\n\t * corrupted or the user space value in *uaddr is simply bogus.\n\t * Give up and tell user space.\n\t */\n\treturn -ESRCH;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int handle_exit_race(u32 __user *uaddr, u32 uval,\n\t\t\t    struct task_struct *tsk)\n{\n\tu32 uval2;\n\n\t/*\n\t * If the futex exit state is not yet FUTEX_STATE_DEAD, tell the\n\t * caller that the alleged owner is busy.\n\t */\n\tif (tsk && tsk->futex_state != FUTEX_STATE_DEAD)\n\t\treturn -EBUSY;\n\n\t/*\n\t * Reread the user space value to handle the following situation:\n\t *\n\t * CPU0\t\t\t\tCPU1\n\t *\n\t * sys_exit()\t\t\tsys_futex()\n\t *  do_exit()\t\t\t futex_lock_pi()\n\t *                                futex_lock_pi_atomic()\n\t *   exit_signals(tsk)\t\t    No waiters:\n\t *    tsk->flags |= PF_EXITING;\t    *uaddr == 0x00000PID\n\t *  mm_release(tsk)\t\t    Set waiter bit\n\t *   exit_robust_list(tsk) {\t    *uaddr = 0x80000PID;\n\t *      Set owner died\t\t    attach_to_pi_owner() {\n\t *    *uaddr = 0xC0000000;\t     tsk = get_task(PID);\n\t *   }\t\t\t\t     if (!tsk->flags & PF_EXITING) {\n\t *  ...\t\t\t\t       attach();\n\t *  tsk->futex_state =               } else {\n\t *\tFUTEX_STATE_DEAD;              if (tsk->futex_state !=\n\t *\t\t\t\t\t  FUTEX_STATE_DEAD)\n\t *\t\t\t\t         return -EAGAIN;\n\t *\t\t\t\t       return -ESRCH; <--- FAIL\n\t *\t\t\t\t     }\n\t *\n\t * Returning ESRCH unconditionally is wrong here because the\n\t * user space value has been changed by the exiting task.\n\t *\n\t * The same logic applies to the case where the exiting task is\n\t * already gone.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\treturn -EFAULT;\n\n\t/* If the user space value has changed, try again. */\n\tif (uval2 != uval)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * The exiting task did not have a robust list, the robust list was\n\t * corrupted or the user space value in *uaddr is simply bogus.\n\t * Give up and tell user space.\n\t */\n\treturn -ESRCH;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "p->futex_state != FUTEX_STATE_OK"
          ],
          "line": 441
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&p->pi_lock"
          ],
          "line": 440
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "p->flags & PF_KTHREAD"
          ],
          "line": 430
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_get_task_by_vpid",
          "args": [
            "pid"
          ],
          "line": 426
        },
        "resolved": true,
        "details": {
          "function_name": "find_get_task_by_vpid",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/pid.c",
          "lines": "425-436",
          "snippet": "struct task_struct *find_get_task_by_vpid(pid_t nr)\n{\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\ttask = find_task_by_vpid(nr);\n\tif (task)\n\t\tget_task_struct(task);\n\trcu_read_unlock();\n\n\treturn task;\n}",
          "includes": [
            "#include <uapi/linux/pidfd.h>",
            "#include <net/sock.h>",
            "#include <linux/idr.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/anon_inodes.h>",
            "#include <linux/refcount.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/init_task.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/memblock.h>",
            "#include <linux/rculist.h>",
            "#include <linux/init.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <uapi/linux/pidfd.h>\n#include <net/sock.h>\n#include <linux/idr.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/anon_inodes.h>\n#include <linux/refcount.h>\n#include <linux/proc_ns.h>\n#include <linux/syscalls.h>\n#include <linux/init_task.h>\n#include <linux/pid_namespace.h>\n#include <linux/memblock.h>\n#include <linux/rculist.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n\nstruct task_struct *find_get_task_by_vpid(pid_t nr)\n{\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\ttask = find_task_by_vpid(nr);\n\tif (task)\n\t\tget_task_struct(task);\n\trcu_read_unlock();\n\n\treturn task;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int attach_to_pi_owner(u32 __user *uaddr, u32 uval, union futex_key *key,\n\t\t\t      struct futex_pi_state **ps,\n\t\t\t      struct task_struct **exiting)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tstruct task_struct *p;\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0 [1]\n\t *\n\t * The !pid check is paranoid. None of the call sites should end up\n\t * with pid == 0, but better safe than sorry. Let the caller retry\n\t */\n\tif (!pid)\n\t\treturn -EAGAIN;\n\tp = find_get_task_by_vpid(pid);\n\tif (!p)\n\t\treturn handle_exit_race(uaddr, uval, NULL);\n\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state to figure out, whether the\n\t * task is exiting. To protect against the change of the task state\n\t * in futex_exit_release(), we do this protected by p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->futex_state != FUTEX_STATE_OK)) {\n\t\t/*\n\t\t * The task is on the way out. When the futex state is\n\t\t * FUTEX_STATE_DEAD, we know that the task has finished\n\t\t * the cleanup:\n\t\t */\n\t\tint ret = handle_exit_race(uaddr, uval, p);\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\t/*\n\t\t * If the owner task is between FUTEX_STATE_EXITING and\n\t\t * FUTEX_STATE_DEAD then store the task pointer and keep\n\t\t * the reference on the task struct. The calling code will\n\t\t * drop all locks, wait for the task to reach\n\t\t * FUTEX_STATE_DEAD and then drop the refcount. This is\n\t\t * required to prevent a live lock when the current task\n\t\t * preempted the exiting task between the two states.\n\t\t */\n\t\tif (ret == -EBUSY)\n\t\t\t*exiting = p;\n\t\telse\n\t\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\t__attach_to_pi_owner(p, key, ps);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "__attach_to_pi_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "376-405",
    "snippet": "static void __attach_to_pi_owner(struct task_struct *p, union futex_key *key,\n\t\t\t\t struct futex_pi_state **ps)\n{\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tstruct futex_pi_state *pi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\n\t*ps = pi_state;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "list_add",
          "args": [
            "&pi_state->list",
            "&p->pi_state_list"
          ],
          "line": 397
        },
        "resolved": true,
        "details": {
          "function_name": "cmp_filterlist_addrs",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/kcsan/debugfs.c",
          "lines": "95-101",
          "snippet": "static int cmp_filterlist_addrs(const void *rhs, const void *lhs)\n{\n\tconst unsigned long a = *(const unsigned long *)rhs;\n\tconst unsigned long b = *(const unsigned long *)lhs;\n\n\treturn a < b ? -1 : a == b ? 0 : 1;\n}",
          "includes": [
            "#include \"kcsan.h\"",
            "#include <linux/uaccess.h>",
            "#include <linux/string.h>",
            "#include <linux/sort.h>",
            "#include <linux/slab.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/sched.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/init.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/bug.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"kcsan.h\"\n#include <linux/uaccess.h>\n#include <linux/string.h>\n#include <linux/sort.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/sched.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/debugfs.h>\n#include <linux/bug.h>\n#include <linux/bsearch.h>\n#include <linux/atomic.h>\n\nstatic int cmp_filterlist_addrs(const void *rhs, const void *lhs)\n{\n\tconst unsigned long a = *(const unsigned long *)rhs;\n\tconst unsigned long b = *(const unsigned long *)lhs;\n\n\treturn a < b ? -1 : a == b ? 0 : 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "!list_empty(&pi_state->list)"
          ],
          "line": 396
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&pi_state->list"
          ],
          "line": 396
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "41-44",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !READ_ONCE(rsclp->head);\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "long rcu_segcblist_n_segment_cbs(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_inc_len(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nlong rcu_segcblist_n_segment_cbs(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_inc_len(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !READ_ONCE(rsclp->head);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_init_proxy_locked",
          "args": [
            "&pi_state->pi_mutex",
            "p"
          ],
          "line": 391
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_init_proxy_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_api.c",
          "lines": "236-253",
          "snippet": "void __sched rt_mutex_init_proxy_locked(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct task_struct *proxy_owner)\n{\n\tstatic struct lock_class_key pi_futex_key;\n\n\t__rt_mutex_base_init(lock);\n\t/*\n\t * On PREEMPT_RT the futex hashbucket spinlock becomes 'sleeping'\n\t * and rtmutex based. That causes a lockdep false positive, because\n\t * some of the futex functions invoke spin_unlock(&hb->lock) with\n\t * the wait_lock of the rtmutex associated to the pi_futex held.\n\t * spin_unlock() in turn takes wait_lock of the rtmutex on which\n\t * the spinlock is based, which makes lockdep notice a lock\n\t * recursion. Give the futex/rtmutex wait_lock a separate key.\n\t */\n\tlockdep_set_class(&lock->wait_lock, &pi_futex_key);\n\trt_mutex_set_owner(lock, proxy_owner);\n}",
          "includes": [
            "#include \"rtmutex.c\"",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex.c\"\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_init_proxy_locked(struct rt_mutex_base *lock,\n\t\t\t\t\tstruct task_struct *proxy_owner)\n{\n\tstatic struct lock_class_key pi_futex_key;\n\n\t__rt_mutex_base_init(lock);\n\t/*\n\t * On PREEMPT_RT the futex hashbucket spinlock becomes 'sleeping'\n\t * and rtmutex based. That causes a lockdep false positive, because\n\t * some of the futex functions invoke spin_unlock(&hb->lock) with\n\t * the wait_lock of the rtmutex associated to the pi_futex held.\n\t * spin_unlock() in turn takes wait_lock of the rtmutex on which\n\t * the spinlock is based, which makes lockdep notice a lock\n\t * recursion. Give the futex/rtmutex wait_lock a separate key.\n\t */\n\tlockdep_set_class(&lock->wait_lock, &pi_futex_key);\n\trt_mutex_set_owner(lock, proxy_owner);\n}"
        }
      },
      {
        "call_info": {
          "callee": "alloc_pi_state",
          "args": [],
          "line": 385
        },
        "resolved": true,
        "details": {
          "function_name": "alloc_pi_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "35-43",
          "snippet": "static struct futex_pi_state *alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic struct futex_pi_state *alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic void __attach_to_pi_owner(struct task_struct *p, union futex_key *key,\n\t\t\t\t struct futex_pi_state **ps)\n{\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tstruct futex_pi_state *pi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\n\t*ps = pi_state;\n}"
  },
  {
    "function_name": "handle_exit_race",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "320-374",
    "snippet": "static int handle_exit_race(u32 __user *uaddr, u32 uval,\n\t\t\t    struct task_struct *tsk)\n{\n\tu32 uval2;\n\n\t/*\n\t * If the futex exit state is not yet FUTEX_STATE_DEAD, tell the\n\t * caller that the alleged owner is busy.\n\t */\n\tif (tsk && tsk->futex_state != FUTEX_STATE_DEAD)\n\t\treturn -EBUSY;\n\n\t/*\n\t * Reread the user space value to handle the following situation:\n\t *\n\t * CPU0\t\t\t\tCPU1\n\t *\n\t * sys_exit()\t\t\tsys_futex()\n\t *  do_exit()\t\t\t futex_lock_pi()\n\t *                                futex_lock_pi_atomic()\n\t *   exit_signals(tsk)\t\t    No waiters:\n\t *    tsk->flags |= PF_EXITING;\t    *uaddr == 0x00000PID\n\t *  mm_release(tsk)\t\t    Set waiter bit\n\t *   exit_robust_list(tsk) {\t    *uaddr = 0x80000PID;\n\t *      Set owner died\t\t    attach_to_pi_owner() {\n\t *    *uaddr = 0xC0000000;\t     tsk = get_task(PID);\n\t *   }\t\t\t\t     if (!tsk->flags & PF_EXITING) {\n\t *  ...\t\t\t\t       attach();\n\t *  tsk->futex_state =               } else {\n\t *\tFUTEX_STATE_DEAD;              if (tsk->futex_state !=\n\t *\t\t\t\t\t  FUTEX_STATE_DEAD)\n\t *\t\t\t\t         return -EAGAIN;\n\t *\t\t\t\t       return -ESRCH; <--- FAIL\n\t *\t\t\t\t     }\n\t *\n\t * Returning ESRCH unconditionally is wrong here because the\n\t * user space value has been changed by the exiting task.\n\t *\n\t * The same logic applies to the case where the exiting task is\n\t * already gone.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\treturn -EFAULT;\n\n\t/* If the user space value has changed, try again. */\n\tif (uval2 != uval)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * The exiting task did not have a robust list, the robust list was\n\t * corrupted or the user space value in *uaddr is simply bogus.\n\t * Give up and tell user space.\n\t */\n\treturn -ESRCH;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "futex_get_value_locked",
          "args": [
            "&uval2",
            "uaddr"
          ],
          "line": 361
        },
        "resolved": true,
        "details": {
          "function_name": "futex_get_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "451-460",
          "snippet": "int futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int handle_exit_race(u32 __user *uaddr, u32 uval,\n\t\t\t    struct task_struct *tsk)\n{\n\tu32 uval2;\n\n\t/*\n\t * If the futex exit state is not yet FUTEX_STATE_DEAD, tell the\n\t * caller that the alleged owner is busy.\n\t */\n\tif (tsk && tsk->futex_state != FUTEX_STATE_DEAD)\n\t\treturn -EBUSY;\n\n\t/*\n\t * Reread the user space value to handle the following situation:\n\t *\n\t * CPU0\t\t\t\tCPU1\n\t *\n\t * sys_exit()\t\t\tsys_futex()\n\t *  do_exit()\t\t\t futex_lock_pi()\n\t *                                futex_lock_pi_atomic()\n\t *   exit_signals(tsk)\t\t    No waiters:\n\t *    tsk->flags |= PF_EXITING;\t    *uaddr == 0x00000PID\n\t *  mm_release(tsk)\t\t    Set waiter bit\n\t *   exit_robust_list(tsk) {\t    *uaddr = 0x80000PID;\n\t *      Set owner died\t\t    attach_to_pi_owner() {\n\t *    *uaddr = 0xC0000000;\t     tsk = get_task(PID);\n\t *   }\t\t\t\t     if (!tsk->flags & PF_EXITING) {\n\t *  ...\t\t\t\t       attach();\n\t *  tsk->futex_state =               } else {\n\t *\tFUTEX_STATE_DEAD;              if (tsk->futex_state !=\n\t *\t\t\t\t\t  FUTEX_STATE_DEAD)\n\t *\t\t\t\t         return -EAGAIN;\n\t *\t\t\t\t       return -ESRCH; <--- FAIL\n\t *\t\t\t\t     }\n\t *\n\t * Returning ESRCH unconditionally is wrong here because the\n\t * user space value has been changed by the exiting task.\n\t *\n\t * The same logic applies to the case where the exiting task is\n\t * already gone.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\treturn -EFAULT;\n\n\t/* If the user space value has changed, try again. */\n\tif (uval2 != uval)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * The exiting task did not have a robust list, the robust list was\n\t * corrupted or the user space value in *uaddr is simply bogus.\n\t * Give up and tell user space.\n\t */\n\treturn -ESRCH;\n}"
  },
  {
    "function_name": "attach_to_pi_state",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "202-318",
    "snippet": "static int attach_to_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t      struct futex_pi_state *pi_state,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tu32 uval2;\n\tint ret;\n\n\t/*\n\t * Userspace might have messed up non-PI and PI futexes [3]\n\t */\n\tif (unlikely(!pi_state))\n\t\treturn -EINVAL;\n\n\t/*\n\t * We get here with hb->lock held, and having found a\n\t * futex_top_waiter(). This means that futex_lock_pi() of said futex_q\n\t * has dropped the hb->lock in between futex_queue() and futex_unqueue_pi(),\n\t * which in turn means that futex_lock_pi() still has a reference on\n\t * our pi_state.\n\t *\n\t * The waiter holding a reference on @pi_state also protects against\n\t * the unlocked put_pi_state() in futex_unlock_pi(), futex_lock_pi()\n\t * and futex_wait_requeue_pi() as it cannot go to 0 and consequently\n\t * free pi_state before we can take a reference ourselves.\n\t */\n\tWARN_ON(!refcount_read(&pi_state->refcount));\n\n\t/*\n\t * Now that we have a pi_state, we can acquire wait_lock\n\t * and do the state validation.\n\t */\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Since {uval, pi_state} is serialized by wait_lock, and our current\n\t * uval was read without holding it, it can have changed. Verify it\n\t * still is what we expect it to be, otherwise retry the entire\n\t * operation.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\tgoto out_efault;\n\n\tif (uval != uval2)\n\t\tgoto out_eagain;\n\n\t/*\n\t * Handle the owner died case:\n\t */\n\tif (uval & FUTEX_OWNER_DIED) {\n\t\t/*\n\t\t * exit_pi_state_list sets owner to NULL and wakes the\n\t\t * topmost waiter. The task which acquires the\n\t\t * pi_state->rt_mutex will fixup owner.\n\t\t */\n\t\tif (!pi_state->owner) {\n\t\t\t/*\n\t\t\t * No pi state owner, but the user space TID\n\t\t\t * is not 0. Inconsistent state. [5]\n\t\t\t */\n\t\t\tif (pid)\n\t\t\t\tgoto out_einval;\n\t\t\t/*\n\t\t\t * Take a ref on the state and return success. [4]\n\t\t\t */\n\t\t\tgoto out_attach;\n\t\t}\n\n\t\t/*\n\t\t * If TID is 0, then either the dying owner has not\n\t\t * yet executed exit_pi_state_list() or some waiter\n\t\t * acquired the rtmutex in the pi state, but did not\n\t\t * yet fixup the TID in user space.\n\t\t *\n\t\t * Take a ref on the state and return success. [6]\n\t\t */\n\t\tif (!pid)\n\t\t\tgoto out_attach;\n\t} else {\n\t\t/*\n\t\t * If the owner died bit is not set, then the pi_state\n\t\t * must have an owner. [7]\n\t\t */\n\t\tif (!pi_state->owner)\n\t\t\tgoto out_einval;\n\t}\n\n\t/*\n\t * Bail out if user space manipulated the futex value. If pi\n\t * state exists then the owner TID must be the same as the\n\t * user space TID. [9/10]\n\t */\n\tif (pid != task_pid_vnr(pi_state->owner))\n\t\tgoto out_einval;\n\nout_attach:\n\tget_pi_state(pi_state);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t*ps = pi_state;\n\treturn 0;\n\nout_einval:\n\tret = -EINVAL;\n\tgoto out_error;\n\nout_eagain:\n\tret = -EAGAIN;\n\tgoto out_error;\n\nout_efault:\n\tret = -EFAULT;\n\tgoto out_error;\n\nout_error:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 316
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_pi_state",
          "args": [
            "pi_state"
          ],
          "line": 298
        },
        "resolved": true,
        "details": {
          "function_name": "get_pi_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "68-71",
          "snippet": "void get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!refcount_inc_not_zero(&pi_state->refcount));\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nvoid get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!refcount_inc_not_zero(&pi_state->refcount));\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_pid_vnr",
          "args": [
            "pi_state->owner"
          ],
          "line": 294
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "futex_get_value_locked",
          "args": [
            "&uval2",
            "uaddr"
          ],
          "line": 242
        },
        "resolved": true,
        "details": {
          "function_name": "futex_get_value_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/core.c",
          "lines": "451-460",
          "snippet": "int futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/slab.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/memblock.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/jhash.h>",
            "#include <linux/compat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/slab.h>\n#include <linux/fault-inject.h>\n#include <linux/memblock.h>\n#include <linux/pagemap.h>\n#include <linux/jhash.h>\n#include <linux/compat.h>\n\nint futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 234
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "!refcount_read(&pi_state->refcount)"
          ],
          "line": 228
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "refcount_read",
          "args": [
            "&pi_state->refcount"
          ],
          "line": 228
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!pi_state"
          ],
          "line": 213
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic int attach_to_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t      struct futex_pi_state *pi_state,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tu32 uval2;\n\tint ret;\n\n\t/*\n\t * Userspace might have messed up non-PI and PI futexes [3]\n\t */\n\tif (unlikely(!pi_state))\n\t\treturn -EINVAL;\n\n\t/*\n\t * We get here with hb->lock held, and having found a\n\t * futex_top_waiter(). This means that futex_lock_pi() of said futex_q\n\t * has dropped the hb->lock in between futex_queue() and futex_unqueue_pi(),\n\t * which in turn means that futex_lock_pi() still has a reference on\n\t * our pi_state.\n\t *\n\t * The waiter holding a reference on @pi_state also protects against\n\t * the unlocked put_pi_state() in futex_unlock_pi(), futex_lock_pi()\n\t * and futex_wait_requeue_pi() as it cannot go to 0 and consequently\n\t * free pi_state before we can take a reference ourselves.\n\t */\n\tWARN_ON(!refcount_read(&pi_state->refcount));\n\n\t/*\n\t * Now that we have a pi_state, we can acquire wait_lock\n\t * and do the state validation.\n\t */\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Since {uval, pi_state} is serialized by wait_lock, and our current\n\t * uval was read without holding it, it can have changed. Verify it\n\t * still is what we expect it to be, otherwise retry the entire\n\t * operation.\n\t */\n\tif (futex_get_value_locked(&uval2, uaddr))\n\t\tgoto out_efault;\n\n\tif (uval != uval2)\n\t\tgoto out_eagain;\n\n\t/*\n\t * Handle the owner died case:\n\t */\n\tif (uval & FUTEX_OWNER_DIED) {\n\t\t/*\n\t\t * exit_pi_state_list sets owner to NULL and wakes the\n\t\t * topmost waiter. The task which acquires the\n\t\t * pi_state->rt_mutex will fixup owner.\n\t\t */\n\t\tif (!pi_state->owner) {\n\t\t\t/*\n\t\t\t * No pi state owner, but the user space TID\n\t\t\t * is not 0. Inconsistent state. [5]\n\t\t\t */\n\t\t\tif (pid)\n\t\t\t\tgoto out_einval;\n\t\t\t/*\n\t\t\t * Take a ref on the state and return success. [4]\n\t\t\t */\n\t\t\tgoto out_attach;\n\t\t}\n\n\t\t/*\n\t\t * If TID is 0, then either the dying owner has not\n\t\t * yet executed exit_pi_state_list() or some waiter\n\t\t * acquired the rtmutex in the pi state, but did not\n\t\t * yet fixup the TID in user space.\n\t\t *\n\t\t * Take a ref on the state and return success. [6]\n\t\t */\n\t\tif (!pid)\n\t\t\tgoto out_attach;\n\t} else {\n\t\t/*\n\t\t * If the owner died bit is not set, then the pi_state\n\t\t * must have an owner. [7]\n\t\t */\n\t\tif (!pi_state->owner)\n\t\t\tgoto out_einval;\n\t}\n\n\t/*\n\t * Bail out if user space manipulated the futex value. If pi\n\t * state exists then the owner TID must be the same as the\n\t * user space TID. [9/10]\n\t */\n\tif (pid != task_pid_vnr(pi_state->owner))\n\t\tgoto out_einval;\n\nout_attach:\n\tget_pi_state(pi_state);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t*ps = pi_state;\n\treturn 0;\n\nout_einval:\n\tret = -EINVAL;\n\tgoto out_error;\n\nout_eagain:\n\tret = -EAGAIN;\n\tgoto out_error;\n\nout_efault:\n\tret = -EFAULT;\n\tgoto out_error;\n\nout_error:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}"
  },
  {
    "function_name": "put_pi_state",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "77-110",
    "snippet": "void put_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!pi_state)\n\t\treturn;\n\n\tif (!refcount_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&pi_state->pi_mutex.wait_lock, flags);\n\t\tpi_state_update_owner(pi_state, NULL);\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex);\n\t\traw_spin_unlock_irqrestore(&pi_state->pi_mutex.wait_lock, flags);\n\t}\n\n\tif (current->pi_state_cache) {\n\t\tkfree(pi_state);\n\t} else {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\trefcount_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "refcount_set",
          "args": [
            "&pi_state->refcount",
            "1"
          ],
          "line": 107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "pi_state"
          ],
          "line": 99
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/params.c",
          "lines": "62-75",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/security.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/security.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&pi_state->pi_mutex.wait_lock",
            "flags"
          ],
          "line": 95
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_proxy_unlock",
          "args": [
            "&pi_state->pi_mutex"
          ],
          "line": 94
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_proxy_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "172-176",
          "snippet": "static inline void debug_rt_mutex_proxy_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(!rt_mutex_owner(lock));\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void debug_rt_mutex_proxy_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(!rt_mutex_owner(lock));\n}"
        }
      },
      {
        "call_info": {
          "callee": "pi_state_update_owner",
          "args": [
            "pi_state",
            "NULL"
          ],
          "line": 93
        },
        "resolved": true,
        "details": {
          "function_name": "pi_state_update_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
          "lines": "45-66",
          "snippet": "static void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"futex.h\"",
            "#include <linux/sched/task.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&pi_state->pi_mutex.wait_lock",
            "flags"
          ],
          "line": 92
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "refcount_dec_and_test",
          "args": [
            "&pi_state->refcount"
          ],
          "line": 82
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nvoid put_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!pi_state)\n\t\treturn;\n\n\tif (!refcount_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&pi_state->pi_mutex.wait_lock, flags);\n\t\tpi_state_update_owner(pi_state, NULL);\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex);\n\t\traw_spin_unlock_irqrestore(&pi_state->pi_mutex.wait_lock, flags);\n\t}\n\n\tif (current->pi_state_cache) {\n\t\tkfree(pi_state);\n\t} else {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\trefcount_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}"
  },
  {
    "function_name": "get_pi_state",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "68-71",
    "snippet": "void get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!refcount_inc_not_zero(&pi_state->refcount));\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "!refcount_inc_not_zero(&pi_state->refcount)"
          ],
          "line": 70
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "refcount_inc_not_zero",
          "args": [
            "&pi_state->refcount"
          ],
          "line": 70
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nvoid get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!refcount_inc_not_zero(&pi_state->refcount));\n}"
  },
  {
    "function_name": "pi_state_update_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "45-66",
    "snippet": "static void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&new_owner->pi_lock"
          ],
          "line": 64
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "208-211",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_add",
          "args": [
            "&pi_state->list",
            "&new_owner->pi_state_list"
          ],
          "line": 62
        },
        "resolved": true,
        "details": {
          "function_name": "cmp_filterlist_addrs",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/kcsan/debugfs.c",
          "lines": "95-101",
          "snippet": "static int cmp_filterlist_addrs(const void *rhs, const void *lhs)\n{\n\tconst unsigned long a = *(const unsigned long *)rhs;\n\tconst unsigned long b = *(const unsigned long *)lhs;\n\n\treturn a < b ? -1 : a == b ? 0 : 1;\n}",
          "includes": [
            "#include \"kcsan.h\"",
            "#include <linux/uaccess.h>",
            "#include <linux/string.h>",
            "#include <linux/sort.h>",
            "#include <linux/slab.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/sched.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/init.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/bug.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"kcsan.h\"\n#include <linux/uaccess.h>\n#include <linux/string.h>\n#include <linux/sort.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/sched.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/debugfs.h>\n#include <linux/bug.h>\n#include <linux/bsearch.h>\n#include <linux/atomic.h>\n\nstatic int cmp_filterlist_addrs(const void *rhs, const void *lhs)\n{\n\tconst unsigned long a = *(const unsigned long *)rhs;\n\tconst unsigned long b = *(const unsigned long *)lhs;\n\n\treturn a < b ? -1 : a == b ? 0 : 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "!list_empty(&pi_state->list)"
          ],
          "line": 61
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&pi_state->list"
          ],
          "line": 61
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "41-44",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !READ_ONCE(rsclp->head);\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "long rcu_segcblist_n_segment_cbs(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_inc_len(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nlong rcu_segcblist_n_segment_cbs(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_inc_len(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !READ_ONCE(rsclp->head);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&new_owner->pi_lock"
          ],
          "line": 60
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "176-179",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_del_init",
          "args": [
            "&pi_state->list"
          ],
          "line": 55
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "list_empty(&pi_state->list)"
          ],
          "line": 54
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&pi_state->pi_mutex.wait_lock"
          ],
          "line": 50
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic void pi_state_update_owner(struct futex_pi_state *pi_state,\n\t\t\t\t  struct task_struct *new_owner)\n{\n\tstruct task_struct *old_owner = pi_state->owner;\n\n\tlockdep_assert_held(&pi_state->pi_mutex.wait_lock);\n\n\tif (old_owner) {\n\t\traw_spin_lock(&old_owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&old_owner->pi_lock);\n\t}\n\n\tif (new_owner) {\n\t\traw_spin_lock(&new_owner->pi_lock);\n\t\tWARN_ON(!list_empty(&pi_state->list));\n\t\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\t\tpi_state->owner = new_owner;\n\t\traw_spin_unlock(&new_owner->pi_lock);\n\t}\n}"
  },
  {
    "function_name": "alloc_pi_state",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "35-43",
    "snippet": "static struct futex_pi_state *alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "!pi_state"
          ],
          "line": 39
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nstatic struct futex_pi_state *alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}"
  },
  {
    "function_name": "refill_pi_state_cache",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/futex/pi.c",
    "lines": "12-33",
    "snippet": "int refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\trefcount_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}",
    "includes": [
      "#include \"../locking/rtmutex_common.h\"",
      "#include \"futex.h\"",
      "#include <linux/sched/task.h>",
      "#include <linux/slab.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "refcount_set",
          "args": [
            "&pi_state->refcount",
            "1"
          ],
          "line": 27
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_LIST_HEAD",
          "args": [
            "&pi_state->list"
          ],
          "line": 24
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kzalloc",
          "args": [
            "sizeof(*pi_state)",
            "GFP_KERNEL"
          ],
          "line": 19
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "current->pi_state_cache"
          ],
          "line": 16
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"futex.h\"\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n\nint refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\trefcount_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}"
  }
]