[
  {
    "function_name": "irq_affinity_online_cpu",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
    "lines": "218-233",
    "snippet": "int irq_affinity_online_cpu(unsigned int cpu)\n{\n\tstruct irq_desc *desc;\n\tunsigned int irq;\n\n\tirq_lock_sparse();\n\tfor_each_active_irq(irq) {\n\t\tdesc = irq_to_desc(irq);\n\t\traw_spin_lock_irq(&desc->lock);\n\t\tirq_restore_affinity_of_irq(desc, cpu);\n\t\traw_spin_unlock_irq(&desc->lock);\n\t}\n\tirq_unlock_sparse();\n\n\treturn 0;\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/sched/isolation.h>",
      "#include <linux/irq.h>",
      "#include <linux/ratelimit.h>",
      "#include <linux/interrupt.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "irq_unlock_sparse",
          "args": [],
          "line": 230
        },
        "resolved": true,
        "details": {
          "function_name": "irq_unlock_sparse",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/irqdesc.c",
          "lines": "384-387",
          "snippet": "void irq_unlock_sparse(void)\n{\n\tmutex_unlock(&sparse_irq_lock);\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sysfs.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/radix-tree.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_MUTEX(sparse_irq_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sysfs.h>\n#include <linux/irqdomain.h>\n#include <linux/bitmap.h>\n#include <linux/radix-tree.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/irq.h>\n\nstatic DEFINE_MUTEX(sparse_irq_lock);\n\nvoid irq_unlock_sparse(void)\n{\n\tmutex_unlock(&sparse_irq_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&desc->lock"
          ],
          "line": 228
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_restore_affinity_of_irq",
          "args": [
            "desc",
            "cpu"
          ],
          "line": 227
        },
        "resolved": true,
        "details": {
          "function_name": "irq_restore_affinity_of_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
          "lines": "189-212",
          "snippet": "static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)\n{\n\tstruct irq_data *data = irq_desc_get_irq_data(desc);\n\tconst struct cpumask *affinity = irq_data_get_affinity_mask(data);\n\n\tif (!irqd_affinity_is_managed(data) || !desc->action ||\n\t    !irq_data_get_irq_chip(data) || !cpumask_test_cpu(cpu, affinity))\n\t\treturn;\n\n\tif (irqd_is_managed_and_shutdown(data)) {\n\t\tirq_startup(desc, IRQ_RESEND, IRQ_START_COND);\n\t\treturn;\n\t}\n\n\t/*\n\t * If the interrupt can only be directed to a single target\n\t * CPU then it is already assigned to a CPU in the affinity\n\t * mask. No point in trying to move it around unless the\n\t * isolation mechanism requests to move it to an upcoming\n\t * housekeeping CPU.\n\t */\n\tif (!irqd_is_single_target(data) || hk_should_isolate(data, cpu))\n\t\tirq_set_affinity_locked(data, affinity, false);\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/irq.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/interrupt.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)\n{\n\tstruct irq_data *data = irq_desc_get_irq_data(desc);\n\tconst struct cpumask *affinity = irq_data_get_affinity_mask(data);\n\n\tif (!irqd_affinity_is_managed(data) || !desc->action ||\n\t    !irq_data_get_irq_chip(data) || !cpumask_test_cpu(cpu, affinity))\n\t\treturn;\n\n\tif (irqd_is_managed_and_shutdown(data)) {\n\t\tirq_startup(desc, IRQ_RESEND, IRQ_START_COND);\n\t\treturn;\n\t}\n\n\t/*\n\t * If the interrupt can only be directed to a single target\n\t * CPU then it is already assigned to a CPU in the affinity\n\t * mask. No point in trying to move it around unless the\n\t * isolation mechanism requests to move it to an upcoming\n\t * housekeeping CPU.\n\t */\n\tif (!irqd_is_single_target(data) || hk_should_isolate(data, cpu))\n\t\tirq_set_affinity_locked(data, affinity, false);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&desc->lock"
          ],
          "line": 226
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_to_desc",
          "args": [
            "irq"
          ],
          "line": 225
        },
        "resolved": true,
        "details": {
          "function_name": "irq_to_desc",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/irqdesc.c",
          "lines": "583-586",
          "snippet": "struct irq_desc *irq_to_desc(unsigned int irq)\n{\n\treturn (irq < NR_IRQS) ? irq_desc + irq : NULL;\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sysfs.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/radix-tree.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sysfs.h>\n#include <linux/irqdomain.h>\n#include <linux/bitmap.h>\n#include <linux/radix-tree.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/irq.h>\n\nstruct irq_desc *irq_to_desc(unsigned int irq)\n{\n\treturn (irq < NR_IRQS) ? irq_desc + irq : NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_lock_sparse",
          "args": [],
          "line": 223
        },
        "resolved": true,
        "details": {
          "function_name": "irq_lock_sparse",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/irqdesc.c",
          "lines": "379-382",
          "snippet": "void irq_lock_sparse(void)\n{\n\tmutex_lock(&sparse_irq_lock);\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sysfs.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/radix-tree.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_MUTEX(sparse_irq_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sysfs.h>\n#include <linux/irqdomain.h>\n#include <linux/bitmap.h>\n#include <linux/radix-tree.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/irq.h>\n\nstatic DEFINE_MUTEX(sparse_irq_lock);\n\nvoid irq_lock_sparse(void)\n{\n\tmutex_lock(&sparse_irq_lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nint irq_affinity_online_cpu(unsigned int cpu)\n{\n\tstruct irq_desc *desc;\n\tunsigned int irq;\n\n\tirq_lock_sparse();\n\tfor_each_active_irq(irq) {\n\t\tdesc = irq_to_desc(irq);\n\t\traw_spin_lock_irq(&desc->lock);\n\t\tirq_restore_affinity_of_irq(desc, cpu);\n\t\traw_spin_unlock_irq(&desc->lock);\n\t}\n\tirq_unlock_sparse();\n\n\treturn 0;\n}"
  },
  {
    "function_name": "irq_restore_affinity_of_irq",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
    "lines": "189-212",
    "snippet": "static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)\n{\n\tstruct irq_data *data = irq_desc_get_irq_data(desc);\n\tconst struct cpumask *affinity = irq_data_get_affinity_mask(data);\n\n\tif (!irqd_affinity_is_managed(data) || !desc->action ||\n\t    !irq_data_get_irq_chip(data) || !cpumask_test_cpu(cpu, affinity))\n\t\treturn;\n\n\tif (irqd_is_managed_and_shutdown(data)) {\n\t\tirq_startup(desc, IRQ_RESEND, IRQ_START_COND);\n\t\treturn;\n\t}\n\n\t/*\n\t * If the interrupt can only be directed to a single target\n\t * CPU then it is already assigned to a CPU in the affinity\n\t * mask. No point in trying to move it around unless the\n\t * isolation mechanism requests to move it to an upcoming\n\t * housekeeping CPU.\n\t */\n\tif (!irqd_is_single_target(data) || hk_should_isolate(data, cpu))\n\t\tirq_set_affinity_locked(data, affinity, false);\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/sched/isolation.h>",
      "#include <linux/irq.h>",
      "#include <linux/ratelimit.h>",
      "#include <linux/interrupt.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "irq_set_affinity_locked",
          "args": [
            "data",
            "affinity",
            "false"
          ],
          "line": 211
        },
        "resolved": true,
        "details": {
          "function_name": "irq_set_affinity_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/manage.c",
          "lines": "340-371",
          "snippet": "int irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,\n\t\t\t    bool force)\n{\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tint ret = 0;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\tif (irq_set_affinity_deactivated(data, mask, force))\n\t\treturn 0;\n\n\tif (irq_can_move_pcntxt(data) && !irqd_is_setaffinity_pending(data)) {\n\t\tret = irq_try_set_affinity(data, mask, force);\n\t} else {\n\t\tirqd_set_move_pending(data);\n\t\tirq_copy_pending(desc, mask);\n\t}\n\n\tif (desc->affinity_notify) {\n\t\tkref_get(&desc->affinity_notify->kref);\n\t\tif (!schedule_work(&desc->affinity_notify->work)) {\n\t\t\t/* Work was already scheduled, drop our extra ref */\n\t\t\tkref_put(&desc->affinity_notify->kref,\n\t\t\t\t desc->affinity_notify->release);\n\t\t}\n\t}\n\tirqd_set(data, IRQD_AFFINITY_SET);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/task_work.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/random.h>",
            "#include <linux/module.h>",
            "#include <linux/kthread.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/task_work.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/task.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/irqdomain.h>\n#include <linux/interrupt.h>\n#include <linux/random.h>\n#include <linux/module.h>\n#include <linux/kthread.h>\n#include <linux/irq.h>\n\nint irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,\n\t\t\t    bool force)\n{\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tint ret = 0;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\tif (irq_set_affinity_deactivated(data, mask, force))\n\t\treturn 0;\n\n\tif (irq_can_move_pcntxt(data) && !irqd_is_setaffinity_pending(data)) {\n\t\tret = irq_try_set_affinity(data, mask, force);\n\t} else {\n\t\tirqd_set_move_pending(data);\n\t\tirq_copy_pending(desc, mask);\n\t}\n\n\tif (desc->affinity_notify) {\n\t\tkref_get(&desc->affinity_notify->kref);\n\t\tif (!schedule_work(&desc->affinity_notify->work)) {\n\t\t\t/* Work was already scheduled, drop our extra ref */\n\t\t\tkref_put(&desc->affinity_notify->kref,\n\t\t\t\t desc->affinity_notify->release);\n\t\t}\n\t}\n\tirqd_set(data, IRQD_AFFINITY_SET);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "hk_should_isolate",
          "args": [
            "data",
            "cpu"
          ],
          "line": 210
        },
        "resolved": true,
        "details": {
          "function_name": "hk_should_isolate",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
          "lines": "175-187",
          "snippet": "static bool hk_should_isolate(struct irq_data *data, unsigned int cpu)\n{\n\tconst struct cpumask *hk_mask;\n\n\tif (!housekeeping_enabled(HK_FLAG_MANAGED_IRQ))\n\t\treturn false;\n\n\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\tif (cpumask_subset(irq_data_get_effective_affinity_mask(data), hk_mask))\n\t\treturn false;\n\n\treturn cpumask_test_cpu(cpu, hk_mask);\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/irq.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/interrupt.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic bool hk_should_isolate(struct irq_data *data, unsigned int cpu)\n{\n\tconst struct cpumask *hk_mask;\n\n\tif (!housekeeping_enabled(HK_FLAG_MANAGED_IRQ))\n\t\treturn false;\n\n\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\tif (cpumask_subset(irq_data_get_effective_affinity_mask(data), hk_mask))\n\t\treturn false;\n\n\treturn cpumask_test_cpu(cpu, hk_mask);\n}"
        }
      },
      {
        "call_info": {
          "callee": "irqd_is_single_target",
          "args": [
            "data"
          ],
          "line": 210
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_startup",
          "args": [
            "desc",
            "IRQ_RESEND",
            "IRQ_START_COND"
          ],
          "line": 199
        },
        "resolved": true,
        "details": {
          "function_name": "irq_startup",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/chip.c",
          "lines": "255-287",
          "snippet": "int irq_startup(struct irq_desc *desc, bool resend, bool force)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct cpumask *aff = irq_data_get_affinity_mask(d);\n\tint ret = 0;\n\n\tdesc->depth = 0;\n\n\tif (irqd_is_started(d)) {\n\t\tirq_enable(desc);\n\t} else {\n\t\tswitch (__irq_startup_managed(desc, aff, force)) {\n\t\tcase IRQ_STARTUP_NORMAL:\n\t\t\tif (d->chip->flags & IRQCHIP_AFFINITY_PRE_STARTUP)\n\t\t\t\tirq_setup_affinity(desc);\n\t\t\tret = __irq_startup(desc);\n\t\t\tif (!(d->chip->flags & IRQCHIP_AFFINITY_PRE_STARTUP))\n\t\t\t\tirq_setup_affinity(desc);\n\t\t\tbreak;\n\t\tcase IRQ_STARTUP_MANAGED:\n\t\t\tirq_do_set_affinity(d, aff, false);\n\t\t\tret = __irq_startup(desc);\n\t\t\tbreak;\n\t\tcase IRQ_STARTUP_ABORT:\n\t\t\tirqd_set_managed_shutdown(d);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (resend)\n\t\tcheck_irq_resend(desc, false);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <trace/events/irq.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/module.h>",
            "#include <linux/msi.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <trace/events/irq.h>\n#include <linux/irqdomain.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/msi.h>\n#include <linux/irq.h>\n\nint irq_startup(struct irq_desc *desc, bool resend, bool force)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct cpumask *aff = irq_data_get_affinity_mask(d);\n\tint ret = 0;\n\n\tdesc->depth = 0;\n\n\tif (irqd_is_started(d)) {\n\t\tirq_enable(desc);\n\t} else {\n\t\tswitch (__irq_startup_managed(desc, aff, force)) {\n\t\tcase IRQ_STARTUP_NORMAL:\n\t\t\tif (d->chip->flags & IRQCHIP_AFFINITY_PRE_STARTUP)\n\t\t\t\tirq_setup_affinity(desc);\n\t\t\tret = __irq_startup(desc);\n\t\t\tif (!(d->chip->flags & IRQCHIP_AFFINITY_PRE_STARTUP))\n\t\t\t\tirq_setup_affinity(desc);\n\t\t\tbreak;\n\t\tcase IRQ_STARTUP_MANAGED:\n\t\t\tirq_do_set_affinity(d, aff, false);\n\t\t\tret = __irq_startup(desc);\n\t\t\tbreak;\n\t\tcase IRQ_STARTUP_ABORT:\n\t\t\tirqd_set_managed_shutdown(d);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (resend)\n\t\tcheck_irq_resend(desc, false);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irqd_is_managed_and_shutdown",
          "args": [
            "data"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "affinity"
          ],
          "line": 195
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_get_irq_chip",
          "args": [
            "data"
          ],
          "line": 195
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_affinity_is_managed",
          "args": [
            "data"
          ],
          "line": 194
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_get_affinity_mask",
          "args": [
            "data"
          ],
          "line": 192
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_desc_get_irq_data",
          "args": [
            "desc"
          ],
          "line": 191
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)\n{\n\tstruct irq_data *data = irq_desc_get_irq_data(desc);\n\tconst struct cpumask *affinity = irq_data_get_affinity_mask(data);\n\n\tif (!irqd_affinity_is_managed(data) || !desc->action ||\n\t    !irq_data_get_irq_chip(data) || !cpumask_test_cpu(cpu, affinity))\n\t\treturn;\n\n\tif (irqd_is_managed_and_shutdown(data)) {\n\t\tirq_startup(desc, IRQ_RESEND, IRQ_START_COND);\n\t\treturn;\n\t}\n\n\t/*\n\t * If the interrupt can only be directed to a single target\n\t * CPU then it is already assigned to a CPU in the affinity\n\t * mask. No point in trying to move it around unless the\n\t * isolation mechanism requests to move it to an upcoming\n\t * housekeeping CPU.\n\t */\n\tif (!irqd_is_single_target(data) || hk_should_isolate(data, cpu))\n\t\tirq_set_affinity_locked(data, affinity, false);\n}"
  },
  {
    "function_name": "hk_should_isolate",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
    "lines": "175-187",
    "snippet": "static bool hk_should_isolate(struct irq_data *data, unsigned int cpu)\n{\n\tconst struct cpumask *hk_mask;\n\n\tif (!housekeeping_enabled(HK_FLAG_MANAGED_IRQ))\n\t\treturn false;\n\n\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\tif (cpumask_subset(irq_data_get_effective_affinity_mask(data), hk_mask))\n\t\treturn false;\n\n\treturn cpumask_test_cpu(cpu, hk_mask);\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/sched/isolation.h>",
      "#include <linux/irq.h>",
      "#include <linux/ratelimit.h>",
      "#include <linux/interrupt.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "hk_mask"
          ],
          "line": 186
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_subset",
          "args": [
            "irq_data_get_effective_affinity_mask(data)",
            "hk_mask"
          ],
          "line": 183
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_get_effective_affinity_mask",
          "args": [
            "data"
          ],
          "line": 183
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "housekeeping_cpumask",
          "args": [
            "HK_FLAG_MANAGED_IRQ"
          ],
          "line": 182
        },
        "resolved": true,
        "details": {
          "function_name": "housekeeping_cpumask",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/isolation.c",
          "lines": "40-46",
          "snippet": "const struct cpumask *housekeeping_cpumask(enum hk_flags flags)\n{\n\tif (static_branch_unlikely(&housekeeping_overridden))\n\t\tif (housekeeping_flags & flags)\n\t\t\treturn housekeeping_mask;\n\treturn cpu_possible_mask;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static cpumask_var_t housekeeping_mask;",
            "static unsigned int housekeeping_flags;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic cpumask_var_t housekeeping_mask;\nstatic unsigned int housekeeping_flags;\n\nconst struct cpumask *housekeeping_cpumask(enum hk_flags flags)\n{\n\tif (static_branch_unlikely(&housekeeping_overridden))\n\t\tif (housekeeping_flags & flags)\n\t\t\treturn housekeeping_mask;\n\treturn cpu_possible_mask;\n}"
        }
      },
      {
        "call_info": {
          "callee": "housekeeping_enabled",
          "args": [
            "HK_FLAG_MANAGED_IRQ"
          ],
          "line": 179
        },
        "resolved": true,
        "details": {
          "function_name": "housekeeping_enabled",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/isolation.c",
          "lines": "17-20",
          "snippet": "bool housekeeping_enabled(enum hk_flags flags)\n{\n\treturn !!(housekeeping_flags & flags);\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static unsigned int housekeeping_flags;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic unsigned int housekeeping_flags;\n\nbool housekeeping_enabled(enum hk_flags flags)\n{\n\treturn !!(housekeeping_flags & flags);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic bool hk_should_isolate(struct irq_data *data, unsigned int cpu)\n{\n\tconst struct cpumask *hk_mask;\n\n\tif (!housekeeping_enabled(HK_FLAG_MANAGED_IRQ))\n\t\treturn false;\n\n\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\tif (cpumask_subset(irq_data_get_effective_affinity_mask(data), hk_mask))\n\t\treturn false;\n\n\treturn cpumask_test_cpu(cpu, hk_mask);\n}"
  },
  {
    "function_name": "irq_migrate_all_off_this_cpu",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
    "lines": "155-173",
    "snippet": "void irq_migrate_all_off_this_cpu(void)\n{\n\tstruct irq_desc *desc;\n\tunsigned int irq;\n\n\tfor_each_active_irq(irq) {\n\t\tbool affinity_broken;\n\n\t\tdesc = irq_to_desc(irq);\n\t\traw_spin_lock(&desc->lock);\n\t\taffinity_broken = migrate_one_irq(desc);\n\t\traw_spin_unlock(&desc->lock);\n\n\t\tif (affinity_broken) {\n\t\t\tpr_debug_ratelimited(\"IRQ %u: no longer affine to CPU%u\\n\",\n\t\t\t\t\t    irq, smp_processor_id());\n\t\t}\n\t}\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/sched/isolation.h>",
      "#include <linux/irq.h>",
      "#include <linux/ratelimit.h>",
      "#include <linux/interrupt.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "pr_debug_ratelimited",
          "args": [
            "\"IRQ %u: no longer affine to CPU%u\\n\"",
            "irq",
            "smp_processor_id()"
          ],
          "line": 169
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 170
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&desc->lock"
          ],
          "line": 166
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "208-211",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "migrate_one_irq",
          "args": [
            "desc"
          ],
          "line": 165
        },
        "resolved": true,
        "details": {
          "function_name": "migrate_one_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
          "lines": "53-143",
          "snippet": "static bool migrate_one_irq(struct irq_desc *desc)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(d);\n\tbool maskchip = !irq_can_move_pcntxt(d) && !irqd_irq_masked(d);\n\tconst struct cpumask *affinity;\n\tbool brokeaff = false;\n\tint err;\n\n\t/*\n\t * IRQ chip might be already torn down, but the irq descriptor is\n\t * still in the radix tree. Also if the chip has no affinity setter,\n\t * nothing can be done here.\n\t */\n\tif (!chip || !chip->irq_set_affinity) {\n\t\tpr_debug(\"IRQ %u: Unable to migrate away\\n\", d->irq);\n\t\treturn false;\n\t}\n\n\t/*\n\t * No move required, if:\n\t * - Interrupt is per cpu\n\t * - Interrupt is not started\n\t * - Affinity mask does not include this CPU.\n\t *\n\t * Note: Do not check desc->action as this might be a chained\n\t * interrupt.\n\t */\n\tif (irqd_is_per_cpu(d) || !irqd_is_started(d) || !irq_needs_fixup(d)) {\n\t\t/*\n\t\t * If an irq move is pending, abort it if the dying CPU is\n\t\t * the sole target.\n\t\t */\n\t\tirq_fixup_move_pending(desc, false);\n\t\treturn false;\n\t}\n\n\t/*\n\t * Complete an eventually pending irq move cleanup. If this\n\t * interrupt was moved in hard irq context, then the vectors need\n\t * to be cleaned up. It can't wait until this interrupt actually\n\t * happens and this CPU was involved.\n\t */\n\tirq_force_complete_move(desc);\n\n\t/*\n\t * If there is a setaffinity pending, then try to reuse the pending\n\t * mask, so the last change of the affinity does not get lost. If\n\t * there is no move pending or the pending mask does not contain\n\t * any online CPU, use the current affinity mask.\n\t */\n\tif (irq_fixup_move_pending(desc, true))\n\t\taffinity = irq_desc_get_pending_mask(desc);\n\telse\n\t\taffinity = irq_data_get_affinity_mask(d);\n\n\t/* Mask the chip for interrupts which cannot move in process context */\n\tif (maskchip && chip->irq_mask)\n\t\tchip->irq_mask(d);\n\n\tif (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If the interrupt is managed, then shut it down and leave\n\t\t * the affinity untouched.\n\t\t */\n\t\tif (irqd_affinity_is_managed(d)) {\n\t\t\tirqd_set_managed_shutdown(d);\n\t\t\tirq_shutdown_and_deactivate(desc);\n\t\t\treturn false;\n\t\t}\n\t\taffinity = cpu_online_mask;\n\t\tbrokeaff = true;\n\t}\n\t/*\n\t * Do not set the force argument of irq_do_set_affinity() as this\n\t * disables the masking of offline CPUs from the supplied affinity\n\t * mask and therefore might keep/reassign the irq to the outgoing\n\t * CPU.\n\t */\n\terr = irq_do_set_affinity(d, affinity, false);\n\tif (err) {\n\t\tpr_warn_ratelimited(\"IRQ%u: set affinity failed(%d).\\n\",\n\t\t\t\t    d->irq, err);\n\t\tbrokeaff = false;\n\t}\n\n\tif (maskchip && chip->irq_unmask)\n\t\tchip->irq_unmask(d);\n\n\treturn brokeaff;\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/irq.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/interrupt.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic bool migrate_one_irq(struct irq_desc *desc)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(d);\n\tbool maskchip = !irq_can_move_pcntxt(d) && !irqd_irq_masked(d);\n\tconst struct cpumask *affinity;\n\tbool brokeaff = false;\n\tint err;\n\n\t/*\n\t * IRQ chip might be already torn down, but the irq descriptor is\n\t * still in the radix tree. Also if the chip has no affinity setter,\n\t * nothing can be done here.\n\t */\n\tif (!chip || !chip->irq_set_affinity) {\n\t\tpr_debug(\"IRQ %u: Unable to migrate away\\n\", d->irq);\n\t\treturn false;\n\t}\n\n\t/*\n\t * No move required, if:\n\t * - Interrupt is per cpu\n\t * - Interrupt is not started\n\t * - Affinity mask does not include this CPU.\n\t *\n\t * Note: Do not check desc->action as this might be a chained\n\t * interrupt.\n\t */\n\tif (irqd_is_per_cpu(d) || !irqd_is_started(d) || !irq_needs_fixup(d)) {\n\t\t/*\n\t\t * If an irq move is pending, abort it if the dying CPU is\n\t\t * the sole target.\n\t\t */\n\t\tirq_fixup_move_pending(desc, false);\n\t\treturn false;\n\t}\n\n\t/*\n\t * Complete an eventually pending irq move cleanup. If this\n\t * interrupt was moved in hard irq context, then the vectors need\n\t * to be cleaned up. It can't wait until this interrupt actually\n\t * happens and this CPU was involved.\n\t */\n\tirq_force_complete_move(desc);\n\n\t/*\n\t * If there is a setaffinity pending, then try to reuse the pending\n\t * mask, so the last change of the affinity does not get lost. If\n\t * there is no move pending or the pending mask does not contain\n\t * any online CPU, use the current affinity mask.\n\t */\n\tif (irq_fixup_move_pending(desc, true))\n\t\taffinity = irq_desc_get_pending_mask(desc);\n\telse\n\t\taffinity = irq_data_get_affinity_mask(d);\n\n\t/* Mask the chip for interrupts which cannot move in process context */\n\tif (maskchip && chip->irq_mask)\n\t\tchip->irq_mask(d);\n\n\tif (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If the interrupt is managed, then shut it down and leave\n\t\t * the affinity untouched.\n\t\t */\n\t\tif (irqd_affinity_is_managed(d)) {\n\t\t\tirqd_set_managed_shutdown(d);\n\t\t\tirq_shutdown_and_deactivate(desc);\n\t\t\treturn false;\n\t\t}\n\t\taffinity = cpu_online_mask;\n\t\tbrokeaff = true;\n\t}\n\t/*\n\t * Do not set the force argument of irq_do_set_affinity() as this\n\t * disables the masking of offline CPUs from the supplied affinity\n\t * mask and therefore might keep/reassign the irq to the outgoing\n\t * CPU.\n\t */\n\terr = irq_do_set_affinity(d, affinity, false);\n\tif (err) {\n\t\tpr_warn_ratelimited(\"IRQ%u: set affinity failed(%d).\\n\",\n\t\t\t\t    d->irq, err);\n\t\tbrokeaff = false;\n\t}\n\n\tif (maskchip && chip->irq_unmask)\n\t\tchip->irq_unmask(d);\n\n\treturn brokeaff;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&desc->lock"
          ],
          "line": 164
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "176-179",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_to_desc",
          "args": [
            "irq"
          ],
          "line": 163
        },
        "resolved": true,
        "details": {
          "function_name": "irq_to_desc",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/irqdesc.c",
          "lines": "583-586",
          "snippet": "struct irq_desc *irq_to_desc(unsigned int irq)\n{\n\treturn (irq < NR_IRQS) ? irq_desc + irq : NULL;\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sysfs.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/radix-tree.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sysfs.h>\n#include <linux/irqdomain.h>\n#include <linux/bitmap.h>\n#include <linux/radix-tree.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/irq.h>\n\nstruct irq_desc *irq_to_desc(unsigned int irq)\n{\n\treturn (irq < NR_IRQS) ? irq_desc + irq : NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nvoid irq_migrate_all_off_this_cpu(void)\n{\n\tstruct irq_desc *desc;\n\tunsigned int irq;\n\n\tfor_each_active_irq(irq) {\n\t\tbool affinity_broken;\n\n\t\tdesc = irq_to_desc(irq);\n\t\traw_spin_lock(&desc->lock);\n\t\taffinity_broken = migrate_one_irq(desc);\n\t\traw_spin_unlock(&desc->lock);\n\n\t\tif (affinity_broken) {\n\t\t\tpr_debug_ratelimited(\"IRQ %u: no longer affine to CPU%u\\n\",\n\t\t\t\t\t    irq, smp_processor_id());\n\t\t}\n\t}\n}"
  },
  {
    "function_name": "migrate_one_irq",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
    "lines": "53-143",
    "snippet": "static bool migrate_one_irq(struct irq_desc *desc)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(d);\n\tbool maskchip = !irq_can_move_pcntxt(d) && !irqd_irq_masked(d);\n\tconst struct cpumask *affinity;\n\tbool brokeaff = false;\n\tint err;\n\n\t/*\n\t * IRQ chip might be already torn down, but the irq descriptor is\n\t * still in the radix tree. Also if the chip has no affinity setter,\n\t * nothing can be done here.\n\t */\n\tif (!chip || !chip->irq_set_affinity) {\n\t\tpr_debug(\"IRQ %u: Unable to migrate away\\n\", d->irq);\n\t\treturn false;\n\t}\n\n\t/*\n\t * No move required, if:\n\t * - Interrupt is per cpu\n\t * - Interrupt is not started\n\t * - Affinity mask does not include this CPU.\n\t *\n\t * Note: Do not check desc->action as this might be a chained\n\t * interrupt.\n\t */\n\tif (irqd_is_per_cpu(d) || !irqd_is_started(d) || !irq_needs_fixup(d)) {\n\t\t/*\n\t\t * If an irq move is pending, abort it if the dying CPU is\n\t\t * the sole target.\n\t\t */\n\t\tirq_fixup_move_pending(desc, false);\n\t\treturn false;\n\t}\n\n\t/*\n\t * Complete an eventually pending irq move cleanup. If this\n\t * interrupt was moved in hard irq context, then the vectors need\n\t * to be cleaned up. It can't wait until this interrupt actually\n\t * happens and this CPU was involved.\n\t */\n\tirq_force_complete_move(desc);\n\n\t/*\n\t * If there is a setaffinity pending, then try to reuse the pending\n\t * mask, so the last change of the affinity does not get lost. If\n\t * there is no move pending or the pending mask does not contain\n\t * any online CPU, use the current affinity mask.\n\t */\n\tif (irq_fixup_move_pending(desc, true))\n\t\taffinity = irq_desc_get_pending_mask(desc);\n\telse\n\t\taffinity = irq_data_get_affinity_mask(d);\n\n\t/* Mask the chip for interrupts which cannot move in process context */\n\tif (maskchip && chip->irq_mask)\n\t\tchip->irq_mask(d);\n\n\tif (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If the interrupt is managed, then shut it down and leave\n\t\t * the affinity untouched.\n\t\t */\n\t\tif (irqd_affinity_is_managed(d)) {\n\t\t\tirqd_set_managed_shutdown(d);\n\t\t\tirq_shutdown_and_deactivate(desc);\n\t\t\treturn false;\n\t\t}\n\t\taffinity = cpu_online_mask;\n\t\tbrokeaff = true;\n\t}\n\t/*\n\t * Do not set the force argument of irq_do_set_affinity() as this\n\t * disables the masking of offline CPUs from the supplied affinity\n\t * mask and therefore might keep/reassign the irq to the outgoing\n\t * CPU.\n\t */\n\terr = irq_do_set_affinity(d, affinity, false);\n\tif (err) {\n\t\tpr_warn_ratelimited(\"IRQ%u: set affinity failed(%d).\\n\",\n\t\t\t\t    d->irq, err);\n\t\tbrokeaff = false;\n\t}\n\n\tif (maskchip && chip->irq_unmask)\n\t\tchip->irq_unmask(d);\n\n\treturn brokeaff;\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/sched/isolation.h>",
      "#include <linux/irq.h>",
      "#include <linux/ratelimit.h>",
      "#include <linux/interrupt.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "chip->irq_unmask",
          "args": [
            "d"
          ],
          "line": 140
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_warn_ratelimited",
          "args": [
            "\"IRQ%u: set affinity failed(%d).\\n\"",
            "d->irq",
            "err"
          ],
          "line": 134
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_do_set_affinity",
          "args": [
            "d",
            "affinity",
            "false"
          ],
          "line": 132
        },
        "resolved": true,
        "details": {
          "function_name": "irq_do_set_affinity",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/manage.c",
          "lines": "220-281",
          "snippet": "int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,\n\t\t\tbool force)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tint ret;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If this is a managed interrupt and housekeeping is enabled on\n\t * it check whether the requested affinity mask intersects with\n\t * a housekeeping CPU. If so, then remove the isolated CPUs from\n\t * the mask and just keep the housekeeping CPU(s). This prevents\n\t * the affinity setter from routing the interrupt to an isolated\n\t * CPU to avoid that I/O submitted from a housekeeping CPU causes\n\t * interrupts on an isolated one.\n\t *\n\t * If the masks do not intersect or include online CPU(s) then\n\t * keep the requested mask. The isolated target CPUs are only\n\t * receiving interrupts when the I/O operation was submitted\n\t * directly from them.\n\t *\n\t * If all housekeeping CPUs in the affinity mask are offline, the\n\t * interrupt will be migrated by the CPU hotplug code once a\n\t * housekeeping CPU which belongs to the affinity mask comes\n\t * online.\n\t */\n\tif (irqd_affinity_is_managed(data) &&\n\t    housekeeping_enabled(HK_FLAG_MANAGED_IRQ)) {\n\t\tconst struct cpumask *hk_mask, *prog_mask;\n\n\t\tstatic DEFINE_RAW_SPINLOCK(tmp_mask_lock);\n\t\tstatic struct cpumask tmp_mask;\n\n\t\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\n\t\traw_spin_lock(&tmp_mask_lock);\n\t\tcpumask_and(&tmp_mask, mask, hk_mask);\n\t\tif (!cpumask_intersects(&tmp_mask, cpu_online_mask))\n\t\t\tprog_mask = mask;\n\t\telse\n\t\t\tprog_mask = &tmp_mask;\n\t\tret = chip->irq_set_affinity(data, prog_mask, force);\n\t\traw_spin_unlock(&tmp_mask_lock);\n\t} else {\n\t\tret = chip->irq_set_affinity(data, mask, force);\n\t}\n\tswitch (ret) {\n\tcase IRQ_SET_MASK_OK:\n\tcase IRQ_SET_MASK_OK_DONE:\n\t\tcpumask_copy(desc->irq_common_data.affinity, mask);\n\t\tfallthrough;\n\tcase IRQ_SET_MASK_OK_NOCOPY:\n\t\tirq_validate_effective_affinity(data);\n\t\tirq_set_thread_affinity(desc);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/task_work.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/random.h>",
            "#include <linux/module.h>",
            "#include <linux/kthread.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/task_work.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/task.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/irqdomain.h>\n#include <linux/interrupt.h>\n#include <linux/random.h>\n#include <linux/module.h>\n#include <linux/kthread.h>\n#include <linux/irq.h>\n\nint irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,\n\t\t\tbool force)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tint ret;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If this is a managed interrupt and housekeeping is enabled on\n\t * it check whether the requested affinity mask intersects with\n\t * a housekeeping CPU. If so, then remove the isolated CPUs from\n\t * the mask and just keep the housekeeping CPU(s). This prevents\n\t * the affinity setter from routing the interrupt to an isolated\n\t * CPU to avoid that I/O submitted from a housekeeping CPU causes\n\t * interrupts on an isolated one.\n\t *\n\t * If the masks do not intersect or include online CPU(s) then\n\t * keep the requested mask. The isolated target CPUs are only\n\t * receiving interrupts when the I/O operation was submitted\n\t * directly from them.\n\t *\n\t * If all housekeeping CPUs in the affinity mask are offline, the\n\t * interrupt will be migrated by the CPU hotplug code once a\n\t * housekeeping CPU which belongs to the affinity mask comes\n\t * online.\n\t */\n\tif (irqd_affinity_is_managed(data) &&\n\t    housekeeping_enabled(HK_FLAG_MANAGED_IRQ)) {\n\t\tconst struct cpumask *hk_mask, *prog_mask;\n\n\t\tstatic DEFINE_RAW_SPINLOCK(tmp_mask_lock);\n\t\tstatic struct cpumask tmp_mask;\n\n\t\thk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);\n\n\t\traw_spin_lock(&tmp_mask_lock);\n\t\tcpumask_and(&tmp_mask, mask, hk_mask);\n\t\tif (!cpumask_intersects(&tmp_mask, cpu_online_mask))\n\t\t\tprog_mask = mask;\n\t\telse\n\t\t\tprog_mask = &tmp_mask;\n\t\tret = chip->irq_set_affinity(data, prog_mask, force);\n\t\traw_spin_unlock(&tmp_mask_lock);\n\t} else {\n\t\tret = chip->irq_set_affinity(data, mask, force);\n\t}\n\tswitch (ret) {\n\tcase IRQ_SET_MASK_OK:\n\tcase IRQ_SET_MASK_OK_DONE:\n\t\tcpumask_copy(desc->irq_common_data.affinity, mask);\n\t\tfallthrough;\n\tcase IRQ_SET_MASK_OK_NOCOPY:\n\t\tirq_validate_effective_affinity(data);\n\t\tirq_set_thread_affinity(desc);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_shutdown_and_deactivate",
          "args": [
            "desc"
          ],
          "line": 120
        },
        "resolved": true,
        "details": {
          "function_name": "irq_shutdown_and_deactivate",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/chip.c",
          "lines": "323-333",
          "snippet": "void irq_shutdown_and_deactivate(struct irq_desc *desc)\n{\n\tirq_shutdown(desc);\n\t/*\n\t * This must be called even if the interrupt was never started up,\n\t * because the activation can happen before the interrupt is\n\t * available for request/startup. It has it's own state tracking so\n\t * it's safe to call it unconditionally.\n\t */\n\tirq_domain_deactivate_irq(&desc->irq_data);\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <trace/events/irq.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/module.h>",
            "#include <linux/msi.h>",
            "#include <linux/irq.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <trace/events/irq.h>\n#include <linux/irqdomain.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/msi.h>\n#include <linux/irq.h>\n\nvoid irq_shutdown_and_deactivate(struct irq_desc *desc)\n{\n\tirq_shutdown(desc);\n\t/*\n\t * This must be called even if the interrupt was never started up,\n\t * because the activation can happen before the interrupt is\n\t * available for request/startup. It has it's own state tracking so\n\t * it's safe to call it unconditionally.\n\t */\n\tirq_domain_deactivate_irq(&desc->irq_data);\n}"
        }
      },
      {
        "call_info": {
          "callee": "irqd_set_managed_shutdown",
          "args": [
            "d"
          ],
          "line": 119
        },
        "resolved": true,
        "details": {
          "function_name": "irqd_set_managed_shutdown",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/internals.h",
          "lines": "213-216",
          "snippet": "static inline void irqd_set_managed_shutdown(struct irq_data *d)\n{\n\t__irqd_to_state(d) |= IRQD_MANAGED_SHUTDOWN;\n}",
          "includes": [
            "#include <linux/debugfs.h>",
            "#include \"settings.h\"",
            "#include \"debug.h\"",
            "#include <linux/sched/clock.h>",
            "#include <linux/pm_runtime.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/irqdesc.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/debugfs.h>\n#include \"settings.h\"\n#include \"debug.h\"\n#include <linux/sched/clock.h>\n#include <linux/pm_runtime.h>\n#include <linux/kernel_stat.h>\n#include <linux/irqdesc.h>\n\nstatic inline void irqd_set_managed_shutdown(struct irq_data *d)\n{\n\t__irqd_to_state(d) |= IRQD_MANAGED_SHUTDOWN;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irqd_affinity_is_managed",
          "args": [
            "d"
          ],
          "line": 118
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_any_and",
          "args": [
            "affinity",
            "cpu_online_mask"
          ],
          "line": 113
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "chip->irq_mask",
          "args": [
            "d"
          ],
          "line": 111
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_get_affinity_mask",
          "args": [
            "d"
          ],
          "line": 107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_desc_get_pending_mask",
          "args": [
            "desc"
          ],
          "line": 105
        },
        "resolved": true,
        "details": {
          "function_name": "irq_desc_get_pending_mask",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/internals.h",
          "lines": "450-453",
          "snippet": "static inline struct cpumask *irq_desc_get_pending_mask(struct irq_desc *desc)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/debugfs.h>",
            "#include \"settings.h\"",
            "#include \"debug.h\"",
            "#include <linux/sched/clock.h>",
            "#include <linux/pm_runtime.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/irqdesc.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void __disable_irq(struct irq_desc *desc);",
            "extern void __enable_irq(struct irq_desc *desc);",
            "extern int irq_activate(struct irq_desc *desc);",
            "extern void irq_shutdown(struct irq_desc *desc);",
            "extern void irq_shutdown_and_deactivate(struct irq_desc *desc);",
            "extern void irq_enable(struct irq_desc *desc);",
            "extern void irq_disable(struct irq_desc *desc);",
            "extern void mask_irq(struct irq_desc *desc);",
            "extern void unmask_irq(struct irq_desc *desc);",
            "extern void unmask_threaded_irq(struct irq_desc *desc);",
            "irqreturn_t __handle_irq_event_percpu(struct irq_desc *desc);",
            "irqreturn_t handle_irq_event_percpu(struct irq_desc *desc);",
            "irqreturn_t handle_irq_event(struct irq_desc *desc);",
            "bool irq_wait_for_poll(struct irq_desc *desc);",
            "extern void irq_set_thread_affinity(struct irq_desc *desc);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/debugfs.h>\n#include \"settings.h\"\n#include \"debug.h\"\n#include <linux/sched/clock.h>\n#include <linux/pm_runtime.h>\n#include <linux/kernel_stat.h>\n#include <linux/irqdesc.h>\n\nextern void __disable_irq(struct irq_desc *desc);\nextern void __enable_irq(struct irq_desc *desc);\nextern int irq_activate(struct irq_desc *desc);\nextern void irq_shutdown(struct irq_desc *desc);\nextern void irq_shutdown_and_deactivate(struct irq_desc *desc);\nextern void irq_enable(struct irq_desc *desc);\nextern void irq_disable(struct irq_desc *desc);\nextern void mask_irq(struct irq_desc *desc);\nextern void unmask_irq(struct irq_desc *desc);\nextern void unmask_threaded_irq(struct irq_desc *desc);\nirqreturn_t __handle_irq_event_percpu(struct irq_desc *desc);\nirqreturn_t handle_irq_event_percpu(struct irq_desc *desc);\nirqreturn_t handle_irq_event(struct irq_desc *desc);\nbool irq_wait_for_poll(struct irq_desc *desc);\nextern void irq_set_thread_affinity(struct irq_desc *desc);\n\nstatic inline struct cpumask *irq_desc_get_pending_mask(struct irq_desc *desc)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_fixup_move_pending",
          "args": [
            "desc",
            "true"
          ],
          "line": 104
        },
        "resolved": true,
        "details": {
          "function_name": "irq_fixup_move_pending",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/internals.h",
          "lines": "454-457",
          "snippet": "static inline bool irq_fixup_move_pending(struct irq_desc *desc, bool fclear)\n{\n\treturn false;\n}",
          "includes": [
            "#include <linux/debugfs.h>",
            "#include \"settings.h\"",
            "#include \"debug.h\"",
            "#include <linux/sched/clock.h>",
            "#include <linux/pm_runtime.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/irqdesc.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void __disable_irq(struct irq_desc *desc);",
            "extern void __enable_irq(struct irq_desc *desc);",
            "extern int irq_activate(struct irq_desc *desc);",
            "extern void irq_shutdown(struct irq_desc *desc);",
            "extern void irq_shutdown_and_deactivate(struct irq_desc *desc);",
            "extern void irq_enable(struct irq_desc *desc);",
            "extern void irq_disable(struct irq_desc *desc);",
            "extern void mask_irq(struct irq_desc *desc);",
            "extern void unmask_irq(struct irq_desc *desc);",
            "extern void unmask_threaded_irq(struct irq_desc *desc);",
            "irqreturn_t __handle_irq_event_percpu(struct irq_desc *desc);",
            "irqreturn_t handle_irq_event_percpu(struct irq_desc *desc);",
            "irqreturn_t handle_irq_event(struct irq_desc *desc);",
            "bool irq_wait_for_poll(struct irq_desc *desc);",
            "extern void irq_set_thread_affinity(struct irq_desc *desc);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/debugfs.h>\n#include \"settings.h\"\n#include \"debug.h\"\n#include <linux/sched/clock.h>\n#include <linux/pm_runtime.h>\n#include <linux/kernel_stat.h>\n#include <linux/irqdesc.h>\n\nextern void __disable_irq(struct irq_desc *desc);\nextern void __enable_irq(struct irq_desc *desc);\nextern int irq_activate(struct irq_desc *desc);\nextern void irq_shutdown(struct irq_desc *desc);\nextern void irq_shutdown_and_deactivate(struct irq_desc *desc);\nextern void irq_enable(struct irq_desc *desc);\nextern void irq_disable(struct irq_desc *desc);\nextern void mask_irq(struct irq_desc *desc);\nextern void unmask_irq(struct irq_desc *desc);\nextern void unmask_threaded_irq(struct irq_desc *desc);\nirqreturn_t __handle_irq_event_percpu(struct irq_desc *desc);\nirqreturn_t handle_irq_event_percpu(struct irq_desc *desc);\nirqreturn_t handle_irq_event(struct irq_desc *desc);\nbool irq_wait_for_poll(struct irq_desc *desc);\nextern void irq_set_thread_affinity(struct irq_desc *desc);\n\nstatic inline bool irq_fixup_move_pending(struct irq_desc *desc, bool fclear)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_force_complete_move",
          "args": [
            "desc"
          ],
          "line": 96
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_needs_fixup",
          "args": [
            "d"
          ],
          "line": 81
        },
        "resolved": true,
        "details": {
          "function_name": "irq_needs_fixup",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
          "lines": "20-51",
          "snippet": "static inline bool irq_needs_fixup(struct irq_data *d)\n{\n\tconst struct cpumask *m = irq_data_get_effective_affinity_mask(d);\n\tunsigned int cpu = smp_processor_id();\n\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\n\t/*\n\t * The cpumask_empty() check is a workaround for interrupt chips,\n\t * which do not implement effective affinity, but the architecture has\n\t * enabled the config switch. Use the general affinity mask instead.\n\t */\n\tif (cpumask_empty(m))\n\t\tm = irq_data_get_affinity_mask(d);\n\n\t/*\n\t * Sanity check. If the mask is not empty when excluding the outgoing\n\t * CPU then it must contain at least one online CPU. The outgoing CPU\n\t * has been removed from the online mask already.\n\t */\n\tif (cpumask_any_but(m, cpu) < nr_cpu_ids &&\n\t    cpumask_any_and(m, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If this happens then there was a missed IRQ fixup at some\n\t\t * point. Warn about it and enforce fixup.\n\t\t */\n\t\tpr_warn(\"Eff. affinity %*pbl of IRQ %u contains only offline CPUs after offlining CPU %u\\n\",\n\t\t\tcpumask_pr_args(m), d->irq, cpu);\n\t\treturn true;\n\t}\n#endif\n\treturn cpumask_test_cpu(cpu, m);\n}",
          "includes": [
            "#include \"internals.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/irq.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/interrupt.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic inline bool irq_needs_fixup(struct irq_data *d)\n{\n\tconst struct cpumask *m = irq_data_get_effective_affinity_mask(d);\n\tunsigned int cpu = smp_processor_id();\n\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\n\t/*\n\t * The cpumask_empty() check is a workaround for interrupt chips,\n\t * which do not implement effective affinity, but the architecture has\n\t * enabled the config switch. Use the general affinity mask instead.\n\t */\n\tif (cpumask_empty(m))\n\t\tm = irq_data_get_affinity_mask(d);\n\n\t/*\n\t * Sanity check. If the mask is not empty when excluding the outgoing\n\t * CPU then it must contain at least one online CPU. The outgoing CPU\n\t * has been removed from the online mask already.\n\t */\n\tif (cpumask_any_but(m, cpu) < nr_cpu_ids &&\n\t    cpumask_any_and(m, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If this happens then there was a missed IRQ fixup at some\n\t\t * point. Warn about it and enforce fixup.\n\t\t */\n\t\tpr_warn(\"Eff. affinity %*pbl of IRQ %u contains only offline CPUs after offlining CPU %u\\n\",\n\t\t\tcpumask_pr_args(m), d->irq, cpu);\n\t\treturn true;\n\t}\n#endif\n\treturn cpumask_test_cpu(cpu, m);\n}"
        }
      },
      {
        "call_info": {
          "callee": "irqd_is_started",
          "args": [
            "d"
          ],
          "line": 81
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_is_per_cpu",
          "args": [
            "d"
          ],
          "line": 81
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_debug",
          "args": [
            "\"IRQ %u: Unable to migrate away\\n\"",
            "d->irq"
          ],
          "line": 68
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqd_irq_masked",
          "args": [
            "d"
          ],
          "line": 57
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_can_move_pcntxt",
          "args": [
            "d"
          ],
          "line": 57
        },
        "resolved": true,
        "details": {
          "function_name": "irq_can_move_pcntxt",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/internals.h",
          "lines": "434-437",
          "snippet": "static inline bool irq_can_move_pcntxt(struct irq_data *data)\n{\n\treturn true;\n}",
          "includes": [
            "#include <linux/debugfs.h>",
            "#include \"settings.h\"",
            "#include \"debug.h\"",
            "#include <linux/sched/clock.h>",
            "#include <linux/pm_runtime.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/irqdesc.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/debugfs.h>\n#include \"settings.h\"\n#include \"debug.h\"\n#include <linux/sched/clock.h>\n#include <linux/pm_runtime.h>\n#include <linux/kernel_stat.h>\n#include <linux/irqdesc.h>\n\nstatic inline bool irq_can_move_pcntxt(struct irq_data *data)\n{\n\treturn true;\n}"
        }
      },
      {
        "call_info": {
          "callee": "irq_data_get_irq_chip",
          "args": [
            "d"
          ],
          "line": 56
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_desc_get_irq_data",
          "args": [
            "desc"
          ],
          "line": 55
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic bool migrate_one_irq(struct irq_desc *desc)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(d);\n\tbool maskchip = !irq_can_move_pcntxt(d) && !irqd_irq_masked(d);\n\tconst struct cpumask *affinity;\n\tbool brokeaff = false;\n\tint err;\n\n\t/*\n\t * IRQ chip might be already torn down, but the irq descriptor is\n\t * still in the radix tree. Also if the chip has no affinity setter,\n\t * nothing can be done here.\n\t */\n\tif (!chip || !chip->irq_set_affinity) {\n\t\tpr_debug(\"IRQ %u: Unable to migrate away\\n\", d->irq);\n\t\treturn false;\n\t}\n\n\t/*\n\t * No move required, if:\n\t * - Interrupt is per cpu\n\t * - Interrupt is not started\n\t * - Affinity mask does not include this CPU.\n\t *\n\t * Note: Do not check desc->action as this might be a chained\n\t * interrupt.\n\t */\n\tif (irqd_is_per_cpu(d) || !irqd_is_started(d) || !irq_needs_fixup(d)) {\n\t\t/*\n\t\t * If an irq move is pending, abort it if the dying CPU is\n\t\t * the sole target.\n\t\t */\n\t\tirq_fixup_move_pending(desc, false);\n\t\treturn false;\n\t}\n\n\t/*\n\t * Complete an eventually pending irq move cleanup. If this\n\t * interrupt was moved in hard irq context, then the vectors need\n\t * to be cleaned up. It can't wait until this interrupt actually\n\t * happens and this CPU was involved.\n\t */\n\tirq_force_complete_move(desc);\n\n\t/*\n\t * If there is a setaffinity pending, then try to reuse the pending\n\t * mask, so the last change of the affinity does not get lost. If\n\t * there is no move pending or the pending mask does not contain\n\t * any online CPU, use the current affinity mask.\n\t */\n\tif (irq_fixup_move_pending(desc, true))\n\t\taffinity = irq_desc_get_pending_mask(desc);\n\telse\n\t\taffinity = irq_data_get_affinity_mask(d);\n\n\t/* Mask the chip for interrupts which cannot move in process context */\n\tif (maskchip && chip->irq_mask)\n\t\tchip->irq_mask(d);\n\n\tif (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If the interrupt is managed, then shut it down and leave\n\t\t * the affinity untouched.\n\t\t */\n\t\tif (irqd_affinity_is_managed(d)) {\n\t\t\tirqd_set_managed_shutdown(d);\n\t\t\tirq_shutdown_and_deactivate(desc);\n\t\t\treturn false;\n\t\t}\n\t\taffinity = cpu_online_mask;\n\t\tbrokeaff = true;\n\t}\n\t/*\n\t * Do not set the force argument of irq_do_set_affinity() as this\n\t * disables the masking of offline CPUs from the supplied affinity\n\t * mask and therefore might keep/reassign the irq to the outgoing\n\t * CPU.\n\t */\n\terr = irq_do_set_affinity(d, affinity, false);\n\tif (err) {\n\t\tpr_warn_ratelimited(\"IRQ%u: set affinity failed(%d).\\n\",\n\t\t\t\t    d->irq, err);\n\t\tbrokeaff = false;\n\t}\n\n\tif (maskchip && chip->irq_unmask)\n\t\tchip->irq_unmask(d);\n\n\treturn brokeaff;\n}"
  },
  {
    "function_name": "irq_needs_fixup",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/irq/cpuhotplug.c",
    "lines": "20-51",
    "snippet": "static inline bool irq_needs_fixup(struct irq_data *d)\n{\n\tconst struct cpumask *m = irq_data_get_effective_affinity_mask(d);\n\tunsigned int cpu = smp_processor_id();\n\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\n\t/*\n\t * The cpumask_empty() check is a workaround for interrupt chips,\n\t * which do not implement effective affinity, but the architecture has\n\t * enabled the config switch. Use the general affinity mask instead.\n\t */\n\tif (cpumask_empty(m))\n\t\tm = irq_data_get_affinity_mask(d);\n\n\t/*\n\t * Sanity check. If the mask is not empty when excluding the outgoing\n\t * CPU then it must contain at least one online CPU. The outgoing CPU\n\t * has been removed from the online mask already.\n\t */\n\tif (cpumask_any_but(m, cpu) < nr_cpu_ids &&\n\t    cpumask_any_and(m, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If this happens then there was a missed IRQ fixup at some\n\t\t * point. Warn about it and enforce fixup.\n\t\t */\n\t\tpr_warn(\"Eff. affinity %*pbl of IRQ %u contains only offline CPUs after offlining CPU %u\\n\",\n\t\t\tcpumask_pr_args(m), d->irq, cpu);\n\t\treturn true;\n\t}\n#endif\n\treturn cpumask_test_cpu(cpu, m);\n}",
    "includes": [
      "#include \"internals.h\"",
      "#include <linux/sched/isolation.h>",
      "#include <linux/irq.h>",
      "#include <linux/ratelimit.h>",
      "#include <linux/interrupt.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "m"
          ],
          "line": 50
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"Eff. affinity %*pbl of IRQ %u contains only offline CPUs after offlining CPU %u\\n\"",
            "cpumask_pr_args(m)",
            "d->irq",
            "cpu"
          ],
          "line": 45
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_pr_args",
          "args": [
            "m"
          ],
          "line": 46
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_any_and",
          "args": [
            "m",
            "cpu_online_mask"
          ],
          "line": 40
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_any_but",
          "args": [
            "m",
            "cpu"
          ],
          "line": 39
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_get_affinity_mask",
          "args": [
            "d"
          ],
          "line": 32
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_empty",
          "args": [
            "m"
          ],
          "line": 31
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 23
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irq_data_get_effective_affinity_mask",
          "args": [
            "d"
          ],
          "line": 22
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internals.h\"\n#include <linux/sched/isolation.h>\n#include <linux/irq.h>\n#include <linux/ratelimit.h>\n#include <linux/interrupt.h>\n\nstatic inline bool irq_needs_fixup(struct irq_data *d)\n{\n\tconst struct cpumask *m = irq_data_get_effective_affinity_mask(d);\n\tunsigned int cpu = smp_processor_id();\n\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\n\t/*\n\t * The cpumask_empty() check is a workaround for interrupt chips,\n\t * which do not implement effective affinity, but the architecture has\n\t * enabled the config switch. Use the general affinity mask instead.\n\t */\n\tif (cpumask_empty(m))\n\t\tm = irq_data_get_affinity_mask(d);\n\n\t/*\n\t * Sanity check. If the mask is not empty when excluding the outgoing\n\t * CPU then it must contain at least one online CPU. The outgoing CPU\n\t * has been removed from the online mask already.\n\t */\n\tif (cpumask_any_but(m, cpu) < nr_cpu_ids &&\n\t    cpumask_any_and(m, cpu_online_mask) >= nr_cpu_ids) {\n\t\t/*\n\t\t * If this happens then there was a missed IRQ fixup at some\n\t\t * point. Warn about it and enforce fixup.\n\t\t */\n\t\tpr_warn(\"Eff. affinity %*pbl of IRQ %u contains only offline CPUs after offlining CPU %u\\n\",\n\t\t\tcpumask_pr_args(m), d->irq, cpu);\n\t\treturn true;\n\t}\n#endif\n\treturn cpumask_test_cpu(cpu, m);\n}"
  }
]