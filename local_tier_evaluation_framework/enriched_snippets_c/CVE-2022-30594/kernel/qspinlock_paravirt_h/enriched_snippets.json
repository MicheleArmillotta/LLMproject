[
  {
    "function_name": "__pv_queued_spin_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "547-561",
    "snippet": "__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__pv_queued_spin_unlock_slowpath",
          "args": [
            "lock",
            "locked"
          ],
          "line": 560
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_queued_spin_unlock_slowpath",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "492-534",
          "snippet": "__visible void\n__pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)\n{\n\tstruct pv_node *node;\n\n\tif (unlikely(locked != _Q_SLOW_VAL)) {\n\t\tWARN(!debug_locks_silent,\n\t\t     \"pvqspinlock: lock 0x%lx has corrupted value 0x%x!\\n\",\n\t\t     (unsigned long)lock, atomic_read(&lock->val));\n\t\treturn;\n\t}\n\n\t/*\n\t * A failed cmpxchg doesn't provide any memory-ordering guarantees,\n\t * so we need a barrier to order the read of the node data in\n\t * pv_unhash *after* we've read the lock being _Q_SLOW_VAL.\n\t *\n\t * Matches the cmpxchg() in pv_wait_head_or_lock() setting _Q_SLOW_VAL.\n\t */\n\tsmp_rmb();\n\n\t/*\n\t * Since the above failed to release, this must be the SLOW path.\n\t * Therefore start by looking up the blocked node and unhashing it.\n\t */\n\tnode = pv_unhash(lock);\n\n\t/*\n\t * Now that we have a reference to the (likely) blocked pv_node,\n\t * release the lock.\n\t */\n\tsmp_store_release(&lock->locked, 0);\n\n\t/*\n\t * At this point the memory pointed at by lock can be freed/reused,\n\t * however we can still use the pv_node to kick the CPU.\n\t * The other vCPU may not really be halted, but kicking an active\n\t * vCPU is harmless other than the additional latency in completing\n\t * the unlock.\n\t */\n\tlockevent_inc(pv_kick_unlock);\n\tpv_kick(node->cpu);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [
            "#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)\n\n__visible void\n__pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)\n{\n\tstruct pv_node *node;\n\n\tif (unlikely(locked != _Q_SLOW_VAL)) {\n\t\tWARN(!debug_locks_silent,\n\t\t     \"pvqspinlock: lock 0x%lx has corrupted value 0x%x!\\n\",\n\t\t     (unsigned long)lock, atomic_read(&lock->val));\n\t\treturn;\n\t}\n\n\t/*\n\t * A failed cmpxchg doesn't provide any memory-ordering guarantees,\n\t * so we need a barrier to order the read of the node data in\n\t * pv_unhash *after* we've read the lock being _Q_SLOW_VAL.\n\t *\n\t * Matches the cmpxchg() in pv_wait_head_or_lock() setting _Q_SLOW_VAL.\n\t */\n\tsmp_rmb();\n\n\t/*\n\t * Since the above failed to release, this must be the SLOW path.\n\t * Therefore start by looking up the blocked node and unhashing it.\n\t */\n\tnode = pv_unhash(lock);\n\n\t/*\n\t * Now that we have a reference to the (likely) blocked pv_node,\n\t * release the lock.\n\t */\n\tsmp_store_release(&lock->locked, 0);\n\n\t/*\n\t * At this point the memory pointed at by lock can be freed/reused,\n\t * however we can still use the pv_node to kick the CPU.\n\t * The other vCPU may not really be halted, but kicking an active\n\t * vCPU is harmless other than the additional latency in completing\n\t * the unlock.\n\t */\n\tlockevent_inc(pv_kick_unlock);\n\tpv_kick(node->cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "locked == _Q_LOCKED_VAL"
          ],
          "line": 557
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cmpxchg_release",
          "args": [
            "&lock->locked",
            "_Q_LOCKED_VAL",
            "0"
          ],
          "line": 556
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_release",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "261-266",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}"
  },
  {
    "function_name": "__pv_queued_spin_unlock_slowpath",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "492-534",
    "snippet": "__visible void\n__pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)\n{\n\tstruct pv_node *node;\n\n\tif (unlikely(locked != _Q_SLOW_VAL)) {\n\t\tWARN(!debug_locks_silent,\n\t\t     \"pvqspinlock: lock 0x%lx has corrupted value 0x%x!\\n\",\n\t\t     (unsigned long)lock, atomic_read(&lock->val));\n\t\treturn;\n\t}\n\n\t/*\n\t * A failed cmpxchg doesn't provide any memory-ordering guarantees,\n\t * so we need a barrier to order the read of the node data in\n\t * pv_unhash *after* we've read the lock being _Q_SLOW_VAL.\n\t *\n\t * Matches the cmpxchg() in pv_wait_head_or_lock() setting _Q_SLOW_VAL.\n\t */\n\tsmp_rmb();\n\n\t/*\n\t * Since the above failed to release, this must be the SLOW path.\n\t * Therefore start by looking up the blocked node and unhashing it.\n\t */\n\tnode = pv_unhash(lock);\n\n\t/*\n\t * Now that we have a reference to the (likely) blocked pv_node,\n\t * release the lock.\n\t */\n\tsmp_store_release(&lock->locked, 0);\n\n\t/*\n\t * At this point the memory pointed at by lock can be freed/reused,\n\t * however we can still use the pv_node to kick the CPU.\n\t * The other vCPU may not really be halted, but kicking an active\n\t * vCPU is harmless other than the additional latency in completing\n\t * the unlock.\n\t */\n\tlockevent_inc(pv_kick_unlock);\n\tpv_kick(node->cpu);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [
      "#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "pv_kick",
          "args": [
            "node->cpu"
          ],
          "line": 533
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_kick",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_stat.h",
          "lines": "108-115",
          "snippet": "static inline void __pv_kick(int cpu)\n{\n\tu64 start = sched_clock();\n\n\tper_cpu(pv_kick_time, cpu) = start;\n\tpv_kick(cpu);\n\tthis_cpu_add(EVENT_COUNT(pv_latency_kick), sched_clock() - start);\n}",
          "includes": [
            "#include <linux/fs.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include \"lock_events.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/fs.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include \"lock_events.h\"\n\nstatic inline void __pv_kick(int cpu)\n{\n\tu64 start = sched_clock();\n\n\tper_cpu(pv_kick_time, cpu) = start;\n\tpv_kick(cpu);\n\tthis_cpu_add(EVENT_COUNT(pv_latency_kick), sched_clock() - start);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "pv_kick_unlock"
          ],
          "line": 532
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_store_release",
          "args": [
            "&lock->locked",
            "0"
          ],
          "line": 523
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_unhash",
          "args": [
            "lock"
          ],
          "line": 517
        },
        "resolved": true,
        "details": {
          "function_name": "pv_unhash",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "239-260",
          "snippet": "static struct pv_node *pv_unhash(struct qspinlock *lock)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tstruct pv_node *node;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\tif (READ_ONCE(he->lock) == lock) {\n\t\t\tnode = READ_ONCE(he->node);\n\t\t\tWRITE_ONCE(he->lock, NULL);\n\t\t\treturn node;\n\t\t}\n\t}\n\t/*\n\t * Hard assume we'll find an entry.\n\t *\n\t * This guarantees a limited lookup time and is itself guaranteed by\n\t * having the lock owner do the unhash -- IFF the unlock sees the\n\t * SLOW flag, there MUST be a hash entry.\n\t */\n\tBUG();\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static unsigned int pv_lock_hash_bits"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic unsigned int pv_lock_hash_bits;\n\nstatic struct pv_node *pv_unhash(struct qspinlock *lock)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tstruct pv_node *node;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\tif (READ_ONCE(he->lock) == lock) {\n\t\t\tnode = READ_ONCE(he->node);\n\t\t\tWRITE_ONCE(he->lock, NULL);\n\t\t\treturn node;\n\t\t}\n\t}\n\t/*\n\t * Hard assume we'll find an entry.\n\t *\n\t * This guarantees a limited lookup time and is itself guaranteed by\n\t * having the lock owner do the unhash -- IFF the unlock sees the\n\t * SLOW flag, there MUST be a hash entry.\n\t */\n\tBUG();\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 511
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN",
          "args": [
            "!debug_locks_silent",
            "\"pvqspinlock: lock 0x%lx has corrupted value 0x%x!\\n\"",
            "(unsigned long)lock",
            "atomic_read(&lock->val)"
          ],
          "line": 498
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&lock->val"
          ],
          "line": 500
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "locked != _Q_SLOW_VAL"
          ],
          "line": 497
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)\n\n__visible void\n__pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)\n{\n\tstruct pv_node *node;\n\n\tif (unlikely(locked != _Q_SLOW_VAL)) {\n\t\tWARN(!debug_locks_silent,\n\t\t     \"pvqspinlock: lock 0x%lx has corrupted value 0x%x!\\n\",\n\t\t     (unsigned long)lock, atomic_read(&lock->val));\n\t\treturn;\n\t}\n\n\t/*\n\t * A failed cmpxchg doesn't provide any memory-ordering guarantees,\n\t * so we need a barrier to order the read of the node data in\n\t * pv_unhash *after* we've read the lock being _Q_SLOW_VAL.\n\t *\n\t * Matches the cmpxchg() in pv_wait_head_or_lock() setting _Q_SLOW_VAL.\n\t */\n\tsmp_rmb();\n\n\t/*\n\t * Since the above failed to release, this must be the SLOW path.\n\t * Therefore start by looking up the blocked node and unhashing it.\n\t */\n\tnode = pv_unhash(lock);\n\n\t/*\n\t * Now that we have a reference to the (likely) blocked pv_node,\n\t * release the lock.\n\t */\n\tsmp_store_release(&lock->locked, 0);\n\n\t/*\n\t * At this point the memory pointed at by lock can be freed/reused,\n\t * however we can still use the pv_node to kick the CPU.\n\t * The other vCPU may not really be halted, but kicking an active\n\t * vCPU is harmless other than the additional latency in completing\n\t * the unlock.\n\t */\n\tlockevent_inc(pv_kick_unlock);\n\tpv_kick(node->cpu);\n}"
  },
  {
    "function_name": "pv_wait_head_or_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "402-486",
    "snippet": "static u32\npv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct qspinlock **lp = NULL;\n\tint waitcnt = 0;\n\tint loop;\n\n\t/*\n\t * If pv_kick_node() already advanced our state, we don't need to\n\t * insert ourselves into the hash table anymore.\n\t */\n\tif (READ_ONCE(pn->state) == vcpu_hashed)\n\t\tlp = (struct qspinlock **)1;\n\n\t/*\n\t * Tracking # of slowpath locking operations\n\t */\n\tlockevent_inc(lock_slowpath);\n\n\tfor (;; waitcnt++) {\n\t\t/*\n\t\t * Set correct vCPU state to be used by queue node wait-early\n\t\t * mechanism.\n\t\t */\n\t\tWRITE_ONCE(pn->state, vcpu_running);\n\n\t\t/*\n\t\t * Set the pending bit in the active lock spinning loop to\n\t\t * disable lock stealing before attempting to acquire the lock.\n\t\t */\n\t\tset_pending(lock);\n\t\tfor (loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (trylock_clear_pending(lock))\n\t\t\t\tgoto gotlock;\n\t\t\tcpu_relax();\n\t\t}\n\t\tclear_pending(lock);\n\n\n\t\tif (!lp) { /* ONCE */\n\t\t\tlp = pv_hash(lock, pn);\n\n\t\t\t/*\n\t\t\t * We must hash before setting _Q_SLOW_VAL, such that\n\t\t\t * when we observe _Q_SLOW_VAL in __pv_queued_spin_unlock()\n\t\t\t * we'll be sure to be able to observe our hash entry.\n\t\t\t *\n\t\t\t *   [S] <hash>                 [Rmw] l->locked == _Q_SLOW_VAL\n\t\t\t *       MB                           RMB\n\t\t\t * [RmW] l->locked = _Q_SLOW_VAL  [L] <unhash>\n\t\t\t *\n\t\t\t * Matches the smp_rmb() in __pv_queued_spin_unlock().\n\t\t\t */\n\t\t\tif (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {\n\t\t\t\t/*\n\t\t\t\t * The lock was free and now we own the lock.\n\t\t\t\t * Change the lock value back to _Q_LOCKED_VAL\n\t\t\t\t * and unhash the table.\n\t\t\t\t */\n\t\t\t\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n\t\t\t\tWRITE_ONCE(*lp, NULL);\n\t\t\t\tgoto gotlock;\n\t\t\t}\n\t\t}\n\t\tWRITE_ONCE(pn->state, vcpu_hashed);\n\t\tlockevent_inc(pv_wait_head);\n\t\tlockevent_cond_inc(pv_wait_again, waitcnt);\n\t\tpv_wait(&lock->locked, _Q_SLOW_VAL);\n\n\t\t/*\n\t\t * Because of lock stealing, the queue head vCPU may not be\n\t\t * able to acquire the lock before it has to wait again.\n\t\t */\n\t}\n\n\t/*\n\t * The cmpxchg() or xchg() call before coming here provides the\n\t * acquire semantics for locking. The dummy ORing of _Q_LOCKED_VAL\n\t * here is to indicate to the compiler that the value will always\n\t * be nozero to enable better code optimization.\n\t */\ngotlock:\n\treturn (u32)(atomic_read(&lock->val) | _Q_LOCKED_VAL);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [
      "#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "",
          "args": [
            "atomic_read(&lock->val) | _Q_LOCKED_VAL"
          ],
          "line": 485
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&lock->val"
          ],
          "line": 485
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_wait",
          "args": [
            "&lock->locked",
            "_Q_SLOW_VAL"
          ],
          "line": 470
        },
        "resolved": true,
        "details": {
          "function_name": "pv_wait_head_or_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "402-486",
          "snippet": "static u32\npv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct qspinlock **lp = NULL;\n\tint waitcnt = 0;\n\tint loop;\n\n\t/*\n\t * If pv_kick_node() already advanced our state, we don't need to\n\t * insert ourselves into the hash table anymore.\n\t */\n\tif (READ_ONCE(pn->state) == vcpu_hashed)\n\t\tlp = (struct qspinlock **)1;\n\n\t/*\n\t * Tracking # of slowpath locking operations\n\t */\n\tlockevent_inc(lock_slowpath);\n\n\tfor (;; waitcnt++) {\n\t\t/*\n\t\t * Set correct vCPU state to be used by queue node wait-early\n\t\t * mechanism.\n\t\t */\n\t\tWRITE_ONCE(pn->state, vcpu_running);\n\n\t\t/*\n\t\t * Set the pending bit in the active lock spinning loop to\n\t\t * disable lock stealing before attempting to acquire the lock.\n\t\t */\n\t\tset_pending(lock);\n\t\tfor (loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (trylock_clear_pending(lock))\n\t\t\t\tgoto gotlock;\n\t\t\tcpu_relax();\n\t\t}\n\t\tclear_pending(lock);\n\n\n\t\tif (!lp) { /* ONCE */\n\t\t\tlp = pv_hash(lock, pn);\n\n\t\t\t/*\n\t\t\t * We must hash before setting _Q_SLOW_VAL, such that\n\t\t\t * when we observe _Q_SLOW_VAL in __pv_queued_spin_unlock()\n\t\t\t * we'll be sure to be able to observe our hash entry.\n\t\t\t *\n\t\t\t *   [S] <hash>                 [Rmw] l->locked == _Q_SLOW_VAL\n\t\t\t *       MB                           RMB\n\t\t\t * [RmW] l->locked = _Q_SLOW_VAL  [L] <unhash>\n\t\t\t *\n\t\t\t * Matches the smp_rmb() in __pv_queued_spin_unlock().\n\t\t\t */\n\t\t\tif (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {\n\t\t\t\t/*\n\t\t\t\t * The lock was free and now we own the lock.\n\t\t\t\t * Change the lock value back to _Q_LOCKED_VAL\n\t\t\t\t * and unhash the table.\n\t\t\t\t */\n\t\t\t\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n\t\t\t\tWRITE_ONCE(*lp, NULL);\n\t\t\t\tgoto gotlock;\n\t\t\t}\n\t\t}\n\t\tWRITE_ONCE(pn->state, vcpu_hashed);\n\t\tlockevent_inc(pv_wait_head);\n\t\tlockevent_cond_inc(pv_wait_again, waitcnt);\n\t\tpv_wait(&lock->locked, _Q_SLOW_VAL);\n\n\t\t/*\n\t\t * Because of lock stealing, the queue head vCPU may not be\n\t\t * able to acquire the lock before it has to wait again.\n\t\t */\n\t}\n\n\t/*\n\t * The cmpxchg() or xchg() call before coming here provides the\n\t * acquire semantics for locking. The dummy ORing of _Q_LOCKED_VAL\n\t * here is to indicate to the compiler that the value will always\n\t * be nozero to enable better code optimization.\n\t */\ngotlock:\n\treturn (u32)(atomic_read(&lock->val) | _Q_LOCKED_VAL);\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "lockevent_cond_inc",
          "args": [
            "pv_wait_again",
            "waitcnt"
          ],
          "line": 469
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "pv_wait_head"
          ],
          "line": 468
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "pn->state",
            "vcpu_hashed"
          ],
          "line": 467
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "*lp",
            "NULL"
          ],
          "line": 463
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "lock->locked",
            "_Q_LOCKED_VAL"
          ],
          "line": 462
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "xchg",
          "args": [
            "&lock->locked",
            "_Q_SLOW_VAL"
          ],
          "line": 456
        },
        "resolved": true,
        "details": {
          "function_name": "xchg_tail",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock.c",
          "lines": "220-238",
          "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [
            "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}"
        }
      },
      {
        "call_info": {
          "callee": "pv_hash",
          "args": [
            "lock",
            "pn"
          ],
          "line": 443
        },
        "resolved": true,
        "details": {
          "function_name": "pv_hash",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "212-237",
          "snippet": "static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tint hopcnt = 0;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\thopcnt++;\n\t\tif (!cmpxchg(&he->lock, NULL, lock)) {\n\t\t\tWRITE_ONCE(he->node, node);\n\t\t\tlockevent_pv_hop(hopcnt);\n\t\t\treturn &he->lock;\n\t\t}\n\t}\n\t/*\n\t * Hard assume there is a free entry for us.\n\t *\n\t * This is guaranteed by ensuring every blocked lock only ever consumes\n\t * a single entry, and since we only have 4 nesting levels per CPU\n\t * and allocated 4*nr_possible_cpus(), this must be so.\n\t *\n\t * The single entry is guaranteed by having the lock owner unhash\n\t * before it releases.\n\t */\n\tBUG();\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static unsigned int pv_lock_hash_bits"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic unsigned int pv_lock_hash_bits;\n\nstatic struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tint hopcnt = 0;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\thopcnt++;\n\t\tif (!cmpxchg(&he->lock, NULL, lock)) {\n\t\t\tWRITE_ONCE(he->node, node);\n\t\t\tlockevent_pv_hop(hopcnt);\n\t\t\treturn &he->lock;\n\t\t}\n\t}\n\t/*\n\t * Hard assume there is a free entry for us.\n\t *\n\t * This is guaranteed by ensuring every blocked lock only ever consumes\n\t * a single entry, and since we only have 4 nesting levels per CPU\n\t * and allocated 4*nr_possible_cpus(), this must be so.\n\t *\n\t * The single entry is guaranteed by having the lock owner unhash\n\t * before it releases.\n\t */\n\tBUG();\n}"
        }
      },
      {
        "call_info": {
          "callee": "clear_pending",
          "args": [
            "lock"
          ],
          "line": 439
        },
        "resolved": true,
        "details": {
          "function_name": "trylock_clear_pending",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "131-152",
          "snippet": "static __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\tint val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tint old, new;\n\n\t\tif (val  & _Q_LOCKED_MASK)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Try to clear pending bit & set locked bit\n\t\t */\n\t\told = val;\n\t\tnew = (val & ~_Q_PENDING_MASK) | _Q_LOCKED_VAL;\n\t\tval = atomic_cmpxchg_acquire(&lock->val, old, new);\n\n\t\tif (val == old)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\tint val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tint old, new;\n\n\t\tif (val  & _Q_LOCKED_MASK)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Try to clear pending bit & set locked bit\n\t\t */\n\t\told = val;\n\t\tnew = (val & ~_Q_PENDING_MASK) | _Q_LOCKED_VAL;\n\t\tval = atomic_cmpxchg_acquire(&lock->val, old, new);\n\n\t\tif (val == old)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 437
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_pending",
          "args": [
            "lock"
          ],
          "line": 433
        },
        "resolved": true,
        "details": {
          "function_name": "set_pending",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "126-129",
          "snippet": "static __always_inline void set_pending(struct qspinlock *lock)\n{\n\tatomic_or(_Q_PENDING_VAL, &lock->val);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic __always_inline void set_pending(struct qspinlock *lock)\n{\n\tatomic_or(_Q_PENDING_VAL, &lock->val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "pn->state",
            "vcpu_running"
          ],
          "line": 427
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "lock_slowpath"
          ],
          "line": 420
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "pn->state"
          ],
          "line": 414
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)\n\nstatic u32\npv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct qspinlock **lp = NULL;\n\tint waitcnt = 0;\n\tint loop;\n\n\t/*\n\t * If pv_kick_node() already advanced our state, we don't need to\n\t * insert ourselves into the hash table anymore.\n\t */\n\tif (READ_ONCE(pn->state) == vcpu_hashed)\n\t\tlp = (struct qspinlock **)1;\n\n\t/*\n\t * Tracking # of slowpath locking operations\n\t */\n\tlockevent_inc(lock_slowpath);\n\n\tfor (;; waitcnt++) {\n\t\t/*\n\t\t * Set correct vCPU state to be used by queue node wait-early\n\t\t * mechanism.\n\t\t */\n\t\tWRITE_ONCE(pn->state, vcpu_running);\n\n\t\t/*\n\t\t * Set the pending bit in the active lock spinning loop to\n\t\t * disable lock stealing before attempting to acquire the lock.\n\t\t */\n\t\tset_pending(lock);\n\t\tfor (loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (trylock_clear_pending(lock))\n\t\t\t\tgoto gotlock;\n\t\t\tcpu_relax();\n\t\t}\n\t\tclear_pending(lock);\n\n\n\t\tif (!lp) { /* ONCE */\n\t\t\tlp = pv_hash(lock, pn);\n\n\t\t\t/*\n\t\t\t * We must hash before setting _Q_SLOW_VAL, such that\n\t\t\t * when we observe _Q_SLOW_VAL in __pv_queued_spin_unlock()\n\t\t\t * we'll be sure to be able to observe our hash entry.\n\t\t\t *\n\t\t\t *   [S] <hash>                 [Rmw] l->locked == _Q_SLOW_VAL\n\t\t\t *       MB                           RMB\n\t\t\t * [RmW] l->locked = _Q_SLOW_VAL  [L] <unhash>\n\t\t\t *\n\t\t\t * Matches the smp_rmb() in __pv_queued_spin_unlock().\n\t\t\t */\n\t\t\tif (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {\n\t\t\t\t/*\n\t\t\t\t * The lock was free and now we own the lock.\n\t\t\t\t * Change the lock value back to _Q_LOCKED_VAL\n\t\t\t\t * and unhash the table.\n\t\t\t\t */\n\t\t\t\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n\t\t\t\tWRITE_ONCE(*lp, NULL);\n\t\t\t\tgoto gotlock;\n\t\t\t}\n\t\t}\n\t\tWRITE_ONCE(pn->state, vcpu_hashed);\n\t\tlockevent_inc(pv_wait_head);\n\t\tlockevent_cond_inc(pv_wait_again, waitcnt);\n\t\tpv_wait(&lock->locked, _Q_SLOW_VAL);\n\n\t\t/*\n\t\t * Because of lock stealing, the queue head vCPU may not be\n\t\t * able to acquire the lock before it has to wait again.\n\t\t */\n\t}\n\n\t/*\n\t * The cmpxchg() or xchg() call before coming here provides the\n\t * acquire semantics for locking. The dummy ORing of _Q_LOCKED_VAL\n\t * here is to indicate to the compiler that the value will always\n\t * be nozero to enable better code optimization.\n\t */\ngotlock:\n\treturn (u32)(atomic_read(&lock->val) | _Q_LOCKED_VAL);\n}"
  },
  {
    "function_name": "pv_kick_node",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "360-393",
    "snippet": "static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\n\t/*\n\t * If the vCPU is indeed halted, advance its state to match that of\n\t * pv_wait_node(). If OTOH this fails, the vCPU was running and will\n\t * observe its next->locked value and advance itself.\n\t *\n\t * Matches with smp_store_mb() and cmpxchg() in pv_wait_node()\n\t *\n\t * The write to next->locked in arch_mcs_spin_unlock_contended()\n\t * must be ordered before the read of pn->state in the cmpxchg()\n\t * below for the code to work correctly. To guarantee full ordering\n\t * irrespective of the success or failure of the cmpxchg(),\n\t * a relaxed version with explicit barrier is used. The control\n\t * dependency will order the reading of pn->state before any\n\t * subsequent writes.\n\t */\n\tsmp_mb__before_atomic();\n\tif (cmpxchg_relaxed(&pn->state, vcpu_halted, vcpu_hashed)\n\t    != vcpu_halted)\n\t\treturn;\n\n\t/*\n\t * Put the lock into the hash table and set the _Q_SLOW_VAL.\n\t *\n\t * As this is the same vCPU that will check the _Q_SLOW_VAL value and\n\t * the hash table later on at unlock time, no atomic instruction is\n\t * needed.\n\t */\n\tWRITE_ONCE(lock->locked, _Q_SLOW_VAL);\n\t(void)pv_hash(lock, pn);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [
      "#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "pv_hash",
          "args": [
            "lock",
            "pn"
          ],
          "line": 392
        },
        "resolved": true,
        "details": {
          "function_name": "pv_hash",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "212-237",
          "snippet": "static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tint hopcnt = 0;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\thopcnt++;\n\t\tif (!cmpxchg(&he->lock, NULL, lock)) {\n\t\t\tWRITE_ONCE(he->node, node);\n\t\t\tlockevent_pv_hop(hopcnt);\n\t\t\treturn &he->lock;\n\t\t}\n\t}\n\t/*\n\t * Hard assume there is a free entry for us.\n\t *\n\t * This is guaranteed by ensuring every blocked lock only ever consumes\n\t * a single entry, and since we only have 4 nesting levels per CPU\n\t * and allocated 4*nr_possible_cpus(), this must be so.\n\t *\n\t * The single entry is guaranteed by having the lock owner unhash\n\t * before it releases.\n\t */\n\tBUG();\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static unsigned int pv_lock_hash_bits"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic unsigned int pv_lock_hash_bits;\n\nstatic struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tint hopcnt = 0;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\thopcnt++;\n\t\tif (!cmpxchg(&he->lock, NULL, lock)) {\n\t\t\tWRITE_ONCE(he->node, node);\n\t\t\tlockevent_pv_hop(hopcnt);\n\t\t\treturn &he->lock;\n\t\t}\n\t}\n\t/*\n\t * Hard assume there is a free entry for us.\n\t *\n\t * This is guaranteed by ensuring every blocked lock only ever consumes\n\t * a single entry, and since we only have 4 nesting levels per CPU\n\t * and allocated 4*nr_possible_cpus(), this must be so.\n\t *\n\t * The single entry is guaranteed by having the lock owner unhash\n\t * before it releases.\n\t */\n\tBUG();\n}"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "lock->locked",
            "_Q_SLOW_VAL"
          ],
          "line": 391
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cmpxchg_relaxed",
          "args": [
            "&pn->state",
            "vcpu_halted",
            "vcpu_hashed"
          ],
          "line": 380
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb__before_atomic",
          "args": [],
          "line": 379
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)\n\nstatic void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\n\t/*\n\t * If the vCPU is indeed halted, advance its state to match that of\n\t * pv_wait_node(). If OTOH this fails, the vCPU was running and will\n\t * observe its next->locked value and advance itself.\n\t *\n\t * Matches with smp_store_mb() and cmpxchg() in pv_wait_node()\n\t *\n\t * The write to next->locked in arch_mcs_spin_unlock_contended()\n\t * must be ordered before the read of pn->state in the cmpxchg()\n\t * below for the code to work correctly. To guarantee full ordering\n\t * irrespective of the success or failure of the cmpxchg(),\n\t * a relaxed version with explicit barrier is used. The control\n\t * dependency will order the reading of pn->state before any\n\t * subsequent writes.\n\t */\n\tsmp_mb__before_atomic();\n\tif (cmpxchg_relaxed(&pn->state, vcpu_halted, vcpu_hashed)\n\t    != vcpu_halted)\n\t\treturn;\n\n\t/*\n\t * Put the lock into the hash table and set the _Q_SLOW_VAL.\n\t *\n\t * As this is the same vCPU that will check the _Q_SLOW_VAL value and\n\t * the hash table later on at unlock time, no atomic instruction is\n\t * needed.\n\t */\n\tWRITE_ONCE(lock->locked, _Q_SLOW_VAL);\n\t(void)pv_hash(lock, pn);\n}"
  },
  {
    "function_name": "pv_wait_node",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "293-351",
    "snippet": "static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct pv_node *pp = (struct pv_node *)prev;\n\tint loop;\n\tbool wait_early;\n\n\tfor (;;) {\n\t\tfor (wait_early = false, loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (READ_ONCE(node->locked))\n\t\t\t\treturn;\n\t\t\tif (pv_wait_early(pp, loop)) {\n\t\t\t\twait_early = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t/*\n\t\t * Order pn->state vs pn->locked thusly:\n\t\t *\n\t\t * [S] pn->state = vcpu_halted\t  [S] next->locked = 1\n\t\t *     MB\t\t\t      MB\n\t\t * [L] pn->locked\t\t[RmW] pn->state = vcpu_hashed\n\t\t *\n\t\t * Matches the cmpxchg() from pv_kick_node().\n\t\t */\n\t\tsmp_store_mb(pn->state, vcpu_halted);\n\n\t\tif (!READ_ONCE(node->locked)) {\n\t\t\tlockevent_inc(pv_wait_node);\n\t\t\tlockevent_cond_inc(pv_wait_early, wait_early);\n\t\t\tpv_wait(&pn->state, vcpu_halted);\n\t\t}\n\n\t\t/*\n\t\t * If pv_kick_node() changed us to vcpu_hashed, retain that\n\t\t * value so that pv_wait_head_or_lock() knows to not also try\n\t\t * to hash this lock.\n\t\t */\n\t\tcmpxchg(&pn->state, vcpu_halted, vcpu_running);\n\n\t\t/*\n\t\t * If the locked flag is still not set after wakeup, it is a\n\t\t * spurious wakeup and the vCPU should wait again. However,\n\t\t * there is a pretty high overhead for CPU halting and kicking.\n\t\t * So it is better to spin for a while in the hope that the\n\t\t * MCS lock will be released soon.\n\t\t */\n\t\tlockevent_cond_inc(pv_spurious_wakeup,\n\t\t\t\t  !READ_ONCE(node->locked));\n\t}\n\n\t/*\n\t * By now our node->locked should be 1 and our caller will not actually\n\t * spin-wait for it. We do however rely on our caller to do a\n\t * load-acquire for us.\n\t */\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "lockevent_cond_inc",
          "args": [
            "pv_spurious_wakeup",
            "!READ_ONCE(node->locked)"
          ],
          "line": 342
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "node->locked"
          ],
          "line": 343
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cmpxchg",
          "args": [
            "&pn->state",
            "vcpu_halted",
            "vcpu_running"
          ],
          "line": 333
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_release",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "261-266",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "pv_wait",
          "args": [
            "&pn->state",
            "vcpu_halted"
          ],
          "line": 325
        },
        "resolved": true,
        "details": {
          "function_name": "pv_wait_head_or_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "402-486",
          "snippet": "static u32\npv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct qspinlock **lp = NULL;\n\tint waitcnt = 0;\n\tint loop;\n\n\t/*\n\t * If pv_kick_node() already advanced our state, we don't need to\n\t * insert ourselves into the hash table anymore.\n\t */\n\tif (READ_ONCE(pn->state) == vcpu_hashed)\n\t\tlp = (struct qspinlock **)1;\n\n\t/*\n\t * Tracking # of slowpath locking operations\n\t */\n\tlockevent_inc(lock_slowpath);\n\n\tfor (;; waitcnt++) {\n\t\t/*\n\t\t * Set correct vCPU state to be used by queue node wait-early\n\t\t * mechanism.\n\t\t */\n\t\tWRITE_ONCE(pn->state, vcpu_running);\n\n\t\t/*\n\t\t * Set the pending bit in the active lock spinning loop to\n\t\t * disable lock stealing before attempting to acquire the lock.\n\t\t */\n\t\tset_pending(lock);\n\t\tfor (loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (trylock_clear_pending(lock))\n\t\t\t\tgoto gotlock;\n\t\t\tcpu_relax();\n\t\t}\n\t\tclear_pending(lock);\n\n\n\t\tif (!lp) { /* ONCE */\n\t\t\tlp = pv_hash(lock, pn);\n\n\t\t\t/*\n\t\t\t * We must hash before setting _Q_SLOW_VAL, such that\n\t\t\t * when we observe _Q_SLOW_VAL in __pv_queued_spin_unlock()\n\t\t\t * we'll be sure to be able to observe our hash entry.\n\t\t\t *\n\t\t\t *   [S] <hash>                 [Rmw] l->locked == _Q_SLOW_VAL\n\t\t\t *       MB                           RMB\n\t\t\t * [RmW] l->locked = _Q_SLOW_VAL  [L] <unhash>\n\t\t\t *\n\t\t\t * Matches the smp_rmb() in __pv_queued_spin_unlock().\n\t\t\t */\n\t\t\tif (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {\n\t\t\t\t/*\n\t\t\t\t * The lock was free and now we own the lock.\n\t\t\t\t * Change the lock value back to _Q_LOCKED_VAL\n\t\t\t\t * and unhash the table.\n\t\t\t\t */\n\t\t\t\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n\t\t\t\tWRITE_ONCE(*lp, NULL);\n\t\t\t\tgoto gotlock;\n\t\t\t}\n\t\t}\n\t\tWRITE_ONCE(pn->state, vcpu_hashed);\n\t\tlockevent_inc(pv_wait_head);\n\t\tlockevent_cond_inc(pv_wait_again, waitcnt);\n\t\tpv_wait(&lock->locked, _Q_SLOW_VAL);\n\n\t\t/*\n\t\t * Because of lock stealing, the queue head vCPU may not be\n\t\t * able to acquire the lock before it has to wait again.\n\t\t */\n\t}\n\n\t/*\n\t * The cmpxchg() or xchg() call before coming here provides the\n\t * acquire semantics for locking. The dummy ORing of _Q_LOCKED_VAL\n\t * here is to indicate to the compiler that the value will always\n\t * be nozero to enable better code optimization.\n\t */\ngotlock:\n\treturn (u32)(atomic_read(&lock->val) | _Q_LOCKED_VAL);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [
            "#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)\n\nstatic u32\npv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct qspinlock **lp = NULL;\n\tint waitcnt = 0;\n\tint loop;\n\n\t/*\n\t * If pv_kick_node() already advanced our state, we don't need to\n\t * insert ourselves into the hash table anymore.\n\t */\n\tif (READ_ONCE(pn->state) == vcpu_hashed)\n\t\tlp = (struct qspinlock **)1;\n\n\t/*\n\t * Tracking # of slowpath locking operations\n\t */\n\tlockevent_inc(lock_slowpath);\n\n\tfor (;; waitcnt++) {\n\t\t/*\n\t\t * Set correct vCPU state to be used by queue node wait-early\n\t\t * mechanism.\n\t\t */\n\t\tWRITE_ONCE(pn->state, vcpu_running);\n\n\t\t/*\n\t\t * Set the pending bit in the active lock spinning loop to\n\t\t * disable lock stealing before attempting to acquire the lock.\n\t\t */\n\t\tset_pending(lock);\n\t\tfor (loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (trylock_clear_pending(lock))\n\t\t\t\tgoto gotlock;\n\t\t\tcpu_relax();\n\t\t}\n\t\tclear_pending(lock);\n\n\n\t\tif (!lp) { /* ONCE */\n\t\t\tlp = pv_hash(lock, pn);\n\n\t\t\t/*\n\t\t\t * We must hash before setting _Q_SLOW_VAL, such that\n\t\t\t * when we observe _Q_SLOW_VAL in __pv_queued_spin_unlock()\n\t\t\t * we'll be sure to be able to observe our hash entry.\n\t\t\t *\n\t\t\t *   [S] <hash>                 [Rmw] l->locked == _Q_SLOW_VAL\n\t\t\t *       MB                           RMB\n\t\t\t * [RmW] l->locked = _Q_SLOW_VAL  [L] <unhash>\n\t\t\t *\n\t\t\t * Matches the smp_rmb() in __pv_queued_spin_unlock().\n\t\t\t */\n\t\t\tif (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {\n\t\t\t\t/*\n\t\t\t\t * The lock was free and now we own the lock.\n\t\t\t\t * Change the lock value back to _Q_LOCKED_VAL\n\t\t\t\t * and unhash the table.\n\t\t\t\t */\n\t\t\t\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n\t\t\t\tWRITE_ONCE(*lp, NULL);\n\t\t\t\tgoto gotlock;\n\t\t\t}\n\t\t}\n\t\tWRITE_ONCE(pn->state, vcpu_hashed);\n\t\tlockevent_inc(pv_wait_head);\n\t\tlockevent_cond_inc(pv_wait_again, waitcnt);\n\t\tpv_wait(&lock->locked, _Q_SLOW_VAL);\n\n\t\t/*\n\t\t * Because of lock stealing, the queue head vCPU may not be\n\t\t * able to acquire the lock before it has to wait again.\n\t\t */\n\t}\n\n\t/*\n\t * The cmpxchg() or xchg() call before coming here provides the\n\t * acquire semantics for locking. The dummy ORing of _Q_LOCKED_VAL\n\t * here is to indicate to the compiler that the value will always\n\t * be nozero to enable better code optimization.\n\t */\ngotlock:\n\treturn (u32)(atomic_read(&lock->val) | _Q_LOCKED_VAL);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockevent_cond_inc",
          "args": [
            "pv_wait_early",
            "wait_early"
          ],
          "line": 324
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "pv_wait_node"
          ],
          "line": 323
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "node->locked"
          ],
          "line": 322
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_store_mb",
          "args": [
            "pn->state",
            "vcpu_halted"
          ],
          "line": 320
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 308
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pv_wait_early",
          "args": [
            "pp",
            "loop"
          ],
          "line": 304
        },
        "resolved": true,
        "details": {
          "function_name": "pv_wait_early",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "266-273",
          "snippet": "static inline bool\npv_wait_early(struct pv_node *prev, int loop)\n{\n\tif ((loop & PV_PREV_CHECK_MASK) != 0)\n\t\treturn false;\n\n\treturn READ_ONCE(prev->state) != vcpu_running;\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [
            "#define PV_PREV_CHECK_MASK\t0xff"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define PV_PREV_CHECK_MASK\t0xff\n\nstatic inline bool\npv_wait_early(struct pv_node *prev, int loop)\n{\n\tif ((loop & PV_PREV_CHECK_MASK) != 0)\n\t\treturn false;\n\n\treturn READ_ONCE(prev->state) != vcpu_running;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "node->locked"
          ],
          "line": 302
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct pv_node *pp = (struct pv_node *)prev;\n\tint loop;\n\tbool wait_early;\n\n\tfor (;;) {\n\t\tfor (wait_early = false, loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (READ_ONCE(node->locked))\n\t\t\t\treturn;\n\t\t\tif (pv_wait_early(pp, loop)) {\n\t\t\t\twait_early = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t/*\n\t\t * Order pn->state vs pn->locked thusly:\n\t\t *\n\t\t * [S] pn->state = vcpu_halted\t  [S] next->locked = 1\n\t\t *     MB\t\t\t      MB\n\t\t * [L] pn->locked\t\t[RmW] pn->state = vcpu_hashed\n\t\t *\n\t\t * Matches the cmpxchg() from pv_kick_node().\n\t\t */\n\t\tsmp_store_mb(pn->state, vcpu_halted);\n\n\t\tif (!READ_ONCE(node->locked)) {\n\t\t\tlockevent_inc(pv_wait_node);\n\t\t\tlockevent_cond_inc(pv_wait_early, wait_early);\n\t\t\tpv_wait(&pn->state, vcpu_halted);\n\t\t}\n\n\t\t/*\n\t\t * If pv_kick_node() changed us to vcpu_hashed, retain that\n\t\t * value so that pv_wait_head_or_lock() knows to not also try\n\t\t * to hash this lock.\n\t\t */\n\t\tcmpxchg(&pn->state, vcpu_halted, vcpu_running);\n\n\t\t/*\n\t\t * If the locked flag is still not set after wakeup, it is a\n\t\t * spurious wakeup and the vCPU should wait again. However,\n\t\t * there is a pretty high overhead for CPU halting and kicking.\n\t\t * So it is better to spin for a while in the hope that the\n\t\t * MCS lock will be released soon.\n\t\t */\n\t\tlockevent_cond_inc(pv_spurious_wakeup,\n\t\t\t\t  !READ_ONCE(node->locked));\n\t}\n\n\t/*\n\t * By now our node->locked should be 1 and our caller will not actually\n\t * spin-wait for it. We do however rely on our caller to do a\n\t * load-acquire for us.\n\t */\n}"
  },
  {
    "function_name": "pv_init_node",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "278-286",
    "snippet": "static void pv_init_node(struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\n\tBUILD_BUG_ON(sizeof(struct pv_node) > sizeof(struct qnode));\n\n\tpn->cpu = smp_processor_id();\n\tpn->state = vcpu_running;\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 284
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUILD_BUG_ON",
          "args": [
            "sizeof(struct pv_node) > sizeof(struct qnode)"
          ],
          "line": 282
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic void pv_init_node(struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\n\tBUILD_BUG_ON(sizeof(struct pv_node) > sizeof(struct qnode));\n\n\tpn->cpu = smp_processor_id();\n\tpn->state = vcpu_running;\n}"
  },
  {
    "function_name": "pv_wait_early",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "266-273",
    "snippet": "static inline bool\npv_wait_early(struct pv_node *prev, int loop)\n{\n\tif ((loop & PV_PREV_CHECK_MASK) != 0)\n\t\treturn false;\n\n\treturn READ_ONCE(prev->state) != vcpu_running;\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [
      "#define PV_PREV_CHECK_MASK\t0xff"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "prev->state"
          ],
          "line": 272
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define PV_PREV_CHECK_MASK\t0xff\n\nstatic inline bool\npv_wait_early(struct pv_node *prev, int loop)\n{\n\tif ((loop & PV_PREV_CHECK_MASK) != 0)\n\t\treturn false;\n\n\treturn READ_ONCE(prev->state) != vcpu_running;\n}"
  },
  {
    "function_name": "pv_unhash",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "239-260",
    "snippet": "static struct pv_node *pv_unhash(struct qspinlock *lock)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tstruct pv_node *node;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\tif (READ_ONCE(he->lock) == lock) {\n\t\t\tnode = READ_ONCE(he->node);\n\t\t\tWRITE_ONCE(he->lock, NULL);\n\t\t\treturn node;\n\t\t}\n\t}\n\t/*\n\t * Hard assume we'll find an entry.\n\t *\n\t * This guarantees a limited lookup time and is itself guaranteed by\n\t * having the lock owner do the unhash -- IFF the unlock sees the\n\t * SLOW flag, there MUST be a hash entry.\n\t */\n\tBUG();\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static unsigned int pv_lock_hash_bits"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 259
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "he->lock",
            "NULL"
          ],
          "line": 248
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "he->node"
          ],
          "line": 247
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "he->lock"
          ],
          "line": 246
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_hash_entry",
          "args": [
            "he",
            "offset",
            "hash"
          ],
          "line": 245
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "hash_ptr",
          "args": [
            "lock",
            "pv_lock_hash_bits"
          ],
          "line": 241
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic unsigned int pv_lock_hash_bits;\n\nstatic struct pv_node *pv_unhash(struct qspinlock *lock)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tstruct pv_node *node;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\tif (READ_ONCE(he->lock) == lock) {\n\t\t\tnode = READ_ONCE(he->node);\n\t\t\tWRITE_ONCE(he->lock, NULL);\n\t\t\treturn node;\n\t\t}\n\t}\n\t/*\n\t * Hard assume we'll find an entry.\n\t *\n\t * This guarantees a limited lookup time and is itself guaranteed by\n\t * having the lock owner do the unhash -- IFF the unlock sees the\n\t * SLOW flag, there MUST be a hash entry.\n\t */\n\tBUG();\n}"
  },
  {
    "function_name": "pv_hash",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "212-237",
    "snippet": "static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tint hopcnt = 0;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\thopcnt++;\n\t\tif (!cmpxchg(&he->lock, NULL, lock)) {\n\t\t\tWRITE_ONCE(he->node, node);\n\t\t\tlockevent_pv_hop(hopcnt);\n\t\t\treturn &he->lock;\n\t\t}\n\t}\n\t/*\n\t * Hard assume there is a free entry for us.\n\t *\n\t * This is guaranteed by ensuring every blocked lock only ever consumes\n\t * a single entry, and since we only have 4 nesting levels per CPU\n\t * and allocated 4*nr_possible_cpus(), this must be so.\n\t *\n\t * The single entry is guaranteed by having the lock owner unhash\n\t * before it releases.\n\t */\n\tBUG();\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static unsigned int pv_lock_hash_bits"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 236
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_pv_hop",
          "args": [
            "hopcnt"
          ],
          "line": 222
        },
        "resolved": true,
        "details": {
          "function_name": "lockevent_pv_hop",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_stat.h",
          "lines": "140-140",
          "snippet": "static inline void lockevent_pv_hop(int hopcnt)\t{ }",
          "includes": [
            "#include <linux/fs.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include \"lock_events.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/fs.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include \"lock_events.h\"\n\nstatic inline void lockevent_pv_hop(int hopcnt)\t{ }"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "he->node",
            "node"
          ],
          "line": 221
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cmpxchg",
          "args": [
            "&he->lock",
            "NULL",
            "lock"
          ],
          "line": 220
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_release",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "261-266",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_hash_entry",
          "args": [
            "he",
            "offset",
            "hash"
          ],
          "line": 218
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "hash_ptr",
          "args": [
            "lock",
            "pv_lock_hash_bits"
          ],
          "line": 214
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic unsigned int pv_lock_hash_bits;\n\nstatic struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tint hopcnt = 0;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\thopcnt++;\n\t\tif (!cmpxchg(&he->lock, NULL, lock)) {\n\t\t\tWRITE_ONCE(he->node, node);\n\t\t\tlockevent_pv_hop(hopcnt);\n\t\t\treturn &he->lock;\n\t\t}\n\t}\n\t/*\n\t * Hard assume there is a free entry for us.\n\t *\n\t * This is guaranteed by ensuring every blocked lock only ever consumes\n\t * a single entry, and since we only have 4 nesting levels per CPU\n\t * and allocated 4*nr_possible_cpus(), this must be so.\n\t *\n\t * The single entry is guaranteed by having the lock owner unhash\n\t * before it releases.\n\t */\n\tBUG();\n}"
  },
  {
    "function_name": "__pv_init_lock_hash",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "188-205",
    "snippet": "void __init __pv_init_lock_hash(void)\n{\n\tint pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);\n\n\tif (pv_hash_size < PV_HE_MIN)\n\t\tpv_hash_size = PV_HE_MIN;\n\n\t/*\n\t * Allocate space from bootmem which should be page-size aligned\n\t * and hence cacheline aligned.\n\t */\n\tpv_lock_hash = alloc_large_system_hash(\"PV qspinlock\",\n\t\t\t\t\t       sizeof(struct pv_hash_entry),\n\t\t\t\t\t       pv_hash_size, 0,\n\t\t\t\t\t       HASH_EARLY | HASH_ZERO,\n\t\t\t\t\t       &pv_lock_hash_bits, NULL,\n\t\t\t\t\t       pv_hash_size, pv_hash_size);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [
      "#define PV_HE_MIN\t(PAGE_SIZE / sizeof(struct pv_hash_entry))",
      "#define PV_HE_PER_LINE\t(SMP_CACHE_BYTES / sizeof(struct pv_hash_entry))"
    ],
    "globals_used": [
      "static struct pv_hash_entry *pv_lock_hash;",
      "static unsigned int pv_lock_hash_bits"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "alloc_large_system_hash",
          "args": [
            "\"PV qspinlock\"",
            "sizeof(struct pv_hash_entry)",
            "pv_hash_size",
            "0",
            "HASH_EARLY | HASH_ZERO",
            "&pv_lock_hash_bits",
            "NULL",
            "pv_hash_size",
            "pv_hash_size"
          ],
          "line": 199
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ALIGN",
          "args": [
            "4 * num_possible_cpus()",
            "PV_HE_PER_LINE"
          ],
          "line": 190
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "num_possible_cpus",
          "args": [],
          "line": 190
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n#define PV_HE_MIN\t(PAGE_SIZE / sizeof(struct pv_hash_entry))\n#define PV_HE_PER_LINE\t(SMP_CACHE_BYTES / sizeof(struct pv_hash_entry))\n\nstatic struct pv_hash_entry *pv_lock_hash;\nstatic unsigned int pv_lock_hash_bits;\n\nvoid __init __pv_init_lock_hash(void)\n{\n\tint pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);\n\n\tif (pv_hash_size < PV_HE_MIN)\n\t\tpv_hash_size = PV_HE_MIN;\n\n\t/*\n\t * Allocate space from bootmem which should be page-size aligned\n\t * and hence cacheline aligned.\n\t */\n\tpv_lock_hash = alloc_large_system_hash(\"PV qspinlock\",\n\t\t\t\t\t       sizeof(struct pv_hash_entry),\n\t\t\t\t\t       pv_hash_size, 0,\n\t\t\t\t\t       HASH_EARLY | HASH_ZERO,\n\t\t\t\t\t       &pv_lock_hash_bits, NULL,\n\t\t\t\t\t       pv_hash_size, pv_hash_size);\n}"
  },
  {
    "function_name": "trylock_clear_pending",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "131-152",
    "snippet": "static __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\tint val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tint old, new;\n\n\t\tif (val  & _Q_LOCKED_MASK)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Try to clear pending bit & set locked bit\n\t\t */\n\t\told = val;\n\t\tnew = (val & ~_Q_PENDING_MASK) | _Q_LOCKED_VAL;\n\t\tval = atomic_cmpxchg_acquire(&lock->val, old, new);\n\n\t\tif (val == old)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_cmpxchg_acquire",
          "args": [
            "&lock->val",
            "old",
            "new"
          ],
          "line": 146
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&lock->val"
          ],
          "line": 133
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\tint val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tint old, new;\n\n\t\tif (val  & _Q_LOCKED_MASK)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Try to clear pending bit & set locked bit\n\t\t */\n\t\told = val;\n\t\tnew = (val & ~_Q_PENDING_MASK) | _Q_LOCKED_VAL;\n\t\tval = atomic_cmpxchg_acquire(&lock->val, old, new);\n\n\t\tif (val == old)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}"
  },
  {
    "function_name": "set_pending",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "126-129",
    "snippet": "static __always_inline void set_pending(struct qspinlock *lock)\n{\n\tatomic_or(_Q_PENDING_VAL, &lock->val);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_or",
          "args": [
            "_Q_PENDING_VAL",
            "&lock->val"
          ],
          "line": 128
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic __always_inline void set_pending(struct qspinlock *lock)\n{\n\tatomic_or(_Q_PENDING_VAL, &lock->val);\n}"
  },
  {
    "function_name": "trylock_clear_pending",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "119-124",
    "snippet": "static __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\treturn !READ_ONCE(lock->locked) &&\n\t       (cmpxchg_acquire(&lock->locked_pending, _Q_PENDING_VAL,\n\t\t\t\t_Q_LOCKED_VAL) == _Q_PENDING_VAL);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cmpxchg_acquire",
          "args": [
            "&lock->locked_pending",
            "_Q_PENDING_VAL",
            "_Q_LOCKED_VAL"
          ],
          "line": 122
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_acquire",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "253-259",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "lock->locked"
          ],
          "line": 121
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\treturn !READ_ONCE(lock->locked) &&\n\t       (cmpxchg_acquire(&lock->locked_pending, _Q_PENDING_VAL,\n\t\t\t\t_Q_LOCKED_VAL) == _Q_PENDING_VAL);\n}"
  },
  {
    "function_name": "set_pending",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "109-112",
    "snippet": "static __always_inline void set_pending(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->pending, 1);\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "lock->pending",
            "1"
          ],
          "line": 111
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic __always_inline void set_pending(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->pending, 1);\n}"
  },
  {
    "function_name": "pv_hybrid_queued_unfair_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/qspinlock_paravirt.h",
    "lines": "81-102",
    "snippet": "static inline bool pv_hybrid_queued_unfair_trylock(struct qspinlock *lock)\n{\n\t/*\n\t * Stay in unfair lock mode as long as queued mode waiters are\n\t * present in the MCS wait queue but the pending bit isn't set.\n\t */\n\tfor (;;) {\n\t\tint val = atomic_read(&lock->val);\n\n\t\tif (!(val & _Q_LOCKED_PENDING_MASK) &&\n\t\t   (cmpxchg_acquire(&lock->locked, 0, _Q_LOCKED_VAL) == 0)) {\n\t\t\tlockevent_inc(pv_lock_stealing);\n\t\t\treturn true;\n\t\t}\n\t\tif (!(val & _Q_TAIL_MASK) || (val & _Q_PENDING_MASK))\n\t\t\tbreak;\n\n\t\tcpu_relax();\n\t}\n\n\treturn false;\n}",
    "includes": [
      "#include <asm/qspinlock_paravirt.h>",
      "#include <linux/debug_locks.h>",
      "#include <linux/memblock.h>",
      "#include <linux/hash.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 98
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockevent_inc",
          "args": [
            "pv_lock_stealing"
          ],
          "line": 92
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cmpxchg_acquire",
          "args": [
            "&lock->locked",
            "0",
            "_Q_LOCKED_VAL"
          ],
          "line": 91
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_acquire",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "253-259",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&lock->val"
          ],
          "line": 88
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\nstatic inline bool pv_hybrid_queued_unfair_trylock(struct qspinlock *lock)\n{\n\t/*\n\t * Stay in unfair lock mode as long as queued mode waiters are\n\t * present in the MCS wait queue but the pending bit isn't set.\n\t */\n\tfor (;;) {\n\t\tint val = atomic_read(&lock->val);\n\n\t\tif (!(val & _Q_LOCKED_PENDING_MASK) &&\n\t\t   (cmpxchg_acquire(&lock->locked, 0, _Q_LOCKED_VAL) == 0)) {\n\t\t\tlockevent_inc(pv_lock_stealing);\n\t\t\treturn true;\n\t\t}\n\t\tif (!(val & _Q_TAIL_MASK) || (val & _Q_PENDING_MASK))\n\t\t\tbreak;\n\n\t\tcpu_relax();\n\t}\n\n\treturn false;\n}"
  }
]