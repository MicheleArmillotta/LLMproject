[
  {
    "function_name": "cpupri_cleanup",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
    "lines": "309-316",
    "snippet": "void cpupri_cleanup(struct cpupri *cp)\n{\n\tint i;\n\n\tkfree(cp->cpu_to_pri);\n\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++)\n\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "free_cpumask_var",
          "args": [
            "cp->pri_to_cpu[i].mask"
          ],
          "line": 315
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "cp->cpu_to_pri"
          ],
          "line": 313
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/params.c",
          "lines": "62-75",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/security.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/security.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nvoid cpupri_cleanup(struct cpupri *cp)\n{\n\tint i;\n\n\tkfree(cp->cpu_to_pri);\n\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++)\n\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);\n}"
  },
  {
    "function_name": "cpupri_init",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
    "lines": "278-303",
    "snippet": "int cpupri_init(struct cpupri *cp)\n{\n\tint i;\n\n\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++) {\n\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[i];\n\n\t\tatomic_set(&vec->count, 0);\n\t\tif (!zalloc_cpumask_var(&vec->mask, GFP_KERNEL))\n\t\t\tgoto cleanup;\n\t}\n\n\tcp->cpu_to_pri = kcalloc(nr_cpu_ids, sizeof(int), GFP_KERNEL);\n\tif (!cp->cpu_to_pri)\n\t\tgoto cleanup;\n\n\tfor_each_possible_cpu(i)\n\t\tcp->cpu_to_pri[i] = CPUPRI_INVALID;\n\n\treturn 0;\n\ncleanup:\n\tfor (i--; i >= 0; i--)\n\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);\n\treturn -ENOMEM;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "free_cpumask_var",
          "args": [
            "cp->pri_to_cpu[i].mask"
          ],
          "line": 301
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kcalloc",
          "args": [
            "nr_cpu_ids",
            "sizeof(int)",
            "GFP_KERNEL"
          ],
          "line": 290
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "zalloc_cpumask_var",
          "args": [
            "&vec->mask",
            "GFP_KERNEL"
          ],
          "line": 286
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_set",
          "args": [
            "&vec->count",
            "0"
          ],
          "line": 285
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nint cpupri_init(struct cpupri *cp)\n{\n\tint i;\n\n\tfor (i = 0; i < CPUPRI_NR_PRIORITIES; i++) {\n\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[i];\n\n\t\tatomic_set(&vec->count, 0);\n\t\tif (!zalloc_cpumask_var(&vec->mask, GFP_KERNEL))\n\t\t\tgoto cleanup;\n\t}\n\n\tcp->cpu_to_pri = kcalloc(nr_cpu_ids, sizeof(int), GFP_KERNEL);\n\tif (!cp->cpu_to_pri)\n\t\tgoto cleanup;\n\n\tfor_each_possible_cpu(i)\n\t\tcp->cpu_to_pri[i] = CPUPRI_INVALID;\n\n\treturn 0;\n\ncleanup:\n\tfor (i--; i >= 0; i--)\n\t\tfree_cpumask_var(cp->pri_to_cpu[i].mask);\n\treturn -ENOMEM;\n}"
  },
  {
    "function_name": "cpupri_set",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
    "lines": "210-270",
    "snippet": "void cpupri_set(struct cpupri *cp, int cpu, int newpri)\n{\n\tint *currpri = &cp->cpu_to_pri[cpu];\n\tint oldpri = *currpri;\n\tint do_mb = 0;\n\n\tnewpri = convert_prio(newpri);\n\n\tBUG_ON(newpri >= CPUPRI_NR_PRIORITIES);\n\n\tif (newpri == oldpri)\n\t\treturn;\n\n\t/*\n\t * If the CPU was currently mapped to a different value, we\n\t * need to map it to the new value then remove the old value.\n\t * Note, we must add the new value first, otherwise we risk the\n\t * cpu being missed by the priority loop in cpupri_find.\n\t */\n\tif (likely(newpri != CPUPRI_INVALID)) {\n\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[newpri];\n\n\t\tcpumask_set_cpu(cpu, vec->mask);\n\t\t/*\n\t\t * When adding a new vector, we update the mask first,\n\t\t * do a write memory barrier, and then update the count, to\n\t\t * make sure the vector is visible when count is set.\n\t\t */\n\t\tsmp_mb__before_atomic();\n\t\tatomic_inc(&(vec)->count);\n\t\tdo_mb = 1;\n\t}\n\tif (likely(oldpri != CPUPRI_INVALID)) {\n\t\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[oldpri];\n\n\t\t/*\n\t\t * Because the order of modification of the vec->count\n\t\t * is important, we must make sure that the update\n\t\t * of the new prio is seen before we decrement the\n\t\t * old prio. This makes sure that the loop sees\n\t\t * one or the other when we raise the priority of\n\t\t * the run queue. We don't care about when we lower the\n\t\t * priority, as that will trigger an rt pull anyway.\n\t\t *\n\t\t * We only need to do a memory barrier if we updated\n\t\t * the new priority vec.\n\t\t */\n\t\tif (do_mb)\n\t\t\tsmp_mb__after_atomic();\n\n\t\t/*\n\t\t * When removing from the vector, we decrement the counter first\n\t\t * do a memory barrier and then clear the mask.\n\t\t */\n\t\tatomic_dec(&(vec)->count);\n\t\tsmp_mb__after_atomic();\n\t\tcpumask_clear_cpu(cpu, vec->mask);\n\t}\n\n\t*currpri = newpri;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpumask_clear_cpu",
          "args": [
            "cpu",
            "vec->mask"
          ],
          "line": 266
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb__after_atomic",
          "args": [],
          "line": 265
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_dec",
          "args": [
            "&(vec)->count"
          ],
          "line": 264
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb__after_atomic",
          "args": [],
          "line": 258
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "oldpri != CPUPRI_INVALID"
          ],
          "line": 242
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_inc",
          "args": [
            "&(vec)->count"
          ],
          "line": 239
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb__before_atomic",
          "args": [],
          "line": 238
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_set_cpu",
          "args": [
            "cpu",
            "vec->mask"
          ],
          "line": 232
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "newpri != CPUPRI_INVALID"
          ],
          "line": 229
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "newpri >= CPUPRI_NR_PRIORITIES"
          ],
          "line": 218
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "convert_prio",
          "args": [
            "newpri"
          ],
          "line": 216
        },
        "resolved": true,
        "details": {
          "function_name": "convert_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
          "lines": "43-66",
          "snippet": "static int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nvoid cpupri_set(struct cpupri *cp, int cpu, int newpri)\n{\n\tint *currpri = &cp->cpu_to_pri[cpu];\n\tint oldpri = *currpri;\n\tint do_mb = 0;\n\n\tnewpri = convert_prio(newpri);\n\n\tBUG_ON(newpri >= CPUPRI_NR_PRIORITIES);\n\n\tif (newpri == oldpri)\n\t\treturn;\n\n\t/*\n\t * If the CPU was currently mapped to a different value, we\n\t * need to map it to the new value then remove the old value.\n\t * Note, we must add the new value first, otherwise we risk the\n\t * cpu being missed by the priority loop in cpupri_find.\n\t */\n\tif (likely(newpri != CPUPRI_INVALID)) {\n\t\tstruct cpupri_vec *vec = &cp->pri_to_cpu[newpri];\n\n\t\tcpumask_set_cpu(cpu, vec->mask);\n\t\t/*\n\t\t * When adding a new vector, we update the mask first,\n\t\t * do a write memory barrier, and then update the count, to\n\t\t * make sure the vector is visible when count is set.\n\t\t */\n\t\tsmp_mb__before_atomic();\n\t\tatomic_inc(&(vec)->count);\n\t\tdo_mb = 1;\n\t}\n\tif (likely(oldpri != CPUPRI_INVALID)) {\n\t\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[oldpri];\n\n\t\t/*\n\t\t * Because the order of modification of the vec->count\n\t\t * is important, we must make sure that the update\n\t\t * of the new prio is seen before we decrement the\n\t\t * old prio. This makes sure that the loop sees\n\t\t * one or the other when we raise the priority of\n\t\t * the run queue. We don't care about when we lower the\n\t\t * priority, as that will trigger an rt pull anyway.\n\t\t *\n\t\t * We only need to do a memory barrier if we updated\n\t\t * the new priority vec.\n\t\t */\n\t\tif (do_mb)\n\t\t\tsmp_mb__after_atomic();\n\n\t\t/*\n\t\t * When removing from the vector, we decrement the counter first\n\t\t * do a memory barrier and then clear the mask.\n\t\t */\n\t\tatomic_dec(&(vec)->count);\n\t\tsmp_mb__after_atomic();\n\t\tcpumask_clear_cpu(cpu, vec->mask);\n\t}\n\n\t*currpri = newpri;\n}"
  },
  {
    "function_name": "cpupri_find_fitness",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
    "lines": "144-198",
    "snippet": "int cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask,\n\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))\n{\n\tint task_pri = convert_prio(p->prio);\n\tint idx, cpu;\n\n\tBUG_ON(task_pri >= CPUPRI_NR_PRIORITIES);\n\n\tfor (idx = 0; idx < task_pri; idx++) {\n\n\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))\n\t\t\tcontinue;\n\n\t\tif (!lowest_mask || !fitness_fn)\n\t\t\treturn 1;\n\n\t\t/* Ensure the capacity of the CPUs fit the task */\n\t\tfor_each_cpu(cpu, lowest_mask) {\n\t\t\tif (!fitness_fn(p, cpu))\n\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);\n\t\t}\n\n\t\t/*\n\t\t * If no CPU at the current priority can fit the task\n\t\t * continue looking\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\tcontinue;\n\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we failed to find a fitting lowest_mask, kick off a new search\n\t * but without taking into account any fitness criteria this time.\n\t *\n\t * This rule favours honouring priority over fitting the task in the\n\t * correct CPU (Capacity Awareness being the only user now).\n\t * The idea is that if a higher priority task can run, then it should\n\t * run even if this ends up being on unfitting CPU.\n\t *\n\t * The cost of this trade-off is not entirely clear and will probably\n\t * be good for some workloads and bad for others.\n\t *\n\t * The main idea here is that if some CPUs were over-committed, we try\n\t * to spread which is what the scheduler traditionally did. Sys admins\n\t * must do proper RT planning to avoid overloading the system if they\n\t * really care.\n\t */\n\tif (fitness_fn)\n\t\treturn cpupri_find(cp, p, lowest_mask);\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpupri_find",
          "args": [
            "cp",
            "p",
            "lowest_mask"
          ],
          "line": 195
        },
        "resolved": true,
        "details": {
          "function_name": "cpupri_find",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
          "lines": "121-125",
          "snippet": "int cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask)\n{\n\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nint cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask)\n{\n\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_empty",
          "args": [
            "lowest_mask"
          ],
          "line": 171
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_clear_cpu",
          "args": [
            "cpu",
            "lowest_mask"
          ],
          "line": 164
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "fitness_fn",
          "args": [
            "p",
            "cpu"
          ],
          "line": 163
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "cpu",
            "lowest_mask"
          ],
          "line": 162
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__cpupri_find",
          "args": [
            "cp",
            "p",
            "lowest_mask",
            "idx"
          ],
          "line": 155
        },
        "resolved": true,
        "details": {
          "function_name": "__cpupri_find",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
          "lines": "68-119",
          "snippet": "static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\t\t\tstruct cpumask *lowest_mask, int idx)\n{\n\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];\n\tint skip = 0;\n\n\tif (!atomic_read(&(vec)->count))\n\t\tskip = 1;\n\t/*\n\t * When looking at the vector, we need to read the counter,\n\t * do a memory barrier, then read the mask.\n\t *\n\t * Note: This is still all racy, but we can deal with it.\n\t *  Ideally, we only want to look at masks that are set.\n\t *\n\t *  If a mask is not set, then the only thing wrong is that we\n\t *  did a little more work than necessary.\n\t *\n\t *  If we read a zero count but the mask is set, because of the\n\t *  memory barriers, that can only happen when the highest prio\n\t *  task for a run queue has left the run queue, in which case,\n\t *  it will be followed by a pull. If the task we are processing\n\t *  fails to find a proper place to go, that pull request will\n\t *  pull this task if the run queue is running at a lower\n\t *  priority.\n\t */\n\tsmp_rmb();\n\n\t/* Need to do the rmb for every iteration */\n\tif (skip)\n\t\treturn 0;\n\n\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)\n\t\treturn 0;\n\n\tif (lowest_mask) {\n\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);\n\n\t\t/*\n\t\t * We have to ensure that we have at least one bit\n\t\t * still set in the array, since the map could have\n\t\t * been concurrently emptied between the first and\n\t\t * second reads of vec->mask.  If we hit this\n\t\t * condition, simply act as though we never hit this\n\t\t * priority level and continue on.\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\t\t\tstruct cpumask *lowest_mask, int idx)\n{\n\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];\n\tint skip = 0;\n\n\tif (!atomic_read(&(vec)->count))\n\t\tskip = 1;\n\t/*\n\t * When looking at the vector, we need to read the counter,\n\t * do a memory barrier, then read the mask.\n\t *\n\t * Note: This is still all racy, but we can deal with it.\n\t *  Ideally, we only want to look at masks that are set.\n\t *\n\t *  If a mask is not set, then the only thing wrong is that we\n\t *  did a little more work than necessary.\n\t *\n\t *  If we read a zero count but the mask is set, because of the\n\t *  memory barriers, that can only happen when the highest prio\n\t *  task for a run queue has left the run queue, in which case,\n\t *  it will be followed by a pull. If the task we are processing\n\t *  fails to find a proper place to go, that pull request will\n\t *  pull this task if the run queue is running at a lower\n\t *  priority.\n\t */\n\tsmp_rmb();\n\n\t/* Need to do the rmb for every iteration */\n\tif (skip)\n\t\treturn 0;\n\n\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)\n\t\treturn 0;\n\n\tif (lowest_mask) {\n\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);\n\n\t\t/*\n\t\t * We have to ensure that we have at least one bit\n\t\t * still set in the array, since the map could have\n\t\t * been concurrently emptied between the first and\n\t\t * second reads of vec->mask.  If we hit this\n\t\t * condition, simply act as though we never hit this\n\t\t * priority level and continue on.\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "task_pri >= CPUPRI_NR_PRIORITIES"
          ],
          "line": 151
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "convert_prio",
          "args": [
            "p->prio"
          ],
          "line": 148
        },
        "resolved": true,
        "details": {
          "function_name": "convert_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
          "lines": "43-66",
          "snippet": "static int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nint cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask,\n\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))\n{\n\tint task_pri = convert_prio(p->prio);\n\tint idx, cpu;\n\n\tBUG_ON(task_pri >= CPUPRI_NR_PRIORITIES);\n\n\tfor (idx = 0; idx < task_pri; idx++) {\n\n\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))\n\t\t\tcontinue;\n\n\t\tif (!lowest_mask || !fitness_fn)\n\t\t\treturn 1;\n\n\t\t/* Ensure the capacity of the CPUs fit the task */\n\t\tfor_each_cpu(cpu, lowest_mask) {\n\t\t\tif (!fitness_fn(p, cpu))\n\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);\n\t\t}\n\n\t\t/*\n\t\t * If no CPU at the current priority can fit the task\n\t\t * continue looking\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\tcontinue;\n\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we failed to find a fitting lowest_mask, kick off a new search\n\t * but without taking into account any fitness criteria this time.\n\t *\n\t * This rule favours honouring priority over fitting the task in the\n\t * correct CPU (Capacity Awareness being the only user now).\n\t * The idea is that if a higher priority task can run, then it should\n\t * run even if this ends up being on unfitting CPU.\n\t *\n\t * The cost of this trade-off is not entirely clear and will probably\n\t * be good for some workloads and bad for others.\n\t *\n\t * The main idea here is that if some CPUs were over-committed, we try\n\t * to spread which is what the scheduler traditionally did. Sys admins\n\t * must do proper RT planning to avoid overloading the system if they\n\t * really care.\n\t */\n\tif (fitness_fn)\n\t\treturn cpupri_find(cp, p, lowest_mask);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "cpupri_find",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
    "lines": "121-125",
    "snippet": "int cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask)\n{\n\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpupri_find_fitness",
          "args": [
            "cp",
            "p",
            "lowest_mask",
            "NULL"
          ],
          "line": 124
        },
        "resolved": true,
        "details": {
          "function_name": "cpupri_find_fitness",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
          "lines": "144-198",
          "snippet": "int cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask,\n\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))\n{\n\tint task_pri = convert_prio(p->prio);\n\tint idx, cpu;\n\n\tBUG_ON(task_pri >= CPUPRI_NR_PRIORITIES);\n\n\tfor (idx = 0; idx < task_pri; idx++) {\n\n\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))\n\t\t\tcontinue;\n\n\t\tif (!lowest_mask || !fitness_fn)\n\t\t\treturn 1;\n\n\t\t/* Ensure the capacity of the CPUs fit the task */\n\t\tfor_each_cpu(cpu, lowest_mask) {\n\t\t\tif (!fitness_fn(p, cpu))\n\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);\n\t\t}\n\n\t\t/*\n\t\t * If no CPU at the current priority can fit the task\n\t\t * continue looking\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\tcontinue;\n\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we failed to find a fitting lowest_mask, kick off a new search\n\t * but without taking into account any fitness criteria this time.\n\t *\n\t * This rule favours honouring priority over fitting the task in the\n\t * correct CPU (Capacity Awareness being the only user now).\n\t * The idea is that if a higher priority task can run, then it should\n\t * run even if this ends up being on unfitting CPU.\n\t *\n\t * The cost of this trade-off is not entirely clear and will probably\n\t * be good for some workloads and bad for others.\n\t *\n\t * The main idea here is that if some CPUs were over-committed, we try\n\t * to spread which is what the scheduler traditionally did. Sys admins\n\t * must do proper RT planning to avoid overloading the system if they\n\t * really care.\n\t */\n\tif (fitness_fn)\n\t\treturn cpupri_find(cp, p, lowest_mask);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nint cpupri_find_fitness(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask,\n\t\tbool (*fitness_fn)(struct task_struct *p, int cpu))\n{\n\tint task_pri = convert_prio(p->prio);\n\tint idx, cpu;\n\n\tBUG_ON(task_pri >= CPUPRI_NR_PRIORITIES);\n\n\tfor (idx = 0; idx < task_pri; idx++) {\n\n\t\tif (!__cpupri_find(cp, p, lowest_mask, idx))\n\t\t\tcontinue;\n\n\t\tif (!lowest_mask || !fitness_fn)\n\t\t\treturn 1;\n\n\t\t/* Ensure the capacity of the CPUs fit the task */\n\t\tfor_each_cpu(cpu, lowest_mask) {\n\t\t\tif (!fitness_fn(p, cpu))\n\t\t\t\tcpumask_clear_cpu(cpu, lowest_mask);\n\t\t}\n\n\t\t/*\n\t\t * If no CPU at the current priority can fit the task\n\t\t * continue looking\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\tcontinue;\n\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we failed to find a fitting lowest_mask, kick off a new search\n\t * but without taking into account any fitness criteria this time.\n\t *\n\t * This rule favours honouring priority over fitting the task in the\n\t * correct CPU (Capacity Awareness being the only user now).\n\t * The idea is that if a higher priority task can run, then it should\n\t * run even if this ends up being on unfitting CPU.\n\t *\n\t * The cost of this trade-off is not entirely clear and will probably\n\t * be good for some workloads and bad for others.\n\t *\n\t * The main idea here is that if some CPUs were over-committed, we try\n\t * to spread which is what the scheduler traditionally did. Sys admins\n\t * must do proper RT planning to avoid overloading the system if they\n\t * really care.\n\t */\n\tif (fitness_fn)\n\t\treturn cpupri_find(cp, p, lowest_mask);\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nint cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\tstruct cpumask *lowest_mask)\n{\n\treturn cpupri_find_fitness(cp, p, lowest_mask, NULL);\n}"
  },
  {
    "function_name": "__cpupri_find",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
    "lines": "68-119",
    "snippet": "static inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\t\t\tstruct cpumask *lowest_mask, int idx)\n{\n\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];\n\tint skip = 0;\n\n\tif (!atomic_read(&(vec)->count))\n\t\tskip = 1;\n\t/*\n\t * When looking at the vector, we need to read the counter,\n\t * do a memory barrier, then read the mask.\n\t *\n\t * Note: This is still all racy, but we can deal with it.\n\t *  Ideally, we only want to look at masks that are set.\n\t *\n\t *  If a mask is not set, then the only thing wrong is that we\n\t *  did a little more work than necessary.\n\t *\n\t *  If we read a zero count but the mask is set, because of the\n\t *  memory barriers, that can only happen when the highest prio\n\t *  task for a run queue has left the run queue, in which case,\n\t *  it will be followed by a pull. If the task we are processing\n\t *  fails to find a proper place to go, that pull request will\n\t *  pull this task if the run queue is running at a lower\n\t *  priority.\n\t */\n\tsmp_rmb();\n\n\t/* Need to do the rmb for every iteration */\n\tif (skip)\n\t\treturn 0;\n\n\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)\n\t\treturn 0;\n\n\tif (lowest_mask) {\n\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);\n\n\t\t/*\n\t\t * We have to ensure that we have at least one bit\n\t\t * still set in the array, since the map could have\n\t\t * been concurrently emptied between the first and\n\t\t * second reads of vec->mask.  If we hit this\n\t\t * condition, simply act as though we never hit this\n\t\t * priority level and continue on.\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpumask_empty",
          "args": [
            "lowest_mask"
          ],
          "line": 114
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_and",
          "args": [
            "lowest_mask",
            "&p->cpus_mask",
            "vec->mask"
          ],
          "line": 104
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_any_and",
          "args": [
            "&p->cpus_mask",
            "vec->mask"
          ],
          "line": 100
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 94
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&(vec)->count"
          ],
          "line": 74
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nstatic inline int __cpupri_find(struct cpupri *cp, struct task_struct *p,\n\t\t\t\tstruct cpumask *lowest_mask, int idx)\n{\n\tstruct cpupri_vec *vec  = &cp->pri_to_cpu[idx];\n\tint skip = 0;\n\n\tif (!atomic_read(&(vec)->count))\n\t\tskip = 1;\n\t/*\n\t * When looking at the vector, we need to read the counter,\n\t * do a memory barrier, then read the mask.\n\t *\n\t * Note: This is still all racy, but we can deal with it.\n\t *  Ideally, we only want to look at masks that are set.\n\t *\n\t *  If a mask is not set, then the only thing wrong is that we\n\t *  did a little more work than necessary.\n\t *\n\t *  If we read a zero count but the mask is set, because of the\n\t *  memory barriers, that can only happen when the highest prio\n\t *  task for a run queue has left the run queue, in which case,\n\t *  it will be followed by a pull. If the task we are processing\n\t *  fails to find a proper place to go, that pull request will\n\t *  pull this task if the run queue is running at a lower\n\t *  priority.\n\t */\n\tsmp_rmb();\n\n\t/* Need to do the rmb for every iteration */\n\tif (skip)\n\t\treturn 0;\n\n\tif (cpumask_any_and(&p->cpus_mask, vec->mask) >= nr_cpu_ids)\n\t\treturn 0;\n\n\tif (lowest_mask) {\n\t\tcpumask_and(lowest_mask, &p->cpus_mask, vec->mask);\n\n\t\t/*\n\t\t * We have to ensure that we have at least one bit\n\t\t * still set in the array, since the map could have\n\t\t * been concurrently emptied between the first and\n\t\t * second reads of vec->mask.  If we hit this\n\t\t * condition, simply act as though we never hit this\n\t\t * priority level and continue on.\n\t\t */\n\t\tif (cpumask_empty(lowest_mask))\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}"
  },
  {
    "function_name": "convert_prio",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
    "lines": "43-66",
    "snippet": "static int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched.h\"\n\nstatic int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}"
  }
]