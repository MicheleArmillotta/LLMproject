[
  {
    "function_name": "rtlock_slowlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1717-1724",
    "snippet": "__sched rtlock_slowlock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\trtlock_slowlock_locked(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1723
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rtlock_slowlock_locked",
          "args": [
            "lock"
          ],
          "line": 1722
        },
        "resolved": true,
        "details": {
          "function_name": "rtlock_slowlock_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1671-1715",
          "snippet": "static void __sched rtlock_slowlock_locked(struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter waiter;\n\tstruct task_struct *owner;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, current, NULL))\n\t\treturn;\n\n\trt_mutex_init_rtlock_waiter(&waiter);\n\n\t/* Save current state and set state to TASK_RTLOCK_WAIT */\n\tcurrent_save_and_set_rtlock_wait_state();\n\n\ttask_blocks_on_rt_mutex(lock, &waiter, current, NULL, RT_MUTEX_MIN_CHAINWALK);\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock again */\n\t\tif (try_to_take_rt_mutex(lock, current, &waiter))\n\t\t\tbreak;\n\n\t\tif (&waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, &waiter, owner))\n\t\t\tschedule_rtlock();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(TASK_RTLOCK_WAIT);\n\t}\n\n\t/* Restore the task state */\n\tcurrent_restore_rtlock_saved_state();\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally.\n\t * We might have to fix that up:\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\tdebug_rt_mutex_free_waiter(&waiter);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched rtlock_slowlock_locked(struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter waiter;\n\tstruct task_struct *owner;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, current, NULL))\n\t\treturn;\n\n\trt_mutex_init_rtlock_waiter(&waiter);\n\n\t/* Save current state and set state to TASK_RTLOCK_WAIT */\n\tcurrent_save_and_set_rtlock_wait_state();\n\n\ttask_blocks_on_rt_mutex(lock, &waiter, current, NULL, RT_MUTEX_MIN_CHAINWALK);\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock again */\n\t\tif (try_to_take_rt_mutex(lock, current, &waiter))\n\t\t\tbreak;\n\n\t\tif (&waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, &waiter, owner))\n\t\t\tschedule_rtlock();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(TASK_RTLOCK_WAIT);\n\t}\n\n\t/* Restore the task state */\n\tcurrent_restore_rtlock_saved_state();\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally.\n\t * We might have to fix that up:\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\tdebug_rt_mutex_free_waiter(&waiter);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1721
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\n__sched rtlock_slowlock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\trtlock_slowlock_locked(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n}"
  },
  {
    "function_name": "rtlock_slowlock_locked",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1671-1715",
    "snippet": "static void __sched rtlock_slowlock_locked(struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter waiter;\n\tstruct task_struct *owner;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, current, NULL))\n\t\treturn;\n\n\trt_mutex_init_rtlock_waiter(&waiter);\n\n\t/* Save current state and set state to TASK_RTLOCK_WAIT */\n\tcurrent_save_and_set_rtlock_wait_state();\n\n\ttask_blocks_on_rt_mutex(lock, &waiter, current, NULL, RT_MUTEX_MIN_CHAINWALK);\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock again */\n\t\tif (try_to_take_rt_mutex(lock, current, &waiter))\n\t\t\tbreak;\n\n\t\tif (&waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, &waiter, owner))\n\t\t\tschedule_rtlock();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(TASK_RTLOCK_WAIT);\n\t}\n\n\t/* Restore the task state */\n\tcurrent_restore_rtlock_saved_state();\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally.\n\t * We might have to fix that up:\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\tdebug_rt_mutex_free_waiter(&waiter);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "debug_rt_mutex_free_waiter",
          "args": [
            "&waiter"
          ],
          "line": 1714
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_free_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "184-188",
          "snippet": "static inline void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tmemset(waiter, 0x22, sizeof(*waiter));\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tmemset(waiter, 0x22, sizeof(*waiter));\n}"
        }
      },
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 1713
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "107-175",
          "snippet": "static __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "current_restore_rtlock_saved_state",
          "args": [],
          "line": 1707
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "TASK_RTLOCK_WAIT"
          ],
          "line": 1703
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1702
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedule_rtlock",
          "args": [],
          "line": 1700
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_rtlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "6433-6440",
          "snippet": "notrace schedule_rtlock(void)\n{\n\tdo {\n\t\tpreempt_disable();\n\t\t__schedule(SM_RTLOCK_WAIT);\n\t\tsched_preempt_enable_no_resched();\n\t} while (need_resched());\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [
            "#define SM_RTLOCK_WAIT\t\t0x2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\n#define SM_RTLOCK_WAIT\t\t0x2\n\nnotrace schedule_rtlock(void)\n{\n\tdo {\n\t\tpreempt_disable();\n\t\t__schedule(SM_RTLOCK_WAIT);\n\t\tsched_preempt_enable_no_resched();\n\t} while (need_resched());\n}"
        }
      },
      {
        "call_info": {
          "callee": "rtmutex_spin_on_owner",
          "args": [
            "lock",
            "&waiter",
            "owner"
          ],
          "line": 1699
        },
        "resolved": true,
        "details": {
          "function_name": "rtmutex_spin_on_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1396-1401",
          "snippet": "static bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1697
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1694
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1693
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "&waiter"
          ],
          "line": 1690
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "968-1076",
          "snippet": "static int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocks_on_rt_mutex",
          "args": [
            "lock",
            "&waiter",
            "current",
            "NULL",
            "RT_MUTEX_MIN_CHAINWALK"
          ],
          "line": 1686
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocks_on_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1085-1184",
          "snippet": "static int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}"
        }
      },
      {
        "call_info": {
          "callee": "current_save_and_set_rtlock_wait_state",
          "args": [],
          "line": 1684
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_init_rtlock_waiter",
          "args": [
            "&waiter"
          ],
          "line": 1681
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_init_rtlock_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "199-203",
          "snippet": "static inline void rt_mutex_init_rtlock_waiter(struct rt_mutex_waiter *waiter)\n{\n\trt_mutex_init_waiter(waiter);\n\twaiter->wake_state = TASK_RTLOCK_WAIT;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void rt_mutex_init_rtlock_waiter(struct rt_mutex_waiter *waiter)\n{\n\trt_mutex_init_waiter(waiter);\n\twaiter->wake_state = TASK_RTLOCK_WAIT;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1676
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched rtlock_slowlock_locked(struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter waiter;\n\tstruct task_struct *owner;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, current, NULL))\n\t\treturn;\n\n\trt_mutex_init_rtlock_waiter(&waiter);\n\n\t/* Save current state and set state to TASK_RTLOCK_WAIT */\n\tcurrent_save_and_set_rtlock_wait_state();\n\n\ttask_blocks_on_rt_mutex(lock, &waiter, current, NULL, RT_MUTEX_MIN_CHAINWALK);\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock again */\n\t\tif (try_to_take_rt_mutex(lock, current, &waiter))\n\t\t\tbreak;\n\n\t\tif (&waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, &waiter, owner))\n\t\t\tschedule_rtlock();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(TASK_RTLOCK_WAIT);\n\t}\n\n\t/* Restore the task state */\n\tcurrent_restore_rtlock_saved_state();\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally.\n\t * We might have to fix that up:\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\tdebug_rt_mutex_free_waiter(&waiter);\n}"
  },
  {
    "function_name": "__rt_mutex_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1652-1659",
    "snippet": "static __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t   unsigned int state)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn rt_mutex_slowlock(lock, NULL, state);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_slowlock",
          "args": [
            "lock",
            "NULL",
            "state"
          ],
          "line": 1658
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1630-1650",
          "snippet": "static int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t     unsigned int state)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t     unsigned int state)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_acquire(lock, NULL, current)"
          ],
          "line": 1655
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_acquire",
          "args": [
            "lock",
            "NULL",
            "current"
          ],
          "line": 1655
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_acquire",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "253-259",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t   unsigned int state)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn rt_mutex_slowlock(lock, NULL, state);\n}"
  },
  {
    "function_name": "rt_mutex_slowlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1630-1650",
    "snippet": "static int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t     unsigned int state)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1647
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_slowlock_locked",
          "args": [
            "lock",
            "ww_ctx",
            "state"
          ],
          "line": 1646
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowlock_locked",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1607-1622",
          "snippet": "static inline int __rt_mutex_slowlock_locked(struct rt_mutex_base *lock,\n\t\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t     unsigned int state)\n{\n\tstruct rt_mutex_waiter waiter;\n\tint ret;\n\n\trt_mutex_init_waiter(&waiter);\n\twaiter.ww_ctx = ww_ctx;\n\n\tret = __rt_mutex_slowlock(lock, ww_ctx, state, RT_MUTEX_MIN_CHAINWALK,\n\t\t\t\t  &waiter);\n\n\tdebug_rt_mutex_free_waiter(&waiter);\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline int __rt_mutex_slowlock_locked(struct rt_mutex_base *lock,\n\t\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t     unsigned int state)\n{\n\tstruct rt_mutex_waiter waiter;\n\tint ret;\n\n\trt_mutex_init_waiter(&waiter);\n\twaiter.ww_ctx = ww_ctx;\n\n\tret = __rt_mutex_slowlock(lock, ww_ctx, state, RT_MUTEX_MIN_CHAINWALK,\n\t\t\t\t  &waiter);\n\n\tdebug_rt_mutex_free_waiter(&waiter);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1645
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t     unsigned int state)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "__rt_mutex_slowlock_locked",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1607-1622",
    "snippet": "static inline int __rt_mutex_slowlock_locked(struct rt_mutex_base *lock,\n\t\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t     unsigned int state)\n{\n\tstruct rt_mutex_waiter waiter;\n\tint ret;\n\n\trt_mutex_init_waiter(&waiter);\n\twaiter.ww_ctx = ww_ctx;\n\n\tret = __rt_mutex_slowlock(lock, ww_ctx, state, RT_MUTEX_MIN_CHAINWALK,\n\t\t\t\t  &waiter);\n\n\tdebug_rt_mutex_free_waiter(&waiter);\n\treturn ret;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "debug_rt_mutex_free_waiter",
          "args": [
            "&waiter"
          ],
          "line": 1620
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_free_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "184-188",
          "snippet": "static inline void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tmemset(waiter, 0x22, sizeof(*waiter));\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tmemset(waiter, 0x22, sizeof(*waiter));\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_slowlock",
          "args": [
            "lock",
            "ww_ctx",
            "state",
            "RT_MUTEX_MIN_CHAINWALK",
            "&waiter"
          ],
          "line": 1617
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1559-1605",
          "snippet": "static int __sched __rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t       struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t       unsigned int state,\n\t\t\t\t       enum rtmutex_chainwalk chwalk,\n\t\t\t\t       struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct ww_mutex *ww = ww_container_of(rtm);\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/* Try to acquire the lock again: */\n\tif (try_to_take_rt_mutex(lock, current, NULL)) {\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tset_current_state(state);\n\n\tret = task_blocks_on_rt_mutex(lock, waiter, current, ww_ctx, chwalk);\n\tif (likely(!ret))\n\t\tret = rt_mutex_slowlock_block(lock, ww_ctx, state, NULL, waiter);\n\n\tif (likely(!ret)) {\n\t\t/* acquired the lock */\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tif (!ww_ctx->is_wait_die)\n\t\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t} else {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_waiter(lock, waiter);\n\t\trt_mutex_handle_deadlock(ret, chwalk, waiter);\n\t}\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit\n\t * unconditionally. We might have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched __rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t       struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t       unsigned int state,\n\t\t\t\t       enum rtmutex_chainwalk chwalk,\n\t\t\t\t       struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct ww_mutex *ww = ww_container_of(rtm);\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/* Try to acquire the lock again: */\n\tif (try_to_take_rt_mutex(lock, current, NULL)) {\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tset_current_state(state);\n\n\tret = task_blocks_on_rt_mutex(lock, waiter, current, ww_ctx, chwalk);\n\tif (likely(!ret))\n\t\tret = rt_mutex_slowlock_block(lock, ww_ctx, state, NULL, waiter);\n\n\tif (likely(!ret)) {\n\t\t/* acquired the lock */\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tif (!ww_ctx->is_wait_die)\n\t\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t} else {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_waiter(lock, waiter);\n\t\trt_mutex_handle_deadlock(ret, chwalk, waiter);\n\t}\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit\n\t * unconditionally. We might have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_init_waiter",
          "args": [
            "&waiter"
          ],
          "line": 1614
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_init_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "190-197",
          "snippet": "static inline void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->wake_state = TASK_NORMAL;\n\twaiter->task = NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->wake_state = TASK_NORMAL;\n\twaiter->task = NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline int __rt_mutex_slowlock_locked(struct rt_mutex_base *lock,\n\t\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t     unsigned int state)\n{\n\tstruct rt_mutex_waiter waiter;\n\tint ret;\n\n\trt_mutex_init_waiter(&waiter);\n\twaiter.ww_ctx = ww_ctx;\n\n\tret = __rt_mutex_slowlock(lock, ww_ctx, state, RT_MUTEX_MIN_CHAINWALK,\n\t\t\t\t  &waiter);\n\n\tdebug_rt_mutex_free_waiter(&waiter);\n\treturn ret;\n}"
  },
  {
    "function_name": "__rt_mutex_slowlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1559-1605",
    "snippet": "static int __sched __rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t       struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t       unsigned int state,\n\t\t\t\t       enum rtmutex_chainwalk chwalk,\n\t\t\t\t       struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct ww_mutex *ww = ww_container_of(rtm);\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/* Try to acquire the lock again: */\n\tif (try_to_take_rt_mutex(lock, current, NULL)) {\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tset_current_state(state);\n\n\tret = task_blocks_on_rt_mutex(lock, waiter, current, ww_ctx, chwalk);\n\tif (likely(!ret))\n\t\tret = rt_mutex_slowlock_block(lock, ww_ctx, state, NULL, waiter);\n\n\tif (likely(!ret)) {\n\t\t/* acquired the lock */\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tif (!ww_ctx->is_wait_die)\n\t\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t} else {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_waiter(lock, waiter);\n\t\trt_mutex_handle_deadlock(ret, chwalk, waiter);\n\t}\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit\n\t * unconditionally. We might have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\treturn ret;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 1603
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "107-175",
          "snippet": "static __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_handle_deadlock",
          "args": [
            "ret",
            "chwalk",
            "waiter"
          ],
          "line": 1596
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_handle_deadlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1528-1549",
          "snippet": "static void __sched rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\tif (build_ww_mutex() && w->ww_ctx)\n\t\treturn;\n\n\t/*\n\t * Yell loudly and stop the task right here.\n\t */\n\tWARN(1, \"rtmutex deadlock detected\\n\");\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\tif (build_ww_mutex() && w->ww_ctx)\n\t\treturn;\n\n\t/*\n\t * Yell loudly and stop the task right here.\n\t */\n\tWARN(1, \"rtmutex deadlock detected\\n\");\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "remove_waiter",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 1595
        },
        "resolved": true,
        "details": {
          "function_name": "remove_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1417-1468",
          "snippet": "static void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 1594
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ww_mutex_lock_acquired",
          "args": [
            "ww",
            "ww_ctx"
          ],
          "line": 1591
        },
        "resolved": true,
        "details": {
          "function_name": "ww_mutex_lock_acquired",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "45-48",
          "snippet": "static inline void ww_mutex_lock_acquired(struct ww_mutex *lock,\n\t\t\t\t\t  struct ww_acquire_ctx *ww_ctx)\n{\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline void ww_mutex_lock_acquired(struct ww_mutex *lock,\n\t\t\t\t\t  struct ww_acquire_ctx *ww_ctx)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "__ww_mutex_check_waiters",
          "args": [
            "rtm",
            "ww_ctx"
          ],
          "line": 1590
        },
        "resolved": true,
        "details": {
          "function_name": "__ww_mutex_check_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "40-43",
          "snippet": "static inline void __ww_mutex_check_waiters(struct rt_mutex *lock,\n\t\t\t\t\t    struct ww_acquire_ctx *ww_ctx)\n{\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline void __ww_mutex_check_waiters(struct rt_mutex *lock,\n\t\t\t\t\t    struct ww_acquire_ctx *ww_ctx)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "build_ww_mutex",
          "args": [],
          "line": 1588
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!ret"
          ],
          "line": 1586
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_slowlock_block",
          "args": [
            "lock",
            "ww_ctx",
            "state",
            "NULL",
            "waiter"
          ],
          "line": 1584
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowlock_block",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1481-1526",
          "snippet": "static int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   unsigned int state,\n\t\t\t\t\t   struct hrtimer_sleeper *timeout,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\tif (timeout && !timeout->task) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(rtm, waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))\n\t\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   unsigned int state,\n\t\t\t\t\t   struct hrtimer_sleeper *timeout,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\tif (timeout && !timeout->task) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(rtm, waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))\n\t\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!ret"
          ],
          "line": 1583
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_blocks_on_rt_mutex",
          "args": [
            "lock",
            "waiter",
            "current",
            "ww_ctx",
            "chwalk"
          ],
          "line": 1582
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocks_on_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1085-1184",
          "snippet": "static int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 1580
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "build_ww_mutex",
          "args": [],
          "line": 1573
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "NULL"
          ],
          "line": 1572
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "968-1076",
          "snippet": "static int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1569
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ww_container_of",
          "args": [
            "rtm"
          ],
          "line": 1566
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "lock",
            "structrt_mutex",
            "rtmutex"
          ],
          "line": 1565
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched __rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t       struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t       unsigned int state,\n\t\t\t\t       enum rtmutex_chainwalk chwalk,\n\t\t\t\t       struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct ww_mutex *ww = ww_container_of(rtm);\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/* Try to acquire the lock again: */\n\tif (try_to_take_rt_mutex(lock, current, NULL)) {\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tset_current_state(state);\n\n\tret = task_blocks_on_rt_mutex(lock, waiter, current, ww_ctx, chwalk);\n\tif (likely(!ret))\n\t\tret = rt_mutex_slowlock_block(lock, ww_ctx, state, NULL, waiter);\n\n\tif (likely(!ret)) {\n\t\t/* acquired the lock */\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tif (!ww_ctx->is_wait_die)\n\t\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t} else {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_waiter(lock, waiter);\n\t\trt_mutex_handle_deadlock(ret, chwalk, waiter);\n\t}\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit\n\t * unconditionally. We might have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_handle_deadlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1528-1549",
    "snippet": "static void __sched rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\tif (build_ww_mutex() && w->ww_ctx)\n\t\treturn;\n\n\t/*\n\t * Yell loudly and stop the task right here.\n\t */\n\tWARN(1, \"rtmutex deadlock detected\\n\");\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "schedule",
          "args": [],
          "line": 1547
        },
        "resolved": true,
        "details": {
          "function_name": "audit_schedule_prune",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/audit_tree.c",
          "lines": "963-966",
          "snippet": "static void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}",
          "includes": [
            "#include <linux/slab.h>",
            "#include <linux/refcount.h>",
            "#include <linux/kthread.h>",
            "#include <linux/mount.h>",
            "#include <linux/namei.h>",
            "#include <linux/fsnotify_backend.h>",
            "#include \"audit.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct task_struct *prune_thread;",
            "static void audit_schedule_prune(void);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/slab.h>\n#include <linux/refcount.h>\n#include <linux/kthread.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/fsnotify_backend.h>\n#include \"audit.h\"\n\nstatic struct task_struct *prune_thread;\nstatic void audit_schedule_prune(void);\n\nstatic void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "TASK_INTERRUPTIBLE"
          ],
          "line": 1546
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN",
          "args": [
            "1",
            "\"rtmutex deadlock detected\\n\""
          ],
          "line": 1544
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "build_ww_mutex",
          "args": [],
          "line": 1538
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\tif (build_ww_mutex() && w->ww_ctx)\n\t\treturn;\n\n\t/*\n\t * Yell loudly and stop the task right here.\n\t */\n\tWARN(1, \"rtmutex deadlock detected\\n\");\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}"
  },
  {
    "function_name": "rt_mutex_slowlock_block",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1481-1526",
    "snippet": "static int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   unsigned int state,\n\t\t\t\t\t   struct hrtimer_sleeper *timeout,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\tif (timeout && !timeout->task) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(rtm, waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))\n\t\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 1524
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 1521
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1520
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedule",
          "args": [],
          "line": 1518
        },
        "resolved": true,
        "details": {
          "function_name": "audit_schedule_prune",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/audit_tree.c",
          "lines": "963-966",
          "snippet": "static void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}",
          "includes": [
            "#include <linux/slab.h>",
            "#include <linux/refcount.h>",
            "#include <linux/kthread.h>",
            "#include <linux/mount.h>",
            "#include <linux/namei.h>",
            "#include <linux/fsnotify_backend.h>",
            "#include \"audit.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct task_struct *prune_thread;",
            "static void audit_schedule_prune(void);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/slab.h>\n#include <linux/refcount.h>\n#include <linux/kthread.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/fsnotify_backend.h>\n#include \"audit.h\"\n\nstatic struct task_struct *prune_thread;\nstatic void audit_schedule_prune(void);\n\nstatic void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rtmutex_spin_on_owner",
          "args": [
            "lock",
            "waiter",
            "owner"
          ],
          "line": 1517
        },
        "resolved": true,
        "details": {
          "function_name": "rtmutex_spin_on_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1396-1401",
          "snippet": "static bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1515
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1512
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1511
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__ww_mutex_check_kill",
          "args": [
            "rtm",
            "waiter",
            "ww_ctx"
          ],
          "line": 1506
        },
        "resolved": true,
        "details": {
          "function_name": "__ww_mutex_check_kill",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "50-55",
          "snippet": "static inline int __ww_mutex_check_kill(struct rt_mutex *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline int __ww_mutex_check_kill(struct rt_mutex *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "build_ww_mutex",
          "args": [],
          "line": 1505
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "signal_pending_state",
          "args": [
            "state",
            "current"
          ],
          "line": 1500
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "waiter"
          ],
          "line": 1493
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "968-1076",
          "snippet": "static int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "lock",
            "structrt_mutex",
            "rtmutex"
          ],
          "line": 1487
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   unsigned int state,\n\t\t\t\t\t   struct hrtimer_sleeper *timeout,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\tif (timeout && !timeout->task) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(rtm, waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))\n\t\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}"
  },
  {
    "function_name": "remove_waiter",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1417-1468",
    "snippet": "static void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1467
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio_chain",
          "args": [
            "owner",
            "RT_MUTEX_MIN_CHAINWALK",
            "lock",
            "next_lock",
            "NULL",
            "current"
          ],
          "line": 1464
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio_chain",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "574-956",
          "snippet": "static int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1462
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "owner"
          ],
          "line": 1460
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 1450
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "208-211",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocked_on_lock",
          "args": [
            "owner"
          ],
          "line": 1448
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocked_on_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "506-509",
          "snippet": "rt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nrt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "owner"
          ],
          "line": 1445
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "436-446",
          "snippet": "static __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "owner",
            "rt_mutex_top_waiter(lock)"
          ],
          "line": 1443
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "420-424",
          "snippet": "static __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1443
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1442
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "93-96",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "owner",
            "waiter"
          ],
          "line": 1440
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "426-434",
          "snippet": "static __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 1438
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "176-179",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1424
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1421
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
  },
  {
    "function_name": "rtmutex_spin_on_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1396-1401",
    "snippet": "static bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\treturn false;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\treturn false;\n}"
  },
  {
    "function_name": "rtmutex_spin_on_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1359-1394",
    "snippet": "static bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\tbool res = true;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\t/* If owner changed, trylock again. */\n\t\tif (owner != rt_mutex_owner(lock))\n\t\t\tbreak;\n\t\t/*\n\t\t * Ensure that @owner is dereferenced after checking that\n\t\t * the lock owner still matches @owner. If that fails,\n\t\t * @owner might point to freed memory. If it still matches,\n\t\t * the rcu_read_lock() ensures the memory stays valid.\n\t\t */\n\t\tbarrier();\n\t\t/*\n\t\t * Stop spinning when:\n\t\t *  - the lock owner has been scheduled out\n\t\t *  - current is not longer the top waiter\n\t\t *  - current is requested to reschedule (redundant\n\t\t *    for CONFIG_PREEMPT_RCU=y)\n\t\t *  - the VCPU on which owner runs is preempted\n\t\t */\n\t\tif (!owner_on_cpu(owner) || need_resched() ||\n\t\t    !rt_mutex_waiter_is_top_waiter(lock, waiter)) {\n\t\t\tres = false;\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\n\trcu_read_unlock();\n\treturn res;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 1392
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_unlock_strict",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/rcu/tree_plugin.h",
          "lines": "815-824",
          "snippet": "void rcu_read_unlock_strict(void)\n{\n\tstruct rcu_data *rdp;\n\n\tif (irqs_disabled() || preempt_count() || !rcu_state.gp_kthread)\n\t\treturn;\n\trdp = this_cpu_ptr(&rcu_data);\n\trcu_report_qs_rdp(rdp);\n\tudelay(rcu_unlock_delay);\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n\nvoid rcu_read_unlock_strict(void)\n{\n\tstruct rcu_data *rdp;\n\n\tif (irqs_disabled() || preempt_count() || !rcu_state.gp_kthread)\n\t\treturn;\n\trdp = this_cpu_ptr(&rcu_data);\n\trcu_report_qs_rdp(rdp);\n\tudelay(rcu_unlock_delay);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 1390
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_is_top_waiter",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 1386
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_is_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "103-109",
          "snippet": "static inline bool rt_mutex_waiter_is_top_waiter(struct rt_mutex_base *lock,\n\t\t\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\n\treturn rb_entry(leftmost, struct rt_mutex_waiter, tree_entry) == waiter;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline bool rt_mutex_waiter_is_top_waiter(struct rt_mutex_base *lock,\n\t\t\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\n\treturn rb_entry(leftmost, struct rt_mutex_waiter, tree_entry) == waiter;\n}"
        }
      },
      {
        "call_info": {
          "callee": "need_resched",
          "args": [],
          "line": 1385
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "owner_on_cpu",
          "args": [
            "owner"
          ],
          "line": 1385
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "barrier",
          "args": [],
          "line": 1376
        },
        "resolved": true,
        "details": {
          "function_name": "membarrier_register_global_expedited",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/membarrier.c",
          "lines": "482-499",
          "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint ret;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tret = sync_runqueues_membarrier_state(mm);\n\tif (ret)\n\t\treturn ret;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1368
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 1365
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_any_held",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/rcu/update.c",
          "lines": "340-351",
          "snippet": "int rcu_read_lock_any_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\tif (lock_is_held(&rcu_lock_map) ||\n\t    lock_is_held(&rcu_bh_lock_map) ||\n\t    lock_is_held(&rcu_sched_lock_map))\n\t\treturn 1;\n\treturn !preemptible();\n}",
          "includes": [
            "#include \"tasks.h\"",
            "#include \"rcu.h\"",
            "#include <linux/rcupdate_trace.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/slab.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tasks.h\"\n#include \"rcu.h\"\n#include <linux/rcupdate_trace.h>\n#include <linux/irq_work.h>\n#include <linux/slab.h>\n#include <linux/kprobes.h>\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_any_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\tif (lock_is_held(&rcu_lock_map) ||\n\t    lock_is_held(&rcu_bh_lock_map) ||\n\t    lock_is_held(&rcu_sched_lock_map))\n\t\treturn 1;\n\treturn !preemptible();\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\tbool res = true;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\t/* If owner changed, trylock again. */\n\t\tif (owner != rt_mutex_owner(lock))\n\t\t\tbreak;\n\t\t/*\n\t\t * Ensure that @owner is dereferenced after checking that\n\t\t * the lock owner still matches @owner. If that fails,\n\t\t * @owner might point to freed memory. If it still matches,\n\t\t * the rcu_read_lock() ensures the memory stays valid.\n\t\t */\n\t\tbarrier();\n\t\t/*\n\t\t * Stop spinning when:\n\t\t *  - the lock owner has been scheduled out\n\t\t *  - current is not longer the top waiter\n\t\t *  - current is requested to reschedule (redundant\n\t\t *    for CONFIG_PREEMPT_RCU=y)\n\t\t *  - the VCPU on which owner runs is preempted\n\t\t */\n\t\tif (!owner_on_cpu(owner) || need_resched() ||\n\t\t    !rt_mutex_waiter_is_top_waiter(lock, waiter)) {\n\t\t\tres = false;\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\n\trcu_read_unlock();\n\treturn res;\n}"
  },
  {
    "function_name": "__rt_mutex_unlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1350-1356",
    "snippet": "static __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_slowunlock",
          "args": [
            "lock"
          ],
          "line": 1355
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowunlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1289-1348",
          "snippet": "static void __sched rt_mutex_slowunlock(struct rt_mutex_base *lock)\n{\n\tDEFINE_RT_WAKE_Q(wqh);\n\tunsigned long flags;\n\n\t/* irqsave required to support early boot calls */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\t/*\n\t * We must be careful here if the fast path is enabled. If we\n\t * have no waiters queued we cannot set owner to NULL here\n\t * because of:\n\t *\n\t * foo->lock->owner = NULL;\n\t *\t\t\trtmutex_lock(foo->lock);   <- fast path\n\t *\t\t\tfree = atomic_dec_and_test(foo->refcnt);\n\t *\t\t\trtmutex_unlock(foo->lock); <- fast path\n\t *\t\t\tif (free)\n\t *\t\t\t\tkfree(foo);\n\t * raw_spin_unlock(foo->lock->wait_lock);\n\t *\n\t * So for the fastpath enabled kernel:\n\t *\n\t * Nothing can set the waiters bit as long as we hold\n\t * lock->wait_lock. So we do the following sequence:\n\t *\n\t *\towner = rt_mutex_owner(lock);\n\t *\tclear_rt_mutex_waiters(lock);\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t *\tif (cmpxchg(&lock->owner, owner, 0) == owner)\n\t *\t\treturn;\n\t *\tgoto retry;\n\t *\n\t * The fastpath disabled variant is simple as all access to\n\t * lock->owner is serialized by lock->wait_lock:\n\t *\n\t *\tlock->owner = NULL;\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t */\n\twhile (!rt_mutex_has_waiters(lock)) {\n\t\t/* Drops lock->wait_lock ! */\n\t\tif (unlock_rt_mutex_safe(lock, flags) == true)\n\t\t\treturn;\n\t\t/* Relock the rtmutex and try again */\n\t\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\t}\n\n\t/*\n\t * The wakeup next waiter path does not suffer from the above\n\t * race. See the comments there.\n\t *\n\t * Queue the next waiter for wakeup once we release the wait_lock.\n\t */\n\tmark_wakeup_next_waiter(&wqh, lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\trt_mutex_wake_up_q(&wqh);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched rt_mutex_slowunlock(struct rt_mutex_base *lock)\n{\n\tDEFINE_RT_WAKE_Q(wqh);\n\tunsigned long flags;\n\n\t/* irqsave required to support early boot calls */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\t/*\n\t * We must be careful here if the fast path is enabled. If we\n\t * have no waiters queued we cannot set owner to NULL here\n\t * because of:\n\t *\n\t * foo->lock->owner = NULL;\n\t *\t\t\trtmutex_lock(foo->lock);   <- fast path\n\t *\t\t\tfree = atomic_dec_and_test(foo->refcnt);\n\t *\t\t\trtmutex_unlock(foo->lock); <- fast path\n\t *\t\t\tif (free)\n\t *\t\t\t\tkfree(foo);\n\t * raw_spin_unlock(foo->lock->wait_lock);\n\t *\n\t * So for the fastpath enabled kernel:\n\t *\n\t * Nothing can set the waiters bit as long as we hold\n\t * lock->wait_lock. So we do the following sequence:\n\t *\n\t *\towner = rt_mutex_owner(lock);\n\t *\tclear_rt_mutex_waiters(lock);\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t *\tif (cmpxchg(&lock->owner, owner, 0) == owner)\n\t *\t\treturn;\n\t *\tgoto retry;\n\t *\n\t * The fastpath disabled variant is simple as all access to\n\t * lock->owner is serialized by lock->wait_lock:\n\t *\n\t *\tlock->owner = NULL;\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t */\n\twhile (!rt_mutex_has_waiters(lock)) {\n\t\t/* Drops lock->wait_lock ! */\n\t\tif (unlock_rt_mutex_safe(lock, flags) == true)\n\t\t\treturn;\n\t\t/* Relock the rtmutex and try again */\n\t\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\t}\n\n\t/*\n\t * The wakeup next waiter path does not suffer from the above\n\t * race. See the comments there.\n\t *\n\t * Queue the next waiter for wakeup once we release the wait_lock.\n\t */\n\tmark_wakeup_next_waiter(&wqh, lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\trt_mutex_wake_up_q(&wqh);\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_release(lock, current, NULL)"
          ],
          "line": 1352
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_release",
          "args": [
            "lock",
            "current",
            "NULL"
          ],
          "line": 1352
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_release",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "261-266",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}"
  },
  {
    "function_name": "rt_mutex_slowunlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1289-1348",
    "snippet": "static void __sched rt_mutex_slowunlock(struct rt_mutex_base *lock)\n{\n\tDEFINE_RT_WAKE_Q(wqh);\n\tunsigned long flags;\n\n\t/* irqsave required to support early boot calls */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\t/*\n\t * We must be careful here if the fast path is enabled. If we\n\t * have no waiters queued we cannot set owner to NULL here\n\t * because of:\n\t *\n\t * foo->lock->owner = NULL;\n\t *\t\t\trtmutex_lock(foo->lock);   <- fast path\n\t *\t\t\tfree = atomic_dec_and_test(foo->refcnt);\n\t *\t\t\trtmutex_unlock(foo->lock); <- fast path\n\t *\t\t\tif (free)\n\t *\t\t\t\tkfree(foo);\n\t * raw_spin_unlock(foo->lock->wait_lock);\n\t *\n\t * So for the fastpath enabled kernel:\n\t *\n\t * Nothing can set the waiters bit as long as we hold\n\t * lock->wait_lock. So we do the following sequence:\n\t *\n\t *\towner = rt_mutex_owner(lock);\n\t *\tclear_rt_mutex_waiters(lock);\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t *\tif (cmpxchg(&lock->owner, owner, 0) == owner)\n\t *\t\treturn;\n\t *\tgoto retry;\n\t *\n\t * The fastpath disabled variant is simple as all access to\n\t * lock->owner is serialized by lock->wait_lock:\n\t *\n\t *\tlock->owner = NULL;\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t */\n\twhile (!rt_mutex_has_waiters(lock)) {\n\t\t/* Drops lock->wait_lock ! */\n\t\tif (unlock_rt_mutex_safe(lock, flags) == true)\n\t\t\treturn;\n\t\t/* Relock the rtmutex and try again */\n\t\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\t}\n\n\t/*\n\t * The wakeup next waiter path does not suffer from the above\n\t * race. See the comments there.\n\t *\n\t * Queue the next waiter for wakeup once we release the wait_lock.\n\t */\n\tmark_wakeup_next_waiter(&wqh, lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\trt_mutex_wake_up_q(&wqh);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_wake_up_q",
          "args": [
            "&wqh"
          ],
          "line": 1347
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_up_q",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "469-482",
          "snippet": "static __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1345
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mark_wakeup_next_waiter",
          "args": [
            "&wqh",
            "lock"
          ],
          "line": 1344
        },
        "resolved": true,
        "details": {
          "function_name": "mark_wakeup_next_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1192-1234",
          "snippet": "static void __sched mark_wakeup_next_waiter(struct rt_wake_q_head *wqh,\n\t\t\t\t\t    struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_wake_up_q();\n\t */\n\tpreempt_disable();\n\trt_mutex_wake_q_add(wqh, waiter);\n\traw_spin_unlock(&current->pi_lock);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched mark_wakeup_next_waiter(struct rt_wake_q_head *wqh,\n\t\t\t\t\t    struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_wake_up_q();\n\t */\n\tpreempt_disable();\n\trt_mutex_wake_q_add(wqh, waiter);\n\traw_spin_unlock(&current->pi_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1335
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlock_rt_mutex_safe",
          "args": [
            "lock",
            "flags"
          ],
          "line": 1332
        },
        "resolved": true,
        "details": {
          "function_name": "unlock_rt_mutex_safe",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "277-284",
          "snippet": "static __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1330
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "93-96",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_unlock",
          "args": [
            "lock"
          ],
          "line": 1297
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_unlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "166-170",
          "snippet": "static inline void debug_rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline void debug_rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_RT_WAKE_Q",
          "args": [
            "wqh"
          ],
          "line": 1291
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched rt_mutex_slowunlock(struct rt_mutex_base *lock)\n{\n\tDEFINE_RT_WAKE_Q(wqh);\n\tunsigned long flags;\n\n\t/* irqsave required to support early boot calls */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\t/*\n\t * We must be careful here if the fast path is enabled. If we\n\t * have no waiters queued we cannot set owner to NULL here\n\t * because of:\n\t *\n\t * foo->lock->owner = NULL;\n\t *\t\t\trtmutex_lock(foo->lock);   <- fast path\n\t *\t\t\tfree = atomic_dec_and_test(foo->refcnt);\n\t *\t\t\trtmutex_unlock(foo->lock); <- fast path\n\t *\t\t\tif (free)\n\t *\t\t\t\tkfree(foo);\n\t * raw_spin_unlock(foo->lock->wait_lock);\n\t *\n\t * So for the fastpath enabled kernel:\n\t *\n\t * Nothing can set the waiters bit as long as we hold\n\t * lock->wait_lock. So we do the following sequence:\n\t *\n\t *\towner = rt_mutex_owner(lock);\n\t *\tclear_rt_mutex_waiters(lock);\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t *\tif (cmpxchg(&lock->owner, owner, 0) == owner)\n\t *\t\treturn;\n\t *\tgoto retry;\n\t *\n\t * The fastpath disabled variant is simple as all access to\n\t * lock->owner is serialized by lock->wait_lock:\n\t *\n\t *\tlock->owner = NULL;\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t */\n\twhile (!rt_mutex_has_waiters(lock)) {\n\t\t/* Drops lock->wait_lock ! */\n\t\tif (unlock_rt_mutex_safe(lock, flags) == true)\n\t\t\treturn;\n\t\t/* Relock the rtmutex and try again */\n\t\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\t}\n\n\t/*\n\t * The wakeup next waiter path does not suffer from the above\n\t * race. See the comments there.\n\t *\n\t * Queue the next waiter for wakeup once we release the wait_lock.\n\t */\n\tmark_wakeup_next_waiter(&wqh, lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\trt_mutex_wake_up_q(&wqh);\n}"
  },
  {
    "function_name": "__rt_mutex_trylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1278-1284",
    "snippet": "static __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_slowtrylock",
          "args": [
            "lock"
          ],
          "line": 1283
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowtrylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1252-1276",
          "snippet": "static int __sched rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_acquire(lock, NULL, current)"
          ],
          "line": 1280
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_acquire",
          "args": [
            "lock",
            "NULL",
            "current"
          ],
          "line": 1280
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_acquire",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "253-259",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}"
  },
  {
    "function_name": "rt_mutex_slowtrylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1252-1276",
    "snippet": "static int __sched rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1273
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_slowtrylock",
          "args": [
            "lock"
          ],
          "line": 1271
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowtrylock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "1236-1247",
          "snippet": "static int __sched __rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched __rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1269
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "383-393",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1262
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "__rt_mutex_slowtrylock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1236-1247",
    "snippet": "static int __sched __rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 1244
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "107-175",
          "snippet": "static __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "NULL"
          ],
          "line": 1238
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "968-1076",
          "snippet": "static int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched __rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "mark_wakeup_next_waiter",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1192-1234",
    "snippet": "static void __sched mark_wakeup_next_waiter(struct rt_wake_q_head *wqh,\n\t\t\t\t\t    struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_wake_up_q();\n\t */\n\tpreempt_disable();\n\trt_mutex_wake_q_add(wqh, waiter);\n\traw_spin_unlock(&current->pi_lock);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&current->pi_lock"
          ],
          "line": 1233
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "208-211",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_wake_q_add",
          "args": [
            "wqh",
            "waiter"
          ],
          "line": 1232
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_q_add",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "463-467",
          "snippet": "static __always_inline void rt_mutex_wake_q_add(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\tstruct rt_mutex_waiter *w)\n{\n\trt_mutex_wake_q_add_task(wqh, w->task, w->wake_state);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_q_add(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\tstruct rt_mutex_waiter *w)\n{\n\trt_mutex_wake_q_add_task(wqh, w->task, w->wake_state);\n}"
        }
      },
      {
        "call_info": {
          "callee": "preempt_disable",
          "args": [],
          "line": 1231
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_preempt_disabled",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "6425-6430",
          "snippet": "void __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic void __sched;\nstatic void __sched;\n\nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "current"
          ],
          "line": 1209
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "436-446",
          "snippet": "static __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "current",
            "waiter"
          ],
          "line": 1208
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "426-434",
          "snippet": "static __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1199
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&current->pi_lock"
          ],
          "line": 1197
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "176-179",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic void __sched mark_wakeup_next_waiter(struct rt_wake_q_head *wqh,\n\t\t\t\t\t    struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_wake_up_q();\n\t */\n\tpreempt_disable();\n\trt_mutex_wake_q_add(wqh, waiter);\n\traw_spin_unlock(&current->pi_lock);\n}"
  },
  {
    "function_name": "task_blocks_on_rt_mutex",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "1085-1184",
    "snippet": "static int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1181
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio_chain",
          "args": [
            "owner",
            "chwalk",
            "lock",
            "next_lock",
            "waiter",
            "task"
          ],
          "line": 1178
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio_chain",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "574-956",
          "snippet": "static int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1176
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "owner"
          ],
          "line": 1174
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 1160
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "208-211",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocked_on_lock",
          "args": [
            "owner"
          ],
          "line": 1158
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocked_on_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "506-509",
          "snippet": "rt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nrt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_cond_detect_deadlock",
          "args": [
            "waiter",
            "chwalk"
          ],
          "line": 1153
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cond_detect_deadlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "497-504",
          "snippet": "static __always_inline bool\nrt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t      enum rtmutex_chainwalk chwalk)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\treturn waiter != NULL;\n\treturn chwalk == RT_MUTEX_FULL_CHAINWALK;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool\nrt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t      enum rtmutex_chainwalk chwalk)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\treturn waiter != NULL;\n\treturn chwalk == RT_MUTEX_FULL_CHAINWALK;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "owner"
          ],
          "line": 1150
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "436-446",
          "snippet": "static __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "owner",
            "waiter"
          ],
          "line": 1148
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "420-424",
          "snippet": "static __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "owner",
            "top_waiter"
          ],
          "line": 1147
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "426-434",
          "snippet": "static __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1146
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 1145
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "176-179",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__ww_mutex_add_waiter",
          "args": [
            "waiter",
            "rtm",
            "ww_ctx"
          ],
          "line": 1132
        },
        "resolved": true,
        "details": {
          "function_name": "__ww_mutex_add_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "33-38",
          "snippet": "static inline int __ww_mutex_add_waiter(struct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct rt_mutex *lock,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline int __ww_mutex_add_waiter(struct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct rt_mutex *lock,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "lock",
            "structrt_mutex",
            "rtmutex"
          ],
          "line": 1131
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "build_ww_mutex",
          "args": [],
          "line": 1127
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1119
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "93-96",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "waiter_update_prio",
          "args": [
            "waiter",
            "task"
          ],
          "line": 1116
        },
        "resolved": true,
        "details": {
          "function_name": "waiter_update_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "297-302",
          "snippet": "static __always_inline void\nwaiter_update_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\twaiter->prio = __waiter_prio(task);\n\twaiter->deadline = task->dl.deadline;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nwaiter_update_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\twaiter->prio = __waiter_prio(task);\n\twaiter->deadline = task->dl.deadline;\n}"
        }
      },
      {
        "call_info": {
          "callee": "build_ww_mutex",
          "args": [],
          "line": 1110
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1096
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1091
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t *\n\t * Except for ww_mutex, in that case the chain walk must already deal\n\t * with spurious cycles, see the comments at [3] and [6].\n\t */\n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t/* Check whether the waiter should back out immediately */\n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}"
  },
  {
    "function_name": "try_to_take_rt_mutex",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "968-1076",
    "snippet": "static int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_set_owner",
          "args": [
            "lock",
            "task"
          ],
          "line": 1073
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_set_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "90-99",
          "snippet": "static __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 1066
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "208-211",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "task",
            "rt_mutex_top_waiter(lock)"
          ],
          "line": 1065
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "420-424",
          "snippet": "static __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1065
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1064
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "93-96",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 1057
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "176-179",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_steal",
          "args": [
            "task_to_waiter(task)",
            "rt_mutex_top_waiter(lock)"
          ],
          "line": 1031
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_steal",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "346-364",
          "snippet": "static inline bool rt_mutex_steal(struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct rt_mutex_waiter *top_waiter)\n{\n\tif (rt_mutex_waiter_less(waiter, top_waiter))\n\t\treturn true;\n\n#ifdef RT_MUTEX_BUILD_SPINLOCKS\n\t/*\n\t * Note that RT tasks are excluded from same priority (lateral)\n\t * steals to prevent the introduction of an unbounded latency.\n\t */\n\tif (rt_prio(waiter->prio) || dl_prio(waiter->prio))\n\t\treturn false;\n\n\treturn rt_mutex_waiter_equal(waiter, top_waiter);\n#else\n\treturn false;\n#endif\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline bool rt_mutex_steal(struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct rt_mutex_waiter *top_waiter)\n{\n\tif (rt_mutex_waiter_less(waiter, top_waiter))\n\t\treturn true;\n\n#ifdef RT_MUTEX_BUILD_SPINLOCKS\n\t/*\n\t * Note that RT tasks are excluded from same priority (lateral)\n\t * steals to prevent the introduction of an unbounded latency.\n\t */\n\tif (rt_prio(waiter->prio) || dl_prio(waiter->prio))\n\t\treturn false;\n\n\treturn rt_mutex_waiter_equal(waiter, top_waiter);\n#else\n\treturn false;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_to_waiter",
          "args": [
            "task"
          ],
          "line": 1031
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 1016
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "426-434",
          "snippet": "static __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 996
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "mark_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 991
        },
        "resolved": true,
        "details": {
          "function_name": "mark_rt_mutex_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "268-272",
          "snippet": "static __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 972
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/*\n\t\t * If waiter is the highest priority waiter of @lock,\n\t\t * or allowed to steal it, take it over.\n\t\t */\n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t/*\n\t\t\t * We can acquire the lock. Remove the waiter from the\n\t\t\t * lock waiters tree.\n\t\t\t */\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/* Check whether the trylock can steal it. */\n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
  },
  {
    "function_name": "rt_mutex_adjust_prio_chain",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "574-956",
    "snippet": "static int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "put_task_struct",
          "args": [
            "task"
          ],
          "line": 953
        },
        "resolved": true,
        "details": {
          "function_name": "__put_task_struct",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/fork.c",
          "lines": "745-761",
          "snippet": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tio_uring_free(tsk);\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\tbpf_task_storage_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\tsched_core_free(tsk);\n\tfree_task(tsk);\n}",
          "includes": [
            "#include <linux/init_task.h>",
            "#include <trace/events/task.h>",
            "#include <trace/events/sched.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/pgalloc.h>",
            "#include <linux/bpf.h>",
            "#include <linux/io_uring.h>",
            "#include <linux/scs.h>",
            "#include <linux/kasan.h>",
            "#include <linux/stackleak.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/kcov.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/compiler.h>",
            "#include <linux/aio.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/oom.h>",
            "#include <linux/user-return-notifier.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/magic.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tty.h>",
            "#include <linux/random.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/acct.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/profile.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/audit.h>",
            "#include <linux/mount.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/kthread.h>",
            "#include <linux/compat.h>",
            "#include <linux/futex.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/seccomp.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/cpu.h>",
            "#include <linux/capability.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/mman.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/sem.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/module.h>",
            "#include <linux/unistd.h>",
            "#include <linux/init.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/anon_inodes.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __latent_entropy struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/init_task.h>\n#include <trace/events/task.h>\n#include <trace/events/sched.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <linux/bpf.h>\n#include <linux/io_uring.h>\n#include <linux/scs.h>\n#include <linux/kasan.h>\n#include <linux/stackleak.h>\n#include <linux/thread_info.h>\n#include <linux/livepatch.h>\n#include <linux/kcov.h>\n#include <linux/sysctl.h>\n#include <linux/compiler.h>\n#include <linux/aio.h>\n#include <linux/uprobes.h>\n#include <linux/signalfd.h>\n#include <linux/khugepaged.h>\n#include <linux/oom.h>\n#include <linux/user-return-notifier.h>\n#include <linux/posix-timers.h>\n#include <linux/perf_event.h>\n#include <linux/magic.h>\n#include <linux/fs_struct.h>\n#include <linux/tty.h>\n#include <linux/random.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/freezer.h>\n#include <linux/cn_proc.h>\n#include <linux/tsacct_kern.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/acct.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/profile.h>\n#include <linux/proc_fs.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/audit.h>\n#include <linux/mount.h>\n#include <linux/ptrace.h>\n#include <linux/rcupdate.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/kthread.h>\n#include <linux/compat.h>\n#include <linux/futex.h>\n#include <linux/jiffies.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/seccomp.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/cgroup.h>\n#include <linux/cpu.h>\n#include <linux/capability.h>\n#include <linux/nsproxy.h>\n#include <linux/vmacache.h>\n#include <linux/mm_inline.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mman.h>\n#include <linux/binfmts.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/sem.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/vmalloc.h>\n#include <linux/module.h>\n#include <linux/unistd.h>\n#include <linux/init.h>\n#include <linux/rtmutex.h>\n#include <linux/seq_file.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/user.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/anon_inodes.h>\n\nstatic __latent_entropy struct;\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tio_uring_free(tsk);\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\tbpf_task_storage_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\tsched_core_free(tsk);\n\tfree_task(tsk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&task->pi_lock"
          ],
          "line": 951
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "200-203",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 927
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "208-211",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 924
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "111-121",
          "snippet": "static inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *rt_mutex_top_waiter(struct rt_mutex_base *lock)\n{\n\tstruct rb_node *leftmost = rb_first_cached(&lock->waiters);\n\tstruct rt_mutex_waiter *w = NULL;\n\n\tif (leftmost) {\n\t\tw = rb_entry(leftmost, struct rt_mutex_waiter, tree_entry);\n\t\tBUG_ON(w->lock != lock);\n\t}\n\treturn w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocked_on_lock",
          "args": [
            "task"
          ],
          "line": 919
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocked_on_lock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "506-509",
          "snippet": "rt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nrt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "task"
          ],
          "line": 901
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "436-446",
          "snippet": "static __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "task",
            "waiter"
          ],
          "line": 900
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "420-424",
          "snippet": "static __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "task",
            "waiter"
          ],
          "line": 898
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "426-434",
          "snippet": "static __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 873
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "176-179",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "rt_mutex_owner(lock)"
          ],
          "line": 872
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 872
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_up_state",
          "args": [
            "waiter->task",
            "waiter->wake_state"
          ],
          "line": 866
        },
        "resolved": true,
        "details": {
          "function_name": "signal_wake_up_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/signal.c",
          "lines": "763-775",
          "snippet": "void signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}",
          "includes": [
            "#include <linux/kdb.h>",
            "#include <asm/syscall.h>\t/* for syscall_get_* */",
            "#include <asm/cacheflush.h>",
            "#include <asm/siginfo.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/param.h>",
            "#include <trace/events/signal.h>",
            "#include <linux/audit.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/compiler.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/compat.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/user_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/freezer.h>",
            "#include <linux/capability.h>",
            "#include <linux/task_work.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/signal.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/coredump.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/tty.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/fs.h>",
            "#include <linux/file.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/kdb.h>\n#include <asm/syscall.h>\t/* for syscall_get_* */\n#include <asm/cacheflush.h>\n#include <asm/siginfo.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <asm/param.h>\n#include <trace/events/signal.h>\n#include <linux/audit.h>\n#include <linux/cgroup.h>\n#include <linux/posix-timers.h>\n#include <linux/compiler.h>\n#include <linux/cn_proc.h>\n#include <linux/compat.h>\n#include <linux/uprobes.h>\n#include <linux/user_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/pid_namespace.h>\n#include <linux/freezer.h>\n#include <linux/capability.h>\n#include <linux/task_work.h>\n#include <linux/ratelimit.h>\n#include <linux/signalfd.h>\n#include <linux/signal.h>\n#include <linux/ptrace.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/coredump.h>\n#include <linux/binfmts.h>\n#include <linux/tty.h>\n#include <linux/proc_fs.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/user.h>\n#include <linux/sched/mm.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}"
        }
      },
      {
        "call_info": {
          "callee": "waiter_update_prio",
          "args": [
            "waiter",
            "task"
          ],
          "line": 844
        },
        "resolved": true,
        "details": {
          "function_name": "waiter_update_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "297-302",
          "snippet": "static __always_inline void\nwaiter_update_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\twaiter->prio = __waiter_prio(task);\n\twaiter->deadline = task->dl.deadline;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nwaiter_update_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\twaiter->prio = __waiter_prio(task);\n\twaiter->deadline = task->dl.deadline;\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "rt_mutex_owner(lock)"
          ],
          "line": 793
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_PREEMPT_RT"
          ],
          "line": 763
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 738
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_trylock",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 736
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_trylock_bh",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "144-147",
          "snippet": "int __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)\n{\n\treturn __raw_spin_trylock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nint __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)\n{\n\treturn __raw_spin_trylock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_equal",
          "args": [
            "waiter",
            "task_to_waiter(task)"
          ],
          "line": 720
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_equal",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "328-344",
          "snippet": "static __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_to_waiter",
          "args": [
            "task"
          ],
          "line": 720
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_top_pi_waiter",
          "args": [
            "task"
          ],
          "line": 705
        },
        "resolved": true,
        "details": {
          "function_name": "task_top_pi_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "128-132",
          "snippet": "static inline struct rt_mutex_waiter *task_top_pi_waiter(struct task_struct *p)\n{\n\treturn rb_entry(p->pi_waiters.rb_leftmost, struct rt_mutex_waiter,\n\t\t\tpi_tree_entry);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *task_top_pi_waiter(struct task_struct *p)\n{\n\treturn rb_entry(p->pi_waiters.rb_leftmost, struct rt_mutex_waiter,\n\t\t\tpi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_has_pi_waiters",
          "args": [
            "task"
          ],
          "line": 697
        },
        "resolved": true,
        "details": {
          "function_name": "task_has_pi_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "123-126",
          "snippet": "static inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn !RB_EMPTY_ROOT(&p->pi_waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn !RB_EMPTY_ROOT(&p->pi_waiters.rb_root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_PREEMPT_RT"
          ],
          "line": 688
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&task->pi_lock"
          ],
          "line": 628
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "168-171",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "printk",
          "args": [
            "KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\"",
            "max_lock_depth",
            "top_task->comm",
            "task_pid_nr(top_task)"
          ],
          "line": 609
        },
        "resolved": true,
        "details": {
          "function_name": "__warn_printk",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/panic.c",
          "lines": "623-632",
          "snippet": "void __warn_printk(const char *fmt, ...)\n{\n\tva_list args;\n\n\tpr_warn(CUT_HERE);\n\n\tva_start(args, fmt);\n\tvprintk(fmt, args);\n\tva_end(args);\n}",
          "includes": [
            "#include <asm/sections.h>",
            "#include <trace/events/error_report.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/bug.h>",
            "#include <linux/console.h>",
            "#include <linux/nmi.h>",
            "#include <linux/init.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/sched.h>",
            "#include <linux/panic_notifier.h>",
            "#include <linux/kexec.h>",
            "#include <linux/delay.h>",
            "#include <linux/reboot.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/random.h>",
            "#include <linux/module.h>",
            "#include <linux/vt_kern.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/kmsg_dump.h>",
            "#include <linux/kgdb.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/sections.h>\n#include <trace/events/error_report.h>\n#include <linux/debugfs.h>\n#include <linux/ratelimit.h>\n#include <linux/bug.h>\n#include <linux/console.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/sysrq.h>\n#include <linux/sched.h>\n#include <linux/panic_notifier.h>\n#include <linux/kexec.h>\n#include <linux/delay.h>\n#include <linux/reboot.h>\n#include <linux/ftrace.h>\n#include <linux/random.h>\n#include <linux/module.h>\n#include <linux/vt_kern.h>\n#include <linux/notifier.h>\n#include <linux/kallsyms.h>\n#include <linux/kmsg_dump.h>\n#include <linux/kgdb.h>\n#include <linux/interrupt.h>\n#include <linux/sched/debug.h>\n#include <linux/debug_locks.h>\n\nvoid __warn_printk(const char *fmt, ...)\n{\n\tva_list args;\n\n\tpr_warn(CUT_HERE);\n\n\tva_start(args, fmt);\n\tvprintk(fmt, args);\n\tva_end(args);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_pid_nr",
          "args": [
            "top_task"
          ],
          "line": 611
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cond_detect_deadlock",
          "args": [
            "orig_waiter",
            "chwalk"
          ],
          "line": 588
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cond_detect_deadlock",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "497-504",
          "snippet": "static __always_inline bool\nrt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t      enum rtmutex_chainwalk chwalk)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\treturn waiter != NULL;\n\treturn chwalk == RT_MUTEX_FULL_CHAINWALK;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool\nrt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t      enum rtmutex_chainwalk chwalk)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\treturn waiter != NULL;\n\treturn chwalk == RT_MUTEX_FULL_CHAINWALK;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * There could be 'spurious' loops in the lock graph due to ww_mutex,\n\t * consider:\n\t *\n\t *   P1: A, ww_A, ww_B\n\t *   P2: ww_B, ww_A\n\t *   P3: A\n\t *\n\t * P3 should not return -EDEADLK because it gets trapped in the cycle\n\t * created by P1 and P2 (which will resolve -- and runs into\n\t * max_lock_depth above). Therefore disable detect_deadlock such that\n\t * the below termination condition can trigger once all relevant tasks\n\t * are boosted.\n\t *\n\t * Even when we start with ww_mutex we can disable deadlock detection,\n\t * since we would supress a ww_mutex induced deadlock at [6] anyway.\n\t * Supressing it here however is not sufficient since we might still\n\t * hit [6] due to adjustment driven iteration.\n\t *\n\t * NOTE: if someone were to create a deadlock between 2 ww_classes we'd\n\t * utterly fail to report it; lockdep should.\n\t */\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t/*\n\t\t * When the deadlock is due to ww_mutex; also see above. Don't\n\t\t * report the deadlock and instead let the ww_mutex wound/die\n\t\t * logic pick which of the contending threads gets -EDEADLK.\n\t\t *\n\t\t * NOTE: assumes the cycle only contains a single ww_class; any\n\t\t * other configuration and we fail to report; also, see\n\t\t * lockdep.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_state(waiter->task, waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top priority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "task_blocked_on_lock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "506-509",
    "snippet": "rt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nrt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
  },
  {
    "function_name": "rt_mutex_cond_detect_deadlock",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "497-504",
    "snippet": "static __always_inline bool\nrt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t      enum rtmutex_chainwalk chwalk)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\treturn waiter != NULL;\n\treturn chwalk == RT_MUTEX_FULL_CHAINWALK;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_DEBUG_RT_MUTEXES"
          ],
          "line": 501
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool\nrt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t      enum rtmutex_chainwalk chwalk)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\treturn waiter != NULL;\n\treturn chwalk == RT_MUTEX_FULL_CHAINWALK;\n}"
  },
  {
    "function_name": "rt_mutex_wake_up_q",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "469-482",
    "snippet": "static __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "preempt_enable",
          "args": [],
          "line": 481
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "&wqh->head"
          ],
          "line": 478
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_up_q",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "469-482",
          "snippet": "static __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "wake_q_empty",
          "args": [
            "&wqh->head"
          ],
          "line": 477
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "put_task_struct",
          "args": [
            "wqh->rtlock_task"
          ],
          "line": 473
        },
        "resolved": true,
        "details": {
          "function_name": "__put_task_struct",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/fork.c",
          "lines": "745-761",
          "snippet": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tio_uring_free(tsk);\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\tbpf_task_storage_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\tsched_core_free(tsk);\n\tfree_task(tsk);\n}",
          "includes": [
            "#include <linux/init_task.h>",
            "#include <trace/events/task.h>",
            "#include <trace/events/sched.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/pgalloc.h>",
            "#include <linux/bpf.h>",
            "#include <linux/io_uring.h>",
            "#include <linux/scs.h>",
            "#include <linux/kasan.h>",
            "#include <linux/stackleak.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/kcov.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/compiler.h>",
            "#include <linux/aio.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/oom.h>",
            "#include <linux/user-return-notifier.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/magic.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tty.h>",
            "#include <linux/random.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/acct.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/profile.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/audit.h>",
            "#include <linux/mount.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/kthread.h>",
            "#include <linux/compat.h>",
            "#include <linux/futex.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/seccomp.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/cpu.h>",
            "#include <linux/capability.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/mman.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/sem.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/module.h>",
            "#include <linux/unistd.h>",
            "#include <linux/init.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/anon_inodes.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __latent_entropy struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/init_task.h>\n#include <trace/events/task.h>\n#include <trace/events/sched.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <linux/bpf.h>\n#include <linux/io_uring.h>\n#include <linux/scs.h>\n#include <linux/kasan.h>\n#include <linux/stackleak.h>\n#include <linux/thread_info.h>\n#include <linux/livepatch.h>\n#include <linux/kcov.h>\n#include <linux/sysctl.h>\n#include <linux/compiler.h>\n#include <linux/aio.h>\n#include <linux/uprobes.h>\n#include <linux/signalfd.h>\n#include <linux/khugepaged.h>\n#include <linux/oom.h>\n#include <linux/user-return-notifier.h>\n#include <linux/posix-timers.h>\n#include <linux/perf_event.h>\n#include <linux/magic.h>\n#include <linux/fs_struct.h>\n#include <linux/tty.h>\n#include <linux/random.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/freezer.h>\n#include <linux/cn_proc.h>\n#include <linux/tsacct_kern.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/acct.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/profile.h>\n#include <linux/proc_fs.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/audit.h>\n#include <linux/mount.h>\n#include <linux/ptrace.h>\n#include <linux/rcupdate.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/kthread.h>\n#include <linux/compat.h>\n#include <linux/futex.h>\n#include <linux/jiffies.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/seccomp.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/cgroup.h>\n#include <linux/cpu.h>\n#include <linux/capability.h>\n#include <linux/nsproxy.h>\n#include <linux/vmacache.h>\n#include <linux/mm_inline.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mman.h>\n#include <linux/binfmts.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/sem.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/vmalloc.h>\n#include <linux/module.h>\n#include <linux/unistd.h>\n#include <linux/init.h>\n#include <linux/rtmutex.h>\n#include <linux/seq_file.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/user.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/anon_inodes.h>\n\nstatic __latent_entropy struct;\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tio_uring_free(tsk);\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\tbpf_task_storage_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\tsched_core_free(tsk);\n\tfree_task(tsk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_up_state",
          "args": [
            "wqh->rtlock_task",
            "TASK_RTLOCK_WAIT"
          ],
          "line": 472
        },
        "resolved": true,
        "details": {
          "function_name": "signal_wake_up_state",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/signal.c",
          "lines": "763-775",
          "snippet": "void signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}",
          "includes": [
            "#include <linux/kdb.h>",
            "#include <asm/syscall.h>\t/* for syscall_get_* */",
            "#include <asm/cacheflush.h>",
            "#include <asm/siginfo.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/param.h>",
            "#include <trace/events/signal.h>",
            "#include <linux/audit.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/compiler.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/compat.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/user_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/freezer.h>",
            "#include <linux/capability.h>",
            "#include <linux/task_work.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/signal.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/coredump.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/tty.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/fs.h>",
            "#include <linux/file.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/kdb.h>\n#include <asm/syscall.h>\t/* for syscall_get_* */\n#include <asm/cacheflush.h>\n#include <asm/siginfo.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <asm/param.h>\n#include <trace/events/signal.h>\n#include <linux/audit.h>\n#include <linux/cgroup.h>\n#include <linux/posix-timers.h>\n#include <linux/compiler.h>\n#include <linux/cn_proc.h>\n#include <linux/compat.h>\n#include <linux/uprobes.h>\n#include <linux/user_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/pid_namespace.h>\n#include <linux/freezer.h>\n#include <linux/capability.h>\n#include <linux/task_work.h>\n#include <linux/ratelimit.h>\n#include <linux/signalfd.h>\n#include <linux/signal.h>\n#include <linux/ptrace.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/coredump.h>\n#include <linux/binfmts.h>\n#include <linux/tty.h>\n#include <linux/proc_fs.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/user.h>\n#include <linux/sched/mm.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n\nvoid signal_wake_up_state(struct task_struct *t, unsigned int state)\n{\n\tset_tsk_thread_flag(t, TIF_SIGPENDING);\n\t/*\n\t * TASK_WAKEKILL also means wake it up in the stopped/traced/killable\n\t * case. We don't check t->state here because there is a race with it\n\t * executing another processor and just now entering stopped state.\n\t * By using wake_up_state, we ensure the process will wake up and\n\t * handle its death signal.\n\t */\n\tif (!wake_up_state(t, state | TASK_INTERRUPTIBLE))\n\t\tkick_process(t);\n}"
        }
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_PREEMPT_RT"
          ],
          "line": 471
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t/* Pairs with preempt_disable() in mark_wakeup_next_waiter() */\n\tpreempt_enable();\n}"
  },
  {
    "function_name": "rt_mutex_wake_q_add",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "463-467",
    "snippet": "static __always_inline void rt_mutex_wake_q_add(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\tstruct rt_mutex_waiter *w)\n{\n\trt_mutex_wake_q_add_task(wqh, w->task, w->wake_state);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_wake_q_add_task",
          "args": [
            "wqh",
            "w->task",
            "w->wake_state"
          ],
          "line": 466
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_q_add_task",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "449-461",
          "snippet": "static __always_inline void rt_mutex_wake_q_add_task(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\t     struct task_struct *task,\n\t\t\t\t\t\t     unsigned int wake_state)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wake_state == TASK_RTLOCK_WAIT) {\n\t\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\t\tWARN_ON_ONCE(wqh->rtlock_task);\n\t\tget_task_struct(task);\n\t\twqh->rtlock_task = task;\n\t} else {\n\t\twake_q_add(&wqh->head, task);\n\t}\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_q_add_task(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\t     struct task_struct *task,\n\t\t\t\t\t\t     unsigned int wake_state)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wake_state == TASK_RTLOCK_WAIT) {\n\t\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\t\tWARN_ON_ONCE(wqh->rtlock_task);\n\t\tget_task_struct(task);\n\t\twqh->rtlock_task = task;\n\t} else {\n\t\twake_q_add(&wqh->head, task);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_q_add(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\tstruct rt_mutex_waiter *w)\n{\n\trt_mutex_wake_q_add_task(wqh, w->task, w->wake_state);\n}"
  },
  {
    "function_name": "rt_mutex_wake_q_add_task",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "449-461",
    "snippet": "static __always_inline void rt_mutex_wake_q_add_task(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\t     struct task_struct *task,\n\t\t\t\t\t\t     unsigned int wake_state)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wake_state == TASK_RTLOCK_WAIT) {\n\t\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\t\tWARN_ON_ONCE(wqh->rtlock_task);\n\t\tget_task_struct(task);\n\t\twqh->rtlock_task = task;\n\t} else {\n\t\twake_q_add(&wqh->head, task);\n\t}\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "wake_q_add",
          "args": [
            "&wqh->head",
            "task"
          ],
          "line": 459
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_wake_q_add",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "463-467",
          "snippet": "static __always_inline void rt_mutex_wake_q_add(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\tstruct rt_mutex_waiter *w)\n{\n\trt_mutex_wake_q_add_task(wqh, w->task, w->wake_state);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_q_add(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\tstruct rt_mutex_waiter *w)\n{\n\trt_mutex_wake_q_add_task(wqh, w->task, w->wake_state);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "task"
          ],
          "line": 456
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "wqh->rtlock_task"
          ],
          "line": 455
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_PROVE_LOCKING"
          ],
          "line": 454
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_PREEMPT_RT"
          ],
          "line": 453
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_wake_q_add_task(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\t     struct task_struct *task,\n\t\t\t\t\t\t     unsigned int wake_state)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wake_state == TASK_RTLOCK_WAIT) {\n\t\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\t\tWARN_ON_ONCE(wqh->rtlock_task);\n\t\tget_task_struct(task);\n\t\twqh->rtlock_task = task;\n\t} else {\n\t\twake_q_add(&wqh->head, task);\n\t}\n}"
  },
  {
    "function_name": "rt_mutex_adjust_prio",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "436-446",
    "snippet": "static __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_setprio",
          "args": [
            "p",
            "pi_task"
          ],
          "line": 445
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_setprio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/core.c",
          "lines": "6767-6883",
          "snippet": "void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)\n{\n\tint prio, oldprio, queued, running, queue_flag =\n\t\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;\n\tconst struct sched_class *prev_class;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\t/* XXX used to be waiter->prio, not waiter->task->prio */\n\tprio = __rt_effective_prio(pi_task, p->normal_prio);\n\n\t/*\n\t * If nothing changed; bail early.\n\t */\n\tif (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))\n\t\treturn;\n\n\trq = __task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\t/*\n\t * Set under pi_lock && rq->lock, such that the value can be used under\n\t * either lock.\n\t *\n\t * Note that there is loads of tricky to make this pointer cache work\n\t * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to\n\t * ensure a task is de-boosted (pi_task is set to NULL) before the\n\t * task is allowed to run again (and can exit). This ensures the pointer\n\t * points to a blocked task -- which guarantees the task is present.\n\t */\n\tp->pi_top_task = pi_task;\n\n\t/*\n\t * For FIFO/RR we only need to set prio, if that matches we're done.\n\t */\n\tif (prio == p->prio && !dl_prio(prio))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Idle task boosting is a nono in general. There is one\n\t * exception, when PREEMPT_RT and NOHZ is active:\n\t *\n\t * The idle task calls get_next_timer_interrupt() and holds\n\t * the timer wheel base->lock on the CPU and another CPU wants\n\t * to access the timer (probably to cancel it). We can safely\n\t * ignore the boosting request, as the idle CPU runs this code\n\t * with interrupts disabled and will complete the lock\n\t * protected section without being interrupted. So there is no\n\t * real need to boost.\n\t */\n\tif (unlikely(p == rq->idle)) {\n\t\tWARN_ON(p != rq->curr);\n\t\tWARN_ON(p->pi_blocked_on);\n\t\tgoto out_unlock;\n\t}\n\n\ttrace_sched_pi_setprio(p, pi_task);\n\toldprio = p->prio;\n\n\tif (oldprio == prio)\n\t\tqueue_flag &= ~DEQUEUE_MOVE;\n\n\tprev_class = p->sched_class;\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\tif (queued)\n\t\tdequeue_task(rq, p, queue_flag);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\t/*\n\t * Boosting condition are:\n\t * 1. -rt task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A\n\t *\n\t * 2. -dl task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A and could preempt the\n\t *          running task\n\t */\n\tif (dl_prio(prio)) {\n\t\tif (!dl_prio(p->normal_prio) ||\n\t\t    (pi_task && dl_prio(pi_task->prio) &&\n\t\t     dl_entity_preempt(&pi_task->dl, &p->dl))) {\n\t\t\tp->dl.pi_se = pi_task->dl.pi_se;\n\t\t\tqueue_flag |= ENQUEUE_REPLENISH;\n\t\t} else {\n\t\t\tp->dl.pi_se = &p->dl;\n\t\t}\n\t} else if (rt_prio(prio)) {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.pi_se = &p->dl;\n\t\tif (oldprio < prio)\n\t\t\tqueue_flag |= ENQUEUE_HEAD;\n\t} else {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.pi_se = &p->dl;\n\t\tif (rt_prio(oldprio))\n\t\t\tp->rt.timeout = 0;\n\t}\n\n\t__setscheduler_prio(p, prio);\n\n\tif (queued)\n\t\tenqueue_task(rq, p, queue_flag);\n\tif (running)\n\t\tset_next_task(rq, p);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\nout_unlock:\n\t/* Avoid rq from going away on us: */\n\tpreempt_disable();\n\n\trq_unpin_lock(rq, &rf);\n\t__balance_callbacks(rq);\n\traw_spin_rq_unlock(rq);\n\n\tpreempt_enable();\n}",
          "includes": [
            "#include <linux/entry-common.h>",
            "#include \"features.h\"",
            "#include \"smp.h\"",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../../fs/io-wq.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/scs.h>",
            "#include <linux/kcov.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\"",
            "#include <trace/events/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/entry-common.h>\n#include \"features.h\"\n#include \"smp.h\"\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../../fs/io-wq.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/scs.h>\n#include <linux/kcov.h>\n#include <linux/blkdev.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n#include <trace/events/sched.h>\n\nstatic __always_inline struct;\n\nvoid rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)\n{\n\tint prio, oldprio, queued, running, queue_flag =\n\t\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;\n\tconst struct sched_class *prev_class;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\t/* XXX used to be waiter->prio, not waiter->task->prio */\n\tprio = __rt_effective_prio(pi_task, p->normal_prio);\n\n\t/*\n\t * If nothing changed; bail early.\n\t */\n\tif (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))\n\t\treturn;\n\n\trq = __task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\t/*\n\t * Set under pi_lock && rq->lock, such that the value can be used under\n\t * either lock.\n\t *\n\t * Note that there is loads of tricky to make this pointer cache work\n\t * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to\n\t * ensure a task is de-boosted (pi_task is set to NULL) before the\n\t * task is allowed to run again (and can exit). This ensures the pointer\n\t * points to a blocked task -- which guarantees the task is present.\n\t */\n\tp->pi_top_task = pi_task;\n\n\t/*\n\t * For FIFO/RR we only need to set prio, if that matches we're done.\n\t */\n\tif (prio == p->prio && !dl_prio(prio))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Idle task boosting is a nono in general. There is one\n\t * exception, when PREEMPT_RT and NOHZ is active:\n\t *\n\t * The idle task calls get_next_timer_interrupt() and holds\n\t * the timer wheel base->lock on the CPU and another CPU wants\n\t * to access the timer (probably to cancel it). We can safely\n\t * ignore the boosting request, as the idle CPU runs this code\n\t * with interrupts disabled and will complete the lock\n\t * protected section without being interrupted. So there is no\n\t * real need to boost.\n\t */\n\tif (unlikely(p == rq->idle)) {\n\t\tWARN_ON(p != rq->curr);\n\t\tWARN_ON(p->pi_blocked_on);\n\t\tgoto out_unlock;\n\t}\n\n\ttrace_sched_pi_setprio(p, pi_task);\n\toldprio = p->prio;\n\n\tif (oldprio == prio)\n\t\tqueue_flag &= ~DEQUEUE_MOVE;\n\n\tprev_class = p->sched_class;\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\tif (queued)\n\t\tdequeue_task(rq, p, queue_flag);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\t/*\n\t * Boosting condition are:\n\t * 1. -rt task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A\n\t *\n\t * 2. -dl task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A and could preempt the\n\t *          running task\n\t */\n\tif (dl_prio(prio)) {\n\t\tif (!dl_prio(p->normal_prio) ||\n\t\t    (pi_task && dl_prio(pi_task->prio) &&\n\t\t     dl_entity_preempt(&pi_task->dl, &p->dl))) {\n\t\t\tp->dl.pi_se = pi_task->dl.pi_se;\n\t\t\tqueue_flag |= ENQUEUE_REPLENISH;\n\t\t} else {\n\t\t\tp->dl.pi_se = &p->dl;\n\t\t}\n\t} else if (rt_prio(prio)) {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.pi_se = &p->dl;\n\t\tif (oldprio < prio)\n\t\t\tqueue_flag |= ENQUEUE_HEAD;\n\t} else {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.pi_se = &p->dl;\n\t\tif (rt_prio(oldprio))\n\t\t\tp->rt.timeout = 0;\n\t}\n\n\t__setscheduler_prio(p, prio);\n\n\tif (queued)\n\t\tenqueue_task(rq, p, queue_flag);\n\tif (running)\n\t\tset_next_task(rq, p);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\nout_unlock:\n\t/* Avoid rq from going away on us: */\n\tpreempt_disable();\n\n\trq_unpin_lock(rq, &rf);\n\t__balance_callbacks(rq);\n\traw_spin_rq_unlock(rq);\n\n\tpreempt_enable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_top_pi_waiter",
          "args": [
            "p"
          ],
          "line": 443
        },
        "resolved": true,
        "details": {
          "function_name": "task_top_pi_waiter",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "128-132",
          "snippet": "static inline struct rt_mutex_waiter *task_top_pi_waiter(struct task_struct *p)\n{\n\treturn rb_entry(p->pi_waiters.rb_leftmost, struct rt_mutex_waiter,\n\t\t\tpi_tree_entry);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct rt_mutex_waiter *task_top_pi_waiter(struct task_struct *p)\n{\n\treturn rb_entry(p->pi_waiters.rb_leftmost, struct rt_mutex_waiter,\n\t\t\tpi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_has_pi_waiters",
          "args": [
            "p"
          ],
          "line": 442
        },
        "resolved": true,
        "details": {
          "function_name": "task_has_pi_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "123-126",
          "snippet": "static inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn !RB_EMPTY_ROOT(&p->pi_waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn !RB_EMPTY_ROOT(&p->pi_waiters.rb_root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&p->pi_lock"
          ],
          "line": 440
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
  },
  {
    "function_name": "rt_mutex_dequeue_pi",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "426-434",
    "snippet": "static __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "RB_CLEAR_NODE",
          "args": [
            "&waiter->pi_tree_entry"
          ],
          "line": 433
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_erase_cached",
          "args": [
            "&waiter->pi_tree_entry",
            "&task->pi_waiters"
          ],
          "line": 432
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RB_EMPTY_NODE",
          "args": [
            "&waiter->pi_tree_entry"
          ],
          "line": 429
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
  },
  {
    "function_name": "rt_mutex_enqueue_pi",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "420-424",
    "snippet": "static __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_add_cached",
          "args": [
            "&waiter->pi_tree_entry",
            "&task->pi_waiters",
            "__pi_waiter_less"
          ],
          "line": 423
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->pi_tree_entry, &task->pi_waiters, __pi_waiter_less);\n}"
  },
  {
    "function_name": "__pi_waiter_less",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "414-418",
    "snippet": "static __always_inline bool\n__pi_waiter_less(struct rb_node *a, const struct rb_node *b)\n{\n\treturn rt_mutex_waiter_less(__node_2_pi_waiter(a), __node_2_pi_waiter(b));\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_waiter_less",
          "args": [
            "__node_2_pi_waiter(a)",
            "__node_2_pi_waiter(b)"
          ],
          "line": 417
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_less",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "310-326",
          "snippet": "static __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__node_2_pi_waiter",
          "args": [
            "b"
          ],
          "line": 417
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__node_2_pi_waiter",
          "args": [
            "a"
          ],
          "line": 417
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool\n__pi_waiter_less(struct rb_node *a, const struct rb_node *b)\n{\n\treturn rt_mutex_waiter_less(__node_2_pi_waiter(a), __node_2_pi_waiter(b));\n}"
  },
  {
    "function_name": "rt_mutex_dequeue",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "401-409",
    "snippet": "static __always_inline void\nrt_mutex_dequeue(struct rt_mutex_base *lock, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->tree_entry, &lock->waiters);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "RB_CLEAR_NODE",
          "args": [
            "&waiter->tree_entry"
          ],
          "line": 408
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_erase_cached",
          "args": [
            "&waiter->tree_entry",
            "&lock->waiters"
          ],
          "line": 407
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RB_EMPTY_NODE",
          "args": [
            "&waiter->tree_entry"
          ],
          "line": 404
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_dequeue(struct rt_mutex_base *lock, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->tree_entry, &lock->waiters);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n}"
  },
  {
    "function_name": "rt_mutex_enqueue",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "395-399",
    "snippet": "static __always_inline void\nrt_mutex_enqueue(struct rt_mutex_base *lock, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->tree_entry, &lock->waiters, __waiter_less);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_add_cached",
          "args": [
            "&waiter->tree_entry",
            "&lock->waiters",
            "__waiter_less"
          ],
          "line": 398
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_enqueue(struct rt_mutex_base *lock, struct rt_mutex_waiter *waiter)\n{\n\trb_add_cached(&waiter->tree_entry, &lock->waiters, __waiter_less);\n}"
  },
  {
    "function_name": "__waiter_less",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "369-393",
    "snippet": "static __always_inline bool __waiter_less(struct rb_node *a, const struct rb_node *b)\n{\n\tstruct rt_mutex_waiter *aw = __node_2_waiter(a);\n\tstruct rt_mutex_waiter *bw = __node_2_waiter(b);\n\n\tif (rt_mutex_waiter_less(aw, bw))\n\t\treturn 1;\n\n\tif (!build_ww_mutex())\n\t\treturn 0;\n\n\tif (rt_mutex_waiter_less(bw, aw))\n\t\treturn 0;\n\n\t/* NOTE: relies on waiter->ww_ctx being set before insertion */\n\tif (aw->ww_ctx) {\n\t\tif (!bw->ww_ctx)\n\t\t\treturn 1;\n\n\t\treturn (signed long)(aw->ww_ctx->stamp -\n\t\t\t\t     bw->ww_ctx->stamp) < 0;\n\t}\n\n\treturn 0;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_waiter_less",
          "args": [
            "bw",
            "aw"
          ],
          "line": 380
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_less",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "310-326",
          "snippet": "static __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "build_ww_mutex",
          "args": [],
          "line": 377
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__node_2_waiter",
          "args": [
            "b"
          ],
          "line": 372
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__node_2_waiter",
          "args": [
            "a"
          ],
          "line": 371
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool __waiter_less(struct rb_node *a, const struct rb_node *b)\n{\n\tstruct rt_mutex_waiter *aw = __node_2_waiter(a);\n\tstruct rt_mutex_waiter *bw = __node_2_waiter(b);\n\n\tif (rt_mutex_waiter_less(aw, bw))\n\t\treturn 1;\n\n\tif (!build_ww_mutex())\n\t\treturn 0;\n\n\tif (rt_mutex_waiter_less(bw, aw))\n\t\treturn 0;\n\n\t/* NOTE: relies on waiter->ww_ctx being set before insertion */\n\tif (aw->ww_ctx) {\n\t\tif (!bw->ww_ctx)\n\t\t\treturn 1;\n\n\t\treturn (signed long)(aw->ww_ctx->stamp -\n\t\t\t\t     bw->ww_ctx->stamp) < 0;\n\t}\n\n\treturn 0;\n}"
  },
  {
    "function_name": "rt_mutex_steal",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "346-364",
    "snippet": "static inline bool rt_mutex_steal(struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct rt_mutex_waiter *top_waiter)\n{\n\tif (rt_mutex_waiter_less(waiter, top_waiter))\n\t\treturn true;\n\n#ifdef RT_MUTEX_BUILD_SPINLOCKS\n\t/*\n\t * Note that RT tasks are excluded from same priority (lateral)\n\t * steals to prevent the introduction of an unbounded latency.\n\t */\n\tif (rt_prio(waiter->prio) || dl_prio(waiter->prio))\n\t\treturn false;\n\n\treturn rt_mutex_waiter_equal(waiter, top_waiter);\n#else\n\treturn false;\n#endif\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_waiter_equal",
          "args": [
            "waiter",
            "top_waiter"
          ],
          "line": 360
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_equal",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "328-344",
          "snippet": "static __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "dl_prio",
          "args": [
            "waiter->prio"
          ],
          "line": 357
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_prio",
          "args": [
            "waiter->prio"
          ],
          "line": 357
        },
        "resolved": true,
        "details": {
          "function_name": "convert_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
          "lines": "43-66",
          "snippet": "static int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_less",
          "args": [
            "waiter",
            "top_waiter"
          ],
          "line": 349
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_less",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "310-326",
          "snippet": "static __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline bool rt_mutex_steal(struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct rt_mutex_waiter *top_waiter)\n{\n\tif (rt_mutex_waiter_less(waiter, top_waiter))\n\t\treturn true;\n\n#ifdef RT_MUTEX_BUILD_SPINLOCKS\n\t/*\n\t * Note that RT tasks are excluded from same priority (lateral)\n\t * steals to prevent the introduction of an unbounded latency.\n\t */\n\tif (rt_prio(waiter->prio) || dl_prio(waiter->prio))\n\t\treturn false;\n\n\treturn rt_mutex_waiter_equal(waiter, top_waiter);\n#else\n\treturn false;\n#endif\n}"
  },
  {
    "function_name": "rt_mutex_waiter_equal",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "328-344",
    "snippet": "static __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "dl_prio",
          "args": [
            "left->prio"
          ],
          "line": 340
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t\t\t\t\t struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}"
  },
  {
    "function_name": "rt_mutex_waiter_less",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "310-326",
    "snippet": "static __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "dl_time_before",
          "args": [
            "left->deadline",
            "right->deadline"
          ],
          "line": 323
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "dl_prio",
          "args": [
            "left->prio"
          ],
          "line": 322
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t\t\t\t\tstruct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "waiter_update_prio",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "297-302",
    "snippet": "static __always_inline void\nwaiter_update_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\twaiter->prio = __waiter_prio(task);\n\twaiter->deadline = task->dl.deadline;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__waiter_prio",
          "args": [
            "task"
          ],
          "line": 300
        },
        "resolved": true,
        "details": {
          "function_name": "__waiter_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "287-295",
          "snippet": "static __always_inline int __waiter_prio(struct task_struct *task)\n{\n\tint prio = task->prio;\n\n\tif (!rt_prio(prio))\n\t\treturn DEFAULT_PRIO;\n\n\treturn prio;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __waiter_prio(struct task_struct *task)\n{\n\tint prio = task->prio;\n\n\tif (!rt_prio(prio))\n\t\treturn DEFAULT_PRIO;\n\n\treturn prio;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nwaiter_update_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\twaiter->prio = __waiter_prio(task);\n\twaiter->deadline = task->dl.deadline;\n}"
  },
  {
    "function_name": "__waiter_prio",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "287-295",
    "snippet": "static __always_inline int __waiter_prio(struct task_struct *task)\n{\n\tint prio = task->prio;\n\n\tif (!rt_prio(prio))\n\t\treturn DEFAULT_PRIO;\n\n\treturn prio;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_prio",
          "args": [
            "prio"
          ],
          "line": 291
        },
        "resolved": true,
        "details": {
          "function_name": "convert_prio",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/sched/cpupri.c",
          "lines": "43-66",
          "snippet": "static int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int convert_prio(int prio)\n{\n\tint cpupri;\n\n\tswitch (prio) {\n\tcase CPUPRI_INVALID:\n\t\tcpupri = CPUPRI_INVALID;\t/* -1 */\n\t\tbreak;\n\n\tcase 0 ... 98:\n\t\tcpupri = MAX_RT_PRIO-1 - prio;\t/* 1 ... 99 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO-1:\n\t\tcpupri = CPUPRI_NORMAL;\t\t/*  0 */\n\t\tbreak;\n\n\tcase MAX_RT_PRIO:\n\t\tcpupri = CPUPRI_HIGHER;\t\t/* 100 */\n\t\tbreak;\n\t}\n\n\treturn cpupri;\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline int __waiter_prio(struct task_struct *task)\n{\n\tint prio = task->prio;\n\n\tif (!rt_prio(prio))\n\t\treturn DEFAULT_PRIO;\n\n\treturn prio;\n}"
  },
  {
    "function_name": "unlock_rt_mutex_safe",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "277-284",
    "snippet": "static __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 282
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__releases",
          "args": [
            "lock->wait_lock"
          ],
          "line": 279
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}"
  },
  {
    "function_name": "mark_rt_mutex_waiters",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "268-272",
    "snippet": "static __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}"
  },
  {
    "function_name": "rt_mutex_cmpxchg_release",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "261-266",
    "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
  },
  {
    "function_name": "rt_mutex_cmpxchg_acquire",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "253-259",
    "snippet": "static __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}"
  },
  {
    "function_name": "unlock_rt_mutex_safe",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "217-250",
    "snippet": "static __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\n\tclear_rt_mutex_waiters(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\t/*\n\t * If a new waiter comes in between the unlock and the cmpxchg\n\t * we have two situations:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t * cmpxchg(p, owner, 0) == owner\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\t\t\t\t\tacquire(lock);\n\t * or:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\n\t * cmpxchg(p, owner, 0) != owner\n\t *\t\t\t\t\tenqueue_waiter();\n\t *\t\t\t\t\tunlock(wait_lock);\n\t * lock(wait_lock);\n\t * wake waiter();\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tacquire(lock);\n\t */\n\treturn rt_mutex_cmpxchg_release(lock, owner, NULL);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_release",
          "args": [
            "lock",
            "owner",
            "NULL"
          ],
          "line": 249
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cmpxchg_release",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "261-266",
          "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 224
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/spinlock.c",
          "lines": "192-195",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "clear_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 223
        },
        "resolved": true,
        "details": {
          "function_name": "clear_rt_mutex_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
          "lines": "101-105",
          "snippet": "static __always_inline void clear_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"ww_mutex.h\"",
            "#include \"rtmutex_common.h\"",
            "#include <linux/ww_mutex.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void clear_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 221
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "207-210",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex_base *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__releases",
          "args": [
            "lock->wait_lock"
          ],
          "line": 219
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\n\tclear_rt_mutex_waiters(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\t/*\n\t * If a new waiter comes in between the unlock and the cmpxchg\n\t * we have two situations:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t * cmpxchg(p, owner, 0) == owner\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\t\t\t\t\tacquire(lock);\n\t * or:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\n\t * cmpxchg(p, owner, 0) != owner\n\t *\t\t\t\t\tenqueue_waiter();\n\t *\t\t\t\t\tunlock(wait_lock);\n\t * lock(wait_lock);\n\t * wake waiter();\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tacquire(lock);\n\t */\n\treturn rt_mutex_cmpxchg_release(lock, owner, NULL);\n}"
  },
  {
    "function_name": "mark_rt_mutex_waiters",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "201-209",
    "snippet": "static __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tdo {\n\t\towner = *p;\n\t} while (cmpxchg_relaxed(p, owner,\n\t\t\t\t owner | RT_MUTEX_HAS_WAITERS) != owner);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cmpxchg_relaxed",
          "args": [
            "p",
            "owner",
            "owner | RT_MUTEX_HAS_WAITERS"
          ],
          "line": 207
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tdo {\n\t\towner = *p;\n\t} while (cmpxchg_relaxed(p, owner,\n\t\t\t\t owner | RT_MUTEX_HAS_WAITERS) != owner);\n}"
  },
  {
    "function_name": "rt_mutex_cmpxchg_release",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "189-194",
    "snippet": "static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn try_cmpxchg_release(&lock->owner, &old, new);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "try_cmpxchg_release",
          "args": [
            "&lock->owner",
            "&old",
            "new"
          ],
          "line": 193
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn try_cmpxchg_release(&lock->owner, &old, new);\n}"
  },
  {
    "function_name": "rt_mutex_cmpxchg_acquire",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "182-187",
    "snippet": "static __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn try_cmpxchg_acquire(&lock->owner, &old, new);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "try_cmpxchg_acquire",
          "args": [
            "&lock->owner",
            "&old",
            "new"
          ],
          "line": 186
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn try_cmpxchg_acquire(&lock->owner, &old, new);\n}"
  },
  {
    "function_name": "fixup_rt_mutex_waiters",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "107-175",
    "snippet": "static __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "*p",
            "owner & ~RT_MUTEX_HAS_WAITERS"
          ],
          "line": 174
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "*p"
          ],
          "line": 172
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 111
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "93-96",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void fixup_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
  },
  {
    "function_name": "clear_rt_mutex_waiters",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "101-105",
    "snippet": "static __always_inline void clear_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void clear_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}"
  },
  {
    "function_name": "rt_mutex_set_owner",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "90-99",
    "snippet": "static __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "lock->owner",
            "(struct task_struct *)val"
          ],
          "line": 98
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 95
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex_common.h",
          "lines": "93-96",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n#include <linux/debug_locks.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex_base *lock)\n{\n\treturn !RB_EMPTY_ROOT(&lock->waiters.rb_root);\n}"
        }
      }
    ],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tWRITE_ONCE(lock->owner, (struct task_struct *)val);\n}"
  },
  {
    "function_name": "__ww_mutex_check_kill",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "50-55",
    "snippet": "static inline int __ww_mutex_check_kill(struct rt_mutex *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline int __ww_mutex_check_kill(struct rt_mutex *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}"
  },
  {
    "function_name": "ww_mutex_lock_acquired",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "45-48",
    "snippet": "static inline void ww_mutex_lock_acquired(struct ww_mutex *lock,\n\t\t\t\t\t  struct ww_acquire_ctx *ww_ctx)\n{\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline void ww_mutex_lock_acquired(struct ww_mutex *lock,\n\t\t\t\t\t  struct ww_acquire_ctx *ww_ctx)\n{\n}"
  },
  {
    "function_name": "__ww_mutex_check_waiters",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "40-43",
    "snippet": "static inline void __ww_mutex_check_waiters(struct rt_mutex *lock,\n\t\t\t\t\t    struct ww_acquire_ctx *ww_ctx)\n{\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline void __ww_mutex_check_waiters(struct rt_mutex *lock,\n\t\t\t\t\t    struct ww_acquire_ctx *ww_ctx)\n{\n}"
  },
  {
    "function_name": "__ww_mutex_add_waiter",
    "container": null,
    "file": "output_repos_c/CVE-2022-30594/repo/kernel/locking/rtmutex.c",
    "lines": "33-38",
    "snippet": "static inline int __ww_mutex_add_waiter(struct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct rt_mutex *lock,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}",
    "includes": [
      "# include \"ww_mutex.h\"",
      "#include \"rtmutex_common.h\"",
      "#include <linux/ww_mutex.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline struct"
    ],
    "called_functions": [],
    "contextual_snippet": "# include \"ww_mutex.h\"\n#include \"rtmutex_common.h\"\n#include <linux/ww_mutex.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n\nstatic __always_inline struct;\n\nstatic inline int __ww_mutex_add_waiter(struct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct rt_mutex *lock,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}"
  }
]