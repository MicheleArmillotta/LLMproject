[
  {
    "function_name": "rf_DoubleDegSmallWrite",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegwr.c",
    "lines": "657-843",
    "snippet": "void \nrf_DoubleDegSmallWrite(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    char *redundantReadNodeName,\n    char *redundantWriteNodeName,\n    char *recoveryNodeName,\n    int (*recovFunc) (RF_DagNode_t *))\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_DagNode_t *nodes, *wudNodes, *rrdNodes, *recoveryNode, *blockNode,\n\t       *unblockNode, *rpNodes, *rqNodes, *wpNodes, *wqNodes, *termNode;\n\tRF_PhysDiskAddr_t *pda, *pqPDAs;\n\tRF_PhysDiskAddr_t *npdas;\n\tint     nWriteNodes, nNodes, nReadNodes, nRrdNodes, nWudNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tint     nPQNodes;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress, &which_ru);\n\n\t/* simple small write case - First part looks like a reconstruct-read\n\t * of the failed data units. Then a write of all data units not\n\t * failed. */\n\n\n\t/* Hdr | ------Block- /  /         \\   Rrd  Rrd ...  Rrd  Rp Rq \\  \\\n\t * /  -------PQ----- /   \\   \\ Wud   Wp  WQ\t     \\    |   /\n\t * --Unblock- | T\n\t * \n\t * Rrd = read recovery data  (potentially none) Wud = write user data\n\t * (not incl. failed disks) Wp = Write P (could be two) Wq = Write Q\n\t * (could be two)\n\t * \n\t */\n\n\trf_WriteGenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes, &pqPDAs, &nPQNodes, allocList);\n\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\n\tnWudNodes = asmap->numStripeUnitsAccessed - (asmap->numDataFailed);\n\tnReadNodes = nRrdNodes + 2 * nPQNodes;\n\tnWriteNodes = nWudNodes + 2 * nPQNodes;\n\tnNodes = 4 + nReadNodes + nWriteNodes;\n\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\tblockNode = nodes;\n\tunblockNode = blockNode + 1;\n\ttermNode = unblockNode + 1;\n\trecoveryNode = termNode + 1;\n\trrdNodes = recoveryNode + 1;\n\trpNodes = rrdNodes + nRrdNodes;\n\trqNodes = rpNodes + nPQNodes;\n\twudNodes = rqNodes + nPQNodes;\n\twpNodes = wudNodes + nWudNodes;\n\twqNodes = wpNodes + nPQNodes;\n\n\tdag_h->creator = \"PQ_DDSimpleSmallWrite\";\n\tdag_h->numSuccedents = 1;\n\tdag_h->succedents[0] = blockNode;\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n\n\t/* init the block and unblock nodes */\n\t/* The block node has all the read nodes as successors */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\tfor (i = 0; i < nReadNodes; i++)\n\t\tblockNode->succedents[i] = rrdNodes + i;\n\n\t/* The unblock node has all the writes as successors */\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWriteNodes, 0, 0, dag_h, \"Nil\", allocList);\n\tfor (i = 0; i < nWriteNodes; i++) {\n\t\tunblockNode->antecedents[i] = wudNodes + i;\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tunblockNode->succedents[0] = termNode;\n\n#define INIT_READ_NODE(node,name) \\\n  rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, name, allocList); \\\n  (node)->succedents[0] = recoveryNode; \\\n  (node)->antecedents[0] = blockNode; \\\n  (node)->antType[0] = rf_control;\n\n\t/* build the read nodes */\n\tpda = npdas;\n\tfor (i = 0; i < nRrdNodes; i++, pda = pda->next) {\n\t\tINIT_READ_NODE(rrdNodes + i, \"rrd\");\n\t\tDISK_NODE_PARAMS(rrdNodes[i], pda);\n\t}\n\n\t/* read redundancy pdas */\n\tpda = pqPDAs;\n\tINIT_READ_NODE(rpNodes, \"Rp\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rpNodes[0], pda);\n\tpda++;\n\tINIT_READ_NODE(rqNodes, redundantReadNodeName);\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rqNodes[0], pda);\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\tINIT_READ_NODE(rpNodes + 1, \"Rp\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rpNodes[1], pda);\n\t\tpda++;\n\t\tINIT_READ_NODE(rqNodes + 1, redundantReadNodeName);\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rqNodes[1], pda);\n\t}\n\t/* the recovery node has all reads as precedessors and all writes as\n\t * successors. It generates a result for every write P or write Q\n\t * node. As parameters, it takes a pda per read and a pda per stripe\n\t * of user data written. It also takes as the last params the raidPtr\n\t * and asm. For results, it takes PDA for P & Q. */\n\n\n\trf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc, rf_NullNodeUndoFunc, NULL,\n\t    nWriteNodes,\t/* succesors */\n\t    nReadNodes,\t\t/* preds */\n\t    nReadNodes + nWudNodes + 3,\t/* params */\n\t    2 * nPQNodes,\t/* results */\n\t    dag_h, recoveryNodeName, allocList);\n\n\n\n\tfor (i = 0; i < nReadNodes; i++) {\n\t\trecoveryNode->antecedents[i] = rrdNodes + i;\n\t\trecoveryNode->antType[i] = rf_control;\n\t\trecoveryNode->params[i].p = DISK_NODE_PDA(rrdNodes + i);\n\t}\n\tfor (i = 0; i < nWudNodes; i++) {\n\t\trecoveryNode->succedents[i] = wudNodes + i;\n\t}\n\trecoveryNode->params[nReadNodes + nWudNodes].p = asmap->failedPDAs[0];\n\trecoveryNode->params[nReadNodes + nWudNodes + 1].p = raidPtr;\n\trecoveryNode->params[nReadNodes + nWudNodes + 2].p = asmap;\n\n\tfor (; i < nWriteNodes; i++)\n\t\trecoveryNode->succedents[i] = wudNodes + i;\n\n\tpda = pqPDAs;\n\trecoveryNode->results[0] = pda;\n\tpda++;\n\trecoveryNode->results[1] = pda;\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\trecoveryNode->results[2] = pda;\n\t\tpda++;\n\t\trecoveryNode->results[3] = pda;\n\t}\n\t/* fill writes */\n#define INIT_WRITE_NODE(node,name) \\\n  rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, name, allocList); \\\n    (node)->succedents[0] = unblockNode; \\\n    (node)->antecedents[0] = recoveryNode; \\\n    (node)->antType[0] = rf_control;\n\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWudNodes; i++) {\n\t\tINIT_WRITE_NODE(wudNodes + i, \"Wd\");\n\t\tDISK_NODE_PARAMS(wudNodes[i], pda);\n\t\trecoveryNode->params[nReadNodes + i].p = DISK_NODE_PDA(wudNodes + i);\n\t\tpda = pda->next;\n\t}\n\t/* write redundancy pdas */\n\tpda = pqPDAs;\n\tINIT_WRITE_NODE(wpNodes, \"Wp\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(wpNodes[0], pda);\n\tpda++;\n\tINIT_WRITE_NODE(wqNodes, \"Wq\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(wqNodes[0], pda);\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\tINIT_WRITE_NODE(wpNodes + 1, \"Wp\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(wpNodes[1], pda);\n\t\tpda++;\n\t\tINIT_WRITE_NODE(wqNodes + 1, \"Wq\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(wqNodes[1], pda);\n\t}\n}",
    "includes": [
      "#include \"rf_dagdegwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "wqNodes[1]",
            "pda"
          ],
          "line": 841
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 840
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_WRITE_NODE",
          "args": [
            "wqNodes + 1",
            "\"Wq\""
          ],
          "line": 839
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "wpNodes[1]",
            "pda"
          ],
          "line": 837
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 836
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_WRITE_NODE",
          "args": [
            "wpNodes + 1",
            "\"Wp\""
          ],
          "line": 835
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "wqNodes[0]",
            "pda"
          ],
          "line": 832
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 831
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_WRITE_NODE",
          "args": [
            "wqNodes",
            "\"Wq\""
          ],
          "line": 830
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "wpNodes[0]",
            "pda"
          ],
          "line": 828
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 827
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_WRITE_NODE",
          "args": [
            "wpNodes",
            "\"Wp\""
          ],
          "line": 826
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PDA",
          "args": [
            "wudNodes + i"
          ],
          "line": 821
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "wudNodes[i]",
            "pda"
          ],
          "line": 820
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_WRITE_NODE",
          "args": [
            "wudNodes + i",
            "\"Wd\""
          ],
          "line": 819
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PDA",
          "args": [
            "rrdNodes + i"
          ],
          "line": 788
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "recoveryNode",
            "rf_wait",
            "RF_FALSE",
            "recovFunc",
            "rf_NullNodeUndoFunc",
            "NULL",
            "nWriteNodes",
            "/* succesors */nReadNodes",
            "/* preds */nReadNodes + nWudNodes + 3",
            "/* params */2 * nPQNodes",
            "/* results */dag_h",
            "recoveryNodeName",
            "allocList"
          ],
          "line": 776
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rqNodes[1]",
            "pda"
          ],
          "line": 767
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 766
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_READ_NODE",
          "args": [
            "rqNodes + 1",
            "redundantReadNodeName"
          ],
          "line": 765
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rpNodes[1]",
            "pda"
          ],
          "line": 763
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 762
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_READ_NODE",
          "args": [
            "rpNodes + 1",
            "\"Rp\""
          ],
          "line": 761
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rqNodes[0]",
            "pda"
          ],
          "line": 758
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 757
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_READ_NODE",
          "args": [
            "rqNodes",
            "redundantReadNodeName"
          ],
          "line": 756
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rpNodes[0]",
            "pda"
          ],
          "line": 754
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 753
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_READ_NODE",
          "args": [
            "rpNodes",
            "\"Rp\""
          ],
          "line": 752
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rrdNodes[i]",
            "pda"
          ],
          "line": 747
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_READ_NODE",
          "args": [
            "rrdNodes + i",
            "\"rrd\""
          ],
          "line": 746
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 704
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->numDataFailed == 1"
          ],
          "line": 697
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_WriteGenerateFailedAccessASMs",
          "args": [
            "raidPtr",
            "asmap",
            "&npdas",
            "&nRrdNodes",
            "&pqPDAs",
            "&nPQNodes",
            "allocList"
          ],
          "line": 695
        },
        "resolved": true,
        "details": {
          "function_name": "rf_WriteGenerateFailedAccessASMs",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegwr.c",
          "lines": "516-648",
          "snippet": "void \nrf_WriteGenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tunsigned napdas;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - 2);\n\t*nPQNodep = PDAPerDisk;\n\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}",
          "includes": [
            "#include \"rf_dagdegwr.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_dagdegwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_WriteGenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tunsigned napdas;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - 2);\n\t*nPQNodep = PDAPerDisk;\n\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "layoutPtr",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 678
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_DoubleDegSmallWrite(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    char *redundantReadNodeName,\n    char *redundantWriteNodeName,\n    char *recoveryNodeName,\n    int (*recovFunc) (RF_DagNode_t *))\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_DagNode_t *nodes, *wudNodes, *rrdNodes, *recoveryNode, *blockNode,\n\t       *unblockNode, *rpNodes, *rqNodes, *wpNodes, *wqNodes, *termNode;\n\tRF_PhysDiskAddr_t *pda, *pqPDAs;\n\tRF_PhysDiskAddr_t *npdas;\n\tint     nWriteNodes, nNodes, nReadNodes, nRrdNodes, nWudNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tint     nPQNodes;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress, &which_ru);\n\n\t/* simple small write case - First part looks like a reconstruct-read\n\t * of the failed data units. Then a write of all data units not\n\t * failed. */\n\n\n\t/* Hdr | ------Block- /  /         \\   Rrd  Rrd ...  Rrd  Rp Rq \\  \\\n\t * /  -------PQ----- /   \\   \\ Wud   Wp  WQ\t     \\    |   /\n\t * --Unblock- | T\n\t * \n\t * Rrd = read recovery data  (potentially none) Wud = write user data\n\t * (not incl. failed disks) Wp = Write P (could be two) Wq = Write Q\n\t * (could be two)\n\t * \n\t */\n\n\trf_WriteGenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes, &pqPDAs, &nPQNodes, allocList);\n\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\n\tnWudNodes = asmap->numStripeUnitsAccessed - (asmap->numDataFailed);\n\tnReadNodes = nRrdNodes + 2 * nPQNodes;\n\tnWriteNodes = nWudNodes + 2 * nPQNodes;\n\tnNodes = 4 + nReadNodes + nWriteNodes;\n\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\tblockNode = nodes;\n\tunblockNode = blockNode + 1;\n\ttermNode = unblockNode + 1;\n\trecoveryNode = termNode + 1;\n\trrdNodes = recoveryNode + 1;\n\trpNodes = rrdNodes + nRrdNodes;\n\trqNodes = rpNodes + nPQNodes;\n\twudNodes = rqNodes + nPQNodes;\n\twpNodes = wudNodes + nWudNodes;\n\twqNodes = wpNodes + nPQNodes;\n\n\tdag_h->creator = \"PQ_DDSimpleSmallWrite\";\n\tdag_h->numSuccedents = 1;\n\tdag_h->succedents[0] = blockNode;\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n\n\t/* init the block and unblock nodes */\n\t/* The block node has all the read nodes as successors */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\tfor (i = 0; i < nReadNodes; i++)\n\t\tblockNode->succedents[i] = rrdNodes + i;\n\n\t/* The unblock node has all the writes as successors */\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWriteNodes, 0, 0, dag_h, \"Nil\", allocList);\n\tfor (i = 0; i < nWriteNodes; i++) {\n\t\tunblockNode->antecedents[i] = wudNodes + i;\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tunblockNode->succedents[0] = termNode;\n\n#define INIT_READ_NODE(node,name) \\\n  rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, name, allocList); \\\n  (node)->succedents[0] = recoveryNode; \\\n  (node)->antecedents[0] = blockNode; \\\n  (node)->antType[0] = rf_control;\n\n\t/* build the read nodes */\n\tpda = npdas;\n\tfor (i = 0; i < nRrdNodes; i++, pda = pda->next) {\n\t\tINIT_READ_NODE(rrdNodes + i, \"rrd\");\n\t\tDISK_NODE_PARAMS(rrdNodes[i], pda);\n\t}\n\n\t/* read redundancy pdas */\n\tpda = pqPDAs;\n\tINIT_READ_NODE(rpNodes, \"Rp\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rpNodes[0], pda);\n\tpda++;\n\tINIT_READ_NODE(rqNodes, redundantReadNodeName);\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rqNodes[0], pda);\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\tINIT_READ_NODE(rpNodes + 1, \"Rp\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rpNodes[1], pda);\n\t\tpda++;\n\t\tINIT_READ_NODE(rqNodes + 1, redundantReadNodeName);\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rqNodes[1], pda);\n\t}\n\t/* the recovery node has all reads as precedessors and all writes as\n\t * successors. It generates a result for every write P or write Q\n\t * node. As parameters, it takes a pda per read and a pda per stripe\n\t * of user data written. It also takes as the last params the raidPtr\n\t * and asm. For results, it takes PDA for P & Q. */\n\n\n\trf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc, rf_NullNodeUndoFunc, NULL,\n\t    nWriteNodes,\t/* succesors */\n\t    nReadNodes,\t\t/* preds */\n\t    nReadNodes + nWudNodes + 3,\t/* params */\n\t    2 * nPQNodes,\t/* results */\n\t    dag_h, recoveryNodeName, allocList);\n\n\n\n\tfor (i = 0; i < nReadNodes; i++) {\n\t\trecoveryNode->antecedents[i] = rrdNodes + i;\n\t\trecoveryNode->antType[i] = rf_control;\n\t\trecoveryNode->params[i].p = DISK_NODE_PDA(rrdNodes + i);\n\t}\n\tfor (i = 0; i < nWudNodes; i++) {\n\t\trecoveryNode->succedents[i] = wudNodes + i;\n\t}\n\trecoveryNode->params[nReadNodes + nWudNodes].p = asmap->failedPDAs[0];\n\trecoveryNode->params[nReadNodes + nWudNodes + 1].p = raidPtr;\n\trecoveryNode->params[nReadNodes + nWudNodes + 2].p = asmap;\n\n\tfor (; i < nWriteNodes; i++)\n\t\trecoveryNode->succedents[i] = wudNodes + i;\n\n\tpda = pqPDAs;\n\trecoveryNode->results[0] = pda;\n\tpda++;\n\trecoveryNode->results[1] = pda;\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\trecoveryNode->results[2] = pda;\n\t\tpda++;\n\t\trecoveryNode->results[3] = pda;\n\t}\n\t/* fill writes */\n#define INIT_WRITE_NODE(node,name) \\\n  rf_InitNode(node, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, name, allocList); \\\n    (node)->succedents[0] = unblockNode; \\\n    (node)->antecedents[0] = recoveryNode; \\\n    (node)->antType[0] = rf_control;\n\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWudNodes; i++) {\n\t\tINIT_WRITE_NODE(wudNodes + i, \"Wd\");\n\t\tDISK_NODE_PARAMS(wudNodes[i], pda);\n\t\trecoveryNode->params[nReadNodes + i].p = DISK_NODE_PDA(wudNodes + i);\n\t\tpda = pda->next;\n\t}\n\t/* write redundancy pdas */\n\tpda = pqPDAs;\n\tINIT_WRITE_NODE(wpNodes, \"Wp\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(wpNodes[0], pda);\n\tpda++;\n\tINIT_WRITE_NODE(wqNodes, \"Wq\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(wqNodes[0], pda);\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\tINIT_WRITE_NODE(wpNodes + 1, \"Wp\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(wpNodes[1], pda);\n\t\tpda++;\n\t\tINIT_WRITE_NODE(wqNodes + 1, \"Wq\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(wqNodes[1], pda);\n\t}\n}"
  },
  {
    "function_name": "rf_WriteGenerateFailedAccessASMs",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegwr.c",
    "lines": "516-648",
    "snippet": "void \nrf_WriteGenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tunsigned napdas;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - 2);\n\t*nPQNodep = PDAPerDisk;\n\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}",
    "includes": [
      "#include \"rf_dagdegwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda_p - *pdap == napdas"
          ],
          "line": 646
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_PANIC",
          "args": [],
          "line": 641
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 638
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 638
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 634
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 630
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 630
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, secPerSU)",
            "(char *), allocList"
          ],
          "line": 624
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "secPerSU"
          ],
          "line": 624
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 620
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 620
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_DEAD_DISK",
          "args": [
            "raidPtr->Disks[pda_p->row][pda_p->col].status"
          ],
          "line": 613
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 611
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressOfPrevStripeBoundary",
          "args": [
            "layoutPtr",
            "asmap->raidAddress"
          ],
          "line": 605
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "pda_p",
            "napdas",
            "sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 598
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "ftwo_start",
            "ftwo->numSector"
          ],
          "line": 584
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "ftwo_start",
            "ftwo->numSector"
          ],
          "line": 581
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 578
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 575
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "*pqpdap",
            "4 * sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 573
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "0",
            "secPerSU"
          ],
          "line": 567
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "0",
            "secPerSU"
          ],
          "line": 564
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "*pqpdap",
            "2 * sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 562
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_StripeUnitOffset",
          "args": [
            "layoutPtr",
            "ftwo->startSector"
          ],
          "line": 557
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 554
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 550
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "*pqpdap",
            "2 * sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 547
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_StripeUnitOffset",
          "args": [
            "layoutPtr",
            "fone->startSector"
          ],
          "line": 541
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_WriteGenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tunsigned napdas;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - 2);\n\t*nPQNodep = PDAPerDisk;\n\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}"
  },
  {
    "function_name": "rf_CommonCreateSimpleDegradedWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegwr.c",
    "lines": "151-508",
    "snippet": "void \nrf_CommonCreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags,\n    allocList, nfaults, redFunc, allowBufferRecycle)\n\tRF_Raid_t *raidPtr;\n\tRF_AccessStripeMap_t *asmap;\n\tRF_DagHeader_t *dag_h;\n\tvoid   *bp;\n\tRF_RaidAccessFlags_t flags;\n\tRF_AllocListElem_t *allocList;\n\tint     nfaults;\n\tint     (*redFunc) (RF_DagNode_t *);\n\tint     allowBufferRecycle;\n{\n\tint     nNodes, nRrdNodes, nWndNodes, nXorBufs, i, j, paramNum,\n\t        rdnodesFaked;\n\tRF_DagNode_t *blockNode, *unblockNode, *wnpNode, *wnqNode, *termNode;\n\tRF_DagNode_t *nodes, *wndNodes, *rrdNodes, *xorNode, *commitNode;\n\tRF_SectorCount_t sectorsPerSU;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *xorTargetBuf = NULL;\t/* the target buffer for the XOR\n\t\t\t\t\t * operation */\n\tchar   *overlappingPDAs;/* a temporary array of flags */\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_PhysDiskAddr_t *pda, *parityPDA;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tRF_RaidLayout_t *layoutPtr;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,\n\t    &which_ru);\n\tsectorsPerSU = layoutPtr->sectorsPerStripeUnit;\n\t/* failedPDA points to the pda within the asm that targets the failed\n\t * disk */\n\tfailedPDA = asmap->failedPDAs[0];\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating degraded-write DAG]\\n\");\n\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"SimpleDegradedWriteDAG\";\n\n\t/*\n         * Generate two ASMs identifying the surviving data\n         * we need in order to recover the lost data.\n         */\n\t/* overlappingPDAs array must be zero'd */\n\tRF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));\n\trf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h,\n\t    &nXorBufs, NULL, overlappingPDAs, allocList);\n\n\t/* create all the nodes at once */\n\tnWndNodes = asmap->numStripeUnitsAccessed - 1;\t/* no access is\n\t\t\t\t\t\t\t * generated for the\n\t\t\t\t\t\t\t * failed pda */\n\n\tnRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +\n\t    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);\n\t/*\n         * XXX\n         *\n         * There's a bug with a complete stripe overwrite- that means 0 reads\n         * of old data, and the rest of the DAG generation code doesn't like\n         * that. A release is coming, and I don't wanna risk breaking a critical\n         * DAG generator, so here's what I'm gonna do- if there's no read nodes,\n         * I'm gonna fake there being a read node, and I'm gonna swap in a\n         * no-op node in its place (to make all the link-up code happy).\n         * This should be fixed at some point.  --jimz\n         */\n\tif (nRrdNodes == 0) {\n\t\tnRrdNodes = 1;\n\t\trdnodesFaked = 1;\n\t} else {\n\t\trdnodesFaked = 0;\n\t}\n\t/* lock, unlock, xor, Wnd, Rrd, W(nfaults) */\n\tnNodes = 5 + nfaults + nWndNodes + nRrdNodes;\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\tRF_ASSERT(i == nNodes);\n\n\t/* this dag can not commit until all rrd and xor Nodes have completed */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\tRF_ASSERT(nRrdNodes > 0);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nRrdNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nWndNodes + nfaults, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, nWndNodes + nfaults, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,\n\t    nRrdNodes, 2 * nXorBufs + 2, nfaults, dag_h, \"Xrc\", allocList);\n\n\t/*\n         * Fill in the Rrd nodes. If any of the rrd buffers are the same size as\n         * the failed buffer, save a pointer to it so we can use it as the target\n         * of the XOR. The pdas in the rrd nodes have been range-restricted, so if\n         * a buffer is the same size as the failed buffer, it must also be at the\n         * same alignment within the SU.\n         */\n\ti = 0;\n\tif (new_asm_h[0]) {\n\t\tfor (i = 0, pda = new_asm_h[0]->stripeMap->physInfo;\n\t\t    i < new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t\t    i++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i].params[0].p = pda;\n\t\t\trrdNodes[i].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i].params[2].v = parityStripeID;\n\t\t\trrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* i now equals the number of stripe units accessed in new_asm_h[0] */\n\tif (new_asm_h[1]) {\n\t\tfor (j = 0, pda = new_asm_h[1]->stripeMap->physInfo;\n\t\t    j < new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t\t    j++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i + j].params[0].p = pda;\n\t\t\trrdNodes[i + j].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i + j].params[2].v = parityStripeID;\n\t\t\trrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tif (allowBufferRecycle && (pda->numSector == failedPDA->numSector))\n\t\t\t\txorTargetBuf = pda->bufPtr;\n\t\t}\n\t}\n\tif (rdnodesFaked) {\n\t\t/*\n\t         * This is where we'll init that fake noop read node\n\t         * (XXX should the wakeup func be different?)\n\t         */\n\t\trf_InitNode(&rrdNodes[0], rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 1, 0, 0, dag_h, \"RrN\", allocList);\n\t}\n\t/*\n         * Make a PDA for the parity unit.  The parity PDA should start at\n         * the same offset into the SU as the failed PDA.\n         */\n\t/* Danner comment: I don't think this copy is really necessary. We are\n\t * in one of two cases here. (1) The entire failed unit is written.\n\t * Then asmap->parityInfo will describe the entire parity. (2) We are\n\t * only writing a subset of the failed unit and nothing else. Then the\n\t * asmap->parityInfo describes the failed unit and the copy can also\n\t * be avoided. */\n\n\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\tparityPDA->row = asmap->parityInfo->row;\n\tparityPDA->col = asmap->parityInfo->col;\n\tparityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)\n\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\tparityPDA->numSector = failedPDA->numSector;\n\n\tif (!xorTargetBuf) {\n\t\tRF_CallocAndAdd(xorTargetBuf, 1,\n\t\t    rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList);\n\t}\n\t/* init the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = parityPDA;\n\twnpNode->params[1].p = xorTargetBuf;\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t/* fill in the Wnq Node */\n\tif (nfaults == 2) {\n\t\t{\n\t\t\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t),\n\t\t\t    (RF_PhysDiskAddr_t *), allocList);\n\t\t\tparityPDA->row = asmap->qInfo->row;\n\t\t\tparityPDA->col = asmap->qInfo->col;\n\t\t\tparityPDA->startSector = ((asmap->qInfo->startSector / sectorsPerSU)\n\t\t\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\t\t\tparityPDA->numSector = failedPDA->numSector;\n\n\t\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\t\twnqNode->params[0].p = parityPDA;\n\t\t\tRF_CallocAndAdd(xorNode->results[1], 1,\n\t\t\t    rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList);\n\t\t\twnqNode->params[1].p = xorNode->results[1];\n\t\t\twnqNode->params[2].v = parityStripeID;\n\t\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* fill in the Wnd nodes */\n\tfor (pda = asmap->physInfo, i = 0; i < nWndNodes; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* fill in the results of the xor node */\n\txorNode->results[0] = xorTargetBuf;\n\n\t/* fill in the params of the xor node */\n\n\tparamNum = 0;\n\tif (rdnodesFaked == 0) {\n\t\tfor (i = 0; i < nRrdNodes; i++) {\n\t\t\t/* all the Rrd nodes need to be xored together */\n\t\t\txorNode->params[paramNum++] = rrdNodes[i].params[0];\n\t\t\txorNode->params[paramNum++] = rrdNodes[i].params[1];\n\t\t}\n\t}\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\t/* any Wnd nodes that overlap the failed access need to be\n\t\t * xored in */\n\t\tif (overlappingPDAs[i]) {\n\t\t\tRF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tbcopy((char *) wndNodes[i].params[0].p, (char *) pda, sizeof(RF_PhysDiskAddr_t));\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);\n\t\t\txorNode->params[paramNum++].p = pda;\n\t\t\txorNode->params[paramNum++].p = pda->bufPtr;\n\t\t}\n\t}\n\tRF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));\n\n\t/*\n         * Install the failed PDA into the xor param list so that the\n         * new data gets xor'd in.\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = failedPDA->bufPtr;\n\n\t/*\n         * The last 2 params to the recovery xor node are always the failed\n         * PDA and the raidPtr. install the failedPDA even though we have just\n         * done so above. This allows us to use the same XOR function for both\n         * degraded reads and degraded writes.\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = raidPtr;\n\tRF_ASSERT(paramNum == 2 * nXorBufs + 2);\n\n\t/*\n         * Code to link nodes begins here\n         */\n\n\t/* link header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* link block node to rd nodes */\n\tRF_ASSERT(blockNode->numSuccedents == nRrdNodes);\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &rrdNodes[i];\n\t\trrdNodes[i].antecedents[0] = blockNode;\n\t\trrdNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* link read nodes to xor node */\n\tRF_ASSERT(xorNode->numAntecedents == nRrdNodes);\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\trrdNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[i] = &rrdNodes[i];\n\t\txorNode->antType[i] = rf_trueData;\n\t}\n\n\t/* link xor node to commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* link commit node to wnd nodes */\n\tRF_ASSERT(commitNode->numSuccedents == nfaults + nWndNodes);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = commitNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* link the commit node to wnp, wnq nodes */\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\tcommitNode->succedents[nWndNodes] = wnpNode;\n\twnpNode->antecedents[0] = commitNode;\n\twnpNode->antType[0] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\tcommitNode->succedents[nWndNodes + 1] = wnqNode;\n\t\twnqNode->antecedents[0] = commitNode;\n\t\twnqNode->antType[0] = rf_control;\n\t}\n\t/* link write new data nodes to unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nfaults));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes[i].numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNodes[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\n\t/* link write new parity node to unblock node */\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = unblockNode;\n\tunblockNode->antecedents[nWndNodes] = wnpNode;\n\tunblockNode->antType[nWndNodes] = rf_control;\n\n\t/* link write new q node to unblock node */\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\tunblockNode->antType[nWndNodes + 1] = rf_control;\n\t}\n\t/* link unblock node to term node */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}",
    "includes": [
      "#include \"rf_dagdegwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 504
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 503
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numSuccedents == 1"
          ],
          "line": 502
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnqNode->numSuccedents == 1"
          ],
          "line": 496
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnpNode->numSuccedents == 1"
          ],
          "line": 489
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes[i].numSuccedents == 1"
          ],
          "line": 482
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numAntecedents == (nWndNodes + nfaults)"
          ],
          "line": 480
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnqNode->numAntecedents == 1"
          ],
          "line": 474
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnpNode->numAntecedents == 1"
          ],
          "line": 469
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes[i].numAntecedents == 1"
          ],
          "line": 462
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numSuccedents == nfaults + nWndNodes"
          ],
          "line": 460
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numAntecedents == 1"
          ],
          "line": 454
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numSuccedents == 1"
          ],
          "line": 453
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rrdNodes[i].numSuccedents == 1"
          ],
          "line": 446
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numAntecedents == nRrdNodes"
          ],
          "line": 444
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rrdNodes[i].numAntecedents == 1"
          ],
          "line": 437
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == nRrdNodes"
          ],
          "line": 435
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 431
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "paramNum == 2 * nXorBufs + 2"
          ],
          "line": 424
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_Free",
          "args": [
            "overlappingPDAs",
            "asmap->numStripeUnitsAccessed * sizeof(char)"
          ],
          "line": 407
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RangeRestrictPDA",
          "args": [
            "raidPtr",
            "failedPDA",
            "pda",
            "RF_RESTRICT_DOBUFFER",
            "0"
          ],
          "line": 402
        },
        "resolved": true,
        "details": {
          "function_name": "rf_RangeRestrictPDA",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "1050-1075",
          "snippet": "void \nrf_RangeRestrictPDA(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * src,\n    RF_PhysDiskAddr_t * dest,\n    int dobuffer,\n    int doraidaddr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_SectorNum_t soffs = rf_StripeUnitOffset(layoutPtr, src->startSector);\n\tRF_SectorNum_t doffs = rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\tRF_SectorNum_t send = rf_StripeUnitOffset(layoutPtr, src->startSector + src->numSector - 1);\t/* use -1 to be sure we\n\t\t\t\t\t\t\t\t\t\t\t\t\t * stay within SU */\n\tRF_SectorNum_t dend = rf_StripeUnitOffset(layoutPtr, dest->startSector + dest->numSector - 1);\n\tRF_SectorNum_t subAddr = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->startSector);\t/* stripe unit boundary */\n\n\tdest->startSector = subAddr + RF_MAX(soffs, doffs);\n\tdest->numSector = subAddr + RF_MIN(send, dend) + 1 - dest->startSector;\n\n\tif (dobuffer)\n\t\tdest->bufPtr += (soffs > doffs) ? rf_RaidAddressToByte(raidPtr, soffs - doffs) : 0;\n\tif (doraidaddr) {\n\t\tdest->raidAddress = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->raidAddress) +\n\t\t    rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_RangeRestrictPDA(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * src,\n    RF_PhysDiskAddr_t * dest,\n    int dobuffer,\n    int doraidaddr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_SectorNum_t soffs = rf_StripeUnitOffset(layoutPtr, src->startSector);\n\tRF_SectorNum_t doffs = rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\tRF_SectorNum_t send = rf_StripeUnitOffset(layoutPtr, src->startSector + src->numSector - 1);\t/* use -1 to be sure we\n\t\t\t\t\t\t\t\t\t\t\t\t\t * stay within SU */\n\tRF_SectorNum_t dend = rf_StripeUnitOffset(layoutPtr, dest->startSector + dest->numSector - 1);\n\tRF_SectorNum_t subAddr = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->startSector);\t/* stripe unit boundary */\n\n\tdest->startSector = subAddr + RF_MAX(soffs, doffs);\n\tdest->numSector = subAddr + RF_MIN(send, dend) + 1 - dest->startSector;\n\n\tif (dobuffer)\n\t\tdest->bufPtr += (soffs > doffs) ? rf_RaidAddressToByte(raidPtr, soffs - doffs) : 0;\n\tif (doraidaddr) {\n\t\tdest->raidAddress = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->raidAddress) +\n\t\t    rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "bcopy",
          "args": [
            "(char *) wndNodes[i].params[0].p",
            "(char *) pda",
            "sizeof(RF_PhysDiskAddr_t)"
          ],
          "line": 401
        },
        "resolved": true,
        "details": {
          "function_name": "tr_bcopy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/ic/tropic.c",
          "lines": "1618-1663",
          "snippet": "void \ntr_bcopy(sc, dest, len)\nstruct tr_softc *sc;\t/* pointer to softc struct for this adapter */\nu_char *dest;\t\t/* destination address */\nint len;\t\t/* number of bytes to copy */\n{\n\tstruct rbcb *rbc = &sc->rbc;\t/* pointer to rec buf ctl blk */\n\n\t/* While amount of data needed >= amount in current receive buffer. */\n\twhile (len >= rbc->data_len) {\n\t\t/* Copy all data from receive buffer to destination. */\n\n\t\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t\t    rbc->rbuf_datap, dest, (bus_size_t)rbc->data_len);\n\t\tlen -= rbc->data_len;\t/* update length left to transfer */\n\t\tdest += rbc->data_len;\t/* update destination address */\n\n\t\t/* Make next receive buffer current receive buffer. */\n\t\trbc->rbufp = rbc->rbufp_next;\n\t\tif (rbc->rbufp != 0) { /* More receive buffers? */\n\n\t\t\t/* Calculate pointer to next receive buffer. */\n\t\t\trbc->rbufp_next = RB_INW(sc, rbc->rbufp, RB_NEXTBUF);\n\t\t\tif (rbc->rbufp_next != 0)\n\t\t\t\trbc->rbufp_next -= RB_NEXTBUF;\n\n\t\t\t/* Get pointer to data in current receive buffer. */\n\t\t\trbc->rbuf_datap = rbc->rbufp + RB_DATA;\n\n\t\t\t/* Get length of data in current receive buffer. */\n\t\t\trbc->data_len = RB_INW(sc, rbc->rbufp, RB_BUFLEN);\n\t\t}\n\t\telse {\n\t\t\tif (len != 0)\t/* len should equal zero. */\n\t\t\t\tprintf(\"tr_bcopy: residual data not copied\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Amount of data needed is < amount in current receive buffer. */\n\n\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t    rbc->rbuf_datap, dest, (bus_size_t)len);\n\trbc->data_len -= len;\t/* Update count of data in receive buffer. */\n\trbc->rbuf_datap += len;\t/* Update pointer to receive buffer data. */\n}",
          "includes": [
            "#include <dev/ic/tropicvar.h>",
            "#include <dev/ic/tropicreg.h>",
            "#include <machine/bus.h>",
            "#include <machine/cpu.h>",
            "#include <net/bpfdesc.h>",
            "#include <net/bpf.h>",
            "#include <netns/ns_if.h>",
            "#include <netns/ns.h>",
            "#include <net/if_token.h>",
            "#include <netinet/in_var.h>",
            "#include <netinet/ip.h>",
            "#include <netinet/if_ether.h>",
            "#include <netinet/in_systm.h>",
            "#include <netinet/in.h>",
            "#include <net/route.h>",
            "#include <net/netisr.h>",
            "#include <net/if_media.h>",
            "#include <net/if_llc.h>",
            "#include <net/if.h>",
            "#include <sys/device.h>",
            "#include <sys/errno.h>",
            "#include <sys/ioctl.h>",
            "#include <sys/syslog.h>",
            "#include <sys/socket.h>",
            "#include <sys/buf.h>",
            "#include <sys/mbuf.h>",
            "#include <sys/proc.h>",
            "#include <sys/kernel.h>",
            "#include <sys/systm.h>",
            "#include <sys/param.h>",
            "#include \"bpfilter.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "void\ttr_bcopy",
            "struct mbuf *\ntr_get(sc, totlen, ifp)\nstruct tr_softc *sc;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <dev/ic/tropicvar.h>\n#include <dev/ic/tropicreg.h>\n#include <machine/bus.h>\n#include <machine/cpu.h>\n#include <net/bpfdesc.h>\n#include <net/bpf.h>\n#include <netns/ns_if.h>\n#include <netns/ns.h>\n#include <net/if_token.h>\n#include <netinet/in_var.h>\n#include <netinet/ip.h>\n#include <netinet/if_ether.h>\n#include <netinet/in_systm.h>\n#include <netinet/in.h>\n#include <net/route.h>\n#include <net/netisr.h>\n#include <net/if_media.h>\n#include <net/if_llc.h>\n#include <net/if.h>\n#include <sys/device.h>\n#include <sys/errno.h>\n#include <sys/ioctl.h>\n#include <sys/syslog.h>\n#include <sys/socket.h>\n#include <sys/buf.h>\n#include <sys/mbuf.h>\n#include <sys/proc.h>\n#include <sys/kernel.h>\n#include <sys/systm.h>\n#include <sys/param.h>\n#include \"bpfilter.h\"\n\nvoid\ttr_bcopy;\nstruct mbuf *\ntr_get(sc, totlen, ifp)\nstruct tr_softc *sc;\n\nvoid \ntr_bcopy(sc, dest, len)\nstruct tr_softc *sc;\t/* pointer to softc struct for this adapter */\nu_char *dest;\t\t/* destination address */\nint len;\t\t/* number of bytes to copy */\n{\n\tstruct rbcb *rbc = &sc->rbc;\t/* pointer to rec buf ctl blk */\n\n\t/* While amount of data needed >= amount in current receive buffer. */\n\twhile (len >= rbc->data_len) {\n\t\t/* Copy all data from receive buffer to destination. */\n\n\t\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t\t    rbc->rbuf_datap, dest, (bus_size_t)rbc->data_len);\n\t\tlen -= rbc->data_len;\t/* update length left to transfer */\n\t\tdest += rbc->data_len;\t/* update destination address */\n\n\t\t/* Make next receive buffer current receive buffer. */\n\t\trbc->rbufp = rbc->rbufp_next;\n\t\tif (rbc->rbufp != 0) { /* More receive buffers? */\n\n\t\t\t/* Calculate pointer to next receive buffer. */\n\t\t\trbc->rbufp_next = RB_INW(sc, rbc->rbufp, RB_NEXTBUF);\n\t\t\tif (rbc->rbufp_next != 0)\n\t\t\t\trbc->rbufp_next -= RB_NEXTBUF;\n\n\t\t\t/* Get pointer to data in current receive buffer. */\n\t\t\trbc->rbuf_datap = rbc->rbufp + RB_DATA;\n\n\t\t\t/* Get length of data in current receive buffer. */\n\t\t\trbc->data_len = RB_INW(sc, rbc->rbufp, RB_BUFLEN);\n\t\t}\n\t\telse {\n\t\t\tif (len != 0)\t/* len should equal zero. */\n\t\t\t\tprintf(\"tr_bcopy: residual data not copied\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Amount of data needed is < amount in current receive buffer. */\n\n\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t    rbc->rbuf_datap, dest, (bus_size_t)len);\n\trbc->data_len -= len;\t/* Update count of data in receive buffer. */\n\trbc->rbuf_datap += len;\t/* Update pointer to receive buffer data. */\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda",
            "sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 400
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 380
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 376
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "&wndNodes[i]",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskWriteFunc",
            "rf_DiskWriteUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Wnd\"",
            "allocList"
          ],
          "line": 374
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 365
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "xorNode->results[1]",
            "1",
            "rf_RaidAddressToByte(raidPtr, failedPDA->numSector)",
            "(char *), allocList"
          ],
          "line": 361
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "failedPDA->numSector"
          ],
          "line": 362
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "parityPDA",
            "sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 350
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 345
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "xorTargetBuf",
            "1",
            "rf_RaidAddressToByte(raidPtr, failedPDA->numSector)",
            "(char *), allocList"
          ],
          "line": 336
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "failedPDA->numSector"
          ],
          "line": 337
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "parityPDA",
            "sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 328
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 304
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 300
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 290
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 286
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "nRrdNodes > 0"
          ],
          "line": 260
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == nNodes"
          ],
          "line": 253
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 228
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_GenerateFailedAccessASMs",
          "args": [
            "raidPtr",
            "asmap",
            "failedPDA",
            "dag_h",
            "new_asm_h",
            "&nXorBufs",
            "NULL",
            "overlappingPDAs",
            "allocList"
          ],
          "line": 199
        },
        "resolved": true,
        "details": {
          "function_name": "rf_GenerateFailedAccessASMs",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "899-1025",
          "snippet": "void \nrf_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t * failedPDA,\n    RF_DagHeader_t * dag_h,\n    RF_AccessStripeMapHeader_t ** new_asm_h,\n    int *nXorBufs,\n    char **rpBufPtr,\n    char *overlappingPDAs,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\n\t/* s=start, e=end, s=stripe, a=access, f=failed, su=stripe unit */\n\tRF_RaidAddr_t sosAddr, sosEndAddr, eosStartAddr, eosAddr;\n\n\tRF_SectorCount_t numSect[2], numParitySect;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *rdBuf, *bufP;\n\tint     foundit, i;\n\n\tbufP = NULL;\n\tfoundit = 0;\n\t/* first compute the following raid addresses: start of stripe,\n\t * (sosAddr) MIN(start of access, start of failed SU),   (sosEndAddr)\n\t * MAX(end of access, end of failed SU),       (eosStartAddr) end of\n\t * stripe (i.e. start of next stripe)   (eosAddr) */\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tsosEndAddr = RF_MIN(asmap->raidAddress, rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosStartAddr = RF_MAX(asmap->endRaidAddress, rf_RaidAddressOfNextStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosAddr = rf_RaidAddressOfNextStripeBoundary(layoutPtr, asmap->raidAddress);\n\n\t/* now generate access stripe maps for each of the above regions of\n\t * the stripe.  Use a dummy (NULL) buf ptr for now */\n\n\tnew_asm_h[0] = (sosAddr != sosEndAddr) ? rf_MapAccess(raidPtr, sosAddr, sosEndAddr - sosAddr, NULL, RF_DONT_REMAP) : NULL;\n\tnew_asm_h[1] = (eosStartAddr != eosAddr) ? rf_MapAccess(raidPtr, eosStartAddr, eosAddr - eosStartAddr, NULL, RF_DONT_REMAP) : NULL;\n\n\t/* walk through the PDAs and range-restrict each SU to the region of\n\t * the SU touched on the failed PDA.  also compute total data buffer\n\t * space requirements in this step.  Ignore the parity for now. */\n\n\tnumSect[0] = numSect[1] = 0;\n\tif (new_asm_h[0]) {\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[0] += pda->numSector;\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[1] += pda->numSector;\n\t\t}\n\t}\n\tnumParitySect = failedPDA->numSector;\n\n\t/* allocate buffer space for the data & parity we have to read to\n\t * recover from the failure */\n\n\tif (numSect[0] + numSect[1] + ((rpBufPtr) ? numParitySect : 0)) {\t/* don't allocate parity\n\t\t\t\t\t\t\t\t\t\t * buf if not needed */\n\t\tRF_MallocAndAdd(rdBuf, rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (char *), allocList);\n\t\tbufP = rdBuf;\n\t\tif (rf_degDagDebug)\n\t\t\tprintf(\"Newly allocated buffer (%d bytes) is 0x%lx\\n\",\n\t\t\t    (int) rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (unsigned long) bufP);\n\t}\n\t/* now walk through the pdas one last time and assign buffer pointers\n\t * (ugh!).  Again, ignore the parity.  also, count nodes to find out\n\t * how many bufs need to be xored together */\n\t(*nXorBufs) = 1;\t/* in read case, 1 is for parity.  In write\n\t\t\t\t * case, 1 is for failed data */\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t*nXorBufs += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t(*nXorBufs) += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (rpBufPtr)\n\t\t*rpBufPtr = bufP;\t/* the rest of the buffer is for\n\t\t\t\t\t * parity */\n\n\t/* the last step is to figure out how many more distinct buffers need\n\t * to get xor'd to produce the missing unit.  there's one for each\n\t * user-data read node that overlaps the portion of the failed unit\n\t * being accessed */\n\n\tfor (foundit = i = 0, pda = asmap->physInfo; pda; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tfoundit = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (rf_PDAOverlap(layoutPtr, pda, failedPDA)) {\n\t\t\toverlappingPDAs[i] = 1;\n\t\t\t(*nXorBufs)++;\n\t\t}\n\t}\n\tif (!foundit) {\n\t\tRF_ERRORMSG(\"GenerateFailedAccessASMs: did not find failedPDA in asm list\\n\");\n\t\tRF_ASSERT(0);\n\t}\n\tif (rf_degDagDebug) {\n\t\tif (new_asm_h[0]) {\n\t\t\tprintf(\"First asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[0], 1);\n\t\t}\n\t\tif (new_asm_h[1]) {\n\t\t\tprintf(\"Second asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[1], 1);\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t * failedPDA,\n    RF_DagHeader_t * dag_h,\n    RF_AccessStripeMapHeader_t ** new_asm_h,\n    int *nXorBufs,\n    char **rpBufPtr,\n    char *overlappingPDAs,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\n\t/* s=start, e=end, s=stripe, a=access, f=failed, su=stripe unit */\n\tRF_RaidAddr_t sosAddr, sosEndAddr, eosStartAddr, eosAddr;\n\n\tRF_SectorCount_t numSect[2], numParitySect;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *rdBuf, *bufP;\n\tint     foundit, i;\n\n\tbufP = NULL;\n\tfoundit = 0;\n\t/* first compute the following raid addresses: start of stripe,\n\t * (sosAddr) MIN(start of access, start of failed SU),   (sosEndAddr)\n\t * MAX(end of access, end of failed SU),       (eosStartAddr) end of\n\t * stripe (i.e. start of next stripe)   (eosAddr) */\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tsosEndAddr = RF_MIN(asmap->raidAddress, rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosStartAddr = RF_MAX(asmap->endRaidAddress, rf_RaidAddressOfNextStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosAddr = rf_RaidAddressOfNextStripeBoundary(layoutPtr, asmap->raidAddress);\n\n\t/* now generate access stripe maps for each of the above regions of\n\t * the stripe.  Use a dummy (NULL) buf ptr for now */\n\n\tnew_asm_h[0] = (sosAddr != sosEndAddr) ? rf_MapAccess(raidPtr, sosAddr, sosEndAddr - sosAddr, NULL, RF_DONT_REMAP) : NULL;\n\tnew_asm_h[1] = (eosStartAddr != eosAddr) ? rf_MapAccess(raidPtr, eosStartAddr, eosAddr - eosStartAddr, NULL, RF_DONT_REMAP) : NULL;\n\n\t/* walk through the PDAs and range-restrict each SU to the region of\n\t * the SU touched on the failed PDA.  also compute total data buffer\n\t * space requirements in this step.  Ignore the parity for now. */\n\n\tnumSect[0] = numSect[1] = 0;\n\tif (new_asm_h[0]) {\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[0] += pda->numSector;\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[1] += pda->numSector;\n\t\t}\n\t}\n\tnumParitySect = failedPDA->numSector;\n\n\t/* allocate buffer space for the data & parity we have to read to\n\t * recover from the failure */\n\n\tif (numSect[0] + numSect[1] + ((rpBufPtr) ? numParitySect : 0)) {\t/* don't allocate parity\n\t\t\t\t\t\t\t\t\t\t * buf if not needed */\n\t\tRF_MallocAndAdd(rdBuf, rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (char *), allocList);\n\t\tbufP = rdBuf;\n\t\tif (rf_degDagDebug)\n\t\t\tprintf(\"Newly allocated buffer (%d bytes) is 0x%lx\\n\",\n\t\t\t    (int) rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (unsigned long) bufP);\n\t}\n\t/* now walk through the pdas one last time and assign buffer pointers\n\t * (ugh!).  Again, ignore the parity.  also, count nodes to find out\n\t * how many bufs need to be xored together */\n\t(*nXorBufs) = 1;\t/* in read case, 1 is for parity.  In write\n\t\t\t\t * case, 1 is for failed data */\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t*nXorBufs += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t(*nXorBufs) += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (rpBufPtr)\n\t\t*rpBufPtr = bufP;\t/* the rest of the buffer is for\n\t\t\t\t\t * parity */\n\n\t/* the last step is to figure out how many more distinct buffers need\n\t * to get xor'd to produce the missing unit.  there's one for each\n\t * user-data read node that overlaps the portion of the failed unit\n\t * being accessed */\n\n\tfor (foundit = i = 0, pda = asmap->physInfo; pda; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tfoundit = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (rf_PDAOverlap(layoutPtr, pda, failedPDA)) {\n\t\t\toverlappingPDAs[i] = 1;\n\t\t\t(*nXorBufs)++;\n\t\t}\n\t}\n\tif (!foundit) {\n\t\tRF_ERRORMSG(\"GenerateFailedAccessASMs: did not find failedPDA in asm list\\n\");\n\t\tRF_ASSERT(0);\n\t}\n\tif (rf_degDagDebug) {\n\t\tif (new_asm_h[0]) {\n\t\t\tprintf(\"First asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[0], 1);\n\t\t}\n\t\tif (new_asm_h[1]) {\n\t\t\tprintf(\"Second asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[1], 1);\n\t\t}\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_Calloc",
          "args": [
            "overlappingPDAs",
            "asmap->numStripeUnitsAccessed",
            "sizeof(char)",
            "(char *)"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->numDataFailed == 1"
          ],
          "line": 190
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating degraded-write DAG]\\n\""
          ],
          "line": 188
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "layoutPtr",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 180
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CommonCreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags,\n    allocList, nfaults, redFunc, allowBufferRecycle)\n\tRF_Raid_t *raidPtr;\n\tRF_AccessStripeMap_t *asmap;\n\tRF_DagHeader_t *dag_h;\n\tvoid   *bp;\n\tRF_RaidAccessFlags_t flags;\n\tRF_AllocListElem_t *allocList;\n\tint     nfaults;\n\tint     (*redFunc) (RF_DagNode_t *);\n\tint     allowBufferRecycle;\n{\n\tint     nNodes, nRrdNodes, nWndNodes, nXorBufs, i, j, paramNum,\n\t        rdnodesFaked;\n\tRF_DagNode_t *blockNode, *unblockNode, *wnpNode, *wnqNode, *termNode;\n\tRF_DagNode_t *nodes, *wndNodes, *rrdNodes, *xorNode, *commitNode;\n\tRF_SectorCount_t sectorsPerSU;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *xorTargetBuf = NULL;\t/* the target buffer for the XOR\n\t\t\t\t\t * operation */\n\tchar   *overlappingPDAs;/* a temporary array of flags */\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_PhysDiskAddr_t *pda, *parityPDA;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tRF_RaidLayout_t *layoutPtr;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,\n\t    &which_ru);\n\tsectorsPerSU = layoutPtr->sectorsPerStripeUnit;\n\t/* failedPDA points to the pda within the asm that targets the failed\n\t * disk */\n\tfailedPDA = asmap->failedPDAs[0];\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating degraded-write DAG]\\n\");\n\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"SimpleDegradedWriteDAG\";\n\n\t/*\n         * Generate two ASMs identifying the surviving data\n         * we need in order to recover the lost data.\n         */\n\t/* overlappingPDAs array must be zero'd */\n\tRF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));\n\trf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h,\n\t    &nXorBufs, NULL, overlappingPDAs, allocList);\n\n\t/* create all the nodes at once */\n\tnWndNodes = asmap->numStripeUnitsAccessed - 1;\t/* no access is\n\t\t\t\t\t\t\t * generated for the\n\t\t\t\t\t\t\t * failed pda */\n\n\tnRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +\n\t    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);\n\t/*\n         * XXX\n         *\n         * There's a bug with a complete stripe overwrite- that means 0 reads\n         * of old data, and the rest of the DAG generation code doesn't like\n         * that. A release is coming, and I don't wanna risk breaking a critical\n         * DAG generator, so here's what I'm gonna do- if there's no read nodes,\n         * I'm gonna fake there being a read node, and I'm gonna swap in a\n         * no-op node in its place (to make all the link-up code happy).\n         * This should be fixed at some point.  --jimz\n         */\n\tif (nRrdNodes == 0) {\n\t\tnRrdNodes = 1;\n\t\trdnodesFaked = 1;\n\t} else {\n\t\trdnodesFaked = 0;\n\t}\n\t/* lock, unlock, xor, Wnd, Rrd, W(nfaults) */\n\tnNodes = 5 + nfaults + nWndNodes + nRrdNodes;\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\tRF_ASSERT(i == nNodes);\n\n\t/* this dag can not commit until all rrd and xor Nodes have completed */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\tRF_ASSERT(nRrdNodes > 0);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nRrdNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nWndNodes + nfaults, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, nWndNodes + nfaults, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,\n\t    nRrdNodes, 2 * nXorBufs + 2, nfaults, dag_h, \"Xrc\", allocList);\n\n\t/*\n         * Fill in the Rrd nodes. If any of the rrd buffers are the same size as\n         * the failed buffer, save a pointer to it so we can use it as the target\n         * of the XOR. The pdas in the rrd nodes have been range-restricted, so if\n         * a buffer is the same size as the failed buffer, it must also be at the\n         * same alignment within the SU.\n         */\n\ti = 0;\n\tif (new_asm_h[0]) {\n\t\tfor (i = 0, pda = new_asm_h[0]->stripeMap->physInfo;\n\t\t    i < new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t\t    i++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i].params[0].p = pda;\n\t\t\trrdNodes[i].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i].params[2].v = parityStripeID;\n\t\t\trrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* i now equals the number of stripe units accessed in new_asm_h[0] */\n\tif (new_asm_h[1]) {\n\t\tfor (j = 0, pda = new_asm_h[1]->stripeMap->physInfo;\n\t\t    j < new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t\t    j++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i + j].params[0].p = pda;\n\t\t\trrdNodes[i + j].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i + j].params[2].v = parityStripeID;\n\t\t\trrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tif (allowBufferRecycle && (pda->numSector == failedPDA->numSector))\n\t\t\t\txorTargetBuf = pda->bufPtr;\n\t\t}\n\t}\n\tif (rdnodesFaked) {\n\t\t/*\n\t         * This is where we'll init that fake noop read node\n\t         * (XXX should the wakeup func be different?)\n\t         */\n\t\trf_InitNode(&rrdNodes[0], rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 1, 0, 0, dag_h, \"RrN\", allocList);\n\t}\n\t/*\n         * Make a PDA for the parity unit.  The parity PDA should start at\n         * the same offset into the SU as the failed PDA.\n         */\n\t/* Danner comment: I don't think this copy is really necessary. We are\n\t * in one of two cases here. (1) The entire failed unit is written.\n\t * Then asmap->parityInfo will describe the entire parity. (2) We are\n\t * only writing a subset of the failed unit and nothing else. Then the\n\t * asmap->parityInfo describes the failed unit and the copy can also\n\t * be avoided. */\n\n\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\tparityPDA->row = asmap->parityInfo->row;\n\tparityPDA->col = asmap->parityInfo->col;\n\tparityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)\n\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\tparityPDA->numSector = failedPDA->numSector;\n\n\tif (!xorTargetBuf) {\n\t\tRF_CallocAndAdd(xorTargetBuf, 1,\n\t\t    rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList);\n\t}\n\t/* init the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = parityPDA;\n\twnpNode->params[1].p = xorTargetBuf;\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t/* fill in the Wnq Node */\n\tif (nfaults == 2) {\n\t\t{\n\t\t\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t),\n\t\t\t    (RF_PhysDiskAddr_t *), allocList);\n\t\t\tparityPDA->row = asmap->qInfo->row;\n\t\t\tparityPDA->col = asmap->qInfo->col;\n\t\t\tparityPDA->startSector = ((asmap->qInfo->startSector / sectorsPerSU)\n\t\t\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\t\t\tparityPDA->numSector = failedPDA->numSector;\n\n\t\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\t\twnqNode->params[0].p = parityPDA;\n\t\t\tRF_CallocAndAdd(xorNode->results[1], 1,\n\t\t\t    rf_RaidAddressToByte(raidPtr, failedPDA->numSector), (char *), allocList);\n\t\t\twnqNode->params[1].p = xorNode->results[1];\n\t\t\twnqNode->params[2].v = parityStripeID;\n\t\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* fill in the Wnd nodes */\n\tfor (pda = asmap->physInfo, i = 0; i < nWndNodes; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* fill in the results of the xor node */\n\txorNode->results[0] = xorTargetBuf;\n\n\t/* fill in the params of the xor node */\n\n\tparamNum = 0;\n\tif (rdnodesFaked == 0) {\n\t\tfor (i = 0; i < nRrdNodes; i++) {\n\t\t\t/* all the Rrd nodes need to be xored together */\n\t\t\txorNode->params[paramNum++] = rrdNodes[i].params[0];\n\t\t\txorNode->params[paramNum++] = rrdNodes[i].params[1];\n\t\t}\n\t}\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\t/* any Wnd nodes that overlap the failed access need to be\n\t\t * xored in */\n\t\tif (overlappingPDAs[i]) {\n\t\t\tRF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tbcopy((char *) wndNodes[i].params[0].p, (char *) pda, sizeof(RF_PhysDiskAddr_t));\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);\n\t\t\txorNode->params[paramNum++].p = pda;\n\t\t\txorNode->params[paramNum++].p = pda->bufPtr;\n\t\t}\n\t}\n\tRF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));\n\n\t/*\n         * Install the failed PDA into the xor param list so that the\n         * new data gets xor'd in.\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = failedPDA->bufPtr;\n\n\t/*\n         * The last 2 params to the recovery xor node are always the failed\n         * PDA and the raidPtr. install the failedPDA even though we have just\n         * done so above. This allows us to use the same XOR function for both\n         * degraded reads and degraded writes.\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = raidPtr;\n\tRF_ASSERT(paramNum == 2 * nXorBufs + 2);\n\n\t/*\n         * Code to link nodes begins here\n         */\n\n\t/* link header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* link block node to rd nodes */\n\tRF_ASSERT(blockNode->numSuccedents == nRrdNodes);\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &rrdNodes[i];\n\t\trrdNodes[i].antecedents[0] = blockNode;\n\t\trrdNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* link read nodes to xor node */\n\tRF_ASSERT(xorNode->numAntecedents == nRrdNodes);\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\trrdNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[i] = &rrdNodes[i];\n\t\txorNode->antType[i] = rf_trueData;\n\t}\n\n\t/* link xor node to commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* link commit node to wnd nodes */\n\tRF_ASSERT(commitNode->numSuccedents == nfaults + nWndNodes);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = commitNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* link the commit node to wnp, wnq nodes */\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\tcommitNode->succedents[nWndNodes] = wnpNode;\n\twnpNode->antecedents[0] = commitNode;\n\twnpNode->antType[0] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\tcommitNode->succedents[nWndNodes + 1] = wnqNode;\n\t\twnqNode->antecedents[0] = commitNode;\n\t\twnqNode->antType[0] = rf_control;\n\t}\n\t/* link write new data nodes to unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nfaults));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes[i].numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNodes[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\n\t/* link write new parity node to unblock node */\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = unblockNode;\n\tunblockNode->antecedents[nWndNodes] = wnpNode;\n\tunblockNode->antType[nWndNodes] = rf_control;\n\n\t/* link write new q node to unblock node */\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\tunblockNode->antType[nWndNodes + 1] = rf_control;\n\t}\n\t/* link unblock node to term node */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}"
  },
  {
    "function_name": "rf_CreateDegradedWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegwr.c",
    "lines": "82-108",
    "snippet": "void \nrf_CreateDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList)\n\tRF_Raid_t *raidPtr;\n\tRF_AccessStripeMap_t *asmap;\n\tRF_DagHeader_t *dag_h;\n\tvoid   *bp;\n\tRF_RaidAccessFlags_t flags;\n\tRF_AllocListElem_t *allocList;\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_PhysDiskAddr_t *failedPDA = asmap->failedPDAs[0];\n\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"DegradedWriteDAG\";\n\n\t/* if the access writes only a portion of the failed unit, and also\n\t * writes some portion of at least one surviving unit, we create two\n\t * DAGs, one for the failed component and one for the non-failed\n\t * component, and do them sequentially.  Note that the fact that we're\n\t * accessing only a portion of the failed unit indicates that the\n\t * access either starts or ends in the failed unit, and hence we need\n\t * create only two dags.  This is inefficient in that the same data or\n\t * parity can get read and written twice using this structure.  I need\n\t * to fix this to do the access all at once. */\n\tRF_ASSERT(!(asmap->numStripeUnitsAccessed != 1 && failedPDA->numSector != layoutPtr->sectorsPerStripeUnit));\n\trf_CreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList);\n}",
    "includes": [
      "#include \"rf_dagdegwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CreateSimpleDegradedWriteDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList"
          ],
          "line": 107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "!(asmap->numStripeUnitsAccessed != 1 && failedPDA->numSector != layoutPtr->sectorsPerStripeUnit)"
          ],
          "line": 106
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->numDataFailed == 1"
          ],
          "line": 94
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList)\n\tRF_Raid_t *raidPtr;\n\tRF_AccessStripeMap_t *asmap;\n\tRF_DagHeader_t *dag_h;\n\tvoid   *bp;\n\tRF_RaidAccessFlags_t flags;\n\tRF_AllocListElem_t *allocList;\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_PhysDiskAddr_t *failedPDA = asmap->failedPDAs[0];\n\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"DegradedWriteDAG\";\n\n\t/* if the access writes only a portion of the failed unit, and also\n\t * writes some portion of at least one surviving unit, we create two\n\t * DAGs, one for the failed component and one for the non-failed\n\t * component, and do them sequentially.  Note that the fact that we're\n\t * accessing only a portion of the failed unit indicates that the\n\t * access either starts or ends in the failed unit, and hence we need\n\t * create only two dags.  This is inefficient in that the same data or\n\t * parity can get read and written twice using this structure.  I need\n\t * to fix this to do the access all at once. */\n\tRF_ASSERT(!(asmap->numStripeUnitsAccessed != 1 && failedPDA->numSector != layoutPtr->sectorsPerStripeUnit));\n\trf_CreateSimpleDegradedWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList);\n}"
  }
]