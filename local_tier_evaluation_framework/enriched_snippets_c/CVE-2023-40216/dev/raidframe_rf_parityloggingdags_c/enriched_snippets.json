[
  {
    "function_name": "rf_CreateParityLoggingLargeWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_parityloggingdags.c",
    "lines": "659-672",
    "snippet": "void \nrf_CreateParityLoggingLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *))\n{\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\trf_CommonCreateParityLoggingLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList, 1, rf_RegularXorFunc);\n}",
    "includes": [
      "#include \"rf_parityloggingdags.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_paritylog.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\"",
      "#include \"rf_archs.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CommonCreateParityLoggingLargeWriteDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList",
            "1",
            "rf_RegularXorFunc"
          ],
          "line": 671
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CommonCreateParityLoggingLargeWriteDAG",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_parityloggingdags.c",
          "lines": "70-269",
          "snippet": "void \nrf_CommonCreateParityLoggingLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *))\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes = NULL, *syncNode, *xorNode,\n\t       *lpoNode, *blockNode, *unblockNode, *termNode;\n\tint     nWndNodes, nRodNodes, i;\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tint     nodeNum, asmNum;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_PhysDiskAddr_t *pda;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging large-write DAG]\\n\");\n\tRF_ASSERT(nfaults == 1);/* this arch only single fault tolerant */\n\tdag_h->creator = \"ParityLoggingLargeWriteDAG\";\n\n\t/* alloc the Wnd nodes, the xor node, and the Lpo node */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 6, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\tlpoNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\n\tdag_h->numCommitNodes = nWndNodes + 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0)\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\n\t/* begin node initialization */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWndNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_TRUE, redFunc, rf_NullNodeUndoFunc, NULL, 1, 1, 2 * (nWndNodes + nRodNodes) + 1, 1, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tfor (i = 0; i < nRodNodes; i++)\n\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\tbreak;\n\tif (i == nRodNodes) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else {\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\t}\n\n\t/* initialize the Lpo node */\n\trf_InitNode(lpoNode, rf_wait, RF_FALSE, rf_ParityLogOverwriteFunc, rf_ParityLogOverwriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpo\", allocList);\n\n\tlpoNode->params[0].p = asmap->parityInfo;\n\tlpoNode->params[1].p = xorNode->results[0];\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the Rod nodes */\n\tRF_ASSERT(blockNode->numSuccedents == nRodNodes + 1);\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\trodNodes[i].antecedents[0] = blockNode;\n\t\trodNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the block node to the sync node */\n\t/* necessary if nRodNodes == 0 */\n\tRF_ASSERT(syncNode->numAntecedents == nRodNodes + 1);\n\tblockNode->succedents[nRodNodes] = syncNode;\n\tsyncNode->antecedents[0] = blockNode;\n\tsyncNode->antType[0] = rf_control;\n\n\t/* connect the Rod nodes to the syncNode */\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\trodNodes[i].succedents[0] = syncNode;\n\t\tsyncNode->antecedents[1 + i] = &rodNodes[i];\n\t\tsyncNode->antType[1 + i] = rf_control;\n\t}\n\n\t/* connect the sync node to the xor node */\n\tRF_ASSERT(syncNode->numSuccedents == nWndNodes + 1);\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_trueData;\t/* carry forward from sync */\n\n\t/* connect the sync node to the Wnd nodes */\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[1 + i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the xor node to the Lpo node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(lpoNode->numAntecedents == 1);\n\txorNode->succedents[0] = lpoNode;\n\tlpoNode->antecedents[0] = xorNode;\n\tlpoNode->antType[0] = rf_trueData;\n\n\t/* connect the Wnd nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == nWndNodes + 1);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNodes[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\n\t/* connect the Lpo node to the unblock node */\n\tRF_ASSERT(lpoNode->numSuccedents == 1);\n\tlpoNode->succedents[0] = unblockNode;\n\tunblockNode->antecedents[nWndNodes] = lpoNode;\n\tunblockNode->antType[nWndNodes] = rf_control;\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}",
          "includes": [
            "#include \"rf_parityloggingdags.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_paritylog.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_parityloggingdags.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_paritylog.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_CommonCreateParityLoggingLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *))\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes = NULL, *syncNode, *xorNode,\n\t       *lpoNode, *blockNode, *unblockNode, *termNode;\n\tint     nWndNodes, nRodNodes, i;\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tint     nodeNum, asmNum;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_PhysDiskAddr_t *pda;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging large-write DAG]\\n\");\n\tRF_ASSERT(nfaults == 1);/* this arch only single fault tolerant */\n\tdag_h->creator = \"ParityLoggingLargeWriteDAG\";\n\n\t/* alloc the Wnd nodes, the xor node, and the Lpo node */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 6, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\tlpoNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\n\tdag_h->numCommitNodes = nWndNodes + 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0)\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\n\t/* begin node initialization */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWndNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_TRUE, redFunc, rf_NullNodeUndoFunc, NULL, 1, 1, 2 * (nWndNodes + nRodNodes) + 1, 1, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tfor (i = 0; i < nRodNodes; i++)\n\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\tbreak;\n\tif (i == nRodNodes) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else {\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\t}\n\n\t/* initialize the Lpo node */\n\trf_InitNode(lpoNode, rf_wait, RF_FALSE, rf_ParityLogOverwriteFunc, rf_ParityLogOverwriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpo\", allocList);\n\n\tlpoNode->params[0].p = asmap->parityInfo;\n\tlpoNode->params[1].p = xorNode->results[0];\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the Rod nodes */\n\tRF_ASSERT(blockNode->numSuccedents == nRodNodes + 1);\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\trodNodes[i].antecedents[0] = blockNode;\n\t\trodNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the block node to the sync node */\n\t/* necessary if nRodNodes == 0 */\n\tRF_ASSERT(syncNode->numAntecedents == nRodNodes + 1);\n\tblockNode->succedents[nRodNodes] = syncNode;\n\tsyncNode->antecedents[0] = blockNode;\n\tsyncNode->antType[0] = rf_control;\n\n\t/* connect the Rod nodes to the syncNode */\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\trodNodes[i].succedents[0] = syncNode;\n\t\tsyncNode->antecedents[1 + i] = &rodNodes[i];\n\t\tsyncNode->antType[1 + i] = rf_control;\n\t}\n\n\t/* connect the sync node to the xor node */\n\tRF_ASSERT(syncNode->numSuccedents == nWndNodes + 1);\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_trueData;\t/* carry forward from sync */\n\n\t/* connect the sync node to the Wnd nodes */\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[1 + i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the xor node to the Lpo node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(lpoNode->numAntecedents == 1);\n\txorNode->succedents[0] = lpoNode;\n\tlpoNode->antecedents[0] = xorNode;\n\tlpoNode->antType[0] = rf_trueData;\n\n\t/* connect the Wnd nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == nWndNodes + 1);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNodes[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\n\t/* connect the Lpo node to the unblock node */\n\tRF_ASSERT(lpoNode->numSuccedents == 1);\n\tlpoNode->succedents[0] = unblockNode;\n\tunblockNode->antecedents[nWndNodes] = lpoNode;\n\tunblockNode->antType[nWndNodes] = rf_control;\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rf_parityloggingdags.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_paritylog.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_CreateParityLoggingLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *))\n{\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\trf_CommonCreateParityLoggingLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList, 1, rf_RegularXorFunc);\n}"
  },
  {
    "function_name": "rf_CreateParityLoggingSmallWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_parityloggingdags.c",
    "lines": "643-656",
    "snippet": "void \nrf_CreateParityLoggingSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\trf_CommonCreateParityLoggingSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList, &rf_xorFuncs, NULL);\n}",
    "includes": [
      "#include \"rf_parityloggingdags.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_paritylog.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\"",
      "#include \"rf_archs.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CommonCreateParityLoggingSmallWriteDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList",
            "&rf_xorFuncs",
            "NULL"
          ],
          "line": 655
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CommonCreateParityLoggingSmallWriteDAG",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_parityloggingdags.c",
          "lines": "312-640",
          "snippet": "void \nrf_CommonCreateParityLoggingSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *xorNodes, *blockNode, *unblockNode, *nodes;\n\tRF_DagNode_t *readDataNodes, *readParityNodes;\n\tRF_DagNode_t *writeDataNodes, *lpuNodes;\n\tRF_DagNode_t *unlockDataNodes = NULL, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     numDataNodes = asmap->numStripeUnitsAccessed;\n\tint     numParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\tint     i, j, nNodes, totalNumNodes;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t * node), (*undoFunc) (RF_DagNode_t * node);\n\tint     (*qfunc) (RF_DagNode_t * node);\n\tchar   *name, *qname;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tlong    nfaults = qfuncs ? 2 : 1;\n\tint     lu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tRF_ASSERT(nfaults == 1);\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\n\t/* DAG creation occurs in three steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node a read and Lpu for each\n\t * parity unit a block and unblock node (2) a terminator node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + numParityNodes + (2 * numParityNodes) + 3;\n\tif (lu_flag)\n\t\ttotalNumNodes += numDataNodes;\n\n\tnNodes = numDataNodes + numParityNodes;\n\n\tdag_h->numCommitNodes = numDataNodes + numParityNodes;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\tlpuNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize unblock node (Nil) */\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nNodes, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminatory node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\treadDataNodes[i].propList[0] = NULL;\n\t\treadDataNodes[i].propList[1] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, nNodes, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t}\n\n\t/* initialize the log node(s) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda);\n\t\trf_InitNode(&lpuNodes[i], rf_wait, RF_FALSE, rf_ParityLogUpdateFunc, rf_ParityLogUpdateUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpu\", allocList);\n\t\tlpuNodes[i].params[0].p = pda;\t/* PhysDiskAddr of parity */\n\t\tlpuNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer to\n\t\t\t\t\t\t\t\t\t * parity */\n\t\tpda = pda->next;\n\t}\n\n\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + numParityNodes));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\tRF_ASSERT(writeDataNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\tif (i == j)\n\t\t\t\twriteDataNodes[j].antType[i] = rf_antiData;\n\t\t\telse\n\t\t\t\twriteDataNodes[j].antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\n\t/* connect read old parity nodes to write new data nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\treadParityNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\twriteDataNodes[j].antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\treadParityNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\n\t/* connect xor nodes to write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(xorNodes[i].numSuccedents == 1);\n\t\tRF_ASSERT(lpuNodes[i].numAntecedents == 1);\n\t\txorNodes[i].succedents[0] = &lpuNodes[i];\n\t\tlpuNodes[i].antecedents[0] = &xorNodes[i];\n\t\tlpuNodes[i].antType[0] = rf_trueData;\n\t}\n\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to unblock node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\tunlockDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to unblock node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &writeDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect write new parity nodes to unblock node */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(lpuNodes[i].numSuccedents == 1);\n\t\tlpuNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[numDataNodes + i] = &lpuNodes[i];\n\t\tunblockNode->antType[numDataNodes + i] = rf_control;\n\t}\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}",
          "includes": [
            "#include \"rf_parityloggingdags.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_paritylog.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_parityloggingdags.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_paritylog.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_CommonCreateParityLoggingSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *xorNodes, *blockNode, *unblockNode, *nodes;\n\tRF_DagNode_t *readDataNodes, *readParityNodes;\n\tRF_DagNode_t *writeDataNodes, *lpuNodes;\n\tRF_DagNode_t *unlockDataNodes = NULL, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     numDataNodes = asmap->numStripeUnitsAccessed;\n\tint     numParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\tint     i, j, nNodes, totalNumNodes;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t * node), (*undoFunc) (RF_DagNode_t * node);\n\tint     (*qfunc) (RF_DagNode_t * node);\n\tchar   *name, *qname;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tlong    nfaults = qfuncs ? 2 : 1;\n\tint     lu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tRF_ASSERT(nfaults == 1);\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\n\t/* DAG creation occurs in three steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node a read and Lpu for each\n\t * parity unit a block and unblock node (2) a terminator node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + numParityNodes + (2 * numParityNodes) + 3;\n\tif (lu_flag)\n\t\ttotalNumNodes += numDataNodes;\n\n\tnNodes = numDataNodes + numParityNodes;\n\n\tdag_h->numCommitNodes = numDataNodes + numParityNodes;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\tlpuNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize unblock node (Nil) */\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nNodes, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminatory node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\treadDataNodes[i].propList[0] = NULL;\n\t\treadDataNodes[i].propList[1] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, nNodes, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t}\n\n\t/* initialize the log node(s) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda);\n\t\trf_InitNode(&lpuNodes[i], rf_wait, RF_FALSE, rf_ParityLogUpdateFunc, rf_ParityLogUpdateUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpu\", allocList);\n\t\tlpuNodes[i].params[0].p = pda;\t/* PhysDiskAddr of parity */\n\t\tlpuNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer to\n\t\t\t\t\t\t\t\t\t * parity */\n\t\tpda = pda->next;\n\t}\n\n\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + numParityNodes));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\tRF_ASSERT(writeDataNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\tif (i == j)\n\t\t\t\twriteDataNodes[j].antType[i] = rf_antiData;\n\t\t\telse\n\t\t\t\twriteDataNodes[j].antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\n\t/* connect read old parity nodes to write new data nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\treadParityNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\twriteDataNodes[j].antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\treadParityNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\n\t/* connect xor nodes to write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(xorNodes[i].numSuccedents == 1);\n\t\tRF_ASSERT(lpuNodes[i].numAntecedents == 1);\n\t\txorNodes[i].succedents[0] = &lpuNodes[i];\n\t\tlpuNodes[i].antecedents[0] = &xorNodes[i];\n\t\tlpuNodes[i].antType[0] = rf_trueData;\n\t}\n\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to unblock node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\tunlockDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to unblock node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &writeDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect write new parity nodes to unblock node */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(lpuNodes[i].numSuccedents == 1);\n\t\tlpuNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[numDataNodes + i] = &lpuNodes[i];\n\t\tunblockNode->antType[numDataNodes + i] = rf_control;\n\t}\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rf_parityloggingdags.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_paritylog.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_CreateParityLoggingSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\trf_CommonCreateParityLoggingSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList, &rf_xorFuncs, NULL);\n}"
  },
  {
    "function_name": "rf_CommonCreateParityLoggingSmallWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_parityloggingdags.c",
    "lines": "312-640",
    "snippet": "void \nrf_CommonCreateParityLoggingSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *xorNodes, *blockNode, *unblockNode, *nodes;\n\tRF_DagNode_t *readDataNodes, *readParityNodes;\n\tRF_DagNode_t *writeDataNodes, *lpuNodes;\n\tRF_DagNode_t *unlockDataNodes = NULL, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     numDataNodes = asmap->numStripeUnitsAccessed;\n\tint     numParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\tint     i, j, nNodes, totalNumNodes;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t * node), (*undoFunc) (RF_DagNode_t * node);\n\tint     (*qfunc) (RF_DagNode_t * node);\n\tchar   *name, *qname;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tlong    nfaults = qfuncs ? 2 : 1;\n\tint     lu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tRF_ASSERT(nfaults == 1);\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\n\t/* DAG creation occurs in three steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node a read and Lpu for each\n\t * parity unit a block and unblock node (2) a terminator node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + numParityNodes + (2 * numParityNodes) + 3;\n\tif (lu_flag)\n\t\ttotalNumNodes += numDataNodes;\n\n\tnNodes = numDataNodes + numParityNodes;\n\n\tdag_h->numCommitNodes = numDataNodes + numParityNodes;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\tlpuNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize unblock node (Nil) */\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nNodes, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminatory node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\treadDataNodes[i].propList[0] = NULL;\n\t\treadDataNodes[i].propList[1] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, nNodes, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t}\n\n\t/* initialize the log node(s) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda);\n\t\trf_InitNode(&lpuNodes[i], rf_wait, RF_FALSE, rf_ParityLogUpdateFunc, rf_ParityLogUpdateUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpu\", allocList);\n\t\tlpuNodes[i].params[0].p = pda;\t/* PhysDiskAddr of parity */\n\t\tlpuNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer to\n\t\t\t\t\t\t\t\t\t * parity */\n\t\tpda = pda->next;\n\t}\n\n\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + numParityNodes));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\tRF_ASSERT(writeDataNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\tif (i == j)\n\t\t\t\twriteDataNodes[j].antType[i] = rf_antiData;\n\t\t\telse\n\t\t\t\twriteDataNodes[j].antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\n\t/* connect read old parity nodes to write new data nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\treadParityNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\twriteDataNodes[j].antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\treadParityNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\n\t/* connect xor nodes to write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(xorNodes[i].numSuccedents == 1);\n\t\tRF_ASSERT(lpuNodes[i].numAntecedents == 1);\n\t\txorNodes[i].succedents[0] = &lpuNodes[i];\n\t\tlpuNodes[i].antecedents[0] = &xorNodes[i];\n\t\tlpuNodes[i].antType[0] = rf_trueData;\n\t}\n\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to unblock node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\tunlockDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to unblock node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &writeDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect write new parity nodes to unblock node */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(lpuNodes[i].numSuccedents == 1);\n\t\tlpuNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[numDataNodes + i] = &lpuNodes[i];\n\t\tunblockNode->antType[numDataNodes + i] = rf_control;\n\t}\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}",
    "includes": [
      "#include \"rf_parityloggingdags.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_paritylog.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\"",
      "#include \"rf_archs.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 636
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 635
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numSuccedents == 1"
          ],
          "line": 634
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "lpuNodes[i].numSuccedents == 1"
          ],
          "line": 627
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes))"
          ],
          "line": 618
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numSuccedents == 1"
          ],
          "line": 617
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes))"
          ],
          "line": 611
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockDataNodes[i].numSuccedents == 1"
          ],
          "line": 610
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockDataNodes[i].numAntecedents == 1"
          ],
          "line": 604
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numSuccedents == 1"
          ],
          "line": 603
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "lpuNodes[i].numAntecedents == 1"
          ],
          "line": 594
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNodes[i].numSuccedents == 1"
          ],
          "line": 593
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readParityNodes[i].numSuccedents == numDataNodes + numParityNodes"
          ],
          "line": 575
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNodes[j].numAntecedents == numDataNodes + numParityNodes"
          ],
          "line": 567
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[j].numAntecedents == numDataNodes + numParityNodes"
          ],
          "line": 554
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readDataNodes[i].numSuccedents == numDataNodes + numParityNodes"
          ],
          "line": 552
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readParityNodes[i].numAntecedents == 1"
          ],
          "line": 545
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readDataNodes[i].numAntecedents == 1"
          ],
          "line": 537
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == (numDataNodes + numParityNodes)"
          ],
          "line": 534
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 530
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "dag_h->numSuccedents == 1"
          ],
          "line": 529
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "&lpuNodes[i]",
            "rf_wait",
            "RF_FALSE",
            "rf_ParityLogUpdateFunc",
            "rf_ParityLogUpdateUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "2",
            "0",
            "dag_h",
            "\"Lpu\"",
            "allocList"
          ],
          "line": 518
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 517
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "lu_flag",
            "which_ru"
          ],
          "line": 447
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 440
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 433
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 425
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_AllocBuffer",
          "args": [
            "raidPtr",
            "dag_h",
            "pda",
            "allocList"
          ],
          "line": 422
        },
        "resolved": true,
        "details": {
          "function_name": "rf_AllocBuffer",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "263-275",
          "snippet": "void   *\nrf_AllocBuffer(\n    RF_Raid_t * raidPtr,\n    RF_DagHeader_t * dag_h,\n    RF_PhysDiskAddr_t * pda,\n    RF_AllocListElem_t * allocList)\n{\n\tchar   *p;\n\n\tRF_MallocAndAdd(p, pda->numSector << raidPtr->logBytesPerSector,\n\t    (char *), allocList);\n\treturn ((void *) p);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid   *\nrf_AllocBuffer(\n    RF_Raid_t * raidPtr,\n    RF_DagHeader_t * dag_h,\n    RF_PhysDiskAddr_t * pda,\n    RF_AllocListElem_t * allocList)\n{\n\tchar   *p;\n\n\tRF_MallocAndAdd(p, pda->numSector << raidPtr->logBytesPerSector,\n\t    (char *), allocList);\n\treturn ((void *) p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 419
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "lu_flag",
            "0",
            "which_ru"
          ],
          "line": 409
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 403
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == totalNumNodes"
          ],
          "line": 388
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "totalNumNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 366
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "nfaults == 1"
          ],
          "line": 342
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "numDataNodes > 0"
          ],
          "line": 341
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating parity-logging small-write DAG]\\n\""
          ],
          "line": 340
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 335
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_parityloggingdags.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_paritylog.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_CommonCreateParityLoggingSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *xorNodes, *blockNode, *unblockNode, *nodes;\n\tRF_DagNode_t *readDataNodes, *readParityNodes;\n\tRF_DagNode_t *writeDataNodes, *lpuNodes;\n\tRF_DagNode_t *unlockDataNodes = NULL, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     numDataNodes = asmap->numStripeUnitsAccessed;\n\tint     numParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\tint     i, j, nNodes, totalNumNodes;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t * node), (*undoFunc) (RF_DagNode_t * node);\n\tint     (*qfunc) (RF_DagNode_t * node);\n\tchar   *name, *qname;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tlong    nfaults = qfuncs ? 2 : 1;\n\tint     lu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tRF_ASSERT(nfaults == 1);\n\tdag_h->creator = \"ParityLoggingSmallWriteDAG\";\n\n\t/* DAG creation occurs in three steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node a read and Lpu for each\n\t * parity unit a block and unblock node (2) a terminator node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + numParityNodes + (2 * numParityNodes) + 3;\n\tif (lu_flag)\n\t\ttotalNumNodes += numDataNodes;\n\n\tnNodes = numDataNodes + numParityNodes;\n\n\tdag_h->numCommitNodes = numDataNodes + numParityNodes;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\tlpuNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize unblock node (Nil) */\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nNodes, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminatory node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\treadDataNodes[i].propList[0] = NULL;\n\t\treadDataNodes[i].propList[1] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, nNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, nNodes, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_TRUE, func, undoFunc, NULL, 1, nNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t}\n\n\t/* initialize the log node(s) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda);\n\t\trf_InitNode(&lpuNodes[i], rf_wait, RF_FALSE, rf_ParityLogUpdateFunc, rf_ParityLogUpdateUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpu\", allocList);\n\t\tlpuNodes[i].params[0].p = pda;\t/* PhysDiskAddr of parity */\n\t\tlpuNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer to\n\t\t\t\t\t\t\t\t\t * parity */\n\t\tpda = pda->next;\n\t}\n\n\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + numParityNodes));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\tRF_ASSERT(writeDataNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\tif (i == j)\n\t\t\t\twriteDataNodes[j].antType[i] = rf_antiData;\n\t\t\telse\n\t\t\t\twriteDataNodes[j].antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\n\t/* connect read old parity nodes to write new data nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numDataNodes + numParityNodes);\n\t\tfor (j = 0; j < numDataNodes; j++) {\n\t\t\treadParityNodes[i].succedents[j] = &writeDataNodes[j];\n\t\t\twriteDataNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\twriteDataNodes[j].antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++)\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\treadParityNodes[i].succedents[numDataNodes + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\n\t/* connect xor nodes to write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(xorNodes[i].numSuccedents == 1);\n\t\tRF_ASSERT(lpuNodes[i].numAntecedents == 1);\n\t\txorNodes[i].succedents[0] = &lpuNodes[i];\n\t\tlpuNodes[i].antecedents[0] = &xorNodes[i];\n\t\tlpuNodes[i].antType[0] = rf_trueData;\n\t}\n\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to unblock node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\tunlockDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to unblock node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unblockNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = unblockNode;\n\t\t\tunblockNode->antecedents[i] = &writeDataNodes[i];\n\t\t\tunblockNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\t/* connect write new parity nodes to unblock node */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(lpuNodes[i].numSuccedents == 1);\n\t\tlpuNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[numDataNodes + i] = &lpuNodes[i];\n\t\tunblockNode->antType[numDataNodes + i] = rf_control;\n\t}\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}"
  },
  {
    "function_name": "rf_CommonCreateParityLoggingLargeWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_parityloggingdags.c",
    "lines": "70-269",
    "snippet": "void \nrf_CommonCreateParityLoggingLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *))\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes = NULL, *syncNode, *xorNode,\n\t       *lpoNode, *blockNode, *unblockNode, *termNode;\n\tint     nWndNodes, nRodNodes, i;\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tint     nodeNum, asmNum;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_PhysDiskAddr_t *pda;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging large-write DAG]\\n\");\n\tRF_ASSERT(nfaults == 1);/* this arch only single fault tolerant */\n\tdag_h->creator = \"ParityLoggingLargeWriteDAG\";\n\n\t/* alloc the Wnd nodes, the xor node, and the Lpo node */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 6, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\tlpoNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\n\tdag_h->numCommitNodes = nWndNodes + 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0)\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\n\t/* begin node initialization */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWndNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_TRUE, redFunc, rf_NullNodeUndoFunc, NULL, 1, 1, 2 * (nWndNodes + nRodNodes) + 1, 1, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tfor (i = 0; i < nRodNodes; i++)\n\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\tbreak;\n\tif (i == nRodNodes) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else {\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\t}\n\n\t/* initialize the Lpo node */\n\trf_InitNode(lpoNode, rf_wait, RF_FALSE, rf_ParityLogOverwriteFunc, rf_ParityLogOverwriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpo\", allocList);\n\n\tlpoNode->params[0].p = asmap->parityInfo;\n\tlpoNode->params[1].p = xorNode->results[0];\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the Rod nodes */\n\tRF_ASSERT(blockNode->numSuccedents == nRodNodes + 1);\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\trodNodes[i].antecedents[0] = blockNode;\n\t\trodNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the block node to the sync node */\n\t/* necessary if nRodNodes == 0 */\n\tRF_ASSERT(syncNode->numAntecedents == nRodNodes + 1);\n\tblockNode->succedents[nRodNodes] = syncNode;\n\tsyncNode->antecedents[0] = blockNode;\n\tsyncNode->antType[0] = rf_control;\n\n\t/* connect the Rod nodes to the syncNode */\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\trodNodes[i].succedents[0] = syncNode;\n\t\tsyncNode->antecedents[1 + i] = &rodNodes[i];\n\t\tsyncNode->antType[1 + i] = rf_control;\n\t}\n\n\t/* connect the sync node to the xor node */\n\tRF_ASSERT(syncNode->numSuccedents == nWndNodes + 1);\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_trueData;\t/* carry forward from sync */\n\n\t/* connect the sync node to the Wnd nodes */\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[1 + i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the xor node to the Lpo node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(lpoNode->numAntecedents == 1);\n\txorNode->succedents[0] = lpoNode;\n\tlpoNode->antecedents[0] = xorNode;\n\tlpoNode->antType[0] = rf_trueData;\n\n\t/* connect the Wnd nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == nWndNodes + 1);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNodes[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\n\t/* connect the Lpo node to the unblock node */\n\tRF_ASSERT(lpoNode->numSuccedents == 1);\n\tlpoNode->succedents[0] = unblockNode;\n\tunblockNode->antecedents[nWndNodes] = lpoNode;\n\tunblockNode->antType[nWndNodes] = rf_control;\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}",
    "includes": [
      "#include \"rf_parityloggingdags.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_paritylog.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\"",
      "#include \"rf_archs.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 265
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 264
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numSuccedents == 1"
          ],
          "line": 263
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "lpoNode->numSuccedents == 1"
          ],
          "line": 257
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes->numSuccedents == 1"
          ],
          "line": 250
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numAntecedents == nWndNodes + 1"
          ],
          "line": 248
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "lpoNode->numAntecedents == 1"
          ],
          "line": 242
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numSuccedents == 1"
          ],
          "line": 241
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes->numAntecedents == 1"
          ],
          "line": 234
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numAntecedents == 1"
          ],
          "line": 227
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "syncNode->numSuccedents == nWndNodes + 1"
          ],
          "line": 226
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "syncNode->numAntecedents == nRodNodes + 1"
          ],
          "line": 213
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rodNodes[i].numAntecedents == 1"
          ],
          "line": 205
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == nRodNodes + 1"
          ],
          "line": 203
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 199
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "dag_h->numSuccedents == 1"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->parityInfo->next == NULL"
          ],
          "line": 191
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "lpoNode",
            "rf_wait",
            "RF_FALSE",
            "rf_ParityLogOverwriteFunc",
            "rf_ParityLogOverwriteUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "2",
            "0",
            "dag_h",
            "\"Lpo\"",
            "allocList"
          ],
          "line": 187
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "xorNode->results[0]",
            "1",
            "rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit)",
            "(void *), allocList"
          ],
          "line": 181
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "raidPtr->Layout.sectorsPerStripeUnit"
          ],
          "line": 181
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 155
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 151
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "nodeNum == nRodNodes"
          ],
          "line": 145
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 139
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "rodNodes",
            "nRodNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 122
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_MapUnaccessedPortionOfStripe",
          "args": [
            "raidPtr",
            "layoutPtr",
            "asmap",
            "dag_h",
            "new_asm_h",
            "&nRodNodes",
            "&sosBuffer",
            "&eosBuffer",
            "allocList"
          ],
          "line": 120
        },
        "resolved": true,
        "details": {
          "function_name": "rf_MapUnaccessedPortionOfStripe",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "799-851",
          "snippet": "void \nrf_MapUnaccessedPortionOfStripe(\n    RF_Raid_t * raidPtr,\n    RF_RaidLayout_t * layoutPtr,/* in: layout information */\n    RF_AccessStripeMap_t * asmap,\t/* in: access stripe map */\n    RF_DagHeader_t * dag_h,\t/* in: header of the dag to create */\n    RF_AccessStripeMapHeader_t ** new_asm_h,\t/* in: ptr to array of 2\n\t\t\t\t\t\t * headers, to be filled in */\n    int *nRodNodes,\t\t/* out: num nodes to be generated to read\n\t\t\t\t * unaccessed data */\n    char **sosBuffer,\t\t/* out: pointers to newly allocated buffer */\n    char **eosBuffer,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidAddr_t sosRaidAddress, eosRaidAddress;\n\tRF_SectorNum_t sosNumSector, eosNumSector;\n\n\tRF_ASSERT(asmap->numStripeUnitsAccessed > (layoutPtr->numDataCol / 2));\n\t/* generate an access map for the region of the array from start of\n\t * stripe to start of access */\n\tnew_asm_h[0] = new_asm_h[1] = NULL;\n\t*nRodNodes = 0;\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->raidAddress)) {\n\t\tsosRaidAddress = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\t\tsosNumSector = asmap->raidAddress - sosRaidAddress;\n\t\tRF_MallocAndAdd(*sosBuffer, rf_RaidAddressToByte(raidPtr, sosNumSector), (char *), allocList);\n\t\tnew_asm_h[0] = rf_MapAccess(raidPtr, sosRaidAddress, sosNumSector, *sosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\t*nRodNodes += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[0]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[0]->stripeMap);\n\t}\n\t/* generate an access map for the region of the array from end of\n\t * access to end of stripe */\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->endRaidAddress)) {\n\t\teosRaidAddress = asmap->endRaidAddress;\n\t\teosNumSector = rf_RaidAddressOfNextStripeBoundary(layoutPtr, eosRaidAddress) - eosRaidAddress;\n\t\tRF_MallocAndAdd(*eosBuffer, rf_RaidAddressToByte(raidPtr, eosNumSector), (char *), allocList);\n\t\tnew_asm_h[1] = rf_MapAccess(raidPtr, eosRaidAddress, eosNumSector, *eosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\t*nRodNodes += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[1]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[1]->stripeMap);\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_MapUnaccessedPortionOfStripe(\n    RF_Raid_t * raidPtr,\n    RF_RaidLayout_t * layoutPtr,/* in: layout information */\n    RF_AccessStripeMap_t * asmap,\t/* in: access stripe map */\n    RF_DagHeader_t * dag_h,\t/* in: header of the dag to create */\n    RF_AccessStripeMapHeader_t ** new_asm_h,\t/* in: ptr to array of 2\n\t\t\t\t\t\t * headers, to be filled in */\n    int *nRodNodes,\t\t/* out: num nodes to be generated to read\n\t\t\t\t * unaccessed data */\n    char **sosBuffer,\t\t/* out: pointers to newly allocated buffer */\n    char **eosBuffer,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidAddr_t sosRaidAddress, eosRaidAddress;\n\tRF_SectorNum_t sosNumSector, eosNumSector;\n\n\tRF_ASSERT(asmap->numStripeUnitsAccessed > (layoutPtr->numDataCol / 2));\n\t/* generate an access map for the region of the array from start of\n\t * stripe to start of access */\n\tnew_asm_h[0] = new_asm_h[1] = NULL;\n\t*nRodNodes = 0;\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->raidAddress)) {\n\t\tsosRaidAddress = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\t\tsosNumSector = asmap->raidAddress - sosRaidAddress;\n\t\tRF_MallocAndAdd(*sosBuffer, rf_RaidAddressToByte(raidPtr, sosNumSector), (char *), allocList);\n\t\tnew_asm_h[0] = rf_MapAccess(raidPtr, sosRaidAddress, sosNumSector, *sosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\t*nRodNodes += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[0]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[0]->stripeMap);\n\t}\n\t/* generate an access map for the region of the array from end of\n\t * access to end of stripe */\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->endRaidAddress)) {\n\t\teosRaidAddress = asmap->endRaidAddress;\n\t\teosNumSector = rf_RaidAddressOfNextStripeBoundary(layoutPtr, eosRaidAddress) - eosRaidAddress;\n\t\tRF_MallocAndAdd(*eosBuffer, rf_RaidAddressToByte(raidPtr, eosNumSector), (char *), allocList);\n\t\tnew_asm_h[1] = rf_MapAccess(raidPtr, eosRaidAddress, eosNumSector, *eosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\t*nRodNodes += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[1]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[1]->stripeMap);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nWndNodes + 6",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 99
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "nfaults == 1"
          ],
          "line": 94
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating parity-logging large-write DAG]\\n\""
          ],
          "line": 93
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 90
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_parityloggingdags.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_paritylog.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_CommonCreateParityLoggingLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *))\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes = NULL, *syncNode, *xorNode,\n\t       *lpoNode, *blockNode, *unblockNode, *termNode;\n\tint     nWndNodes, nRodNodes, i;\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tint     nodeNum, asmNum;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_PhysDiskAddr_t *pda;\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating parity-logging large-write DAG]\\n\");\n\tRF_ASSERT(nfaults == 1);/* this arch only single fault tolerant */\n\tdag_h->creator = \"ParityLoggingLargeWriteDAG\";\n\n\t/* alloc the Wnd nodes, the xor node, and the Lpo node */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 6, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\tlpoNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\n\tdag_h->numCommitNodes = nWndNodes + 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0)\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\n\t/* begin node initialization */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nWndNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes + 1, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_TRUE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_TRUE, redFunc, rf_NullNodeUndoFunc, NULL, 1, 1, 2 * (nWndNodes + nRodNodes) + 1, 1, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tfor (i = 0; i < nRodNodes; i++)\n\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\tbreak;\n\tif (i == nRodNodes) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else {\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\t}\n\n\t/* initialize the Lpo node */\n\trf_InitNode(lpoNode, rf_wait, RF_FALSE, rf_ParityLogOverwriteFunc, rf_ParityLogOverwriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Lpo\", allocList);\n\n\tlpoNode->params[0].p = asmap->parityInfo;\n\tlpoNode->params[1].p = xorNode->results[0];\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the Rod nodes */\n\tRF_ASSERT(blockNode->numSuccedents == nRodNodes + 1);\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\trodNodes[i].antecedents[0] = blockNode;\n\t\trodNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the block node to the sync node */\n\t/* necessary if nRodNodes == 0 */\n\tRF_ASSERT(syncNode->numAntecedents == nRodNodes + 1);\n\tblockNode->succedents[nRodNodes] = syncNode;\n\tsyncNode->antecedents[0] = blockNode;\n\tsyncNode->antType[0] = rf_control;\n\n\t/* connect the Rod nodes to the syncNode */\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\trodNodes[i].succedents[0] = syncNode;\n\t\tsyncNode->antecedents[1 + i] = &rodNodes[i];\n\t\tsyncNode->antType[1 + i] = rf_control;\n\t}\n\n\t/* connect the sync node to the xor node */\n\tRF_ASSERT(syncNode->numSuccedents == nWndNodes + 1);\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_trueData;\t/* carry forward from sync */\n\n\t/* connect the sync node to the Wnd nodes */\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[1 + i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the xor node to the Lpo node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(lpoNode->numAntecedents == 1);\n\txorNode->succedents[0] = lpoNode;\n\tlpoNode->antecedents[0] = xorNode;\n\tlpoNode->antType[0] = rf_trueData;\n\n\t/* connect the Wnd nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == nWndNodes + 1);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNodes[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\n\t/* connect the Lpo node to the unblock node */\n\tRF_ASSERT(lpoNode->numSuccedents == 1);\n\tlpoNode->succedents[0] = unblockNode;\n\tunblockNode->antecedents[nWndNodes] = lpoNode;\n\tunblockNode->antType[nWndNodes] = rf_control;\n\n\t/* connect unblock node to terminator */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}"
  }
]