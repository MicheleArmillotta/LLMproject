[
  {
    "function_name": "rf_SubmitReconBufferRAID1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "541-688",
    "snippet": "int \nrf_SubmitReconBufferRAID1(rbuf, keep_it, use_committed)\n\tRF_ReconBuffer_t *rbuf;\t/* the recon buffer to submit */\n\tint     keep_it;\t/* whether we can keep this buffer or we have\n\t\t\t\t * to return it */\n\tint     use_committed;\t/* whether to use a committed or an available\n\t\t\t\t * recon buffer */\n{\n\tRF_ReconParityStripeStatus_t *pssPtr;\n\tRF_ReconCtrl_t *reconCtrlPtr;\n\tRF_RaidLayout_t *layoutPtr;\n\tint     retcode, created;\n\tRF_CallbackDesc_t *cb, *p;\n\tRF_ReconBuffer_t *t;\n\tRF_Raid_t *raidPtr;\n\tcaddr_t ta;\n\n\tretcode = 0;\n\tcreated = 0;\n\n\traidPtr = rbuf->raidPtr;\n\tlayoutPtr = &raidPtr->Layout;\n\treconCtrlPtr = raidPtr->reconControl[rbuf->row];\n\n\tRF_ASSERT(rbuf);\n\tRF_ASSERT(rbuf->col != reconCtrlPtr->fcol);\n\n\tif (rf_reconbufferDebug) {\n\t\tprintf(\"raid%d: RAID1 reconbuffer submission r%d c%d psid %ld ru%d (failed offset %ld)\\n\",\n\t\t       raidPtr->raidid, rbuf->row, rbuf->col, \n\t\t       (long) rbuf->parityStripeID, rbuf->which_ru,\n\t\t    (long) rbuf->failedDiskSectorOffset);\n\t}\n\tif (rf_reconDebug) {\n\t\tprintf(\"RAID1 reconbuffer submit psid %ld buf %lx\\n\",\n\t\t    (long) rbuf->parityStripeID, (long) rbuf->buffer);\n\t\tprintf(\"RAID1 psid %ld   %02x %02x %02x %02x %02x\\n\",\n\t\t    (long) rbuf->parityStripeID,\n\t\t    rbuf->buffer[0], rbuf->buffer[1], rbuf->buffer[2], rbuf->buffer[3],\n\t\t    rbuf->buffer[4]);\n\t}\n\tRF_LOCK_PSS_MUTEX(raidPtr, rbuf->row, rbuf->parityStripeID);\n\n\tRF_LOCK_MUTEX(reconCtrlPtr->rb_mutex);\n\n\tpssPtr = rf_LookupRUStatus(raidPtr, reconCtrlPtr->pssTable,\n\t    rbuf->parityStripeID, rbuf->which_ru, RF_PSS_NONE, &created);\n\tRF_ASSERT(pssPtr);\t/* if it didn't exist, we wouldn't have gotten\n\t\t\t\t * an rbuf for it */\n\n\t/*\n         * Since this is simple mirroring, the first submission for a stripe is also\n         * treated as the last.\n         */\n\n\tt = NULL;\n\tif (keep_it) {\n\t\tif (rf_reconbufferDebug) {\n\t\t\tprintf(\"raid%d: RAID1 rbuf submission: keeping rbuf\\n\", \n\t\t\t       raidPtr->raidid);\n\t\t}\n\t\tt = rbuf;\n\t} else {\n\t\tif (use_committed) {\n\t\t\tif (rf_reconbufferDebug) {\n\t\t\t\tprintf(\"raid%d: RAID1 rbuf submission: using committed rbuf\\n\", raidPtr->raidid);\n\t\t\t}\n\t\t\tt = reconCtrlPtr->committedRbufs;\n\t\t\tRF_ASSERT(t);\n\t\t\treconCtrlPtr->committedRbufs = t->next;\n\t\t\tt->next = NULL;\n\t\t} else\n\t\t\tif (reconCtrlPtr->floatingRbufs) {\n\t\t\t\tif (rf_reconbufferDebug) {\n\t\t\t\t\tprintf(\"raid%d: RAID1 rbuf submission: using floating rbuf\\n\", raidPtr->raidid);\n\t\t\t\t}\n\t\t\t\tt = reconCtrlPtr->floatingRbufs;\n\t\t\t\treconCtrlPtr->floatingRbufs = t->next;\n\t\t\t\tt->next = NULL;\n\t\t\t}\n\t}\n\tif (t == NULL) {\n\t\tif (rf_reconbufferDebug) {\n\t\t\tprintf(\"raid%d: RAID1 rbuf submission: waiting for rbuf\\n\", raidPtr->raidid);\n\t\t}\n\t\tRF_ASSERT((keep_it == 0) && (use_committed == 0));\n\t\traidPtr->procsInBufWait++;\n\t\tif ((raidPtr->procsInBufWait == (raidPtr->numCol - 1))\n\t\t    && (raidPtr->numFullReconBuffers == 0)) {\n\t\t\t/* ruh-ro */\n\t\t\tRF_ERRORMSG(\"Buffer wait deadlock\\n\");\n\t\t\trf_PrintPSStatusTable(raidPtr, rbuf->row);\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpssPtr->flags |= RF_PSS_BUFFERWAIT;\n\t\tcb = rf_AllocCallbackDesc();\n\t\tcb->row = rbuf->row;\n\t\tcb->col = rbuf->col;\n\t\tcb->callbackArg.v = rbuf->parityStripeID;\n\t\tcb->callbackArg2.v = rbuf->which_ru;\n\t\tcb->next = NULL;\n\t\tif (reconCtrlPtr->bufferWaitList == NULL) {\n\t\t\t/* we are the wait list- lucky us */\n\t\t\treconCtrlPtr->bufferWaitList = cb;\n\t\t} else {\n\t\t\t/* append to wait list */\n\t\t\tfor (p = reconCtrlPtr->bufferWaitList; p->next; p = p->next);\n\t\t\tp->next = cb;\n\t\t}\n\t\tretcode = 1;\n\t\tgoto out;\n\t}\n\tif (t != rbuf) {\n\t\tt->row = rbuf->row;\n\t\tt->col = reconCtrlPtr->fcol;\n\t\tt->parityStripeID = rbuf->parityStripeID;\n\t\tt->which_ru = rbuf->which_ru;\n\t\tt->failedDiskSectorOffset = rbuf->failedDiskSectorOffset;\n\t\tt->spRow = rbuf->spRow;\n\t\tt->spCol = rbuf->spCol;\n\t\tt->spOffset = rbuf->spOffset;\n\t\t/* Swap buffers. DANCE! */\n\t\tta = t->buffer;\n\t\tt->buffer = rbuf->buffer;\n\t\trbuf->buffer = ta;\n\t}\n\t/*\n         * Use the rbuf we've been given as the target.\n         */\n\tRF_ASSERT(pssPtr->rbuf == NULL);\n\tpssPtr->rbuf = t;\n\n\tt->count = 1;\n\t/*\n         * Below, we use 1 for numDataCol (which is equal to the count in the\n         * previous line), so we'll always be done.\n         */\n\trf_CheckForFullRbuf(raidPtr, reconCtrlPtr, pssPtr, 1);\n\nout:\n\tRF_UNLOCK_PSS_MUTEX(raidPtr, rbuf->row, rbuf->parityStripeID);\n\tRF_UNLOCK_MUTEX(reconCtrlPtr->rb_mutex);\n\tif (rf_reconbufferDebug) {\n\t\tprintf(\"raid%d: RAID1 rbuf submission: returning %d\\n\", \n\t\t       raidPtr->raidid, retcode);\n\t}\n\treturn (retcode);\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"raid%d: RAID1 rbuf submission: returning %d\\n\"",
            "raidPtr->raidid",
            "retcode"
          ],
          "line": 684
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_UNLOCK_MUTEX",
          "args": [
            "reconCtrlPtr->rb_mutex"
          ],
          "line": 682
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_UNLOCK_PSS_MUTEX",
          "args": [
            "raidPtr",
            "rbuf->row",
            "rbuf->parityStripeID"
          ],
          "line": 681
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_CheckForFullRbuf",
          "args": [
            "raidPtr",
            "reconCtrlPtr",
            "pssPtr",
            "1"
          ],
          "line": 678
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CheckForFullRbuf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_reconbuffer.c",
          "lines": "349-386",
          "snippet": "int \nrf_CheckForFullRbuf(raidPtr, reconCtrl, pssPtr, numDataCol)\n\tRF_Raid_t *raidPtr;\n\tRF_ReconCtrl_t *reconCtrl;\n\tRF_ReconParityStripeStatus_t *pssPtr;\n\tint     numDataCol;\n{\n\tRF_ReconBuffer_t *p, *pt, *rbuf = (RF_ReconBuffer_t *) pssPtr->rbuf;\n\n\tif (rbuf->count == numDataCol) {\n\t\traidPtr->numFullReconBuffers++;\n\t\tDprintf2(\"RECON: rbuf for psid %ld ru %d has filled\\n\",\n\t\t    (long) rbuf->parityStripeID, rbuf->which_ru);\n\t\tif (!reconCtrl->fullBufferList || (rbuf->failedDiskSectorOffset < reconCtrl->fullBufferList->failedDiskSectorOffset)) {\n\t\t\tDprintf2(\"RECON: rbuf for psid %ld ru %d is head of list\\n\",\n\t\t\t    (long) rbuf->parityStripeID, rbuf->which_ru);\n\t\t\trbuf->next = reconCtrl->fullBufferList;\n\t\t\treconCtrl->fullBufferList = rbuf;\n\t\t} else {\n\t\t\tfor (pt = reconCtrl->fullBufferList, p = pt->next; p && p->failedDiskSectorOffset < rbuf->failedDiskSectorOffset; pt = p, p = p->next);\n\t\t\trbuf->next = p;\n\t\t\tpt->next = rbuf;\n\t\t\tDprintf2(\"RECON: rbuf for psid %ld ru %d is in list\\n\",\n\t\t\t    (long) rbuf->parityStripeID, rbuf->which_ru);\n\t\t}\n#if 0\n\t\tpssPtr->writeRbuf = pssPtr->rbuf;\t/* DEBUG ONLY:  we like\n\t\t\t\t\t\t\t * to be able to find\n\t\t\t\t\t\t\t * this rbuf while it's\n\t\t\t\t\t\t\t * awaiting write */\n#else\n\t\trbuf->pssPtr = pssPtr;\n#endif\n\t\tpssPtr->rbuf = NULL;\n\t\trf_CauseReconEvent(raidPtr, rbuf->row, rbuf->col, NULL, RF_REVENT_BUFREADY);\n\t}\n\treturn (0);\n}",
          "includes": [
            "#include \"rf_nwayxor.h\"",
            "#include \"rf_reconutil.h\"",
            "#include \"rf_revent.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_etimer.h\"",
            "#include \"rf_acctrace.h\"",
            "#include \"rf_reconbuffer.h\"",
            "#include \"rf_raid.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_nwayxor.h\"\n#include \"rf_reconutil.h\"\n#include \"rf_revent.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_general.h\"\n#include \"rf_etimer.h\"\n#include \"rf_acctrace.h\"\n#include \"rf_reconbuffer.h\"\n#include \"rf_raid.h\"\n\nint \nrf_CheckForFullRbuf(raidPtr, reconCtrl, pssPtr, numDataCol)\n\tRF_Raid_t *raidPtr;\n\tRF_ReconCtrl_t *reconCtrl;\n\tRF_ReconParityStripeStatus_t *pssPtr;\n\tint     numDataCol;\n{\n\tRF_ReconBuffer_t *p, *pt, *rbuf = (RF_ReconBuffer_t *) pssPtr->rbuf;\n\n\tif (rbuf->count == numDataCol) {\n\t\traidPtr->numFullReconBuffers++;\n\t\tDprintf2(\"RECON: rbuf for psid %ld ru %d has filled\\n\",\n\t\t    (long) rbuf->parityStripeID, rbuf->which_ru);\n\t\tif (!reconCtrl->fullBufferList || (rbuf->failedDiskSectorOffset < reconCtrl->fullBufferList->failedDiskSectorOffset)) {\n\t\t\tDprintf2(\"RECON: rbuf for psid %ld ru %d is head of list\\n\",\n\t\t\t    (long) rbuf->parityStripeID, rbuf->which_ru);\n\t\t\trbuf->next = reconCtrl->fullBufferList;\n\t\t\treconCtrl->fullBufferList = rbuf;\n\t\t} else {\n\t\t\tfor (pt = reconCtrl->fullBufferList, p = pt->next; p && p->failedDiskSectorOffset < rbuf->failedDiskSectorOffset; pt = p, p = p->next);\n\t\t\trbuf->next = p;\n\t\t\tpt->next = rbuf;\n\t\t\tDprintf2(\"RECON: rbuf for psid %ld ru %d is in list\\n\",\n\t\t\t    (long) rbuf->parityStripeID, rbuf->which_ru);\n\t\t}\n#if 0\n\t\tpssPtr->writeRbuf = pssPtr->rbuf;\t/* DEBUG ONLY:  we like\n\t\t\t\t\t\t\t * to be able to find\n\t\t\t\t\t\t\t * this rbuf while it's\n\t\t\t\t\t\t\t * awaiting write */\n#else\n\t\trbuf->pssPtr = pssPtr;\n#endif\n\t\tpssPtr->rbuf = NULL;\n\t\trf_CauseReconEvent(raidPtr, rbuf->row, rbuf->col, NULL, RF_REVENT_BUFREADY);\n\t}\n\treturn (0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pssPtr->rbuf == NULL"
          ],
          "line": 670
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_AllocCallbackDesc",
          "args": [],
          "line": 636
        },
        "resolved": true,
        "details": {
          "function_name": "rf_AllocCallbackDesc",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_callback.c",
          "lines": "80-87",
          "snippet": "RF_CallbackDesc_t *\nrf_AllocCallbackDesc()\n{\n\tRF_CallbackDesc_t *p;\n\n\tRF_FREELIST_GET(rf_callback_freelist, p, next, (RF_CallbackDesc_t *));\n\treturn (p);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_callback.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static RF_FreeList_t *rf_callback_freelist;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_freelist.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_callback.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\nstatic RF_FreeList_t *rf_callback_freelist;\n\nRF_CallbackDesc_t *\nrf_AllocCallbackDesc()\n{\n\tRF_CallbackDesc_t *p;\n\n\tRF_FREELIST_GET(rf_callback_freelist, p, next, (RF_CallbackDesc_t *));\n\treturn (p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_PANIC",
          "args": [],
          "line": 633
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_PrintPSStatusTable",
          "args": [
            "raidPtr",
            "rbuf->row"
          ],
          "line": 632
        },
        "resolved": true,
        "details": {
          "function_name": "rf_PrintPSStatusTable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_psstatus.c",
          "lines": "369-376",
          "snippet": "void \nrf_PrintPSStatusTable(raidPtr, row)\n\tRF_Raid_t *raidPtr;\n\tRF_RowCol_t row;\n{\n\tRF_PSStatusHeader_t *pssTable = raidPtr->reconControl[row]->pssTable;\n\tRealPrintPSStatusTable(raidPtr, pssTable);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_psstatus.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void \nRealPrintPSStatusTable(RF_Raid_t * raidPtr,\n    RF_PSStatusHeader_t * pssTable);",
            "static int init_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);",
            "static void clean_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);",
            "RF_PSStatusHeader_t *\nrf_MakeParityStripeStatusTable(raidPtr)\n\tRF_Raid_t *raidPtr;",
            "RF_ReconParityStripeStatus_t *\nrf_AllocPSStatus(raidPtr)\n\tRF_Raid_t *raidPtr;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_psstatus.h\"\n#include \"rf_freelist.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_general.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nstatic void \nRealPrintPSStatusTable(RF_Raid_t * raidPtr,\n    RF_PSStatusHeader_t * pssTable);\nstatic int init_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);\nstatic void clean_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);\nRF_PSStatusHeader_t *\nrf_MakeParityStripeStatusTable(raidPtr)\n\tRF_Raid_t *raidPtr;\nRF_ReconParityStripeStatus_t *\nrf_AllocPSStatus(raidPtr)\n\tRF_Raid_t *raidPtr;\n\nvoid \nrf_PrintPSStatusTable(raidPtr, row)\n\tRF_Raid_t *raidPtr;\n\tRF_RowCol_t row;\n{\n\tRF_PSStatusHeader_t *pssTable = raidPtr->reconControl[row]->pssTable;\n\tRealPrintPSStatusTable(raidPtr, pssTable);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ERRORMSG",
          "args": [
            "\"Buffer wait deadlock\\n\""
          ],
          "line": 631
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "(keep_it == 0) && (use_committed == 0)"
          ],
          "line": 626
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "t"
          ],
          "line": 609
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pssPtr"
          ],
          "line": 588
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_LookupRUStatus",
          "args": [
            "raidPtr",
            "reconCtrlPtr->pssTable",
            "rbuf->parityStripeID",
            "rbuf->which_ru",
            "RF_PSS_NONE",
            "&created"
          ],
          "line": 586
        },
        "resolved": true,
        "details": {
          "function_name": "rf_LookupRUStatus",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_psstatus.c",
          "lines": "170-218",
          "snippet": "RF_ReconParityStripeStatus_t *\nrf_LookupRUStatus(\n    RF_Raid_t * raidPtr,\n    RF_PSStatusHeader_t * pssTable,\n    RF_StripeNum_t psID,\n    RF_ReconUnitNum_t which_ru,\n    RF_PSSFlags_t flags,\t/* whether or not to create it if it doesn't\n\t\t\t\t * exist + what flags to set initially */\n    int *created)\n{\n\tRF_PSStatusHeader_t *hdr = &pssTable[RF_HASH_PSID(raidPtr, psID)];\n\tRF_ReconParityStripeStatus_t *p, *pssPtr = hdr->chain;\n\n\t*created = 0;\n\tfor (p = pssPtr; p; p = p->next) {\n\t\tif (p->parityStripeID == psID && p->which_ru == which_ru)\n\t\t\tbreak;\n\t}\n\n\tif (!p && (flags & RF_PSS_CREATE)) {\n\t\tDprintf2(\"PSS: creating pss for psid %ld ru %d\\n\", psID, which_ru);\n\t\tp = rf_AllocPSStatus(raidPtr);\n\t\tp->next = hdr->chain;\n\t\thdr->chain = p;\n\n\t\tp->parityStripeID = psID;\n\t\tp->which_ru = which_ru;\n\t\tp->flags = flags;\n\t\tp->rbuf = NULL;\n\t\tp->writeRbuf = NULL;\n\t\tp->blockCount = 0;\n\t\tp->procWaitList = NULL;\n\t\tp->blockWaitList = NULL;\n\t\tp->bufWaitList = NULL;\n\t\t*created = 1;\n\t} else\n\t\tif (p) {\t/* we didn't create, but we want to specify\n\t\t\t\t * some new status */\n\t\t\tp->flags |= flags;\t/* add in whatever flags we're\n\t\t\t\t\t\t * specifying */\n\t\t}\n\tif (p && (flags & RF_PSS_RECON_BLOCKED)) {\n\t\tp->blockCount++;/* if we're asking to block recon, bump the\n\t\t\t\t * count */\n\t\tDprintf3(\"raid%d: Blocked recon on psid %ld.  count now %d\\n\",\n\t\t\t raidPtr->raidid, psID, p->blockCount);\n\t}\n\treturn (p);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_psstatus.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void \nRealPrintPSStatusTable(RF_Raid_t * raidPtr,\n    RF_PSStatusHeader_t * pssTable);",
            "static int init_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);",
            "static void clean_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);",
            "RF_PSStatusHeader_t *\nrf_MakeParityStripeStatusTable(raidPtr)\n\tRF_Raid_t *raidPtr;",
            "RF_ReconParityStripeStatus_t *\nrf_AllocPSStatus(raidPtr)\n\tRF_Raid_t *raidPtr;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_psstatus.h\"\n#include \"rf_freelist.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_general.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nstatic void \nRealPrintPSStatusTable(RF_Raid_t * raidPtr,\n    RF_PSStatusHeader_t * pssTable);\nstatic int init_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);\nstatic void clean_pss(RF_ReconParityStripeStatus_t *, RF_Raid_t *);\nRF_PSStatusHeader_t *\nrf_MakeParityStripeStatusTable(raidPtr)\n\tRF_Raid_t *raidPtr;\nRF_ReconParityStripeStatus_t *\nrf_AllocPSStatus(raidPtr)\n\tRF_Raid_t *raidPtr;\n\nRF_ReconParityStripeStatus_t *\nrf_LookupRUStatus(\n    RF_Raid_t * raidPtr,\n    RF_PSStatusHeader_t * pssTable,\n    RF_StripeNum_t psID,\n    RF_ReconUnitNum_t which_ru,\n    RF_PSSFlags_t flags,\t/* whether or not to create it if it doesn't\n\t\t\t\t * exist + what flags to set initially */\n    int *created)\n{\n\tRF_PSStatusHeader_t *hdr = &pssTable[RF_HASH_PSID(raidPtr, psID)];\n\tRF_ReconParityStripeStatus_t *p, *pssPtr = hdr->chain;\n\n\t*created = 0;\n\tfor (p = pssPtr; p; p = p->next) {\n\t\tif (p->parityStripeID == psID && p->which_ru == which_ru)\n\t\t\tbreak;\n\t}\n\n\tif (!p && (flags & RF_PSS_CREATE)) {\n\t\tDprintf2(\"PSS: creating pss for psid %ld ru %d\\n\", psID, which_ru);\n\t\tp = rf_AllocPSStatus(raidPtr);\n\t\tp->next = hdr->chain;\n\t\thdr->chain = p;\n\n\t\tp->parityStripeID = psID;\n\t\tp->which_ru = which_ru;\n\t\tp->flags = flags;\n\t\tp->rbuf = NULL;\n\t\tp->writeRbuf = NULL;\n\t\tp->blockCount = 0;\n\t\tp->procWaitList = NULL;\n\t\tp->blockWaitList = NULL;\n\t\tp->bufWaitList = NULL;\n\t\t*created = 1;\n\t} else\n\t\tif (p) {\t/* we didn't create, but we want to specify\n\t\t\t\t * some new status */\n\t\t\tp->flags |= flags;\t/* add in whatever flags we're\n\t\t\t\t\t\t * specifying */\n\t\t}\n\tif (p && (flags & RF_PSS_RECON_BLOCKED)) {\n\t\tp->blockCount++;/* if we're asking to block recon, bump the\n\t\t\t\t * count */\n\t\tDprintf3(\"raid%d: Blocked recon on psid %ld.  count now %d\\n\",\n\t\t\t raidPtr->raidid, psID, p->blockCount);\n\t}\n\treturn (p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_LOCK_MUTEX",
          "args": [
            "reconCtrlPtr->rb_mutex"
          ],
          "line": 584
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_LOCK_PSS_MUTEX",
          "args": [
            "raidPtr",
            "rbuf->row",
            "rbuf->parityStripeID"
          ],
          "line": 582
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rbuf->col != reconCtrlPtr->fcol"
          ],
          "line": 566
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rbuf"
          ],
          "line": 565
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nint \nrf_SubmitReconBufferRAID1(rbuf, keep_it, use_committed)\n\tRF_ReconBuffer_t *rbuf;\t/* the recon buffer to submit */\n\tint     keep_it;\t/* whether we can keep this buffer or we have\n\t\t\t\t * to return it */\n\tint     use_committed;\t/* whether to use a committed or an available\n\t\t\t\t * recon buffer */\n{\n\tRF_ReconParityStripeStatus_t *pssPtr;\n\tRF_ReconCtrl_t *reconCtrlPtr;\n\tRF_RaidLayout_t *layoutPtr;\n\tint     retcode, created;\n\tRF_CallbackDesc_t *cb, *p;\n\tRF_ReconBuffer_t *t;\n\tRF_Raid_t *raidPtr;\n\tcaddr_t ta;\n\n\tretcode = 0;\n\tcreated = 0;\n\n\traidPtr = rbuf->raidPtr;\n\tlayoutPtr = &raidPtr->Layout;\n\treconCtrlPtr = raidPtr->reconControl[rbuf->row];\n\n\tRF_ASSERT(rbuf);\n\tRF_ASSERT(rbuf->col != reconCtrlPtr->fcol);\n\n\tif (rf_reconbufferDebug) {\n\t\tprintf(\"raid%d: RAID1 reconbuffer submission r%d c%d psid %ld ru%d (failed offset %ld)\\n\",\n\t\t       raidPtr->raidid, rbuf->row, rbuf->col, \n\t\t       (long) rbuf->parityStripeID, rbuf->which_ru,\n\t\t    (long) rbuf->failedDiskSectorOffset);\n\t}\n\tif (rf_reconDebug) {\n\t\tprintf(\"RAID1 reconbuffer submit psid %ld buf %lx\\n\",\n\t\t    (long) rbuf->parityStripeID, (long) rbuf->buffer);\n\t\tprintf(\"RAID1 psid %ld   %02x %02x %02x %02x %02x\\n\",\n\t\t    (long) rbuf->parityStripeID,\n\t\t    rbuf->buffer[0], rbuf->buffer[1], rbuf->buffer[2], rbuf->buffer[3],\n\t\t    rbuf->buffer[4]);\n\t}\n\tRF_LOCK_PSS_MUTEX(raidPtr, rbuf->row, rbuf->parityStripeID);\n\n\tRF_LOCK_MUTEX(reconCtrlPtr->rb_mutex);\n\n\tpssPtr = rf_LookupRUStatus(raidPtr, reconCtrlPtr->pssTable,\n\t    rbuf->parityStripeID, rbuf->which_ru, RF_PSS_NONE, &created);\n\tRF_ASSERT(pssPtr);\t/* if it didn't exist, we wouldn't have gotten\n\t\t\t\t * an rbuf for it */\n\n\t/*\n         * Since this is simple mirroring, the first submission for a stripe is also\n         * treated as the last.\n         */\n\n\tt = NULL;\n\tif (keep_it) {\n\t\tif (rf_reconbufferDebug) {\n\t\t\tprintf(\"raid%d: RAID1 rbuf submission: keeping rbuf\\n\", \n\t\t\t       raidPtr->raidid);\n\t\t}\n\t\tt = rbuf;\n\t} else {\n\t\tif (use_committed) {\n\t\t\tif (rf_reconbufferDebug) {\n\t\t\t\tprintf(\"raid%d: RAID1 rbuf submission: using committed rbuf\\n\", raidPtr->raidid);\n\t\t\t}\n\t\t\tt = reconCtrlPtr->committedRbufs;\n\t\t\tRF_ASSERT(t);\n\t\t\treconCtrlPtr->committedRbufs = t->next;\n\t\t\tt->next = NULL;\n\t\t} else\n\t\t\tif (reconCtrlPtr->floatingRbufs) {\n\t\t\t\tif (rf_reconbufferDebug) {\n\t\t\t\t\tprintf(\"raid%d: RAID1 rbuf submission: using floating rbuf\\n\", raidPtr->raidid);\n\t\t\t\t}\n\t\t\t\tt = reconCtrlPtr->floatingRbufs;\n\t\t\t\treconCtrlPtr->floatingRbufs = t->next;\n\t\t\t\tt->next = NULL;\n\t\t\t}\n\t}\n\tif (t == NULL) {\n\t\tif (rf_reconbufferDebug) {\n\t\t\tprintf(\"raid%d: RAID1 rbuf submission: waiting for rbuf\\n\", raidPtr->raidid);\n\t\t}\n\t\tRF_ASSERT((keep_it == 0) && (use_committed == 0));\n\t\traidPtr->procsInBufWait++;\n\t\tif ((raidPtr->procsInBufWait == (raidPtr->numCol - 1))\n\t\t    && (raidPtr->numFullReconBuffers == 0)) {\n\t\t\t/* ruh-ro */\n\t\t\tRF_ERRORMSG(\"Buffer wait deadlock\\n\");\n\t\t\trf_PrintPSStatusTable(raidPtr, rbuf->row);\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpssPtr->flags |= RF_PSS_BUFFERWAIT;\n\t\tcb = rf_AllocCallbackDesc();\n\t\tcb->row = rbuf->row;\n\t\tcb->col = rbuf->col;\n\t\tcb->callbackArg.v = rbuf->parityStripeID;\n\t\tcb->callbackArg2.v = rbuf->which_ru;\n\t\tcb->next = NULL;\n\t\tif (reconCtrlPtr->bufferWaitList == NULL) {\n\t\t\t/* we are the wait list- lucky us */\n\t\t\treconCtrlPtr->bufferWaitList = cb;\n\t\t} else {\n\t\t\t/* append to wait list */\n\t\t\tfor (p = reconCtrlPtr->bufferWaitList; p->next; p = p->next);\n\t\t\tp->next = cb;\n\t\t}\n\t\tretcode = 1;\n\t\tgoto out;\n\t}\n\tif (t != rbuf) {\n\t\tt->row = rbuf->row;\n\t\tt->col = reconCtrlPtr->fcol;\n\t\tt->parityStripeID = rbuf->parityStripeID;\n\t\tt->which_ru = rbuf->which_ru;\n\t\tt->failedDiskSectorOffset = rbuf->failedDiskSectorOffset;\n\t\tt->spRow = rbuf->spRow;\n\t\tt->spCol = rbuf->spCol;\n\t\tt->spOffset = rbuf->spOffset;\n\t\t/* Swap buffers. DANCE! */\n\t\tta = t->buffer;\n\t\tt->buffer = rbuf->buffer;\n\t\trbuf->buffer = ta;\n\t}\n\t/*\n         * Use the rbuf we've been given as the target.\n         */\n\tRF_ASSERT(pssPtr->rbuf == NULL);\n\tpssPtr->rbuf = t;\n\n\tt->count = 1;\n\t/*\n         * Below, we use 1 for numDataCol (which is equal to the count in the\n         * previous line), so we'll always be done.\n         */\n\trf_CheckForFullRbuf(raidPtr, reconCtrlPtr, pssPtr, 1);\n\nout:\n\tRF_UNLOCK_PSS_MUTEX(raidPtr, rbuf->row, rbuf->parityStripeID);\n\tRF_UNLOCK_MUTEX(reconCtrlPtr->rb_mutex);\n\tif (rf_reconbufferDebug) {\n\t\tprintf(\"raid%d: RAID1 rbuf submission: returning %d\\n\", \n\t\t       raidPtr->raidid, retcode);\n\t}\n\treturn (retcode);\n}"
  },
  {
    "function_name": "rf_VerifyParityRAID1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "273-539",
    "snippet": "int \nrf_VerifyParityRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t raidAddr,\n    RF_PhysDiskAddr_t * parityPDA,\n    int correct_it,\n    RF_RaidAccessFlags_t flags)\n{\n\tint     nbytes, bcount, stripeWidth, ret, i, j, nbad, *bbufs;\n\tRF_DagNode_t *blockNode, *unblockNode, *wrBlock;\n\tRF_DagHeader_t *rd_dag_h, *wr_dag_h;\n\tRF_AccessStripeMapHeader_t *asm_h;\n\tRF_AllocListElem_t *allocList;\n\tRF_AccTraceEntry_t tracerec;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_AccessStripeMap_t *aasm;\n\tRF_SectorCount_t nsector;\n\tRF_RaidAddr_t startAddr;\n\tchar   *buf, *buf1, *buf2;\n\tRF_PhysDiskAddr_t *pda;\n\tRF_StripeNum_t psID;\n\tRF_MCPair_t *mcpair;\n\n\tlayoutPtr = &raidPtr->Layout;\n\tstartAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, raidAddr);\n\tnsector = parityPDA->numSector;\n\tnbytes = rf_RaidAddressToByte(raidPtr, nsector);\n\tpsID = rf_RaidAddressToParityStripeID(layoutPtr, raidAddr, &which_ru);\n\n\tasm_h = NULL;\n\trd_dag_h = wr_dag_h = NULL;\n\tmcpair = NULL;\n\n\tret = RF_PARITY_COULD_NOT_VERIFY;\n\n\trf_MakeAllocList(allocList);\n\tif (allocList == NULL)\n\t\treturn (RF_PARITY_COULD_NOT_VERIFY);\n\tmcpair = rf_AllocMCPair();\n\tif (mcpair == NULL)\n\t\tgoto done;\n\tRF_ASSERT(layoutPtr->numDataCol == layoutPtr->numParityCol);\n\tstripeWidth = layoutPtr->numDataCol + layoutPtr->numParityCol;\n\tbcount = nbytes * (layoutPtr->numDataCol + layoutPtr->numParityCol);\n\tRF_MallocAndAdd(buf, bcount, (char *), allocList);\n\tif (buf == NULL)\n\t\tgoto done;\n\tif (rf_verifyParityDebug) {\n\t\tprintf(\"raid%d: RAID1 parity verify: buf=%lx bcount=%d (%lx - %lx)\\n\",\n\t\t       raidPtr->raidid, (long) buf, bcount, (long) buf, \n\t\t       (long) buf + bcount);\n\t}\n\t/*\n         * Generate a DAG which will read the entire stripe- then we can\n         * just compare data chunks versus \"parity\" chunks.\n         */\n\n\trd_dag_h = rf_MakeSimpleDAG(raidPtr, stripeWidth, nbytes, buf,\n\t    rf_DiskReadFunc, rf_DiskReadUndoFunc, \"Rod\", allocList, flags,\n\t    RF_IO_NORMAL_PRIORITY);\n\tif (rd_dag_h == NULL)\n\t\tgoto done;\n\tblockNode = rd_dag_h->succedents[0];\n\tunblockNode = blockNode->succedents[0]->succedents[0];\n\n\t/*\n         * Map the access to physical disk addresses (PDAs)- this will\n         * get us both a list of data addresses, and \"parity\" addresses\n         * (which are really mirror copies).\n         */\n\tasm_h = rf_MapAccess(raidPtr, startAddr, layoutPtr->dataSectorsPerStripe,\n\t    buf, RF_DONT_REMAP);\n\taasm = asm_h->stripeMap;\n\n\tbuf1 = buf;\n\t/*\n         * Loop through the data blocks, setting up read nodes for each.\n         */\n\tfor (pda = aasm->physInfo, i = 0; i < layoutPtr->numDataCol; i++, pda = pda->next) {\n\t\tRF_ASSERT(pda);\n\n\t\trf_RangeRestrictPDA(raidPtr, parityPDA, pda, 0, 1);\n\n\t\tRF_ASSERT(pda->numSector != 0);\n\t\tif (rf_TryToRedirectPDA(raidPtr, pda, 0)) {\n\t\t\t/* cannot verify parity with dead disk */\n\t\t\tgoto done;\n\t\t}\n\t\tpda->bufPtr = buf1;\n\t\tblockNode->succedents[i]->params[0].p = pda;\n\t\tblockNode->succedents[i]->params[1].p = buf1;\n\t\tblockNode->succedents[i]->params[2].v = psID;\n\t\tblockNode->succedents[i]->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tbuf1 += nbytes;\n\t}\n\tRF_ASSERT(pda == NULL);\n\t/*\n         * keep i, buf1 running\n         *\n         * Loop through parity blocks, setting up read nodes for each.\n         */\n\tfor (pda = aasm->parityInfo; i < layoutPtr->numDataCol + layoutPtr->numParityCol; i++, pda = pda->next) {\n\t\tRF_ASSERT(pda);\n\t\trf_RangeRestrictPDA(raidPtr, parityPDA, pda, 0, 1);\n\t\tRF_ASSERT(pda->numSector != 0);\n\t\tif (rf_TryToRedirectPDA(raidPtr, pda, 0)) {\n\t\t\t/* cannot verify parity with dead disk */\n\t\t\tgoto done;\n\t\t}\n\t\tpda->bufPtr = buf1;\n\t\tblockNode->succedents[i]->params[0].p = pda;\n\t\tblockNode->succedents[i]->params[1].p = buf1;\n\t\tblockNode->succedents[i]->params[2].v = psID;\n\t\tblockNode->succedents[i]->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tbuf1 += nbytes;\n\t}\n\tRF_ASSERT(pda == NULL);\n\n\tbzero((char *) &tracerec, sizeof(tracerec));\n\trd_dag_h->tracerec = &tracerec;\n\n\tif (rf_verifyParityDebug > 1) {\n\t\tprintf(\"raid%d: RAID1 parity verify read dag:\\n\", \n\t\t       raidPtr->raidid);\n\t\trf_PrintDAGList(rd_dag_h);\n\t}\n\tRF_LOCK_MUTEX(mcpair->mutex);\n\tmcpair->flag = 0;\n\trf_DispatchDAG(rd_dag_h, (void (*) (void *)) rf_MCPairWakeupFunc,\n\t    (void *) mcpair);\n\twhile (mcpair->flag == 0) {\n\t\tRF_WAIT_MCPAIR(mcpair);\n\t}\n\tRF_UNLOCK_MUTEX(mcpair->mutex);\n\n\tif (rd_dag_h->status != rf_enable) {\n\t\tRF_ERRORMSG(\"Unable to verify raid1 parity: can't read stripe\\n\");\n\t\tret = RF_PARITY_COULD_NOT_VERIFY;\n\t\tgoto done;\n\t}\n\t/*\n         * buf1 is the beginning of the data blocks chunk\n         * buf2 is the beginning of the parity blocks chunk\n         */\n\tbuf1 = buf;\n\tbuf2 = buf + (nbytes * layoutPtr->numDataCol);\n\tret = RF_PARITY_OKAY;\n\t/*\n         * bbufs is \"bad bufs\"- an array whose entries are the data\n         * column numbers where we had miscompares. (That is, column 0\n         * and column 1 of the array are mirror copies, and are considered\n         * \"data column 0\" for this purpose).\n         */\n\tRF_MallocAndAdd(bbufs, layoutPtr->numParityCol * sizeof(int), (int *),\n\t    allocList);\n\tnbad = 0;\n\t/*\n         * Check data vs \"parity\" (mirror copy).\n         */\n\tfor (i = 0; i < layoutPtr->numDataCol; i++) {\n\t\tif (rf_verifyParityDebug) {\n\t\t\tprintf(\"raid%d: RAID1 parity verify %d bytes: i=%d buf1=%lx buf2=%lx buf=%lx\\n\",\n\t\t\t       raidPtr->raidid, nbytes, i, (long) buf1, \n\t\t\t       (long) buf2, (long) buf);\n\t\t}\n\t\tret = bcmp(buf1, buf2, nbytes);\n\t\tif (ret) {\n\t\t\tif (rf_verifyParityDebug > 1) {\n\t\t\t\tfor (j = 0; j < nbytes; j++) {\n\t\t\t\t\tif (buf1[j] != buf2[j])\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tprintf(\"psid=%ld j=%d\\n\", (long) psID, j);\n\t\t\t\tprintf(\"buf1 %02x %02x %02x %02x %02x\\n\", buf1[0] & 0xff,\n\t\t\t\t    buf1[1] & 0xff, buf1[2] & 0xff, buf1[3] & 0xff, buf1[4] & 0xff);\n\t\t\t\tprintf(\"buf2 %02x %02x %02x %02x %02x\\n\", buf2[0] & 0xff,\n\t\t\t\t    buf2[1] & 0xff, buf2[2] & 0xff, buf2[3] & 0xff, buf2[4] & 0xff);\n\t\t\t}\n\t\t\tif (rf_verifyParityDebug) {\n\t\t\t\tprintf(\"raid%d: RAID1: found bad parity, i=%d\\n\", raidPtr->raidid, i);\n\t\t\t}\n\t\t\t/*\n\t\t         * Parity is bad. Keep track of which columns were bad.\n\t\t         */\n\t\t\tif (bbufs)\n\t\t\t\tbbufs[nbad] = i;\n\t\t\tnbad++;\n\t\t\tret = RF_PARITY_BAD;\n\t\t}\n\t\tbuf1 += nbytes;\n\t\tbuf2 += nbytes;\n\t}\n\n\tif ((ret != RF_PARITY_OKAY) && correct_it) {\n\t\tret = RF_PARITY_COULD_NOT_CORRECT;\n\t\tif (rf_verifyParityDebug) {\n\t\t\tprintf(\"raid%d: RAID1 parity verify: parity not correct\\n\", raidPtr->raidid);\n\t\t}\n\t\tif (bbufs == NULL)\n\t\t\tgoto done;\n\t\t/*\n\t         * Make a DAG with one write node for each bad unit. We'll simply\n\t         * write the contents of the data unit onto the parity unit for\n\t         * correction. (It's possible that the mirror copy was the correct\n\t         * copy, and that we're spooging good data by writing bad over it,\n\t         * but there's no way we can know that.\n\t         */\n\t\twr_dag_h = rf_MakeSimpleDAG(raidPtr, nbad, nbytes, buf,\n\t\t    rf_DiskWriteFunc, rf_DiskWriteUndoFunc, \"Wnp\", allocList, flags,\n\t\t    RF_IO_NORMAL_PRIORITY);\n\t\tif (wr_dag_h == NULL)\n\t\t\tgoto done;\n\t\twrBlock = wr_dag_h->succedents[0];\n\t\t/*\n\t         * Fill in a write node for each bad compare.\n\t         */\n\t\tfor (i = 0; i < nbad; i++) {\n\t\t\tj = i + layoutPtr->numDataCol;\n\t\t\tpda = blockNode->succedents[j]->params[0].p;\n\t\t\tpda->bufPtr = blockNode->succedents[i]->params[1].p;\n\t\t\twrBlock->succedents[i]->params[0].p = pda;\n\t\t\twrBlock->succedents[i]->params[1].p = pda->bufPtr;\n\t\t\twrBlock->succedents[i]->params[2].v = psID;\n\t\t\twrBlock->succedents[0]->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t\tbzero((char *) &tracerec, sizeof(tracerec));\n\t\twr_dag_h->tracerec = &tracerec;\n\t\tif (rf_verifyParityDebug > 1) {\n\t\t\tprintf(\"Parity verify write dag:\\n\");\n\t\t\trf_PrintDAGList(wr_dag_h);\n\t\t}\n\t\tRF_LOCK_MUTEX(mcpair->mutex);\n\t\tmcpair->flag = 0;\n\t\t/* fire off the write DAG */\n\t\trf_DispatchDAG(wr_dag_h, (void (*) (void *)) rf_MCPairWakeupFunc,\n\t\t    (void *) mcpair);\n\t\twhile (!mcpair->flag) {\n\t\t\tRF_WAIT_COND(mcpair->cond, mcpair->mutex);\n\t\t}\n\t\tRF_UNLOCK_MUTEX(mcpair->mutex);\n\t\tif (wr_dag_h->status != rf_enable) {\n\t\t\tRF_ERRORMSG(\"Unable to correct RAID1 parity in VerifyParity\\n\");\n\t\t\tgoto done;\n\t\t}\n\t\tret = RF_PARITY_CORRECTED;\n\t}\ndone:\n\t/*\n         * All done. We might've gotten here without doing part of the function,\n         * so cleanup what we have to and return our running status.\n         */\n\tif (asm_h)\n\t\trf_FreeAccessStripeMap(asm_h);\n\tif (rd_dag_h)\n\t\trf_FreeDAG(rd_dag_h);\n\tif (wr_dag_h)\n\t\trf_FreeDAG(wr_dag_h);\n\tif (mcpair)\n\t\trf_FreeMCPair(mcpair);\n\trf_FreeAllocList(allocList);\n\tif (rf_verifyParityDebug) {\n\t\tprintf(\"raid%d: RAID1 parity verify, returning %d\\n\", \n\t\t       raidPtr->raidid, ret);\n\t}\n\treturn (ret);\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"raid%d: RAID1 parity verify, returning %d\\n\"",
            "raidPtr->raidid",
            "ret"
          ],
          "line": 535
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_FreeAllocList",
          "args": [
            "allocList"
          ],
          "line": 533
        },
        "resolved": true,
        "details": {
          "function_name": "rf_FreeAllocList",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_alloclist.c",
          "lines": "139-164",
          "snippet": "void \nrf_FreeAllocList(l)\n\tRF_AllocListElem_t *l;\n{\n\tint     i;\n\tRF_AllocListElem_t *temp, *p;\n\n\tfor (p = l; p; p = p->next) {\n\t\tRF_ASSERT(p->numPointers >= 0 && p->numPointers <= RF_POINTERS_PER_ALLOC_LIST_ELEMENT);\n\t\tfor (i = 0; i < p->numPointers; i++) {\n\t\t\tRF_ASSERT(p->pointers[i]);\n\t\t\tRF_Free(p->pointers[i], p->sizes[i]);\n\t\t}\n\t}\n\twhile (l) {\n\t\ttemp = l;\n\t\tl = l->next;\n\t\tif (al_free_list_count > RF_AL_FREELIST_MAX) {\n\t\t\tDO_FREE(temp, sizeof(*temp));\n\t\t} else {\n\t\t\ttemp->next = al_free_list;\n\t\t\tal_free_list = temp;\n\t\t\tal_free_list_count++;\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_etimer.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_alloclist.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define RF_AL_FREELIST_MAX 256"
          ],
          "globals_used": [
            "static RF_AllocListElem_t *al_free_list = NULL;",
            "static int al_free_list_count;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_general.h\"\n#include \"rf_etimer.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_alloclist.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define RF_AL_FREELIST_MAX 256\n\nstatic RF_AllocListElem_t *al_free_list = NULL;\nstatic int al_free_list_count;\n\nvoid \nrf_FreeAllocList(l)\n\tRF_AllocListElem_t *l;\n{\n\tint     i;\n\tRF_AllocListElem_t *temp, *p;\n\n\tfor (p = l; p; p = p->next) {\n\t\tRF_ASSERT(p->numPointers >= 0 && p->numPointers <= RF_POINTERS_PER_ALLOC_LIST_ELEMENT);\n\t\tfor (i = 0; i < p->numPointers; i++) {\n\t\t\tRF_ASSERT(p->pointers[i]);\n\t\t\tRF_Free(p->pointers[i], p->sizes[i]);\n\t\t}\n\t}\n\twhile (l) {\n\t\ttemp = l;\n\t\tl = l->next;\n\t\tif (al_free_list_count > RF_AL_FREELIST_MAX) {\n\t\t\tDO_FREE(temp, sizeof(*temp));\n\t\t} else {\n\t\t\ttemp->next = al_free_list;\n\t\t\tal_free_list = temp;\n\t\t\tal_free_list_count++;\n\t\t}\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_FreeMCPair",
          "args": [
            "mcpair"
          ],
          "line": 532
        },
        "resolved": true,
        "details": {
          "function_name": "rf_FreeMCPair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_mcpair.c",
          "lines": "125-130",
          "snippet": "void \nrf_FreeMCPair(t)\n\tRF_MCPair_t *t;\n{\n\tRF_FREELIST_FREE_CLEAN(rf_mcpair_freelist, t, next, clean_mcpair);\n}",
          "includes": [
            "#include <sys/proc.h>",
            "#include \"rf_shutdown.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_mcpair.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static RF_FreeList_t *rf_mcpair_freelist;",
            "static int init_mcpair(RF_MCPair_t *);",
            "static void clean_mcpair(RF_MCPair_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/proc.h>\n#include \"rf_shutdown.h\"\n#include \"rf_freelist.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\nstatic RF_FreeList_t *rf_mcpair_freelist;\nstatic int init_mcpair(RF_MCPair_t *);\nstatic void clean_mcpair(RF_MCPair_t *);\n\nvoid \nrf_FreeMCPair(t)\n\tRF_MCPair_t *t;\n{\n\tRF_FREELIST_FREE_CLEAN(rf_mcpair_freelist, t, next, clean_mcpair);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_FreeDAG",
          "args": [
            "wr_dag_h"
          ],
          "line": 530
        },
        "resolved": true,
        "details": {
          "function_name": "rf_FreeDAGHeader",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "257-261",
          "snippet": "void \nrf_FreeDAGHeader(RF_DagHeader_t * dh)\n{\n\tRF_FREELIST_FREE(rf_dagh_freelist, dh, next);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);",
            "static RF_FreeList_t *rf_dagh_freelist;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\nstatic RF_FreeList_t *rf_dagh_freelist;\n\nvoid \nrf_FreeDAGHeader(RF_DagHeader_t * dh)\n{\n\tRF_FREELIST_FREE(rf_dagh_freelist, dh, next);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_FreeAccessStripeMap",
          "args": [
            "asm_h"
          ],
          "line": 526
        },
        "resolved": true,
        "details": {
          "function_name": "rf_FreeAccessStripeMap",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_map.c",
          "lines": "537-598",
          "snippet": "void \nrf_FreeAccessStripeMap(hdr)\n\tRF_AccessStripeMapHeader_t *hdr;\n{\n\tRF_AccessStripeMap_t *p, *pt = NULL;\n\tRF_PhysDiskAddr_t *pdp, *trailer, *pdaList = NULL, *pdaEnd = NULL;\n\tint     count = 0, t, asm_count = 0;\n\n\tfor (p = hdr->stripeMap; p; p = p->next) {\n\n\t\t/* link the 3 pda lists into the accumulating pda list */\n\n\t\tif (!pdaList)\n\t\t\tpdaList = p->qInfo;\n\t\telse\n\t\t\tpdaEnd->next = p->qInfo;\n\t\tfor (trailer = NULL, pdp = p->qInfo; pdp;) {\n\t\t\ttrailer = pdp;\n\t\t\tpdp = pdp->next;\n\t\t\tcount++;\n\t\t}\n\t\tif (trailer)\n\t\t\tpdaEnd = trailer;\n\n\t\tif (!pdaList)\n\t\t\tpdaList = p->parityInfo;\n\t\telse\n\t\t\tpdaEnd->next = p->parityInfo;\n\t\tfor (trailer = NULL, pdp = p->parityInfo; pdp;) {\n\t\t\ttrailer = pdp;\n\t\t\tpdp = pdp->next;\n\t\t\tcount++;\n\t\t}\n\t\tif (trailer)\n\t\t\tpdaEnd = trailer;\n\n\t\tif (!pdaList)\n\t\t\tpdaList = p->physInfo;\n\t\telse\n\t\t\tpdaEnd->next = p->physInfo;\n\t\tfor (trailer = NULL, pdp = p->physInfo; pdp;) {\n\t\t\ttrailer = pdp;\n\t\t\tpdp = pdp->next;\n\t\t\tcount++;\n\t\t}\n\t\tif (trailer)\n\t\t\tpdaEnd = trailer;\n\n\t\tpt = p;\n\t\tasm_count++;\n\t}\n\n\t/* debug only */\n\tfor (t = 0, pdp = pdaList; pdp; pdp = pdp->next)\n\t\tt++;\n\tRF_ASSERT(t == count);\n\n\tif (pdaList)\n\t\trf_FreePDAList(pdaList, pdaEnd, count);\n\trf_FreeASMList(hdr->stripeMap, pt, asm_count);\n\trf_FreeAccessStripeMapHeader(hdr);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_FreePDAList(RF_PhysDiskAddr_t * start, RF_PhysDiskAddr_t * end, int count);",
            "static void \nrf_FreeASMList(RF_AccessStripeMap_t * start, RF_AccessStripeMap_t * end,\n    int count);",
            "RF_PhysDiskAddr_t *\nrf_DuplicatePDA(pda)\n\tRF_PhysDiskAddr_t *pda;",
            "RF_PhysDiskAddr_t *\nrf_AllocPDAList(count)\n\tint     count;",
            "RF_AccessStripeMap_t *\nrf_AllocASMList(count)\n\tint     count;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_freelist.h\"\n#include \"rf_map.h\"\n#include \"rf_general.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\nstatic void rf_FreePDAList(RF_PhysDiskAddr_t * start, RF_PhysDiskAddr_t * end, int count);\nstatic void \nrf_FreeASMList(RF_AccessStripeMap_t * start, RF_AccessStripeMap_t * end,\n    int count);\nRF_PhysDiskAddr_t *\nrf_DuplicatePDA(pda)\n\tRF_PhysDiskAddr_t *pda;\nRF_PhysDiskAddr_t *\nrf_AllocPDAList(count)\n\tint     count;\nRF_AccessStripeMap_t *\nrf_AllocASMList(count)\n\tint     count;\n\nvoid \nrf_FreeAccessStripeMap(hdr)\n\tRF_AccessStripeMapHeader_t *hdr;\n{\n\tRF_AccessStripeMap_t *p, *pt = NULL;\n\tRF_PhysDiskAddr_t *pdp, *trailer, *pdaList = NULL, *pdaEnd = NULL;\n\tint     count = 0, t, asm_count = 0;\n\n\tfor (p = hdr->stripeMap; p; p = p->next) {\n\n\t\t/* link the 3 pda lists into the accumulating pda list */\n\n\t\tif (!pdaList)\n\t\t\tpdaList = p->qInfo;\n\t\telse\n\t\t\tpdaEnd->next = p->qInfo;\n\t\tfor (trailer = NULL, pdp = p->qInfo; pdp;) {\n\t\t\ttrailer = pdp;\n\t\t\tpdp = pdp->next;\n\t\t\tcount++;\n\t\t}\n\t\tif (trailer)\n\t\t\tpdaEnd = trailer;\n\n\t\tif (!pdaList)\n\t\t\tpdaList = p->parityInfo;\n\t\telse\n\t\t\tpdaEnd->next = p->parityInfo;\n\t\tfor (trailer = NULL, pdp = p->parityInfo; pdp;) {\n\t\t\ttrailer = pdp;\n\t\t\tpdp = pdp->next;\n\t\t\tcount++;\n\t\t}\n\t\tif (trailer)\n\t\t\tpdaEnd = trailer;\n\n\t\tif (!pdaList)\n\t\t\tpdaList = p->physInfo;\n\t\telse\n\t\t\tpdaEnd->next = p->physInfo;\n\t\tfor (trailer = NULL, pdp = p->physInfo; pdp;) {\n\t\t\ttrailer = pdp;\n\t\t\tpdp = pdp->next;\n\t\t\tcount++;\n\t\t}\n\t\tif (trailer)\n\t\t\tpdaEnd = trailer;\n\n\t\tpt = p;\n\t\tasm_count++;\n\t}\n\n\t/* debug only */\n\tfor (t = 0, pdp = pdaList; pdp; pdp = pdp->next)\n\t\tt++;\n\tRF_ASSERT(t == count);\n\n\tif (pdaList)\n\t\trf_FreePDAList(pdaList, pdaEnd, count);\n\trf_FreeASMList(hdr->stripeMap, pt, asm_count);\n\trf_FreeAccessStripeMapHeader(hdr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ERRORMSG",
          "args": [
            "\"Unable to correct RAID1 parity in VerifyParity\\n\""
          ],
          "line": 515
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_UNLOCK_MUTEX",
          "args": [
            "mcpair->mutex"
          ],
          "line": 513
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_WAIT_COND",
          "args": [
            "mcpair->cond",
            "mcpair->mutex"
          ],
          "line": 511
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_DispatchDAG",
          "args": [
            "wr_dag_h",
            "(void (*) (void *)) rf_MCPairWakeupFunc",
            "(void *) mcpair"
          ],
          "line": 508
        },
        "resolved": true,
        "details": {
          "function_name": "rf_DispatchDAG",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_engine.c",
          "lines": "668-695",
          "snippet": "int \nrf_DispatchDAG(\n    RF_DagHeader_t * dag,\n    void (*cbFunc) (void *),\n    void *cbArg)\n{\n\tRF_Raid_t *raidPtr;\n\n\traidPtr = dag->raidPtr;\n\tif (dag->tracerec) {\n\t\tRF_ETIMER_START(dag->tracerec->timer);\n\t}\n\tif (rf_engineDebug || rf_validateDAGDebug) {\n\t\tif (rf_ValidateDAG(dag))\n\t\t\tRF_PANIC();\n\t}\n\tif (rf_engineDebug) {\n\t\tprintf(\"raid%d: Entering DispatchDAG\\n\", raidPtr->raidid);\n\t}\n\traidPtr->dags_in_flight++;\t/* debug only:  blow off proper\n\t\t\t\t\t * locking */\n\tdag->cbFunc = cbFunc;\n\tdag->cbArg = cbArg;\n\tdag->numNodesCompleted = 0;\n\tdag->status = rf_enable;\n\tFireNodeArray(dag->numSuccedents, dag->succedents);\n\treturn (1);\n}",
          "includes": [
            "#include \"rf_raid.h\"",
            "#include \"rf_shutdown.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_etimer.h\"",
            "#include \"rf_engine.h\"",
            "#include \"rf_dag.h\"",
            "#include <sys/errno.h>",
            "#include \"rf_threadstuff.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_raid.h\"\n#include \"rf_shutdown.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_general.h\"\n#include \"rf_etimer.h\"\n#include \"rf_engine.h\"\n#include \"rf_dag.h\"\n#include <sys/errno.h>\n#include \"rf_threadstuff.h\"\n\nint \nrf_DispatchDAG(\n    RF_DagHeader_t * dag,\n    void (*cbFunc) (void *),\n    void *cbArg)\n{\n\tRF_Raid_t *raidPtr;\n\n\traidPtr = dag->raidPtr;\n\tif (dag->tracerec) {\n\t\tRF_ETIMER_START(dag->tracerec->timer);\n\t}\n\tif (rf_engineDebug || rf_validateDAGDebug) {\n\t\tif (rf_ValidateDAG(dag))\n\t\t\tRF_PANIC();\n\t}\n\tif (rf_engineDebug) {\n\t\tprintf(\"raid%d: Entering DispatchDAG\\n\", raidPtr->raidid);\n\t}\n\traidPtr->dags_in_flight++;\t/* debug only:  blow off proper\n\t\t\t\t\t * locking */\n\tdag->cbFunc = cbFunc;\n\tdag->cbArg = cbArg;\n\tdag->numNodesCompleted = 0;\n\tdag->status = rf_enable;\n\tFireNodeArray(dag->numSuccedents, dag->succedents);\n\treturn (1);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_LOCK_MUTEX",
          "args": [
            "mcpair->mutex"
          ],
          "line": 505
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_PrintDAGList",
          "args": [
            "wr_dag_h"
          ],
          "line": 503
        },
        "resolved": true,
        "details": {
          "function_name": "rf_PrintDAGList",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "509-519",
          "snippet": "void \nrf_PrintDAGList(RF_DagHeader_t * dag_h)\n{\n\tint     i = 0;\n\n\tfor (; dag_h; dag_h = dag_h->next) {\n\t\trf_AssignNodeNums(dag_h);\n\t\tprintf(\"\\n\\nDAG %d IN LIST:\\n\", i++);\n\t\trf_PrintDAG(dag_h);\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_PrintDAGList(RF_DagHeader_t * dag_h)\n{\n\tint     i = 0;\n\n\tfor (; dag_h; dag_h = dag_h->next) {\n\t\trf_AssignNodeNums(dag_h);\n\t\tprintf(\"\\n\\nDAG %d IN LIST:\\n\", i++);\n\t\trf_PrintDAG(dag_h);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "bzero",
          "args": [
            "(char *) &tracerec",
            "sizeof(tracerec)"
          ],
          "line": 499
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 497
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_MakeSimpleDAG",
          "args": [
            "raidPtr",
            "nbad",
            "nbytes",
            "buf",
            "rf_DiskWriteFunc",
            "rf_DiskWriteUndoFunc",
            "\"Wnp\"",
            "allocList",
            "flags",
            "RF_IO_NORMAL_PRIORITY"
          ],
          "line": 481
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "bcmp",
          "args": [
            "buf1",
            "buf2",
            "nbytes"
          ],
          "line": 439
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "bbufs",
            "layoutPtr->numParityCol * sizeof(int)",
            "(int *),\n\t    allocList"
          ],
          "line": 427
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ERRORMSG",
          "args": [
            "\"Unable to verify raid1 parity: can't read stripe\\n\""
          ],
          "line": 410
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_UNLOCK_MUTEX",
          "args": [
            "mcpair->mutex"
          ],
          "line": 407
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_WAIT_MCPAIR",
          "args": [
            "mcpair"
          ],
          "line": 405
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_LOCK_MUTEX",
          "args": [
            "mcpair->mutex"
          ],
          "line": 400
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "bzero",
          "args": [
            "(char *) &tracerec",
            "sizeof(tracerec)"
          ],
          "line": 392
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda == NULL"
          ],
          "line": 390
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 387
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_TryToRedirectPDA",
          "args": [
            "raidPtr",
            "pda",
            "0"
          ],
          "line": 379
        },
        "resolved": true,
        "details": {
          "function_name": "rf_TryToRedirectPDA",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_parityscan.c",
          "lines": "315-348",
          "snippet": "int \nrf_TryToRedirectPDA(raidPtr, pda, parity)\n\tRF_Raid_t *raidPtr;\n\tRF_PhysDiskAddr_t *pda;\n\tint     parity;\n{\n\tif (raidPtr->Disks[pda->row][pda->col].status == rf_ds_reconstructing) {\n\t\tif (rf_CheckRUReconstructed(raidPtr->reconControl[pda->row]->reconMap, pda->startSector)) {\n\t\t\tif (raidPtr->Layout.map->flags & RF_DISTRIBUTE_SPARE) {\n\t\t\t\tRF_RowCol_t or = pda->row, oc = pda->col;\n\t\t\t\tRF_SectorNum_t os = pda->startSector;\n\t\t\t\tif (parity) {\n\t\t\t\t\t(raidPtr->Layout.map->MapParity) (raidPtr, pda->raidAddress, &pda->row, &pda->col, &pda->startSector, RF_REMAP);\n\t\t\t\t\tif (rf_verifyParityDebug)\n\t\t\t\t\t\tprintf(\"VerifyParity: Redir P r %d c %d sect %ld -> r %d c %d sect %ld\\n\",\n\t\t\t\t\t\t    or, oc, (long) os, pda->row, pda->col, (long) pda->startSector);\n\t\t\t\t} else {\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda->raidAddress, &pda->row, &pda->col, &pda->startSector, RF_REMAP);\n\t\t\t\t\tif (rf_verifyParityDebug)\n\t\t\t\t\t\tprintf(\"VerifyParity: Redir D r %d c %d sect %ld -> r %d c %d sect %ld\\n\",\n\t\t\t\t\t\t    or, oc, (long) os, pda->row, pda->col, (long) pda->startSector);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tRF_RowCol_t spRow = raidPtr->Disks[pda->row][pda->col].spareRow;\n\t\t\t\tRF_RowCol_t spCol = raidPtr->Disks[pda->row][pda->col].spareCol;\n\t\t\t\tpda->row = spRow;\n\t\t\t\tpda->col = spCol;\n\t\t\t}\n\t\t}\n\t}\n\tif (RF_DEAD_DISK(raidPtr->Disks[pda->row][pda->col].status))\n\t\treturn (1);\n\treturn (0);\n}",
          "includes": [
            "#include \"rf_map.h\"",
            "#include \"rf_parityscan.h\"",
            "#include \"rf_engine.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_mcpair.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "RF_DagHeader_t *\nrf_MakeSimpleDAG(raidPtr, nNodes, bytesPerSU, databuf, doFunc, undoFunc, name, alloclist, flags, priority)\n\tRF_Raid_t *raidPtr;",
            "RF_RaidAccessFlags_t flags;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_map.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_engine.h\"\n#include \"rf_general.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nRF_DagHeader_t *\nrf_MakeSimpleDAG(raidPtr, nNodes, bytesPerSU, databuf, doFunc, undoFunc, name, alloclist, flags, priority)\n\tRF_Raid_t *raidPtr;\nRF_RaidAccessFlags_t flags;\n\nint \nrf_TryToRedirectPDA(raidPtr, pda, parity)\n\tRF_Raid_t *raidPtr;\n\tRF_PhysDiskAddr_t *pda;\n\tint     parity;\n{\n\tif (raidPtr->Disks[pda->row][pda->col].status == rf_ds_reconstructing) {\n\t\tif (rf_CheckRUReconstructed(raidPtr->reconControl[pda->row]->reconMap, pda->startSector)) {\n\t\t\tif (raidPtr->Layout.map->flags & RF_DISTRIBUTE_SPARE) {\n\t\t\t\tRF_RowCol_t or = pda->row, oc = pda->col;\n\t\t\t\tRF_SectorNum_t os = pda->startSector;\n\t\t\t\tif (parity) {\n\t\t\t\t\t(raidPtr->Layout.map->MapParity) (raidPtr, pda->raidAddress, &pda->row, &pda->col, &pda->startSector, RF_REMAP);\n\t\t\t\t\tif (rf_verifyParityDebug)\n\t\t\t\t\t\tprintf(\"VerifyParity: Redir P r %d c %d sect %ld -> r %d c %d sect %ld\\n\",\n\t\t\t\t\t\t    or, oc, (long) os, pda->row, pda->col, (long) pda->startSector);\n\t\t\t\t} else {\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda->raidAddress, &pda->row, &pda->col, &pda->startSector, RF_REMAP);\n\t\t\t\t\tif (rf_verifyParityDebug)\n\t\t\t\t\t\tprintf(\"VerifyParity: Redir D r %d c %d sect %ld -> r %d c %d sect %ld\\n\",\n\t\t\t\t\t\t    or, oc, (long) os, pda->row, pda->col, (long) pda->startSector);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tRF_RowCol_t spRow = raidPtr->Disks[pda->row][pda->col].spareRow;\n\t\t\t\tRF_RowCol_t spCol = raidPtr->Disks[pda->row][pda->col].spareCol;\n\t\t\t\tpda->row = spRow;\n\t\t\t\tpda->col = spCol;\n\t\t\t}\n\t\t}\n\t}\n\tif (RF_DEAD_DISK(raidPtr->Disks[pda->row][pda->col].status))\n\t\treturn (1);\n\treturn (0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda->numSector != 0"
          ],
          "line": 378
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RangeRestrictPDA",
          "args": [
            "raidPtr",
            "parityPDA",
            "pda",
            "0",
            "1"
          ],
          "line": 377
        },
        "resolved": true,
        "details": {
          "function_name": "rf_RangeRestrictPDA",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "1050-1075",
          "snippet": "void \nrf_RangeRestrictPDA(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * src,\n    RF_PhysDiskAddr_t * dest,\n    int dobuffer,\n    int doraidaddr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_SectorNum_t soffs = rf_StripeUnitOffset(layoutPtr, src->startSector);\n\tRF_SectorNum_t doffs = rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\tRF_SectorNum_t send = rf_StripeUnitOffset(layoutPtr, src->startSector + src->numSector - 1);\t/* use -1 to be sure we\n\t\t\t\t\t\t\t\t\t\t\t\t\t * stay within SU */\n\tRF_SectorNum_t dend = rf_StripeUnitOffset(layoutPtr, dest->startSector + dest->numSector - 1);\n\tRF_SectorNum_t subAddr = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->startSector);\t/* stripe unit boundary */\n\n\tdest->startSector = subAddr + RF_MAX(soffs, doffs);\n\tdest->numSector = subAddr + RF_MIN(send, dend) + 1 - dest->startSector;\n\n\tif (dobuffer)\n\t\tdest->bufPtr += (soffs > doffs) ? rf_RaidAddressToByte(raidPtr, soffs - doffs) : 0;\n\tif (doraidaddr) {\n\t\tdest->raidAddress = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->raidAddress) +\n\t\t    rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_RangeRestrictPDA(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * src,\n    RF_PhysDiskAddr_t * dest,\n    int dobuffer,\n    int doraidaddr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_SectorNum_t soffs = rf_StripeUnitOffset(layoutPtr, src->startSector);\n\tRF_SectorNum_t doffs = rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\tRF_SectorNum_t send = rf_StripeUnitOffset(layoutPtr, src->startSector + src->numSector - 1);\t/* use -1 to be sure we\n\t\t\t\t\t\t\t\t\t\t\t\t\t * stay within SU */\n\tRF_SectorNum_t dend = rf_StripeUnitOffset(layoutPtr, dest->startSector + dest->numSector - 1);\n\tRF_SectorNum_t subAddr = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->startSector);\t/* stripe unit boundary */\n\n\tdest->startSector = subAddr + RF_MAX(soffs, doffs);\n\tdest->numSector = subAddr + RF_MIN(send, dend) + 1 - dest->startSector;\n\n\tif (dobuffer)\n\t\tdest->bufPtr += (soffs > doffs) ? rf_RaidAddressToByte(raidPtr, soffs - doffs) : 0;\n\tif (doraidaddr) {\n\t\tdest->raidAddress = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->raidAddress) +\n\t\t    rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 376
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda == NULL"
          ],
          "line": 369
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 366
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda->numSector != 0"
          ],
          "line": 357
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 353
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_MapAccess",
          "args": [
            "raidPtr",
            "startAddr",
            "layoutPtr->dataSectorsPerStripe",
            "buf",
            "RF_DONT_REMAP"
          ],
          "line": 344
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_MakeSimpleDAG",
          "args": [
            "raidPtr",
            "stripeWidth",
            "nbytes",
            "buf",
            "rf_DiskReadFunc",
            "rf_DiskReadUndoFunc",
            "\"Rod\"",
            "allocList",
            "flags",
            "RF_IO_NORMAL_PRIORITY"
          ],
          "line": 331
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "buf",
            "bcount",
            "(char *), allocList"
          ],
          "line": 318
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "layoutPtr->numDataCol == layoutPtr->numParityCol"
          ],
          "line": 315
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_AllocMCPair",
          "args": [],
          "line": 312
        },
        "resolved": true,
        "details": {
          "function_name": "rf_AllocMCPair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_mcpair.c",
          "lines": "112-123",
          "snippet": "RF_MCPair_t *\nrf_AllocMCPair()\n{\n\tRF_MCPair_t *t;\n\n\tRF_FREELIST_GET_INIT(rf_mcpair_freelist, t, next, (RF_MCPair_t *), init_mcpair);\n\tif (t) {\n\t\tt->flag = 0;\n\t\tt->next = NULL;\n\t}\n\treturn (t);\n}",
          "includes": [
            "#include <sys/proc.h>",
            "#include \"rf_shutdown.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_mcpair.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static RF_FreeList_t *rf_mcpair_freelist;",
            "static int init_mcpair(RF_MCPair_t *);",
            "static void clean_mcpair(RF_MCPair_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/proc.h>\n#include \"rf_shutdown.h\"\n#include \"rf_freelist.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\nstatic RF_FreeList_t *rf_mcpair_freelist;\nstatic int init_mcpair(RF_MCPair_t *);\nstatic void clean_mcpair(RF_MCPair_t *);\n\nRF_MCPair_t *\nrf_AllocMCPair()\n{\n\tRF_MCPair_t *t;\n\n\tRF_FREELIST_GET_INIT(rf_mcpair_freelist, t, next, (RF_MCPair_t *), init_mcpair);\n\tif (t) {\n\t\tt->flag = 0;\n\t\tt->next = NULL;\n\t}\n\treturn (t);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_MakeAllocList",
          "args": [
            "allocList"
          ],
          "line": 309
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "layoutPtr",
            "raidAddr",
            "&which_ru"
          ],
          "line": 301
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "nsector"
          ],
          "line": 300
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressOfPrevStripeBoundary",
          "args": [
            "layoutPtr",
            "raidAddr"
          ],
          "line": 298
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nint \nrf_VerifyParityRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t raidAddr,\n    RF_PhysDiskAddr_t * parityPDA,\n    int correct_it,\n    RF_RaidAccessFlags_t flags)\n{\n\tint     nbytes, bcount, stripeWidth, ret, i, j, nbad, *bbufs;\n\tRF_DagNode_t *blockNode, *unblockNode, *wrBlock;\n\tRF_DagHeader_t *rd_dag_h, *wr_dag_h;\n\tRF_AccessStripeMapHeader_t *asm_h;\n\tRF_AllocListElem_t *allocList;\n\tRF_AccTraceEntry_t tracerec;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_AccessStripeMap_t *aasm;\n\tRF_SectorCount_t nsector;\n\tRF_RaidAddr_t startAddr;\n\tchar   *buf, *buf1, *buf2;\n\tRF_PhysDiskAddr_t *pda;\n\tRF_StripeNum_t psID;\n\tRF_MCPair_t *mcpair;\n\n\tlayoutPtr = &raidPtr->Layout;\n\tstartAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, raidAddr);\n\tnsector = parityPDA->numSector;\n\tnbytes = rf_RaidAddressToByte(raidPtr, nsector);\n\tpsID = rf_RaidAddressToParityStripeID(layoutPtr, raidAddr, &which_ru);\n\n\tasm_h = NULL;\n\trd_dag_h = wr_dag_h = NULL;\n\tmcpair = NULL;\n\n\tret = RF_PARITY_COULD_NOT_VERIFY;\n\n\trf_MakeAllocList(allocList);\n\tif (allocList == NULL)\n\t\treturn (RF_PARITY_COULD_NOT_VERIFY);\n\tmcpair = rf_AllocMCPair();\n\tif (mcpair == NULL)\n\t\tgoto done;\n\tRF_ASSERT(layoutPtr->numDataCol == layoutPtr->numParityCol);\n\tstripeWidth = layoutPtr->numDataCol + layoutPtr->numParityCol;\n\tbcount = nbytes * (layoutPtr->numDataCol + layoutPtr->numParityCol);\n\tRF_MallocAndAdd(buf, bcount, (char *), allocList);\n\tif (buf == NULL)\n\t\tgoto done;\n\tif (rf_verifyParityDebug) {\n\t\tprintf(\"raid%d: RAID1 parity verify: buf=%lx bcount=%d (%lx - %lx)\\n\",\n\t\t       raidPtr->raidid, (long) buf, bcount, (long) buf, \n\t\t       (long) buf + bcount);\n\t}\n\t/*\n         * Generate a DAG which will read the entire stripe- then we can\n         * just compare data chunks versus \"parity\" chunks.\n         */\n\n\trd_dag_h = rf_MakeSimpleDAG(raidPtr, stripeWidth, nbytes, buf,\n\t    rf_DiskReadFunc, rf_DiskReadUndoFunc, \"Rod\", allocList, flags,\n\t    RF_IO_NORMAL_PRIORITY);\n\tif (rd_dag_h == NULL)\n\t\tgoto done;\n\tblockNode = rd_dag_h->succedents[0];\n\tunblockNode = blockNode->succedents[0]->succedents[0];\n\n\t/*\n         * Map the access to physical disk addresses (PDAs)- this will\n         * get us both a list of data addresses, and \"parity\" addresses\n         * (which are really mirror copies).\n         */\n\tasm_h = rf_MapAccess(raidPtr, startAddr, layoutPtr->dataSectorsPerStripe,\n\t    buf, RF_DONT_REMAP);\n\taasm = asm_h->stripeMap;\n\n\tbuf1 = buf;\n\t/*\n         * Loop through the data blocks, setting up read nodes for each.\n         */\n\tfor (pda = aasm->physInfo, i = 0; i < layoutPtr->numDataCol; i++, pda = pda->next) {\n\t\tRF_ASSERT(pda);\n\n\t\trf_RangeRestrictPDA(raidPtr, parityPDA, pda, 0, 1);\n\n\t\tRF_ASSERT(pda->numSector != 0);\n\t\tif (rf_TryToRedirectPDA(raidPtr, pda, 0)) {\n\t\t\t/* cannot verify parity with dead disk */\n\t\t\tgoto done;\n\t\t}\n\t\tpda->bufPtr = buf1;\n\t\tblockNode->succedents[i]->params[0].p = pda;\n\t\tblockNode->succedents[i]->params[1].p = buf1;\n\t\tblockNode->succedents[i]->params[2].v = psID;\n\t\tblockNode->succedents[i]->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tbuf1 += nbytes;\n\t}\n\tRF_ASSERT(pda == NULL);\n\t/*\n         * keep i, buf1 running\n         *\n         * Loop through parity blocks, setting up read nodes for each.\n         */\n\tfor (pda = aasm->parityInfo; i < layoutPtr->numDataCol + layoutPtr->numParityCol; i++, pda = pda->next) {\n\t\tRF_ASSERT(pda);\n\t\trf_RangeRestrictPDA(raidPtr, parityPDA, pda, 0, 1);\n\t\tRF_ASSERT(pda->numSector != 0);\n\t\tif (rf_TryToRedirectPDA(raidPtr, pda, 0)) {\n\t\t\t/* cannot verify parity with dead disk */\n\t\t\tgoto done;\n\t\t}\n\t\tpda->bufPtr = buf1;\n\t\tblockNode->succedents[i]->params[0].p = pda;\n\t\tblockNode->succedents[i]->params[1].p = buf1;\n\t\tblockNode->succedents[i]->params[2].v = psID;\n\t\tblockNode->succedents[i]->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tbuf1 += nbytes;\n\t}\n\tRF_ASSERT(pda == NULL);\n\n\tbzero((char *) &tracerec, sizeof(tracerec));\n\trd_dag_h->tracerec = &tracerec;\n\n\tif (rf_verifyParityDebug > 1) {\n\t\tprintf(\"raid%d: RAID1 parity verify read dag:\\n\", \n\t\t       raidPtr->raidid);\n\t\trf_PrintDAGList(rd_dag_h);\n\t}\n\tRF_LOCK_MUTEX(mcpair->mutex);\n\tmcpair->flag = 0;\n\trf_DispatchDAG(rd_dag_h, (void (*) (void *)) rf_MCPairWakeupFunc,\n\t    (void *) mcpair);\n\twhile (mcpair->flag == 0) {\n\t\tRF_WAIT_MCPAIR(mcpair);\n\t}\n\tRF_UNLOCK_MUTEX(mcpair->mutex);\n\n\tif (rd_dag_h->status != rf_enable) {\n\t\tRF_ERRORMSG(\"Unable to verify raid1 parity: can't read stripe\\n\");\n\t\tret = RF_PARITY_COULD_NOT_VERIFY;\n\t\tgoto done;\n\t}\n\t/*\n         * buf1 is the beginning of the data blocks chunk\n         * buf2 is the beginning of the parity blocks chunk\n         */\n\tbuf1 = buf;\n\tbuf2 = buf + (nbytes * layoutPtr->numDataCol);\n\tret = RF_PARITY_OKAY;\n\t/*\n         * bbufs is \"bad bufs\"- an array whose entries are the data\n         * column numbers where we had miscompares. (That is, column 0\n         * and column 1 of the array are mirror copies, and are considered\n         * \"data column 0\" for this purpose).\n         */\n\tRF_MallocAndAdd(bbufs, layoutPtr->numParityCol * sizeof(int), (int *),\n\t    allocList);\n\tnbad = 0;\n\t/*\n         * Check data vs \"parity\" (mirror copy).\n         */\n\tfor (i = 0; i < layoutPtr->numDataCol; i++) {\n\t\tif (rf_verifyParityDebug) {\n\t\t\tprintf(\"raid%d: RAID1 parity verify %d bytes: i=%d buf1=%lx buf2=%lx buf=%lx\\n\",\n\t\t\t       raidPtr->raidid, nbytes, i, (long) buf1, \n\t\t\t       (long) buf2, (long) buf);\n\t\t}\n\t\tret = bcmp(buf1, buf2, nbytes);\n\t\tif (ret) {\n\t\t\tif (rf_verifyParityDebug > 1) {\n\t\t\t\tfor (j = 0; j < nbytes; j++) {\n\t\t\t\t\tif (buf1[j] != buf2[j])\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tprintf(\"psid=%ld j=%d\\n\", (long) psID, j);\n\t\t\t\tprintf(\"buf1 %02x %02x %02x %02x %02x\\n\", buf1[0] & 0xff,\n\t\t\t\t    buf1[1] & 0xff, buf1[2] & 0xff, buf1[3] & 0xff, buf1[4] & 0xff);\n\t\t\t\tprintf(\"buf2 %02x %02x %02x %02x %02x\\n\", buf2[0] & 0xff,\n\t\t\t\t    buf2[1] & 0xff, buf2[2] & 0xff, buf2[3] & 0xff, buf2[4] & 0xff);\n\t\t\t}\n\t\t\tif (rf_verifyParityDebug) {\n\t\t\t\tprintf(\"raid%d: RAID1: found bad parity, i=%d\\n\", raidPtr->raidid, i);\n\t\t\t}\n\t\t\t/*\n\t\t         * Parity is bad. Keep track of which columns were bad.\n\t\t         */\n\t\t\tif (bbufs)\n\t\t\t\tbbufs[nbad] = i;\n\t\t\tnbad++;\n\t\t\tret = RF_PARITY_BAD;\n\t\t}\n\t\tbuf1 += nbytes;\n\t\tbuf2 += nbytes;\n\t}\n\n\tif ((ret != RF_PARITY_OKAY) && correct_it) {\n\t\tret = RF_PARITY_COULD_NOT_CORRECT;\n\t\tif (rf_verifyParityDebug) {\n\t\t\tprintf(\"raid%d: RAID1 parity verify: parity not correct\\n\", raidPtr->raidid);\n\t\t}\n\t\tif (bbufs == NULL)\n\t\t\tgoto done;\n\t\t/*\n\t         * Make a DAG with one write node for each bad unit. We'll simply\n\t         * write the contents of the data unit onto the parity unit for\n\t         * correction. (It's possible that the mirror copy was the correct\n\t         * copy, and that we're spooging good data by writing bad over it,\n\t         * but there's no way we can know that.\n\t         */\n\t\twr_dag_h = rf_MakeSimpleDAG(raidPtr, nbad, nbytes, buf,\n\t\t    rf_DiskWriteFunc, rf_DiskWriteUndoFunc, \"Wnp\", allocList, flags,\n\t\t    RF_IO_NORMAL_PRIORITY);\n\t\tif (wr_dag_h == NULL)\n\t\t\tgoto done;\n\t\twrBlock = wr_dag_h->succedents[0];\n\t\t/*\n\t         * Fill in a write node for each bad compare.\n\t         */\n\t\tfor (i = 0; i < nbad; i++) {\n\t\t\tj = i + layoutPtr->numDataCol;\n\t\t\tpda = blockNode->succedents[j]->params[0].p;\n\t\t\tpda->bufPtr = blockNode->succedents[i]->params[1].p;\n\t\t\twrBlock->succedents[i]->params[0].p = pda;\n\t\t\twrBlock->succedents[i]->params[1].p = pda->bufPtr;\n\t\t\twrBlock->succedents[i]->params[2].v = psID;\n\t\t\twrBlock->succedents[0]->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t\tbzero((char *) &tracerec, sizeof(tracerec));\n\t\twr_dag_h->tracerec = &tracerec;\n\t\tif (rf_verifyParityDebug > 1) {\n\t\t\tprintf(\"Parity verify write dag:\\n\");\n\t\t\trf_PrintDAGList(wr_dag_h);\n\t\t}\n\t\tRF_LOCK_MUTEX(mcpair->mutex);\n\t\tmcpair->flag = 0;\n\t\t/* fire off the write DAG */\n\t\trf_DispatchDAG(wr_dag_h, (void (*) (void *)) rf_MCPairWakeupFunc,\n\t\t    (void *) mcpair);\n\t\twhile (!mcpair->flag) {\n\t\t\tRF_WAIT_COND(mcpair->cond, mcpair->mutex);\n\t\t}\n\t\tRF_UNLOCK_MUTEX(mcpair->mutex);\n\t\tif (wr_dag_h->status != rf_enable) {\n\t\t\tRF_ERRORMSG(\"Unable to correct RAID1 parity in VerifyParity\\n\");\n\t\t\tgoto done;\n\t\t}\n\t\tret = RF_PARITY_CORRECTED;\n\t}\ndone:\n\t/*\n         * All done. We might've gotten here without doing part of the function,\n         * so cleanup what we have to and return our running status.\n         */\n\tif (asm_h)\n\t\trf_FreeAccessStripeMap(asm_h);\n\tif (rd_dag_h)\n\t\trf_FreeDAG(rd_dag_h);\n\tif (wr_dag_h)\n\t\trf_FreeDAG(wr_dag_h);\n\tif (mcpair)\n\t\trf_FreeMCPair(mcpair);\n\trf_FreeAllocList(allocList);\n\tif (rf_verifyParityDebug) {\n\t\tprintf(\"raid%d: RAID1 parity verify, returning %d\\n\", \n\t\t       raidPtr->raidid, ret);\n\t}\n\treturn (ret);\n}"
  },
  {
    "function_name": "rf_RAID1DagSelect",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "189-271",
    "snippet": "void \nrf_RAID1DagSelect(\n    RF_Raid_t * raidPtr,\n    RF_IoType_t type,\n    RF_AccessStripeMap_t * asmap,\n    RF_VoidFuncPtr * createFunc)\n{\n\tRF_RowCol_t frow, fcol, or, oc;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tint     prior_recon;\n\tRF_RowStatus_t rstat;\n\tRF_SectorNum_t oo;\n\n\n\tRF_ASSERT(RF_IO_IS_R_OR_W(type));\n\n\tif (asmap->numDataFailed + asmap->numParityFailed > 1) {\n\t\tRF_ERRORMSG(\"Multiple disks failed in a single group!  Aborting I/O operation.\\n\");\n\t\t*createFunc = NULL;\n\t\treturn;\n\t}\n\tif (asmap->numDataFailed + asmap->numParityFailed) {\n\t\t/*\n\t         * We've got a fault. Re-map to spare space, iff applicable.\n\t         * Shouldn't the arch-independent code do this for us?\n\t         * Anyway, it turns out if we don't do this here, then when\n\t         * we're reconstructing, writes go only to the surviving\n\t         * original disk, and aren't reflected on the reconstructed\n\t         * spare. Oops. --jimz\n\t         */\n\t\tfailedPDA = asmap->failedPDAs[0];\n\t\tfrow = failedPDA->row;\n\t\tfcol = failedPDA->col;\n\t\trstat = raidPtr->status[frow];\n\t\tprior_recon = (rstat == rf_rs_reconfigured) || (\n\t\t    (rstat == rf_rs_reconstructing) ?\n\t\t    rf_CheckRUReconstructed(raidPtr->reconControl[frow]->reconMap, failedPDA->startSector) : 0\n\t\t    );\n\t\tif (prior_recon) {\n\t\t\tor = frow;\n\t\t\toc = fcol;\n\t\t\too = failedPDA->startSector;\n\t\t\t/*\n\t\t         * If we did distributed sparing, we'd monkey with that here.\n\t\t         * But we don't, so we'll\n\t\t         */\n\t\t\tfailedPDA->row = raidPtr->Disks[frow][fcol].spareRow;\n\t\t\tfailedPDA->col = raidPtr->Disks[frow][fcol].spareCol;\n\t\t\t/*\n\t\t         * Redirect other components, iff necessary. This looks\n\t\t         * pretty suspicious to me, but it's what the raid5\n\t\t         * DAG select does.\n\t\t         */\n\t\t\tif (asmap->parityInfo->next) {\n\t\t\t\tif (failedPDA == asmap->parityInfo) {\n\t\t\t\t\tfailedPDA->next->row = failedPDA->row;\n\t\t\t\t\tfailedPDA->next->col = failedPDA->col;\n\t\t\t\t} else {\n\t\t\t\t\tif (failedPDA == asmap->parityInfo->next) {\n\t\t\t\t\t\tasmap->parityInfo->row = failedPDA->row;\n\t\t\t\t\t\tasmap->parityInfo->col = failedPDA->col;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (rf_dagDebug || rf_mapDebug) {\n\t\t\t\tprintf(\"raid%d: Redirected type '%c' r %d c %d o %ld -> r %d c %d o %ld\\n\",\n\t\t\t\t       raidPtr->raidid, type, or, oc, \n\t\t\t\t       (long) oo, failedPDA->row, \n\t\t\t\t       failedPDA->col,\n\t\t\t\t    (long) failedPDA->startSector);\n\t\t\t}\n\t\t\tasmap->numDataFailed = asmap->numParityFailed = 0;\n\t\t}\n\t}\n\tif (type == RF_IO_TYPE_READ) {\n\t\tif (asmap->numDataFailed == 0)\n\t\t\t*createFunc = (RF_VoidFuncPtr) rf_CreateMirrorIdleReadDAG;\n\t\telse\n\t\t\t*createFunc = (RF_VoidFuncPtr) rf_CreateRaidOneDegradedReadDAG;\n\t} else {\n\t\t*createFunc = (RF_VoidFuncPtr) rf_CreateRaidOneWriteDAG;\n\t}\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"raid%d: Redirected type '%c' r %d c %d o %ld -> r %d c %d o %ld\\n\"",
            "raidPtr->raidid",
            "type",
            "or",
            "oc",
            "(long) oo",
            "failedPDA->row",
            "failedPDA->col",
            "(long) failedPDA->startSector"
          ],
          "line": 254
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_CheckRUReconstructed",
          "args": [
            "raidPtr->reconControl[frow]->reconMap",
            "failedPDA->startSector"
          ],
          "line": 225
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CheckRUReconstructed",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_reconmap.c",
          "lines": "308-319",
          "snippet": "int \nrf_CheckRUReconstructed(mapPtr, startSector)\n\tRF_ReconMap_t *mapPtr;\n\tRF_SectorNum_t startSector;\n{\n\tRF_ReconMapListElem_t *l;\t/* used for searching */\n\tRF_ReconUnitNum_t i;\n\n\ti = startSector / mapPtr->sectorsPerReconUnit;\n\tl = mapPtr->status[i];\n\treturn ((l == RU_ALL) ? 1 : 0);\n}",
          "includes": [
            "#include \"rf_utils.h\"",
            "#include \"rf_general.h\"",
            "#include <sys/time.h>",
            "#include \"rf_raid.h\""
          ],
          "macros_used": [
            "#define RU_ALL      ((RF_ReconMapListElem_t *) -1)"
          ],
          "globals_used": [
            "static void \ncompact_stat_entry(RF_Raid_t * raidPtr, RF_ReconMap_t * mapPtr,\n    int i);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include <sys/time.h>\n#include \"rf_raid.h\"\n\n#define RU_ALL      ((RF_ReconMapListElem_t *) -1)\n\nstatic void \ncompact_stat_entry(RF_Raid_t * raidPtr, RF_ReconMap_t * mapPtr,\n    int i);\n\nint \nrf_CheckRUReconstructed(mapPtr, startSector)\n\tRF_ReconMap_t *mapPtr;\n\tRF_SectorNum_t startSector;\n{\n\tRF_ReconMapListElem_t *l;\t/* used for searching */\n\tRF_ReconUnitNum_t i;\n\n\ti = startSector / mapPtr->sectorsPerReconUnit;\n\tl = mapPtr->status[i];\n\treturn ((l == RU_ALL) ? 1 : 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ERRORMSG",
          "args": [
            "\"Multiple disks failed in a single group!  Aborting I/O operation.\\n\""
          ],
          "line": 206
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "RF_IO_IS_R_OR_W(type)"
          ],
          "line": 203
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_IO_IS_R_OR_W",
          "args": [
            "type"
          ],
          "line": 203
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nvoid \nrf_RAID1DagSelect(\n    RF_Raid_t * raidPtr,\n    RF_IoType_t type,\n    RF_AccessStripeMap_t * asmap,\n    RF_VoidFuncPtr * createFunc)\n{\n\tRF_RowCol_t frow, fcol, or, oc;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tint     prior_recon;\n\tRF_RowStatus_t rstat;\n\tRF_SectorNum_t oo;\n\n\n\tRF_ASSERT(RF_IO_IS_R_OR_W(type));\n\n\tif (asmap->numDataFailed + asmap->numParityFailed > 1) {\n\t\tRF_ERRORMSG(\"Multiple disks failed in a single group!  Aborting I/O operation.\\n\");\n\t\t*createFunc = NULL;\n\t\treturn;\n\t}\n\tif (asmap->numDataFailed + asmap->numParityFailed) {\n\t\t/*\n\t         * We've got a fault. Re-map to spare space, iff applicable.\n\t         * Shouldn't the arch-independent code do this for us?\n\t         * Anyway, it turns out if we don't do this here, then when\n\t         * we're reconstructing, writes go only to the surviving\n\t         * original disk, and aren't reflected on the reconstructed\n\t         * spare. Oops. --jimz\n\t         */\n\t\tfailedPDA = asmap->failedPDAs[0];\n\t\tfrow = failedPDA->row;\n\t\tfcol = failedPDA->col;\n\t\trstat = raidPtr->status[frow];\n\t\tprior_recon = (rstat == rf_rs_reconfigured) || (\n\t\t    (rstat == rf_rs_reconstructing) ?\n\t\t    rf_CheckRUReconstructed(raidPtr->reconControl[frow]->reconMap, failedPDA->startSector) : 0\n\t\t    );\n\t\tif (prior_recon) {\n\t\t\tor = frow;\n\t\t\toc = fcol;\n\t\t\too = failedPDA->startSector;\n\t\t\t/*\n\t\t         * If we did distributed sparing, we'd monkey with that here.\n\t\t         * But we don't, so we'll\n\t\t         */\n\t\t\tfailedPDA->row = raidPtr->Disks[frow][fcol].spareRow;\n\t\t\tfailedPDA->col = raidPtr->Disks[frow][fcol].spareCol;\n\t\t\t/*\n\t\t         * Redirect other components, iff necessary. This looks\n\t\t         * pretty suspicious to me, but it's what the raid5\n\t\t         * DAG select does.\n\t\t         */\n\t\t\tif (asmap->parityInfo->next) {\n\t\t\t\tif (failedPDA == asmap->parityInfo) {\n\t\t\t\t\tfailedPDA->next->row = failedPDA->row;\n\t\t\t\t\tfailedPDA->next->col = failedPDA->col;\n\t\t\t\t} else {\n\t\t\t\t\tif (failedPDA == asmap->parityInfo->next) {\n\t\t\t\t\t\tasmap->parityInfo->row = failedPDA->row;\n\t\t\t\t\t\tasmap->parityInfo->col = failedPDA->col;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (rf_dagDebug || rf_mapDebug) {\n\t\t\t\tprintf(\"raid%d: Redirected type '%c' r %d c %d o %ld -> r %d c %d o %ld\\n\",\n\t\t\t\t       raidPtr->raidid, type, or, oc, \n\t\t\t\t       (long) oo, failedPDA->row, \n\t\t\t\t       failedPDA->col,\n\t\t\t\t    (long) failedPDA->startSector);\n\t\t\t}\n\t\t\tasmap->numDataFailed = asmap->numParityFailed = 0;\n\t\t}\n\t}\n\tif (type == RF_IO_TYPE_READ) {\n\t\tif (asmap->numDataFailed == 0)\n\t\t\t*createFunc = (RF_VoidFuncPtr) rf_CreateMirrorIdleReadDAG;\n\t\telse\n\t\t\t*createFunc = (RF_VoidFuncPtr) rf_CreateRaidOneDegradedReadDAG;\n\t} else {\n\t\t*createFunc = (RF_VoidFuncPtr) rf_CreateRaidOneWriteDAG;\n\t}\n}"
  },
  {
    "function_name": "rf_MapSIDToPSIDRAID1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "167-176",
    "snippet": "void \nrf_MapSIDToPSIDRAID1(\n    RF_RaidLayout_t * layoutPtr,\n    RF_StripeNum_t stripeID,\n    RF_StripeNum_t * psID,\n    RF_ReconUnitNum_t * which_ru)\n{\n\t*which_ru = 0;\n\t*psID = stripeID;\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nvoid \nrf_MapSIDToPSIDRAID1(\n    RF_RaidLayout_t * layoutPtr,\n    RF_StripeNum_t stripeID,\n    RF_StripeNum_t * psID,\n    RF_ReconUnitNum_t * which_ru)\n{\n\t*which_ru = 0;\n\t*psID = stripeID;\n}"
  },
  {
    "function_name": "rf_IdentifyStripeRAID1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "146-160",
    "snippet": "void \nrf_IdentifyStripeRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t addr,\n    RF_RowCol_t ** diskids,\n    RF_RowCol_t * outRow)\n{\n\tRF_StripeNum_t stripeID = rf_RaidAddressToStripeID(&raidPtr->Layout, addr);\n\tRF_Raid1ConfigInfo_t *info = raidPtr->Layout.layoutSpecificInfo;\n\tRF_ASSERT(stripeID >= 0);\n\tRF_ASSERT(addr >= 0);\n\t*outRow = 0;\n\t*diskids = info->stripeIdentifier[stripeID % (raidPtr->numCol / 2)];\n\tRF_ASSERT(*diskids);\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "*diskids"
          ],
          "line": 159
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "addr >= 0"
          ],
          "line": 156
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "stripeID >= 0"
          ],
          "line": 155
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToStripeID",
          "args": [
            "&raidPtr->Layout",
            "addr"
          ],
          "line": 153
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nvoid \nrf_IdentifyStripeRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t addr,\n    RF_RowCol_t ** diskids,\n    RF_RowCol_t * outRow)\n{\n\tRF_StripeNum_t stripeID = rf_RaidAddressToStripeID(&raidPtr->Layout, addr);\n\tRF_Raid1ConfigInfo_t *info = raidPtr->Layout.layoutSpecificInfo;\n\tRF_ASSERT(stripeID >= 0);\n\tRF_ASSERT(addr >= 0);\n\t*outRow = 0;\n\t*diskids = info->stripeIdentifier[stripeID % (raidPtr->numCol / 2)];\n\tRF_ASSERT(*diskids);\n}"
  },
  {
    "function_name": "rf_MapParityRAID1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "123-139",
    "snippet": "void \nrf_MapParityRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t raidSector,\n    RF_RowCol_t * row,\n    RF_RowCol_t * col,\n    RF_SectorNum_t * diskSector,\n    int remap)\n{\n\tRF_StripeNum_t SUID = raidSector / raidPtr->Layout.sectorsPerStripeUnit;\n\tRF_RowCol_t mirrorPair = SUID % (raidPtr->numCol / 2);\n\n\t*row = 0;\n\t*col = (2 * mirrorPair) + 1;\n\n\t*diskSector = ((SUID / (raidPtr->numCol / 2)) * raidPtr->Layout.sectorsPerStripeUnit) + (raidSector % raidPtr->Layout.sectorsPerStripeUnit);\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nvoid \nrf_MapParityRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t raidSector,\n    RF_RowCol_t * row,\n    RF_RowCol_t * col,\n    RF_SectorNum_t * diskSector,\n    int remap)\n{\n\tRF_StripeNum_t SUID = raidSector / raidPtr->Layout.sectorsPerStripeUnit;\n\tRF_RowCol_t mirrorPair = SUID % (raidPtr->numCol / 2);\n\n\t*row = 0;\n\t*col = (2 * mirrorPair) + 1;\n\n\t*diskSector = ((SUID / (raidPtr->numCol / 2)) * raidPtr->Layout.sectorsPerStripeUnit) + (raidSector % raidPtr->Layout.sectorsPerStripeUnit);\n}"
  },
  {
    "function_name": "rf_MapSectorRAID1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "100-115",
    "snippet": "void \nrf_MapSectorRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t raidSector,\n    RF_RowCol_t * row,\n    RF_RowCol_t * col,\n    RF_SectorNum_t * diskSector,\n    int remap)\n{\n\tRF_StripeNum_t SUID = raidSector / raidPtr->Layout.sectorsPerStripeUnit;\n\tRF_RowCol_t mirrorPair = SUID % (raidPtr->numCol / 2);\n\n\t*row = 0;\n\t*col = 2 * mirrorPair;\n\t*diskSector = ((SUID / (raidPtr->numCol / 2)) * raidPtr->Layout.sectorsPerStripeUnit) + (raidSector % raidPtr->Layout.sectorsPerStripeUnit);\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nvoid \nrf_MapSectorRAID1(\n    RF_Raid_t * raidPtr,\n    RF_RaidAddr_t raidSector,\n    RF_RowCol_t * row,\n    RF_RowCol_t * col,\n    RF_SectorNum_t * diskSector,\n    int remap)\n{\n\tRF_StripeNum_t SUID = raidSector / raidPtr->Layout.sectorsPerStripeUnit;\n\tRF_RowCol_t mirrorPair = SUID % (raidPtr->numCol / 2);\n\n\t*row = 0;\n\t*col = 2 * mirrorPair;\n\t*diskSector = ((SUID / (raidPtr->numCol / 2)) * raidPtr->Layout.sectorsPerStripeUnit) + (raidSector % raidPtr->Layout.sectorsPerStripeUnit);\n}"
  },
  {
    "function_name": "rf_ConfigureRAID1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_raid1.c",
    "lines": "58-96",
    "snippet": "int \nrf_ConfigureRAID1(\n    RF_ShutdownList_t ** listp,\n    RF_Raid_t * raidPtr,\n    RF_Config_t * cfgPtr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_Raid1ConfigInfo_t *info;\n\tRF_RowCol_t i;\n\n\t/* create a RAID level 1 configuration structure */\n\tRF_MallocAndAdd(info, sizeof(RF_Raid1ConfigInfo_t), (RF_Raid1ConfigInfo_t *), raidPtr->cleanupList);\n\tif (info == NULL)\n\t\treturn (ENOMEM);\n\tlayoutPtr->layoutSpecificInfo = (void *) info;\n\n\t/* ... and fill it in. */\n\tinfo->stripeIdentifier = rf_make_2d_array(raidPtr->numCol / 2, 2, raidPtr->cleanupList);\n\tif (info->stripeIdentifier == NULL)\n\t\treturn (ENOMEM);\n\tfor (i = 0; i < (raidPtr->numCol / 2); i++) {\n\t\tinfo->stripeIdentifier[i][0] = (2 * i);\n\t\tinfo->stripeIdentifier[i][1] = (2 * i) + 1;\n\t}\n\n\tRF_ASSERT(raidPtr->numRow == 1);\n\n\t/* this implementation of RAID level 1 uses one row of numCol disks\n\t * and allows multiple (numCol / 2) stripes per row.  A stripe\n\t * consists of a single data unit and a single parity (mirror) unit.\n\t * stripe id = raidAddr / stripeUnitSize */\n\traidPtr->totalSectors = layoutPtr->stripeUnitsPerDisk * (raidPtr->numCol / 2) * layoutPtr->sectorsPerStripeUnit;\n\tlayoutPtr->numStripe = layoutPtr->stripeUnitsPerDisk * (raidPtr->numCol / 2);\n\tlayoutPtr->dataSectorsPerStripe = layoutPtr->sectorsPerStripeUnit;\n\tlayoutPtr->bytesPerStripeUnit = layoutPtr->sectorsPerStripeUnit << raidPtr->logBytesPerSector;\n\tlayoutPtr->numDataCol = 1;\n\tlayoutPtr->numParityCol = 1;\n\treturn (0);\n}",
    "includes": [
      "#include \"rf_reconbuffer.h\"",
      "#include \"rf_engine.h\"",
      "#include \"rf_map.h\"",
      "#include \"rf_layout.h\"",
      "#include \"rf_mcpair.h\"",
      "#include \"rf_parityscan.h\"",
      "#include \"rf_utils.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_diskqueue.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid1.h\"",
      "#include \"rf_raid.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "raidPtr->numRow == 1"
          ],
          "line": 83
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_make_2d_array",
          "args": [
            "raidPtr->numCol / 2",
            "2",
            "raidPtr->cleanupList"
          ],
          "line": 75
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "info",
            "sizeof(RF_Raid1ConfigInfo_t)",
            "(RF_Raid1ConfigInfo_t *), raidPtr->cleanupList"
          ],
          "line": 69
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_reconbuffer.h\"\n#include \"rf_engine.h\"\n#include \"rf_map.h\"\n#include \"rf_layout.h\"\n#include \"rf_mcpair.h\"\n#include \"rf_parityscan.h\"\n#include \"rf_utils.h\"\n#include \"rf_general.h\"\n#include \"rf_diskqueue.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dagdegrd.h\"\n#include \"rf_dagffwr.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid1.h\"\n#include \"rf_raid.h\"\n\nint \nrf_ConfigureRAID1(\n    RF_ShutdownList_t ** listp,\n    RF_Raid_t * raidPtr,\n    RF_Config_t * cfgPtr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_Raid1ConfigInfo_t *info;\n\tRF_RowCol_t i;\n\n\t/* create a RAID level 1 configuration structure */\n\tRF_MallocAndAdd(info, sizeof(RF_Raid1ConfigInfo_t), (RF_Raid1ConfigInfo_t *), raidPtr->cleanupList);\n\tif (info == NULL)\n\t\treturn (ENOMEM);\n\tlayoutPtr->layoutSpecificInfo = (void *) info;\n\n\t/* ... and fill it in. */\n\tinfo->stripeIdentifier = rf_make_2d_array(raidPtr->numCol / 2, 2, raidPtr->cleanupList);\n\tif (info->stripeIdentifier == NULL)\n\t\treturn (ENOMEM);\n\tfor (i = 0; i < (raidPtr->numCol / 2); i++) {\n\t\tinfo->stripeIdentifier[i][0] = (2 * i);\n\t\tinfo->stripeIdentifier[i][1] = (2 * i) + 1;\n\t}\n\n\tRF_ASSERT(raidPtr->numRow == 1);\n\n\t/* this implementation of RAID level 1 uses one row of numCol disks\n\t * and allows multiple (numCol / 2) stripes per row.  A stripe\n\t * consists of a single data unit and a single parity (mirror) unit.\n\t * stripe id = raidAddr / stripeUnitSize */\n\traidPtr->totalSectors = layoutPtr->stripeUnitsPerDisk * (raidPtr->numCol / 2) * layoutPtr->sectorsPerStripeUnit;\n\tlayoutPtr->numStripe = layoutPtr->stripeUnitsPerDisk * (raidPtr->numCol / 2);\n\tlayoutPtr->dataSectorsPerStripe = layoutPtr->sectorsPerStripeUnit;\n\tlayoutPtr->bytesPerStripeUnit = layoutPtr->sectorsPerStripeUnit << raidPtr->logBytesPerSector;\n\tlayoutPtr->numDataCol = 1;\n\tlayoutPtr->numParityCol = 1;\n\treturn (0);\n}"
  }
]