[
  {
    "function_name": "rf_DoubleDegRead",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
    "lines": "983-1124",
    "snippet": "void \nrf_DoubleDegRead(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    char *redundantReadNodeName,\n    char *recoveryNodeName,\n    int (*recovFunc) (RF_DagNode_t *))\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_DagNode_t *nodes, *rudNodes, *rrdNodes, *recoveryNode, *blockNode,\n\t       *unblockNode, *rpNodes, *rqNodes, *termNode;\n\tRF_PhysDiskAddr_t *pda, *pqPDAs;\n\tRF_PhysDiskAddr_t *npdas;\n\tint     nNodes, nRrdNodes, nRudNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tint     nReadNodes, nPQNodes;\n\tRF_PhysDiskAddr_t *failedPDA = asmap->failedPDAs[0];\n\tRF_PhysDiskAddr_t *failedPDAtwo = asmap->failedPDAs[1];\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating Double Degraded Read DAG]\\n\");\n\trf_DD_GenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes, &pqPDAs, &nPQNodes, allocList);\n\n\tnRudNodes = asmap->numStripeUnitsAccessed - (asmap->numDataFailed);\n\tnReadNodes = nRrdNodes + nRudNodes + 2 * nPQNodes;\n\tnNodes = 4 /* block, unblock, recovery, term */ + nReadNodes;\n\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\trecoveryNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\trudNodes = &nodes[i];\n\ti += nRudNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\trpNodes = &nodes[i];\n\ti += nPQNodes;\n\trqNodes = &nodes[i];\n\ti += nPQNodes;\n\tRF_ASSERT(i == nNodes);\n\n\tdag_h->numSuccedents = 1;\n\tdag_h->succedents[0] = blockNode;\n\tdag_h->creator = \"DoubleDegRead\";\n\tdag_h->numCommits = 0;\n\tdag_h->numCommitNodes = 1;\t/* unblock */\n\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 2, 0, 0, dag_h, \"Trm\", allocList);\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n\ttermNode->antecedents[1] = recoveryNode;\n\ttermNode->antType[1] = rf_control;\n\n\t/* init the block and unblock nodes */\n\t/* The block node has all nodes except itself, unblock and recovery as\n\t * successors. Similarly for predecessors of the unblock. */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nReadNodes, 0, 0, dag_h, \"Nil\", allocList);\n\n\tfor (i = 0; i < nReadNodes; i++) {\n\t\tblockNode->succedents[i] = rudNodes + i;\n\t\tunblockNode->antecedents[i] = rudNodes + i;\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tunblockNode->succedents[0] = termNode;\n\n\t/* The recovery node has all the reads as predecessors, and the term\n\t * node as successors. It gets a pda as a param from each of the read\n\t * nodes plus the raidPtr. For each failed unit is has a result pda. */\n\trf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc, rf_NullNodeUndoFunc, NULL,\n\t    1,\t\t\t/* succesors */\n\t    nReadNodes,\t\t/* preds */\n\t    nReadNodes + 2,\t/* params */\n\t    asmap->numDataFailed,\t/* results */\n\t    dag_h, recoveryNodeName, allocList);\n\n\trecoveryNode->succedents[0] = termNode;\n\tfor (i = 0; i < nReadNodes; i++) {\n\t\trecoveryNode->antecedents[i] = rudNodes + i;\n\t\trecoveryNode->antType[i] = rf_trueData;\n\t}\n\n\t/* build the read nodes, then come back and fill in recovery params\n\t * and results */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nRudNodes; pda = pda->next) {\n\t\tif ((pda == failedPDA) || (pda == failedPDAtwo))\n\t\t\tcontinue;\n\t\tINIT_DISK_NODE(rudNodes + i, \"Rud\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rudNodes[i], pda);\n\t\ti++;\n\t}\n\n\tpda = npdas;\n\tfor (i = 0; i < nRrdNodes; i++, pda = pda->next) {\n\t\tINIT_DISK_NODE(rrdNodes + i, \"Rrd\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rrdNodes[i], pda);\n\t}\n\n\t/* redundancy pdas */\n\tpda = pqPDAs;\n\tINIT_DISK_NODE(rpNodes, \"Rp\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rpNodes[0], pda);\n\tpda++;\n\tINIT_DISK_NODE(rqNodes, redundantReadNodeName);\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rqNodes[0], pda);\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\tINIT_DISK_NODE(rpNodes + 1, \"Rp\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rpNodes[1], pda);\n\t\tpda++;\n\t\tINIT_DISK_NODE(rqNodes + 1, redundantReadNodeName);\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rqNodes[1], pda);\n\t}\n\t/* fill in recovery node params */\n\tfor (i = 0; i < nReadNodes; i++)\n\t\trecoveryNode->params[i] = rudNodes[i].params[0];\t/* pda */\n\trecoveryNode->params[i++].p = (void *) raidPtr;\n\trecoveryNode->params[i++].p = (void *) asmap;\n\trecoveryNode->results[0] = failedPDA;\n\tif (asmap->numDataFailed == 2)\n\t\trecoveryNode->results[1] = failedPDAtwo;\n\n\t/* zero fill the target data buffers? */\n}",
    "includes": [
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rqNodes[1]",
            "pda"
          ],
          "line": 1112
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 1111
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_DISK_NODE",
          "args": [
            "rqNodes + 1",
            "redundantReadNodeName"
          ],
          "line": 1110
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rpNodes[1]",
            "pda"
          ],
          "line": 1108
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 1107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_DISK_NODE",
          "args": [
            "rpNodes + 1",
            "\"Rp\""
          ],
          "line": 1106
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rqNodes[0]",
            "pda"
          ],
          "line": 1103
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 1102
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_DISK_NODE",
          "args": [
            "rqNodes",
            "redundantReadNodeName"
          ],
          "line": 1101
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rpNodes[0]",
            "pda"
          ],
          "line": 1099
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 1098
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_DISK_NODE",
          "args": [
            "rpNodes",
            "\"Rp\""
          ],
          "line": 1097
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rrdNodes[i]",
            "pda"
          ],
          "line": 1092
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 1091
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_DISK_NODE",
          "args": [
            "rrdNodes + i",
            "\"Rrd\""
          ],
          "line": 1090
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DISK_NODE_PARAMS",
          "args": [
            "rudNodes[i]",
            "pda"
          ],
          "line": 1084
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 1083
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_DISK_NODE",
          "args": [
            "rudNodes + i",
            "\"Rud\""
          ],
          "line": 1082
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "recoveryNode",
            "rf_wait",
            "RF_FALSE",
            "recovFunc",
            "rf_NullNodeUndoFunc",
            "NULL",
            "1",
            "/* succesors */nReadNodes",
            "/* preds */nReadNodes + 2",
            "/* params */asmap->numDataFailed",
            "/* results */dag_h",
            "recoveryNodeName",
            "allocList"
          ],
          "line": 1063
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == nNodes"
          ],
          "line": 1033
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 1015
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_DD_GenerateFailedAccessASMs",
          "args": [
            "raidPtr",
            "asmap",
            "&npdas",
            "&nRrdNodes",
            "&pqPDAs",
            "&nPQNodes",
            "allocList"
          ],
          "line": 1009
        },
        "resolved": true,
        "details": {
          "function_name": "rf_DD_GenerateFailedAccessASMs",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
          "lines": "651-969",
          "snippet": "void \nrf_DD_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tRF_SectorNum_t suoff, suend;\n\tunsigned firstDataCol, napdas, count;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end = 0;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_PhysDiskAddr_t *phys_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n#define CONS_PDA(if,start,num) \\\n  pda_p->row = asmap->if->row;    pda_p->col = asmap->if->col; \\\n  pda_p->startSector = ((asmap->if->startSector / secPerSU) * secPerSU) + start; \\\n  pda_p->numSector = num; \\\n  pda_p->next = NULL; \\\n  RF_MallocAndAdd(pda_p->bufPtr,rf_RaidAddressToByte(raidPtr,num),(char *), allocList)\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - asmap->numStripeUnitsAccessed - (ftwo == NULL ? 1 : 0));\n\t*nPQNodep = PDAPerDisk;\n\n\t/* sweep over the over accessed pda's, figuring out the number of\n\t * additional pda's to generate. Of course, skip the failed ones */\n\n\tcount = 0;\n\tfor (pda_p = asmap->physInfo; pda_p; pda_p = pda_p->next) {\n\t\tif ((pda_p == fone) || (pda_p == ftwo))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, pda_p->startSector);\n\t\tsuend = suoff + pda_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* one failed PDA to overlap */\n\t\t\t/* if a PDA doesn't contain the failed unit, it can\n\t\t\t * only miss the start or end, not both */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe */\n\t\t\tif (suoff)\t/* leak at begining */\n\t\t\t\tcount++;\n\t\t\tif (suend < numDataCol)\t/* leak at end */\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 3:\t/* two disjoint units */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\tnapdas += count;\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\t/* march through the one's up to the first accessed disk */\n\tfirstDataCol = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), asmap->physInfo->raidAddress) % numDataCol;\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < firstDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\t/* march through the touched stripe units */\n\tfor (phys_p = asmap->physInfo; phys_p; phys_p = phys_p->next, i++) {\n\t\tif ((phys_p == asmap->failedPDAs[0]) || (phys_p == asmap->failedPDAs[1]))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, phys_p->startSector);\n\t\tsuend = suoff + phys_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* single buffer */\n\t\t\tif (suoff > fone_start) {\n\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t/* The data read starts after the mapped\n\t\t\t\t * access, snip off the begining */\n\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tif (suend < fone_end) {\n\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t/* The data read stops before the end of the\n\t\t\t\t * failed access, extend */\n\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe unit */\n\t\t\tRF_ASSERT((suoff == 0) || (suend == secPerSU));\n\t\t\tif (suend < secPerSU) {\t/* short read, snip from end\n\t\t\t\t\t\t * on */\n\t\t\t\tpda_p->numSector = secPerSU - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t} else\n\t\t\t\tif (suoff > 0) {\t/* short at front */\n\t\t\t\t\tpda_p->numSector = suoff;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\t/* two nonoverlapping failures */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end)) {\n\t\t\t\tif (suoff > fone_start) {\n\t\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < fone_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end)) {\n\t\t\t\tif (suoff > ftwo_start) {\n\t\t\t\t\tRF_ASSERT(suend >= ftwo_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - ftwo_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + ftwo_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < ftwo_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= ftwo_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = ftwo_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\t/* after the last accessed disk */\n\tfor (; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}",
          "includes": [
            "#include \"rf_dagdegrd.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_DD_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tRF_SectorNum_t suoff, suend;\n\tunsigned firstDataCol, napdas, count;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end = 0;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_PhysDiskAddr_t *phys_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n#define CONS_PDA(if,start,num) \\\n  pda_p->row = asmap->if->row;    pda_p->col = asmap->if->col; \\\n  pda_p->startSector = ((asmap->if->startSector / secPerSU) * secPerSU) + start; \\\n  pda_p->numSector = num; \\\n  pda_p->next = NULL; \\\n  RF_MallocAndAdd(pda_p->bufPtr,rf_RaidAddressToByte(raidPtr,num),(char *), allocList)\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - asmap->numStripeUnitsAccessed - (ftwo == NULL ? 1 : 0));\n\t*nPQNodep = PDAPerDisk;\n\n\t/* sweep over the over accessed pda's, figuring out the number of\n\t * additional pda's to generate. Of course, skip the failed ones */\n\n\tcount = 0;\n\tfor (pda_p = asmap->physInfo; pda_p; pda_p = pda_p->next) {\n\t\tif ((pda_p == fone) || (pda_p == ftwo))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, pda_p->startSector);\n\t\tsuend = suoff + pda_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* one failed PDA to overlap */\n\t\t\t/* if a PDA doesn't contain the failed unit, it can\n\t\t\t * only miss the start or end, not both */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe */\n\t\t\tif (suoff)\t/* leak at begining */\n\t\t\t\tcount++;\n\t\t\tif (suend < numDataCol)\t/* leak at end */\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 3:\t/* two disjoint units */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\tnapdas += count;\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\t/* march through the one's up to the first accessed disk */\n\tfirstDataCol = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), asmap->physInfo->raidAddress) % numDataCol;\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < firstDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\t/* march through the touched stripe units */\n\tfor (phys_p = asmap->physInfo; phys_p; phys_p = phys_p->next, i++) {\n\t\tif ((phys_p == asmap->failedPDAs[0]) || (phys_p == asmap->failedPDAs[1]))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, phys_p->startSector);\n\t\tsuend = suoff + phys_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* single buffer */\n\t\t\tif (suoff > fone_start) {\n\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t/* The data read starts after the mapped\n\t\t\t\t * access, snip off the begining */\n\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tif (suend < fone_end) {\n\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t/* The data read stops before the end of the\n\t\t\t\t * failed access, extend */\n\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe unit */\n\t\t\tRF_ASSERT((suoff == 0) || (suend == secPerSU));\n\t\t\tif (suend < secPerSU) {\t/* short read, snip from end\n\t\t\t\t\t\t * on */\n\t\t\t\tpda_p->numSector = secPerSU - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t} else\n\t\t\t\tif (suoff > 0) {\t/* short at front */\n\t\t\t\t\tpda_p->numSector = suoff;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\t/* two nonoverlapping failures */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end)) {\n\t\t\t\tif (suoff > fone_start) {\n\t\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < fone_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end)) {\n\t\t\t\tif (suoff > ftwo_start) {\n\t\t\t\t\tRF_ASSERT(suend >= ftwo_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - ftwo_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + ftwo_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < ftwo_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= ftwo_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = ftwo_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\t/* after the last accessed disk */\n\tfor (; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}"
        }
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating Double Degraded Read DAG]\\n\""
          ],
          "line": 1008
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "layoutPtr",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 1005
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_DoubleDegRead(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    char *redundantReadNodeName,\n    char *recoveryNodeName,\n    int (*recovFunc) (RF_DagNode_t *))\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tRF_DagNode_t *nodes, *rudNodes, *rrdNodes, *recoveryNode, *blockNode,\n\t       *unblockNode, *rpNodes, *rqNodes, *termNode;\n\tRF_PhysDiskAddr_t *pda, *pqPDAs;\n\tRF_PhysDiskAddr_t *npdas;\n\tint     nNodes, nRrdNodes, nRudNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tint     nReadNodes, nPQNodes;\n\tRF_PhysDiskAddr_t *failedPDA = asmap->failedPDAs[0];\n\tRF_PhysDiskAddr_t *failedPDAtwo = asmap->failedPDAs[1];\n\tRF_StripeNum_t parityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating Double Degraded Read DAG]\\n\");\n\trf_DD_GenerateFailedAccessASMs(raidPtr, asmap, &npdas, &nRrdNodes, &pqPDAs, &nPQNodes, allocList);\n\n\tnRudNodes = asmap->numStripeUnitsAccessed - (asmap->numDataFailed);\n\tnReadNodes = nRrdNodes + nRudNodes + 2 * nPQNodes;\n\tnNodes = 4 /* block, unblock, recovery, term */ + nReadNodes;\n\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\trecoveryNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\trudNodes = &nodes[i];\n\ti += nRudNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\trpNodes = &nodes[i];\n\ti += nPQNodes;\n\trqNodes = &nodes[i];\n\ti += nPQNodes;\n\tRF_ASSERT(i == nNodes);\n\n\tdag_h->numSuccedents = 1;\n\tdag_h->succedents[0] = blockNode;\n\tdag_h->creator = \"DoubleDegRead\";\n\tdag_h->numCommits = 0;\n\tdag_h->numCommitNodes = 1;\t/* unblock */\n\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 2, 0, 0, dag_h, \"Trm\", allocList);\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n\ttermNode->antecedents[1] = recoveryNode;\n\ttermNode->antType[1] = rf_control;\n\n\t/* init the block and unblock nodes */\n\t/* The block node has all nodes except itself, unblock and recovery as\n\t * successors. Similarly for predecessors of the unblock. */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nReadNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, nReadNodes, 0, 0, dag_h, \"Nil\", allocList);\n\n\tfor (i = 0; i < nReadNodes; i++) {\n\t\tblockNode->succedents[i] = rudNodes + i;\n\t\tunblockNode->antecedents[i] = rudNodes + i;\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tunblockNode->succedents[0] = termNode;\n\n\t/* The recovery node has all the reads as predecessors, and the term\n\t * node as successors. It gets a pda as a param from each of the read\n\t * nodes plus the raidPtr. For each failed unit is has a result pda. */\n\trf_InitNode(recoveryNode, rf_wait, RF_FALSE, recovFunc, rf_NullNodeUndoFunc, NULL,\n\t    1,\t\t\t/* succesors */\n\t    nReadNodes,\t\t/* preds */\n\t    nReadNodes + 2,\t/* params */\n\t    asmap->numDataFailed,\t/* results */\n\t    dag_h, recoveryNodeName, allocList);\n\n\trecoveryNode->succedents[0] = termNode;\n\tfor (i = 0; i < nReadNodes; i++) {\n\t\trecoveryNode->antecedents[i] = rudNodes + i;\n\t\trecoveryNode->antType[i] = rf_trueData;\n\t}\n\n\t/* build the read nodes, then come back and fill in recovery params\n\t * and results */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nRudNodes; pda = pda->next) {\n\t\tif ((pda == failedPDA) || (pda == failedPDAtwo))\n\t\t\tcontinue;\n\t\tINIT_DISK_NODE(rudNodes + i, \"Rud\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rudNodes[i], pda);\n\t\ti++;\n\t}\n\n\tpda = npdas;\n\tfor (i = 0; i < nRrdNodes; i++, pda = pda->next) {\n\t\tINIT_DISK_NODE(rrdNodes + i, \"Rrd\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rrdNodes[i], pda);\n\t}\n\n\t/* redundancy pdas */\n\tpda = pqPDAs;\n\tINIT_DISK_NODE(rpNodes, \"Rp\");\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rpNodes[0], pda);\n\tpda++;\n\tINIT_DISK_NODE(rqNodes, redundantReadNodeName);\n\tRF_ASSERT(pda);\n\tDISK_NODE_PARAMS(rqNodes[0], pda);\n\tif (nPQNodes == 2) {\n\t\tpda++;\n\t\tINIT_DISK_NODE(rpNodes + 1, \"Rp\");\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rpNodes[1], pda);\n\t\tpda++;\n\t\tINIT_DISK_NODE(rqNodes + 1, redundantReadNodeName);\n\t\tRF_ASSERT(pda);\n\t\tDISK_NODE_PARAMS(rqNodes[1], pda);\n\t}\n\t/* fill in recovery node params */\n\tfor (i = 0; i < nReadNodes; i++)\n\t\trecoveryNode->params[i] = rudNodes[i].params[0];\t/* pda */\n\trecoveryNode->params[i++].p = (void *) raidPtr;\n\trecoveryNode->params[i++].p = (void *) asmap;\n\trecoveryNode->results[0] = failedPDA;\n\tif (asmap->numDataFailed == 2)\n\t\trecoveryNode->results[1] = failedPDAtwo;\n\n\t/* zero fill the target data buffers? */\n}"
  },
  {
    "function_name": "rf_DD_GenerateFailedAccessASMs",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
    "lines": "651-969",
    "snippet": "void \nrf_DD_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tRF_SectorNum_t suoff, suend;\n\tunsigned firstDataCol, napdas, count;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end = 0;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_PhysDiskAddr_t *phys_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n#define CONS_PDA(if,start,num) \\\n  pda_p->row = asmap->if->row;    pda_p->col = asmap->if->col; \\\n  pda_p->startSector = ((asmap->if->startSector / secPerSU) * secPerSU) + start; \\\n  pda_p->numSector = num; \\\n  pda_p->next = NULL; \\\n  RF_MallocAndAdd(pda_p->bufPtr,rf_RaidAddressToByte(raidPtr,num),(char *), allocList)\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - asmap->numStripeUnitsAccessed - (ftwo == NULL ? 1 : 0));\n\t*nPQNodep = PDAPerDisk;\n\n\t/* sweep over the over accessed pda's, figuring out the number of\n\t * additional pda's to generate. Of course, skip the failed ones */\n\n\tcount = 0;\n\tfor (pda_p = asmap->physInfo; pda_p; pda_p = pda_p->next) {\n\t\tif ((pda_p == fone) || (pda_p == ftwo))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, pda_p->startSector);\n\t\tsuend = suoff + pda_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* one failed PDA to overlap */\n\t\t\t/* if a PDA doesn't contain the failed unit, it can\n\t\t\t * only miss the start or end, not both */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe */\n\t\t\tif (suoff)\t/* leak at begining */\n\t\t\t\tcount++;\n\t\t\tif (suend < numDataCol)\t/* leak at end */\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 3:\t/* two disjoint units */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\tnapdas += count;\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\t/* march through the one's up to the first accessed disk */\n\tfirstDataCol = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), asmap->physInfo->raidAddress) % numDataCol;\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < firstDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\t/* march through the touched stripe units */\n\tfor (phys_p = asmap->physInfo; phys_p; phys_p = phys_p->next, i++) {\n\t\tif ((phys_p == asmap->failedPDAs[0]) || (phys_p == asmap->failedPDAs[1]))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, phys_p->startSector);\n\t\tsuend = suoff + phys_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* single buffer */\n\t\t\tif (suoff > fone_start) {\n\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t/* The data read starts after the mapped\n\t\t\t\t * access, snip off the begining */\n\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tif (suend < fone_end) {\n\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t/* The data read stops before the end of the\n\t\t\t\t * failed access, extend */\n\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe unit */\n\t\t\tRF_ASSERT((suoff == 0) || (suend == secPerSU));\n\t\t\tif (suend < secPerSU) {\t/* short read, snip from end\n\t\t\t\t\t\t * on */\n\t\t\t\tpda_p->numSector = secPerSU - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t} else\n\t\t\t\tif (suoff > 0) {\t/* short at front */\n\t\t\t\t\tpda_p->numSector = suoff;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\t/* two nonoverlapping failures */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end)) {\n\t\t\t\tif (suoff > fone_start) {\n\t\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < fone_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end)) {\n\t\t\t\tif (suoff > ftwo_start) {\n\t\t\t\t\tRF_ASSERT(suend >= ftwo_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - ftwo_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + ftwo_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < ftwo_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= ftwo_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = ftwo_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\t/* after the last accessed disk */\n\tfor (; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}",
    "includes": [
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda_p - *pdap == napdas"
          ],
          "line": 967
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_PANIC",
          "args": [],
          "line": 962
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 959
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 959
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 955
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 951
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 951
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, secPerSU)",
            "(char *), allocList"
          ],
          "line": 945
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "secPerSU"
          ],
          "line": 945
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 941
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 941
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_DEAD_DISK",
          "args": [
            "raidPtr->Disks[pda_p->row][pda_p->col].status"
          ],
          "line": 934
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 932
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_PANIC",
          "args": [],
          "line": 922
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 916
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 916
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 915
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "suoff <= ftwo_start"
          ],
          "line": 910
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 906
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 906
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 905
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "suend >= ftwo_end"
          ],
          "line": 899
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 893
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 893
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 892
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "suoff <= fone_start"
          ],
          "line": 887
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 883
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 883
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 882
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "suend >= fone_end"
          ],
          "line": 876
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 869
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 869
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 868
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 862
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 862
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 861
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "(suoff == 0) || (suend == secPerSU)"
          ],
          "line": 856
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 851
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 851
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 850
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "suoff <= fone_start"
          ],
          "line": 845
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 841
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 841
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 840
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "suend >= fone_end"
          ],
          "line": 835
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_StripeUnitOffset",
          "args": [
            "layoutPtr",
            "phys_p->startSector"
          ],
          "line": 830
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_PANIC",
          "args": [],
          "line": 821
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 818
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 818
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 814
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 810
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 810
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, secPerSU)",
            "(char *), allocList"
          ],
          "line": 804
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "secPerSU"
          ],
          "line": 804
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda_p->bufPtr",
            "rf_RaidAddressToByte(raidPtr, pda_p->numSector)",
            "(char *), allocList"
          ],
          "line": 800
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "pda_p->numSector"
          ],
          "line": 800
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_DEAD_DISK",
          "args": [
            "raidPtr->Disks[pda_p->row][pda_p->col].status"
          ],
          "line": 793
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "raidPtr",
            "pda_p->raidAddress",
            "&(pda_p->row)",
            "&(pda_p->col)",
            "&(pda_p->startSector)",
            "0"
          ],
          "line": 791
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressOfPrevStripeBoundary",
          "args": [
            "layoutPtr",
            "asmap->raidAddress"
          ],
          "line": 785
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToStripeUnitID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->physInfo->raidAddress"
          ],
          "line": 784
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "pda_p",
            "napdas",
            "sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 776
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_PANIC",
          "args": [],
          "line": 765
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_StripeUnitOffset",
          "args": [
            "layoutPtr",
            "pda_p->startSector"
          ],
          "line": 743
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "ftwo_start",
            "ftwo->numSector"
          ],
          "line": 728
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "ftwo_start",
            "ftwo->numSector"
          ],
          "line": 725
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 722
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 719
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "*pqpdap",
            "4 * sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 717
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "0",
            "secPerSU"
          ],
          "line": 711
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "0",
            "secPerSU"
          ],
          "line": 708
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "*pqpdap",
            "2 * sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 706
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_StripeUnitOffset",
          "args": [
            "layoutPtr",
            "ftwo->startSector"
          ],
          "line": 701
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "qInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 698
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "CONS_PDA",
          "args": [
            "parityInfo",
            "fone_start",
            "fone->numSector"
          ],
          "line": 694
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "*pqpdap",
            "2 * sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 691
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_StripeUnitOffset",
          "args": [
            "layoutPtr",
            "fone->startSector"
          ],
          "line": 678
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_DD_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t ** pdap,\n    int *nNodep,\n    RF_PhysDiskAddr_t ** pqpdap,\n    int *nPQNodep,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\tint     PDAPerDisk, i;\n\tRF_SectorCount_t secPerSU = layoutPtr->sectorsPerStripeUnit;\n\tint     numDataCol = layoutPtr->numDataCol;\n\tint     state;\n\tRF_SectorNum_t suoff, suend;\n\tunsigned firstDataCol, napdas, count;\n\tRF_SectorNum_t fone_start, fone_end, ftwo_start = 0, ftwo_end = 0;\n\tRF_PhysDiskAddr_t *fone = asmap->failedPDAs[0], *ftwo = asmap->failedPDAs[1];\n\tRF_PhysDiskAddr_t *pda_p;\n\tRF_PhysDiskAddr_t *phys_p;\n\tRF_RaidAddr_t sosAddr;\n\n\t/* determine how many pda's we will have to generate per unaccess\n\t * stripe. If there is only one failed data unit, it is one; if two,\n\t * possibly two, depending wether they overlap. */\n\n\tfone_start = rf_StripeUnitOffset(layoutPtr, fone->startSector);\n\tfone_end = fone_start + fone->numSector;\n\n#define CONS_PDA(if,start,num) \\\n  pda_p->row = asmap->if->row;    pda_p->col = asmap->if->col; \\\n  pda_p->startSector = ((asmap->if->startSector / secPerSU) * secPerSU) + start; \\\n  pda_p->numSector = num; \\\n  pda_p->next = NULL; \\\n  RF_MallocAndAdd(pda_p->bufPtr,rf_RaidAddressToByte(raidPtr,num),(char *), allocList)\n\n\tif (asmap->numDataFailed == 1) {\n\t\tPDAPerDisk = 1;\n\t\tstate = 1;\n\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\tpda_p = *pqpdap;\n\t\t/* build p */\n\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\tpda_p++;\n\t\t/* build q */\n\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t} else {\n\t\tftwo_start = rf_StripeUnitOffset(layoutPtr, ftwo->startSector);\n\t\tftwo_end = ftwo_start + ftwo->numSector;\n\t\tif (fone->numSector + ftwo->numSector > secPerSU) {\n\t\t\tPDAPerDisk = 1;\n\t\t\tstate = 2;\n\t\t\tRF_MallocAndAdd(*pqpdap, 2 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, 0, secPerSU);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t} else {\n\t\t\tPDAPerDisk = 2;\n\t\t\tstate = 3;\n\t\t\t/* four of them, fone, then ftwo */\n\t\t\tRF_MallocAndAdd(*pqpdap, 4 * sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tpda_p = *pqpdap;\n\t\t\tCONS_PDA(parityInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, fone_start, fone->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(parityInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_PARITY;\n\t\t\tpda_p++;\n\t\t\tCONS_PDA(qInfo, ftwo_start, ftwo->numSector);\n\t\t\tpda_p->type = RF_PDA_TYPE_Q;\n\t\t}\n\t}\n\t/* figure out number of nonaccessed pda */\n\tnapdas = PDAPerDisk * (numDataCol - asmap->numStripeUnitsAccessed - (ftwo == NULL ? 1 : 0));\n\t*nPQNodep = PDAPerDisk;\n\n\t/* sweep over the over accessed pda's, figuring out the number of\n\t * additional pda's to generate. Of course, skip the failed ones */\n\n\tcount = 0;\n\tfor (pda_p = asmap->physInfo; pda_p; pda_p = pda_p->next) {\n\t\tif ((pda_p == fone) || (pda_p == ftwo))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, pda_p->startSector);\n\t\tsuend = suoff + pda_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* one failed PDA to overlap */\n\t\t\t/* if a PDA doesn't contain the failed unit, it can\n\t\t\t * only miss the start or end, not both */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe */\n\t\t\tif (suoff)\t/* leak at begining */\n\t\t\t\tcount++;\n\t\t\tif (suend < numDataCol)\t/* leak at end */\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase 3:\t/* two disjoint units */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end))\n\t\t\t\tcount++;\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end))\n\t\t\t\tcount++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\tnapdas += count;\n\t*nNodep = napdas;\n\tif (napdas == 0)\n\t\treturn;\t\t/* short circuit */\n\n\t/* allocate up our list of pda's */\n\n\tRF_CallocAndAdd(pda_p, napdas, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t*pdap = pda_p;\n\n\t/* linkem together */\n\tfor (i = 0; i < (napdas - 1); i++)\n\t\tpda_p[i].next = pda_p + (i + 1);\n\n\t/* march through the one's up to the first accessed disk */\n\tfirstDataCol = rf_RaidAddressToStripeUnitID(&(raidPtr->Layout), asmap->physInfo->raidAddress) % numDataCol;\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tfor (i = 0; i < firstDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\t/* march through the touched stripe units */\n\tfor (phys_p = asmap->physInfo; phys_p; phys_p = phys_p->next, i++) {\n\t\tif ((phys_p == asmap->failedPDAs[0]) || (phys_p == asmap->failedPDAs[1]))\n\t\t\tcontinue;\n\t\tsuoff = rf_StripeUnitOffset(layoutPtr, phys_p->startSector);\n\t\tsuend = suoff + phys_p->numSector;\n\t\tswitch (state) {\n\t\tcase 1:\t/* single buffer */\n\t\t\tif (suoff > fone_start) {\n\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t/* The data read starts after the mapped\n\t\t\t\t * access, snip off the begining */\n\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tif (suend < fone_end) {\n\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t/* The data read stops before the end of the\n\t\t\t\t * failed access, extend */\n\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\t/* whole stripe unit */\n\t\t\tRF_ASSERT((suoff == 0) || (suend == secPerSU));\n\t\t\tif (suend < secPerSU) {\t/* short read, snip from end\n\t\t\t\t\t\t * on */\n\t\t\t\tpda_p->numSector = secPerSU - suend;\n\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\tpda_p++;\n\t\t\t} else\n\t\t\t\tif (suoff > 0) {\t/* short at front */\n\t\t\t\t\tpda_p->numSector = suoff;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\t/* two nonoverlapping failures */\n\t\t\tif ((suoff > fone_start) || (suend < fone_end)) {\n\t\t\t\tif (suoff > fone_start) {\n\t\t\t\t\tRF_ASSERT(suend >= fone_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - fone_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + fone_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < fone_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= fone_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = fone_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ((suoff > ftwo_start) || (suend < ftwo_end)) {\n\t\t\t\tif (suoff > ftwo_start) {\n\t\t\t\t\tRF_ASSERT(suend >= ftwo_end);\n\t\t\t\t\t/* The data read starts after the\n\t\t\t\t\t * mapped access, snip off the\n\t\t\t\t\t * begining */\n\t\t\t\t\tpda_p->numSector = suoff - ftwo_start;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + ftwo_start;\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t\tif (suend < ftwo_end) {\n\t\t\t\t\tRF_ASSERT(suoff <= ftwo_start);\n\t\t\t\t\t/* The data read stops before the end\n\t\t\t\t\t * of the failed access, extend */\n\t\t\t\t\tpda_p->numSector = ftwo_end - suend;\n\t\t\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU) + suend;\t/* off by one? */\n\t\t\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\t\t\tpda_p++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t}\n\n\t/* after the last accessed disk */\n\tfor (; i < numDataCol; i++) {\n\t\tif ((pda_p - (*pdap)) == napdas)\n\t\t\tcontinue;\n\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t/* skip over dead disks */\n\t\tif (RF_DEAD_DISK(raidPtr->Disks[pda_p->row][pda_p->col].status))\n\t\t\tcontinue;\n\t\tswitch (state) {\n\t\tcase 1:\t/* fone */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 2:\t/* full stripe */\n\t\t\tpda_p->numSector = secPerSU;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, secPerSU), (char *), allocList);\n\t\t\tbreak;\n\t\tcase 3:\t/* two slabs */\n\t\t\tpda_p->numSector = fone->numSector;\n\t\t\tpda_p->raidAddress += fone_start;\n\t\t\tpda_p->startSector += fone_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tpda_p++;\n\t\t\tpda_p->type = RF_PDA_TYPE_DATA;\n\t\t\tpda_p->raidAddress = sosAddr + (i * secPerSU);\n\t\t\t(raidPtr->Layout.map->MapSector) (raidPtr, pda_p->raidAddress, &(pda_p->row), &(pda_p->col), &(pda_p->startSector), 0);\n\t\t\tpda_p->numSector = ftwo->numSector;\n\t\t\tpda_p->raidAddress += ftwo_start;\n\t\t\tpda_p->startSector += ftwo_start;\n\t\t\tRF_MallocAndAdd(pda_p->bufPtr, rf_RaidAddressToByte(raidPtr, pda_p->numSector), (char *), allocList);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tRF_PANIC();\n\t\t}\n\t\tpda_p++;\n\t}\n\n\tRF_ASSERT(pda_p - *pdap == napdas);\n\treturn;\n}"
  },
  {
    "function_name": "rf_CreateRaidCDegradedReadDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
    "lines": "526-647",
    "snippet": "void \nrf_CreateRaidCDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *nodes, *rdNode, *blockNode, *commitNode, *termNode;\n\tRF_StripeNum_t parityStripeID;\n\tint     useMirror, i, shiftable;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda;\n\n\tif ((asmap->numDataFailed + asmap->numParityFailed) == 0) {\n\t\tshiftable = RF_TRUE;\n\t} else {\n\t\tshiftable = RF_FALSE;\n\t}\n\tuseMirror = 0;\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID C degraded read DAG]\\n\");\n\t}\n\tdag_h->creator = \"RaidCDegradedReadDAG\";\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 0)\n\t\tuseMirror = RF_FALSE;\n\telse\n\t\tuseMirror = RF_TRUE;\n\n\t/* total number of nodes = 1 + (block + commit + terminator) */\n\tRF_CallocAndAdd(nodes, 4, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\trdNode = &nodes[i];\n\ti++;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\n\t/*\n         * This dag can not commit until the commit node is reached.\n         * Errors prior to the commit point imply the dag has failed\n         * and must be retried.\n         */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the block, commit, and terminator nodes */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\tpda = asmap->physInfo;\n\tRF_ASSERT(pda != NULL);\n\t/* parityInfo must describe entire parity unit */\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\n\t/* initialize the data node */\n\tif (!useMirror) {\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rpd\", allocList);\n\t\tif (shiftable && rf_compute_workload_shift(raidPtr, pda)) {\n\t\t\t/* shift this read to the next disk in line */\n\t\t\trdNode->params[0].p = asmap->parityInfo;\n\t\t\trdNode->params[1].p = pda->bufPtr;\n\t\t\trdNode->params[2].v = parityStripeID;\n\t\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t} else {\n\t\t\t/* read primary copy */\n\t\t\trdNode->params[0].p = pda;\n\t\t\trdNode->params[1].p = pda->bufPtr;\n\t\t\trdNode->params[2].v = parityStripeID;\n\t\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t} else {\n\t\t/* read secondary copy of data */\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rsd\", allocList);\n\t\trdNode->params[0].p = asmap->parityInfo;\n\t\trdNode->params[1].p = pda->bufPtr;\n\t\trdNode->params[2].v = parityStripeID;\n\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to rdnode */\n\tRF_ASSERT(blockNode->numSuccedents == 1);\n\tRF_ASSERT(rdNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rdNode;\n\trdNode->antecedents[0] = blockNode;\n\trdNode->antType[0] = rf_control;\n\n\t/* connect rdnode to commit node */\n\tRF_ASSERT(rdNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\trdNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = rdNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect commit node to terminator */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = commitNode;\n\ttermNode->antType[0] = rf_control;\n}",
    "includes": [
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 643
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 642
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numSuccedents == 1"
          ],
          "line": 641
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numAntecedents == 1"
          ],
          "line": 635
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rdNode->numSuccedents == 1"
          ],
          "line": 634
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rdNode->numAntecedents == 1"
          ],
          "line": 628
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == 1"
          ],
          "line": 627
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 623
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "dag_h->numSuccedents == 1"
          ],
          "line": 622
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 618
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "rdNode",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskReadFunc",
            "rf_DiskReadUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Rsd\"",
            "allocList"
          ],
          "line": 613
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 609
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 603
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_compute_workload_shift",
          "args": [
            "raidPtr",
            "pda"
          ],
          "line": 598
        },
        "resolved": true,
        "details": {
          "function_name": "rf_compute_workload_shift",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "1090-1166",
          "snippet": "int \nrf_compute_workload_shift(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * pda)\n{\n\t/*\n         * variables:\n         *  d   = column of disk containing primary\n         *  f   = column of failed disk\n         *  n   = number of disks in array\n         *  sd  = \"shift distance\" (number of columns that d is to the right of f)\n         *  row = row of array the access is in\n         *  v   = numerator of redirection ratio\n         *  k   = denominator of redirection ratio\n         */\n\tRF_RowCol_t d, f, sd, row, n;\n\tint     k, v, ret, i;\n\n\trow = pda->row;\n\tn = raidPtr->numCol;\n\n\t/* assign column of primary copy to d */\n\td = pda->col;\n\n\t/* assign column of dead disk to f */\n\tfor (f = 0; ((!RF_DEAD_DISK(raidPtr->Disks[row][f].status)) && (f < n)); f++);\n\n\tRF_ASSERT(f < n);\n\tRF_ASSERT(f != d);\n\n\tsd = (f > d) ? (n + d - f) : (d - f);\n\tRF_ASSERT(sd < n);\n\n\t/*\n         * v of every k accesses should be redirected\n         *\n         * v/k := (n-1-sd)/(n-1)\n         */\n\tv = (n - 1 - sd);\n\tk = (n - 1);\n\n#if 1\n\t/*\n         * XXX\n         * Is this worth it?\n         *\n         * Now reduce the fraction, by repeatedly factoring\n         * out primes (just like they teach in elementary school!)\n         */\n\tfor (i = 0; i < NLOWPRIMES; i++) {\n\t\tif (lowprimes[i] > v)\n\t\t\tbreak;\n\t\twhile (((v % lowprimes[i]) == 0) && ((k % lowprimes[i]) == 0)) {\n\t\t\tv /= lowprimes[i];\n\t\t\tk /= lowprimes[i];\n\t\t}\n\t}\n#endif\n\n\traidPtr->hist_diskreq[row][d]++;\n\tif (raidPtr->hist_diskreq[row][d] > v) {\n\t\tret = 0;\t/* do not redirect */\n\t} else {\n\t\tret = 1;\t/* redirect */\n\t}\n\n#if 0\n\tprintf(\"d=%d f=%d sd=%d v=%d k=%d ret=%d h=%d\\n\", d, f, sd, v, k, ret,\n\t    raidPtr->hist_diskreq[row][d]);\n#endif\n\n\tif (raidPtr->hist_diskreq[row][d] >= k) {\n\t\t/* reset counter */\n\t\traidPtr->hist_diskreq[row][d] = 0;\n\t}\n\treturn (ret);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [
            "#define NLOWPRIMES 8"
          ],
          "globals_used": [
            "static int lowprimes[NLOWPRIMES] = {2, 3, 5, 7, 11, 13, 17, 19};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\n#define NLOWPRIMES 8\n\nstatic int lowprimes[NLOWPRIMES] = {2, 3, 5, 7, 11, 13, 17, 19};\n\nint \nrf_compute_workload_shift(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * pda)\n{\n\t/*\n         * variables:\n         *  d   = column of disk containing primary\n         *  f   = column of failed disk\n         *  n   = number of disks in array\n         *  sd  = \"shift distance\" (number of columns that d is to the right of f)\n         *  row = row of array the access is in\n         *  v   = numerator of redirection ratio\n         *  k   = denominator of redirection ratio\n         */\n\tRF_RowCol_t d, f, sd, row, n;\n\tint     k, v, ret, i;\n\n\trow = pda->row;\n\tn = raidPtr->numCol;\n\n\t/* assign column of primary copy to d */\n\td = pda->col;\n\n\t/* assign column of dead disk to f */\n\tfor (f = 0; ((!RF_DEAD_DISK(raidPtr->Disks[row][f].status)) && (f < n)); f++);\n\n\tRF_ASSERT(f < n);\n\tRF_ASSERT(f != d);\n\n\tsd = (f > d) ? (n + d - f) : (d - f);\n\tRF_ASSERT(sd < n);\n\n\t/*\n         * v of every k accesses should be redirected\n         *\n         * v/k := (n-1-sd)/(n-1)\n         */\n\tv = (n - 1 - sd);\n\tk = (n - 1);\n\n#if 1\n\t/*\n         * XXX\n         * Is this worth it?\n         *\n         * Now reduce the fraction, by repeatedly factoring\n         * out primes (just like they teach in elementary school!)\n         */\n\tfor (i = 0; i < NLOWPRIMES; i++) {\n\t\tif (lowprimes[i] > v)\n\t\t\tbreak;\n\t\twhile (((v % lowprimes[i]) == 0) && ((k % lowprimes[i]) == 0)) {\n\t\t\tv /= lowprimes[i];\n\t\t\tk /= lowprimes[i];\n\t\t}\n\t}\n#endif\n\n\traidPtr->hist_diskreq[row][d]++;\n\tif (raidPtr->hist_diskreq[row][d] > v) {\n\t\tret = 0;\t/* do not redirect */\n\t} else {\n\t\tret = 1;\t/* redirect */\n\t}\n\n#if 0\n\tprintf(\"d=%d f=%d sd=%d v=%d k=%d ret=%d h=%d\\n\", d, f, sd, v, k, ret,\n\t    raidPtr->hist_diskreq[row][d]);\n#endif\n\n\tif (raidPtr->hist_diskreq[row][d] >= k) {\n\t\t/* reset counter */\n\t\traidPtr->hist_diskreq[row][d] = 0;\n\t}\n\treturn (ret);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->parityInfo->next == NULL"
          ],
          "line": 592
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 590
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "4",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 561
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating RAID C degraded read DAG]\\n\""
          ],
          "line": 551
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 547
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateRaidCDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *nodes, *rdNode, *blockNode, *commitNode, *termNode;\n\tRF_StripeNum_t parityStripeID;\n\tint     useMirror, i, shiftable;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda;\n\n\tif ((asmap->numDataFailed + asmap->numParityFailed) == 0) {\n\t\tshiftable = RF_TRUE;\n\t} else {\n\t\tshiftable = RF_FALSE;\n\t}\n\tuseMirror = 0;\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID C degraded read DAG]\\n\");\n\t}\n\tdag_h->creator = \"RaidCDegradedReadDAG\";\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 0)\n\t\tuseMirror = RF_FALSE;\n\telse\n\t\tuseMirror = RF_TRUE;\n\n\t/* total number of nodes = 1 + (block + commit + terminator) */\n\tRF_CallocAndAdd(nodes, 4, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\trdNode = &nodes[i];\n\ti++;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\n\t/*\n         * This dag can not commit until the commit node is reached.\n         * Errors prior to the commit point imply the dag has failed\n         * and must be retried.\n         */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the block, commit, and terminator nodes */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\tpda = asmap->physInfo;\n\tRF_ASSERT(pda != NULL);\n\t/* parityInfo must describe entire parity unit */\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\n\t/* initialize the data node */\n\tif (!useMirror) {\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rpd\", allocList);\n\t\tif (shiftable && rf_compute_workload_shift(raidPtr, pda)) {\n\t\t\t/* shift this read to the next disk in line */\n\t\t\trdNode->params[0].p = asmap->parityInfo;\n\t\t\trdNode->params[1].p = pda->bufPtr;\n\t\t\trdNode->params[2].v = parityStripeID;\n\t\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t} else {\n\t\t\t/* read primary copy */\n\t\t\trdNode->params[0].p = pda;\n\t\t\trdNode->params[1].p = pda->bufPtr;\n\t\t\trdNode->params[2].v = parityStripeID;\n\t\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t} else {\n\t\t/* read secondary copy of data */\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rsd\", allocList);\n\t\trdNode->params[0].p = asmap->parityInfo;\n\t\trdNode->params[1].p = pda->bufPtr;\n\t\trdNode->params[2].v = parityStripeID;\n\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to rdnode */\n\tRF_ASSERT(blockNode->numSuccedents == 1);\n\tRF_ASSERT(rdNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rdNode;\n\trdNode->antecedents[0] = blockNode;\n\trdNode->antType[0] = rf_control;\n\n\t/* connect rdnode to commit node */\n\tRF_ASSERT(rdNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\trdNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = rdNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect commit node to terminator */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = commitNode;\n\ttermNode->antType[0] = rf_control;\n}"
  },
  {
    "function_name": "rf_CreateDegradedReadDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
    "lines": "249-507",
    "snippet": "void \nrf_CreateDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * recFunc)\n{\n\tRF_DagNode_t *nodes, *rudNodes, *rrdNodes, *xorNode, *blockNode;\n\tRF_DagNode_t *commitNode, *rpNode, *termNode;\n\tint     nNodes, nRrdNodes, nRudNodes, nXorBufs, i;\n\tint     j, paramNum;\n\tRF_SectorCount_t sectorsPerSU;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *overlappingPDAs;/* a temporary array of flags */\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_PhysDiskAddr_t *pda, *parityPDA;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tRF_RaidLayout_t *layoutPtr;\n\tchar   *rpBuf;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\t/* failedPDA points to the pda within the asm that targets the failed\n\t * disk */\n\tfailedPDA = asmap->failedPDAs[0];\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,\n\t    asmap->raidAddress, &which_ru);\n\tsectorsPerSU = layoutPtr->sectorsPerStripeUnit;\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating degraded read DAG]\\n\");\n\t}\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"DegradedReadDAG\";\n\n\t/*\n         * generate two ASMs identifying the surviving data we need\n         * in order to recover the lost data\n         */\n\n\t/* overlappingPDAs array must be zero'd */\n\tRF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));\n\trf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h, &nXorBufs,\n\t    &rpBuf, overlappingPDAs, allocList);\n\n\t/*\n         * create all the nodes at once\n         *\n         * -1 because no access is generated for the failed pda\n         */\n\tnRudNodes = asmap->numStripeUnitsAccessed - 1;\n\tnRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +\n\t    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);\n\tnNodes = 5 + nRudNodes + nRrdNodes;\t/* lock, unlock, xor, Rp, Rud,\n\t\t\t\t\t\t * Rrd */\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *),\n\t    allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\txorNode = &nodes[i];\n\ti++;\n\trpNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\trudNodes = &nodes[i];\n\ti += nRudNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\tRF_ASSERT(i == nNodes);\n\n\t/* initialize nodes */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\t/* this dag can not commit until the commit node is reached errors\n\t * prior to the commit point imply the dag has failed */\n\tdag_h->numSuccedents = 1;\n\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nRudNodes + nRrdNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, recFunc->simple, rf_NullNodeUndoFunc,\n\t    NULL, 1, nRudNodes + nRrdNodes + 1, 2 * nXorBufs + 2, 1, dag_h,\n\t    recFunc->SimpleName, allocList);\n\n\t/* fill in the Rud nodes */\n\tfor (pda = asmap->physInfo, i = 0; i < nRudNodes; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\trf_InitNode(&rudNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Rud\", allocList);\n\t\tRF_ASSERT(pda);\n\t\trudNodes[i].params[0].p = pda;\n\t\trudNodes[i].params[1].p = pda->bufPtr;\n\t\trudNodes[i].params[2].v = parityStripeID;\n\t\trudNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* fill in the Rrd nodes */\n\ti = 0;\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo;\n\t\t    i < new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t\t    i++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i].params[0].p = pda;\n\t\t\trrdNodes[i].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i].params[2].v = parityStripeID;\n\t\t\trrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (j = 0, pda = new_asm_h[1]->stripeMap->physInfo;\n\t\t    j < new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t\t    j++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i + j].params[0].p = pda;\n\t\t\trrdNodes[i + j].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i + j].params[2].v = parityStripeID;\n\t\t\trrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* make a PDA for the parity unit */\n\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\tparityPDA->row = asmap->parityInfo->row;\n\tparityPDA->col = asmap->parityInfo->col;\n\tparityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)\n\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\tparityPDA->numSector = failedPDA->numSector;\n\n\t/* initialize the Rp node */\n\trf_InitNode(rpNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rp \", allocList);\n\trpNode->params[0].p = parityPDA;\n\trpNode->params[1].p = rpBuf;\n\trpNode->params[2].v = parityStripeID;\n\trpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t/*\n         * the last and nastiest step is to assign all\n         * the parameters of the Xor node\n         */\n\tparamNum = 0;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\t/* all the Rrd nodes need to be xored together */\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[0];\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[1];\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\t/* any Rud nodes that overlap the failed access need to be\n\t\t * xored in */\n\t\tif (overlappingPDAs[i]) {\n\t\t\tRF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tbcopy((char *) rudNodes[i].params[0].p, (char *) pda, sizeof(RF_PhysDiskAddr_t));\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);\n\t\t\txorNode->params[paramNum++].p = pda;\n\t\t\txorNode->params[paramNum++].p = pda->bufPtr;\n\t\t}\n\t}\n\tRF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));\n\n\t/* install parity pda as last set of params to be xor'd */\n\txorNode->params[paramNum++].p = parityPDA;\n\txorNode->params[paramNum++].p = rpBuf;\n\n\t/*\n         * the last 2 params to the recovery xor node are\n         * the failed PDA and the raidPtr\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = raidPtr;\n\tRF_ASSERT(paramNum == 2 * nXorBufs + 2);\n\n\t/*\n         * The xor node uses results[0] as the target buffer.\n         * Set pointer and zero the buffer. In the kernel, this\n         * may be a user buffer in which case we have to remap it.\n         */\n\txorNode->results[0] = failedPDA->bufPtr;\n\tRF_BZERO(bp, failedPDA->bufPtr, rf_RaidAddressToByte(raidPtr,\n\t\tfailedPDA->numSector));\n\n\t/* connect nodes to form graph */\n\t/* connect the header to the block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the read nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rpNode;\n\trpNode->antecedents[0] = blockNode;\n\trpNode->antType[0] = rf_control;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + i] = &rrdNodes[i];\n\t\trrdNodes[i].antecedents[0] = blockNode;\n\t\trrdNodes[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\trudNodes[i].antecedents[0] = blockNode;\n\t\trudNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the read nodes to the xor node */\n\tRF_ASSERT(xorNode->numAntecedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numSuccedents == 1);\n\trpNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = rpNode;\n\txorNode->antType[0] = rf_trueData;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\trrdNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + i] = &rrdNodes[i];\n\t\txorNode->antType[1 + i] = rf_trueData;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\trudNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\txorNode->antType[1 + nRrdNodes + i] = rf_trueData;\n\t}\n\n\t/* connect the xor node to the commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect the termNode to the commit node */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antType[0] = rf_control;\n\ttermNode->antecedents[0] = commitNode;\n}",
    "includes": [
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 503
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 502
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numSuccedents == 1"
          ],
          "line": 501
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numAntecedents == 1"
          ],
          "line": 495
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numSuccedents == 1"
          ],
          "line": 494
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rudNodes[i].numSuccedents == 1"
          ],
          "line": 487
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rrdNodes[i].numSuccedents == 1"
          ],
          "line": 481
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rpNode->numSuccedents == 1"
          ],
          "line": 476
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numAntecedents == (1 + nRrdNodes + nRudNodes)"
          ],
          "line": 475
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rudNodes[i].numSuccedents == 1"
          ],
          "line": 468
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rrdNodes[i].numSuccedents == 1"
          ],
          "line": 462
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rpNode->numAntecedents == 1"
          ],
          "line": 457
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == (1 + nRrdNodes + nRudNodes)"
          ],
          "line": 456
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 452
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "dag_h->numSuccedents == 1"
          ],
          "line": 451
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_BZERO",
          "args": [
            "bp",
            "failedPDA->bufPtr",
            "rf_RaidAddressToByte(raidPtr,\n\t\tfailedPDA->numSector)"
          ],
          "line": 446
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "failedPDA->numSector"
          ],
          "line": 446
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "paramNum == 2 * nXorBufs + 2"
          ],
          "line": 438
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_Free",
          "args": [
            "overlappingPDAs",
            "asmap->numStripeUnitsAccessed * sizeof(char)"
          ],
          "line": 426
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RangeRestrictPDA",
          "args": [
            "raidPtr",
            "failedPDA",
            "pda",
            "RF_RESTRICT_DOBUFFER",
            "0"
          ],
          "line": 421
        },
        "resolved": true,
        "details": {
          "function_name": "rf_RangeRestrictPDA",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "1050-1075",
          "snippet": "void \nrf_RangeRestrictPDA(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * src,\n    RF_PhysDiskAddr_t * dest,\n    int dobuffer,\n    int doraidaddr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_SectorNum_t soffs = rf_StripeUnitOffset(layoutPtr, src->startSector);\n\tRF_SectorNum_t doffs = rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\tRF_SectorNum_t send = rf_StripeUnitOffset(layoutPtr, src->startSector + src->numSector - 1);\t/* use -1 to be sure we\n\t\t\t\t\t\t\t\t\t\t\t\t\t * stay within SU */\n\tRF_SectorNum_t dend = rf_StripeUnitOffset(layoutPtr, dest->startSector + dest->numSector - 1);\n\tRF_SectorNum_t subAddr = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->startSector);\t/* stripe unit boundary */\n\n\tdest->startSector = subAddr + RF_MAX(soffs, doffs);\n\tdest->numSector = subAddr + RF_MIN(send, dend) + 1 - dest->startSector;\n\n\tif (dobuffer)\n\t\tdest->bufPtr += (soffs > doffs) ? rf_RaidAddressToByte(raidPtr, soffs - doffs) : 0;\n\tif (doraidaddr) {\n\t\tdest->raidAddress = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->raidAddress) +\n\t\t    rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nvoid \nrf_RangeRestrictPDA(\n    RF_Raid_t * raidPtr,\n    RF_PhysDiskAddr_t * src,\n    RF_PhysDiskAddr_t * dest,\n    int dobuffer,\n    int doraidaddr)\n{\n\tRF_RaidLayout_t *layoutPtr = &raidPtr->Layout;\n\tRF_SectorNum_t soffs = rf_StripeUnitOffset(layoutPtr, src->startSector);\n\tRF_SectorNum_t doffs = rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\tRF_SectorNum_t send = rf_StripeUnitOffset(layoutPtr, src->startSector + src->numSector - 1);\t/* use -1 to be sure we\n\t\t\t\t\t\t\t\t\t\t\t\t\t * stay within SU */\n\tRF_SectorNum_t dend = rf_StripeUnitOffset(layoutPtr, dest->startSector + dest->numSector - 1);\n\tRF_SectorNum_t subAddr = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->startSector);\t/* stripe unit boundary */\n\n\tdest->startSector = subAddr + RF_MAX(soffs, doffs);\n\tdest->numSector = subAddr + RF_MIN(send, dend) + 1 - dest->startSector;\n\n\tif (dobuffer)\n\t\tdest->bufPtr += (soffs > doffs) ? rf_RaidAddressToByte(raidPtr, soffs - doffs) : 0;\n\tif (doraidaddr) {\n\t\tdest->raidAddress = rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, dest->raidAddress) +\n\t\t    rf_StripeUnitOffset(layoutPtr, dest->startSector);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "bcopy",
          "args": [
            "(char *) rudNodes[i].params[0].p",
            "(char *) pda",
            "sizeof(RF_PhysDiskAddr_t)"
          ],
          "line": 420
        },
        "resolved": true,
        "details": {
          "function_name": "tr_bcopy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/ic/tropic.c",
          "lines": "1618-1663",
          "snippet": "void \ntr_bcopy(sc, dest, len)\nstruct tr_softc *sc;\t/* pointer to softc struct for this adapter */\nu_char *dest;\t\t/* destination address */\nint len;\t\t/* number of bytes to copy */\n{\n\tstruct rbcb *rbc = &sc->rbc;\t/* pointer to rec buf ctl blk */\n\n\t/* While amount of data needed >= amount in current receive buffer. */\n\twhile (len >= rbc->data_len) {\n\t\t/* Copy all data from receive buffer to destination. */\n\n\t\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t\t    rbc->rbuf_datap, dest, (bus_size_t)rbc->data_len);\n\t\tlen -= rbc->data_len;\t/* update length left to transfer */\n\t\tdest += rbc->data_len;\t/* update destination address */\n\n\t\t/* Make next receive buffer current receive buffer. */\n\t\trbc->rbufp = rbc->rbufp_next;\n\t\tif (rbc->rbufp != 0) { /* More receive buffers? */\n\n\t\t\t/* Calculate pointer to next receive buffer. */\n\t\t\trbc->rbufp_next = RB_INW(sc, rbc->rbufp, RB_NEXTBUF);\n\t\t\tif (rbc->rbufp_next != 0)\n\t\t\t\trbc->rbufp_next -= RB_NEXTBUF;\n\n\t\t\t/* Get pointer to data in current receive buffer. */\n\t\t\trbc->rbuf_datap = rbc->rbufp + RB_DATA;\n\n\t\t\t/* Get length of data in current receive buffer. */\n\t\t\trbc->data_len = RB_INW(sc, rbc->rbufp, RB_BUFLEN);\n\t\t}\n\t\telse {\n\t\t\tif (len != 0)\t/* len should equal zero. */\n\t\t\t\tprintf(\"tr_bcopy: residual data not copied\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Amount of data needed is < amount in current receive buffer. */\n\n\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t    rbc->rbuf_datap, dest, (bus_size_t)len);\n\trbc->data_len -= len;\t/* Update count of data in receive buffer. */\n\trbc->rbuf_datap += len;\t/* Update pointer to receive buffer data. */\n}",
          "includes": [
            "#include <dev/ic/tropicvar.h>",
            "#include <dev/ic/tropicreg.h>",
            "#include <machine/bus.h>",
            "#include <machine/cpu.h>",
            "#include <net/bpfdesc.h>",
            "#include <net/bpf.h>",
            "#include <netns/ns_if.h>",
            "#include <netns/ns.h>",
            "#include <net/if_token.h>",
            "#include <netinet/in_var.h>",
            "#include <netinet/ip.h>",
            "#include <netinet/if_ether.h>",
            "#include <netinet/in_systm.h>",
            "#include <netinet/in.h>",
            "#include <net/route.h>",
            "#include <net/netisr.h>",
            "#include <net/if_media.h>",
            "#include <net/if_llc.h>",
            "#include <net/if.h>",
            "#include <sys/device.h>",
            "#include <sys/errno.h>",
            "#include <sys/ioctl.h>",
            "#include <sys/syslog.h>",
            "#include <sys/socket.h>",
            "#include <sys/buf.h>",
            "#include <sys/mbuf.h>",
            "#include <sys/proc.h>",
            "#include <sys/kernel.h>",
            "#include <sys/systm.h>",
            "#include <sys/param.h>",
            "#include \"bpfilter.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "void\ttr_bcopy",
            "struct mbuf *\ntr_get(sc, totlen, ifp)\nstruct tr_softc *sc;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <dev/ic/tropicvar.h>\n#include <dev/ic/tropicreg.h>\n#include <machine/bus.h>\n#include <machine/cpu.h>\n#include <net/bpfdesc.h>\n#include <net/bpf.h>\n#include <netns/ns_if.h>\n#include <netns/ns.h>\n#include <net/if_token.h>\n#include <netinet/in_var.h>\n#include <netinet/ip.h>\n#include <netinet/if_ether.h>\n#include <netinet/in_systm.h>\n#include <netinet/in.h>\n#include <net/route.h>\n#include <net/netisr.h>\n#include <net/if_media.h>\n#include <net/if_llc.h>\n#include <net/if.h>\n#include <sys/device.h>\n#include <sys/errno.h>\n#include <sys/ioctl.h>\n#include <sys/syslog.h>\n#include <sys/socket.h>\n#include <sys/buf.h>\n#include <sys/mbuf.h>\n#include <sys/proc.h>\n#include <sys/kernel.h>\n#include <sys/systm.h>\n#include <sys/param.h>\n#include \"bpfilter.h\"\n\nvoid\ttr_bcopy;\nstruct mbuf *\ntr_get(sc, totlen, ifp)\nstruct tr_softc *sc;\n\nvoid \ntr_bcopy(sc, dest, len)\nstruct tr_softc *sc;\t/* pointer to softc struct for this adapter */\nu_char *dest;\t\t/* destination address */\nint len;\t\t/* number of bytes to copy */\n{\n\tstruct rbcb *rbc = &sc->rbc;\t/* pointer to rec buf ctl blk */\n\n\t/* While amount of data needed >= amount in current receive buffer. */\n\twhile (len >= rbc->data_len) {\n\t\t/* Copy all data from receive buffer to destination. */\n\n\t\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t\t    rbc->rbuf_datap, dest, (bus_size_t)rbc->data_len);\n\t\tlen -= rbc->data_len;\t/* update length left to transfer */\n\t\tdest += rbc->data_len;\t/* update destination address */\n\n\t\t/* Make next receive buffer current receive buffer. */\n\t\trbc->rbufp = rbc->rbufp_next;\n\t\tif (rbc->rbufp != 0) { /* More receive buffers? */\n\n\t\t\t/* Calculate pointer to next receive buffer. */\n\t\t\trbc->rbufp_next = RB_INW(sc, rbc->rbufp, RB_NEXTBUF);\n\t\t\tif (rbc->rbufp_next != 0)\n\t\t\t\trbc->rbufp_next -= RB_NEXTBUF;\n\n\t\t\t/* Get pointer to data in current receive buffer. */\n\t\t\trbc->rbuf_datap = rbc->rbufp + RB_DATA;\n\n\t\t\t/* Get length of data in current receive buffer. */\n\t\t\trbc->data_len = RB_INW(sc, rbc->rbufp, RB_BUFLEN);\n\t\t}\n\t\telse {\n\t\t\tif (len != 0)\t/* len should equal zero. */\n\t\t\t\tprintf(\"tr_bcopy: residual data not copied\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Amount of data needed is < amount in current receive buffer. */\n\n\tbus_space_read_region_1(sc->sc_memt, sc->sc_sramh,\n\t    rbc->rbuf_datap, dest, (bus_size_t)len);\n\trbc->data_len -= len;\t/* Update count of data in receive buffer. */\n\trbc->rbuf_datap += len;\t/* Update pointer to receive buffer data. */\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "pda",
            "sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 419
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 403
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "rpNode",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskReadFunc",
            "rf_DiskReadUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Rp \"",
            "allocList"
          ],
          "line": 398
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_MallocAndAdd",
          "args": [
            "parityPDA",
            "sizeof(RF_PhysDiskAddr_t)",
            "(RF_PhysDiskAddr_t *), allocList"
          ],
          "line": 390
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 386
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 382
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 372
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 368
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 356
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda"
          ],
          "line": 352
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == nNodes"
          ],
          "line": 324
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *),\n\t    allocList"
          ],
          "line": 307
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_GenerateFailedAccessASMs",
          "args": [
            "raidPtr",
            "asmap",
            "failedPDA",
            "dag_h",
            "new_asm_h",
            "&nXorBufs",
            "&rpBuf",
            "overlappingPDAs",
            "allocList"
          ],
          "line": 294
        },
        "resolved": true,
        "details": {
          "function_name": "rf_GenerateFailedAccessASMs",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "899-1025",
          "snippet": "void \nrf_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t * failedPDA,\n    RF_DagHeader_t * dag_h,\n    RF_AccessStripeMapHeader_t ** new_asm_h,\n    int *nXorBufs,\n    char **rpBufPtr,\n    char *overlappingPDAs,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\n\t/* s=start, e=end, s=stripe, a=access, f=failed, su=stripe unit */\n\tRF_RaidAddr_t sosAddr, sosEndAddr, eosStartAddr, eosAddr;\n\n\tRF_SectorCount_t numSect[2], numParitySect;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *rdBuf, *bufP;\n\tint     foundit, i;\n\n\tbufP = NULL;\n\tfoundit = 0;\n\t/* first compute the following raid addresses: start of stripe,\n\t * (sosAddr) MIN(start of access, start of failed SU),   (sosEndAddr)\n\t * MAX(end of access, end of failed SU),       (eosStartAddr) end of\n\t * stripe (i.e. start of next stripe)   (eosAddr) */\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tsosEndAddr = RF_MIN(asmap->raidAddress, rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosStartAddr = RF_MAX(asmap->endRaidAddress, rf_RaidAddressOfNextStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosAddr = rf_RaidAddressOfNextStripeBoundary(layoutPtr, asmap->raidAddress);\n\n\t/* now generate access stripe maps for each of the above regions of\n\t * the stripe.  Use a dummy (NULL) buf ptr for now */\n\n\tnew_asm_h[0] = (sosAddr != sosEndAddr) ? rf_MapAccess(raidPtr, sosAddr, sosEndAddr - sosAddr, NULL, RF_DONT_REMAP) : NULL;\n\tnew_asm_h[1] = (eosStartAddr != eosAddr) ? rf_MapAccess(raidPtr, eosStartAddr, eosAddr - eosStartAddr, NULL, RF_DONT_REMAP) : NULL;\n\n\t/* walk through the PDAs and range-restrict each SU to the region of\n\t * the SU touched on the failed PDA.  also compute total data buffer\n\t * space requirements in this step.  Ignore the parity for now. */\n\n\tnumSect[0] = numSect[1] = 0;\n\tif (new_asm_h[0]) {\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[0] += pda->numSector;\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[1] += pda->numSector;\n\t\t}\n\t}\n\tnumParitySect = failedPDA->numSector;\n\n\t/* allocate buffer space for the data & parity we have to read to\n\t * recover from the failure */\n\n\tif (numSect[0] + numSect[1] + ((rpBufPtr) ? numParitySect : 0)) {\t/* don't allocate parity\n\t\t\t\t\t\t\t\t\t\t * buf if not needed */\n\t\tRF_MallocAndAdd(rdBuf, rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (char *), allocList);\n\t\tbufP = rdBuf;\n\t\tif (rf_degDagDebug)\n\t\t\tprintf(\"Newly allocated buffer (%d bytes) is 0x%lx\\n\",\n\t\t\t    (int) rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (unsigned long) bufP);\n\t}\n\t/* now walk through the pdas one last time and assign buffer pointers\n\t * (ugh!).  Again, ignore the parity.  also, count nodes to find out\n\t * how many bufs need to be xored together */\n\t(*nXorBufs) = 1;\t/* in read case, 1 is for parity.  In write\n\t\t\t\t * case, 1 is for failed data */\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t*nXorBufs += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t(*nXorBufs) += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (rpBufPtr)\n\t\t*rpBufPtr = bufP;\t/* the rest of the buffer is for\n\t\t\t\t\t * parity */\n\n\t/* the last step is to figure out how many more distinct buffers need\n\t * to get xor'd to produce the missing unit.  there's one for each\n\t * user-data read node that overlaps the portion of the failed unit\n\t * being accessed */\n\n\tfor (foundit = i = 0, pda = asmap->physInfo; pda; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tfoundit = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (rf_PDAOverlap(layoutPtr, pda, failedPDA)) {\n\t\t\toverlappingPDAs[i] = 1;\n\t\t\t(*nXorBufs)++;\n\t\t}\n\t}\n\tif (!foundit) {\n\t\tRF_ERRORMSG(\"GenerateFailedAccessASMs: did not find failedPDA in asm list\\n\");\n\t\tRF_ASSERT(0);\n\t}\n\tif (rf_degDagDebug) {\n\t\tif (new_asm_h[0]) {\n\t\t\tprintf(\"First asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[0], 1);\n\t\t}\n\t\tif (new_asm_h[1]) {\n\t\t\tprintf(\"Second asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[1], 1);\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_GenerateFailedAccessASMs(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_PhysDiskAddr_t * failedPDA,\n    RF_DagHeader_t * dag_h,\n    RF_AccessStripeMapHeader_t ** new_asm_h,\n    int *nXorBufs,\n    char **rpBufPtr,\n    char *overlappingPDAs,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidLayout_t *layoutPtr = &(raidPtr->Layout);\n\n\t/* s=start, e=end, s=stripe, a=access, f=failed, su=stripe unit */\n\tRF_RaidAddr_t sosAddr, sosEndAddr, eosStartAddr, eosAddr;\n\n\tRF_SectorCount_t numSect[2], numParitySect;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *rdBuf, *bufP;\n\tint     foundit, i;\n\n\tbufP = NULL;\n\tfoundit = 0;\n\t/* first compute the following raid addresses: start of stripe,\n\t * (sosAddr) MIN(start of access, start of failed SU),   (sosEndAddr)\n\t * MAX(end of access, end of failed SU),       (eosStartAddr) end of\n\t * stripe (i.e. start of next stripe)   (eosAddr) */\n\tsosAddr = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\tsosEndAddr = RF_MIN(asmap->raidAddress, rf_RaidAddressOfPrevStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosStartAddr = RF_MAX(asmap->endRaidAddress, rf_RaidAddressOfNextStripeUnitBoundary(layoutPtr, failedPDA->raidAddress));\n\teosAddr = rf_RaidAddressOfNextStripeBoundary(layoutPtr, asmap->raidAddress);\n\n\t/* now generate access stripe maps for each of the above regions of\n\t * the stripe.  Use a dummy (NULL) buf ptr for now */\n\n\tnew_asm_h[0] = (sosAddr != sosEndAddr) ? rf_MapAccess(raidPtr, sosAddr, sosEndAddr - sosAddr, NULL, RF_DONT_REMAP) : NULL;\n\tnew_asm_h[1] = (eosStartAddr != eosAddr) ? rf_MapAccess(raidPtr, eosStartAddr, eosAddr - eosStartAddr, NULL, RF_DONT_REMAP) : NULL;\n\n\t/* walk through the PDAs and range-restrict each SU to the region of\n\t * the SU touched on the failed PDA.  also compute total data buffer\n\t * space requirements in this step.  Ignore the parity for now. */\n\n\tnumSect[0] = numSect[1] = 0;\n\tif (new_asm_h[0]) {\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[0] += pda->numSector;\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_NOBUFFER, 0);\n\t\t\tnumSect[1] += pda->numSector;\n\t\t}\n\t}\n\tnumParitySect = failedPDA->numSector;\n\n\t/* allocate buffer space for the data & parity we have to read to\n\t * recover from the failure */\n\n\tif (numSect[0] + numSect[1] + ((rpBufPtr) ? numParitySect : 0)) {\t/* don't allocate parity\n\t\t\t\t\t\t\t\t\t\t * buf if not needed */\n\t\tRF_MallocAndAdd(rdBuf, rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (char *), allocList);\n\t\tbufP = rdBuf;\n\t\tif (rf_degDagDebug)\n\t\t\tprintf(\"Newly allocated buffer (%d bytes) is 0x%lx\\n\",\n\t\t\t    (int) rf_RaidAddressToByte(raidPtr, numSect[0] + numSect[1] + numParitySect), (unsigned long) bufP);\n\t}\n\t/* now walk through the pdas one last time and assign buffer pointers\n\t * (ugh!).  Again, ignore the parity.  also, count nodes to find out\n\t * how many bufs need to be xored together */\n\t(*nXorBufs) = 1;\t/* in read case, 1 is for parity.  In write\n\t\t\t\t * case, 1 is for failed data */\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t*nXorBufs += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (pda = new_asm_h[1]->stripeMap->physInfo; pda; pda = pda->next) {\n\t\t\tpda->bufPtr = bufP;\n\t\t\tbufP += rf_RaidAddressToByte(raidPtr, pda->numSector);\n\t\t}\n\t\t(*nXorBufs) += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t}\n\tif (rpBufPtr)\n\t\t*rpBufPtr = bufP;\t/* the rest of the buffer is for\n\t\t\t\t\t * parity */\n\n\t/* the last step is to figure out how many more distinct buffers need\n\t * to get xor'd to produce the missing unit.  there's one for each\n\t * user-data read node that overlaps the portion of the failed unit\n\t * being accessed */\n\n\tfor (foundit = i = 0, pda = asmap->physInfo; pda; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tfoundit = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (rf_PDAOverlap(layoutPtr, pda, failedPDA)) {\n\t\t\toverlappingPDAs[i] = 1;\n\t\t\t(*nXorBufs)++;\n\t\t}\n\t}\n\tif (!foundit) {\n\t\tRF_ERRORMSG(\"GenerateFailedAccessASMs: did not find failedPDA in asm list\\n\");\n\t\tRF_ASSERT(0);\n\t}\n\tif (rf_degDagDebug) {\n\t\tif (new_asm_h[0]) {\n\t\t\tprintf(\"First asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[0], 1);\n\t\t}\n\t\tif (new_asm_h[1]) {\n\t\t\tprintf(\"Second asm:\\n\");\n\t\t\trf_PrintFullAccessStripeMap(new_asm_h[1], 1);\n\t\t}\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_Calloc",
          "args": [
            "overlappingPDAs",
            "asmap->numStripeUnitsAccessed",
            "sizeof(char)",
            "(char *)"
          ],
          "line": 293
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->numDataFailed == 1"
          ],
          "line": 284
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating degraded read DAG]\\n\""
          ],
          "line": 282
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "layoutPtr",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 277
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * recFunc)\n{\n\tRF_DagNode_t *nodes, *rudNodes, *rrdNodes, *xorNode, *blockNode;\n\tRF_DagNode_t *commitNode, *rpNode, *termNode;\n\tint     nNodes, nRrdNodes, nRudNodes, nXorBufs, i;\n\tint     j, paramNum;\n\tRF_SectorCount_t sectorsPerSU;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *overlappingPDAs;/* a temporary array of flags */\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_PhysDiskAddr_t *pda, *parityPDA;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tRF_RaidLayout_t *layoutPtr;\n\tchar   *rpBuf;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\t/* failedPDA points to the pda within the asm that targets the failed\n\t * disk */\n\tfailedPDA = asmap->failedPDAs[0];\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,\n\t    asmap->raidAddress, &which_ru);\n\tsectorsPerSU = layoutPtr->sectorsPerStripeUnit;\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating degraded read DAG]\\n\");\n\t}\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"DegradedReadDAG\";\n\n\t/*\n         * generate two ASMs identifying the surviving data we need\n         * in order to recover the lost data\n         */\n\n\t/* overlappingPDAs array must be zero'd */\n\tRF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));\n\trf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h, &nXorBufs,\n\t    &rpBuf, overlappingPDAs, allocList);\n\n\t/*\n         * create all the nodes at once\n         *\n         * -1 because no access is generated for the failed pda\n         */\n\tnRudNodes = asmap->numStripeUnitsAccessed - 1;\n\tnRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +\n\t    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);\n\tnNodes = 5 + nRudNodes + nRrdNodes;\t/* lock, unlock, xor, Rp, Rud,\n\t\t\t\t\t\t * Rrd */\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *),\n\t    allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\txorNode = &nodes[i];\n\ti++;\n\trpNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\trudNodes = &nodes[i];\n\ti += nRudNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\tRF_ASSERT(i == nNodes);\n\n\t/* initialize nodes */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\t/* this dag can not commit until the commit node is reached errors\n\t * prior to the commit point imply the dag has failed */\n\tdag_h->numSuccedents = 1;\n\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nRudNodes + nRrdNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, recFunc->simple, rf_NullNodeUndoFunc,\n\t    NULL, 1, nRudNodes + nRrdNodes + 1, 2 * nXorBufs + 2, 1, dag_h,\n\t    recFunc->SimpleName, allocList);\n\n\t/* fill in the Rud nodes */\n\tfor (pda = asmap->physInfo, i = 0; i < nRudNodes; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\trf_InitNode(&rudNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Rud\", allocList);\n\t\tRF_ASSERT(pda);\n\t\trudNodes[i].params[0].p = pda;\n\t\trudNodes[i].params[1].p = pda->bufPtr;\n\t\trudNodes[i].params[2].v = parityStripeID;\n\t\trudNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* fill in the Rrd nodes */\n\ti = 0;\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo;\n\t\t    i < new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t\t    i++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i].params[0].p = pda;\n\t\t\trrdNodes[i].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i].params[2].v = parityStripeID;\n\t\t\trrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (j = 0, pda = new_asm_h[1]->stripeMap->physInfo;\n\t\t    j < new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t\t    j++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i + j].params[0].p = pda;\n\t\t\trrdNodes[i + j].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i + j].params[2].v = parityStripeID;\n\t\t\trrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* make a PDA for the parity unit */\n\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\tparityPDA->row = asmap->parityInfo->row;\n\tparityPDA->col = asmap->parityInfo->col;\n\tparityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)\n\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\tparityPDA->numSector = failedPDA->numSector;\n\n\t/* initialize the Rp node */\n\trf_InitNode(rpNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rp \", allocList);\n\trpNode->params[0].p = parityPDA;\n\trpNode->params[1].p = rpBuf;\n\trpNode->params[2].v = parityStripeID;\n\trpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t/*\n         * the last and nastiest step is to assign all\n         * the parameters of the Xor node\n         */\n\tparamNum = 0;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\t/* all the Rrd nodes need to be xored together */\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[0];\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[1];\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\t/* any Rud nodes that overlap the failed access need to be\n\t\t * xored in */\n\t\tif (overlappingPDAs[i]) {\n\t\t\tRF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tbcopy((char *) rudNodes[i].params[0].p, (char *) pda, sizeof(RF_PhysDiskAddr_t));\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);\n\t\t\txorNode->params[paramNum++].p = pda;\n\t\t\txorNode->params[paramNum++].p = pda->bufPtr;\n\t\t}\n\t}\n\tRF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));\n\n\t/* install parity pda as last set of params to be xor'd */\n\txorNode->params[paramNum++].p = parityPDA;\n\txorNode->params[paramNum++].p = rpBuf;\n\n\t/*\n         * the last 2 params to the recovery xor node are\n         * the failed PDA and the raidPtr\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = raidPtr;\n\tRF_ASSERT(paramNum == 2 * nXorBufs + 2);\n\n\t/*\n         * The xor node uses results[0] as the target buffer.\n         * Set pointer and zero the buffer. In the kernel, this\n         * may be a user buffer in which case we have to remap it.\n         */\n\txorNode->results[0] = failedPDA->bufPtr;\n\tRF_BZERO(bp, failedPDA->bufPtr, rf_RaidAddressToByte(raidPtr,\n\t\tfailedPDA->numSector));\n\n\t/* connect nodes to form graph */\n\t/* connect the header to the block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the read nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rpNode;\n\trpNode->antecedents[0] = blockNode;\n\trpNode->antType[0] = rf_control;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + i] = &rrdNodes[i];\n\t\trrdNodes[i].antecedents[0] = blockNode;\n\t\trrdNodes[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\trudNodes[i].antecedents[0] = blockNode;\n\t\trudNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the read nodes to the xor node */\n\tRF_ASSERT(xorNode->numAntecedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numSuccedents == 1);\n\trpNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = rpNode;\n\txorNode->antType[0] = rf_trueData;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\trrdNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + i] = &rrdNodes[i];\n\t\txorNode->antType[1 + i] = rf_trueData;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\trudNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\txorNode->antType[1 + nRrdNodes + i] = rf_trueData;\n\t}\n\n\t/* connect the xor node to the commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect the termNode to the commit node */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antType[0] = rf_control;\n\ttermNode->antecedents[0] = commitNode;\n}"
  },
  {
    "function_name": "rf_CreateRaidOneDegradedReadDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
    "lines": "110-215",
    "snippet": "void \nrf_CreateRaidOneDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *nodes, *rdNode, *blockNode, *commitNode, *termNode;\n\tRF_StripeNum_t parityStripeID;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda;\n\tint     useMirror, i;\n\n\tuseMirror = 0;\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID level 1 degraded read DAG]\\n\");\n\t}\n\tdag_h->creator = \"RaidOneDegradedReadDAG\";\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 0)\n\t\tuseMirror = RF_FALSE;\n\telse\n\t\tuseMirror = RF_TRUE;\n\n\t/* total number of nodes = 1 + (block + commit + terminator) */\n\tRF_CallocAndAdd(nodes, 4, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\trdNode = &nodes[i];\n\ti++;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\n\t/* this dag can not commit until the commit node is reached.   errors\n\t * prior to the commit point imply the dag has failed and must be\n\t * retried */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the block, commit, and terminator nodes */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\tpda = asmap->physInfo;\n\tRF_ASSERT(pda != NULL);\n\t/* parityInfo must describe entire parity unit */\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\n\t/* initialize the data node */\n\tif (!useMirror) {\n\t\t/* read primary copy of data */\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rpd\", allocList);\n\t\trdNode->params[0].p = pda;\n\t\trdNode->params[1].p = pda->bufPtr;\n\t\trdNode->params[2].v = parityStripeID;\n\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t} else {\n\t\t/* read secondary copy of data */\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rsd\", allocList);\n\t\trdNode->params[0].p = asmap->parityInfo;\n\t\trdNode->params[1].p = pda->bufPtr;\n\t\trdNode->params[2].v = parityStripeID;\n\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to rdnode */\n\tRF_ASSERT(blockNode->numSuccedents == 1);\n\tRF_ASSERT(rdNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rdNode;\n\trdNode->antecedents[0] = blockNode;\n\trdNode->antType[0] = rf_control;\n\n\t/* connect rdnode to commit node */\n\tRF_ASSERT(rdNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\trdNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = rdNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect commit node to terminator */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = commitNode;\n\ttermNode->antType[0] = rf_control;\n}",
    "includes": [
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 211
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 210
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numSuccedents == 1"
          ],
          "line": 209
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numAntecedents == 1"
          ],
          "line": 203
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rdNode->numSuccedents == 1"
          ],
          "line": 202
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rdNode->numAntecedents == 1"
          ],
          "line": 196
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == 1"
          ],
          "line": 195
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 191
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "dag_h->numSuccedents == 1"
          ],
          "line": 190
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 186
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "rdNode",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskReadFunc",
            "rf_DiskReadUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Rsd\"",
            "allocList"
          ],
          "line": 181
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 178
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->parityInfo->next == NULL"
          ],
          "line": 168
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 166
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "4",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 139
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating RAID level 1 degraded read DAG]\\n\""
          ],
          "line": 129
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 126
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateRaidOneDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *nodes, *rdNode, *blockNode, *commitNode, *termNode;\n\tRF_StripeNum_t parityStripeID;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda;\n\tint     useMirror, i;\n\n\tuseMirror = 0;\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID level 1 degraded read DAG]\\n\");\n\t}\n\tdag_h->creator = \"RaidOneDegradedReadDAG\";\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 0)\n\t\tuseMirror = RF_FALSE;\n\telse\n\t\tuseMirror = RF_TRUE;\n\n\t/* total number of nodes = 1 + (block + commit + terminator) */\n\tRF_CallocAndAdd(nodes, 4, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\trdNode = &nodes[i];\n\ti++;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\n\t/* this dag can not commit until the commit node is reached.   errors\n\t * prior to the commit point imply the dag has failed and must be\n\t * retried */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the block, commit, and terminator nodes */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\tpda = asmap->physInfo;\n\tRF_ASSERT(pda != NULL);\n\t/* parityInfo must describe entire parity unit */\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\n\t/* initialize the data node */\n\tif (!useMirror) {\n\t\t/* read primary copy of data */\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rpd\", allocList);\n\t\trdNode->params[0].p = pda;\n\t\trdNode->params[1].p = pda->bufPtr;\n\t\trdNode->params[2].v = parityStripeID;\n\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t} else {\n\t\t/* read secondary copy of data */\n\t\trf_InitNode(rdNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rsd\", allocList);\n\t\trdNode->params[0].p = asmap->parityInfo;\n\t\trdNode->params[1].p = pda->bufPtr;\n\t\trdNode->params[2].v = parityStripeID;\n\t\trdNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* connect header to block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to rdnode */\n\tRF_ASSERT(blockNode->numSuccedents == 1);\n\tRF_ASSERT(rdNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rdNode;\n\trdNode->antecedents[0] = blockNode;\n\trdNode->antType[0] = rf_control;\n\n\t/* connect rdnode to commit node */\n\tRF_ASSERT(rdNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\trdNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = rdNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect commit node to terminator */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = commitNode;\n\ttermNode->antType[0] = rf_control;\n}"
  },
  {
    "function_name": "rf_CreateRaidFiveDegradedReadDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
    "lines": "74-85",
    "snippet": "void \nrf_CreateRaidFiveDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\trf_CreateDegradedReadDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    &rf_xorRecoveryFuncs);\n}",
    "includes": [
      "#include \"rf_dagdegrd.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CreateDegradedReadDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList",
            "&rf_xorRecoveryFuncs"
          ],
          "line": 83
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CreateDegradedReadDAG",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagdegrd.c",
          "lines": "249-507",
          "snippet": "void \nrf_CreateDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * recFunc)\n{\n\tRF_DagNode_t *nodes, *rudNodes, *rrdNodes, *xorNode, *blockNode;\n\tRF_DagNode_t *commitNode, *rpNode, *termNode;\n\tint     nNodes, nRrdNodes, nRudNodes, nXorBufs, i;\n\tint     j, paramNum;\n\tRF_SectorCount_t sectorsPerSU;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *overlappingPDAs;/* a temporary array of flags */\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_PhysDiskAddr_t *pda, *parityPDA;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tRF_RaidLayout_t *layoutPtr;\n\tchar   *rpBuf;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\t/* failedPDA points to the pda within the asm that targets the failed\n\t * disk */\n\tfailedPDA = asmap->failedPDAs[0];\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,\n\t    asmap->raidAddress, &which_ru);\n\tsectorsPerSU = layoutPtr->sectorsPerStripeUnit;\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating degraded read DAG]\\n\");\n\t}\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"DegradedReadDAG\";\n\n\t/*\n         * generate two ASMs identifying the surviving data we need\n         * in order to recover the lost data\n         */\n\n\t/* overlappingPDAs array must be zero'd */\n\tRF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));\n\trf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h, &nXorBufs,\n\t    &rpBuf, overlappingPDAs, allocList);\n\n\t/*\n         * create all the nodes at once\n         *\n         * -1 because no access is generated for the failed pda\n         */\n\tnRudNodes = asmap->numStripeUnitsAccessed - 1;\n\tnRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +\n\t    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);\n\tnNodes = 5 + nRudNodes + nRrdNodes;\t/* lock, unlock, xor, Rp, Rud,\n\t\t\t\t\t\t * Rrd */\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *),\n\t    allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\txorNode = &nodes[i];\n\ti++;\n\trpNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\trudNodes = &nodes[i];\n\ti += nRudNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\tRF_ASSERT(i == nNodes);\n\n\t/* initialize nodes */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\t/* this dag can not commit until the commit node is reached errors\n\t * prior to the commit point imply the dag has failed */\n\tdag_h->numSuccedents = 1;\n\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nRudNodes + nRrdNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, recFunc->simple, rf_NullNodeUndoFunc,\n\t    NULL, 1, nRudNodes + nRrdNodes + 1, 2 * nXorBufs + 2, 1, dag_h,\n\t    recFunc->SimpleName, allocList);\n\n\t/* fill in the Rud nodes */\n\tfor (pda = asmap->physInfo, i = 0; i < nRudNodes; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\trf_InitNode(&rudNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Rud\", allocList);\n\t\tRF_ASSERT(pda);\n\t\trudNodes[i].params[0].p = pda;\n\t\trudNodes[i].params[1].p = pda->bufPtr;\n\t\trudNodes[i].params[2].v = parityStripeID;\n\t\trudNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* fill in the Rrd nodes */\n\ti = 0;\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo;\n\t\t    i < new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t\t    i++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i].params[0].p = pda;\n\t\t\trrdNodes[i].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i].params[2].v = parityStripeID;\n\t\t\trrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (j = 0, pda = new_asm_h[1]->stripeMap->physInfo;\n\t\t    j < new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t\t    j++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i + j].params[0].p = pda;\n\t\t\trrdNodes[i + j].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i + j].params[2].v = parityStripeID;\n\t\t\trrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* make a PDA for the parity unit */\n\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\tparityPDA->row = asmap->parityInfo->row;\n\tparityPDA->col = asmap->parityInfo->col;\n\tparityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)\n\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\tparityPDA->numSector = failedPDA->numSector;\n\n\t/* initialize the Rp node */\n\trf_InitNode(rpNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rp \", allocList);\n\trpNode->params[0].p = parityPDA;\n\trpNode->params[1].p = rpBuf;\n\trpNode->params[2].v = parityStripeID;\n\trpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t/*\n         * the last and nastiest step is to assign all\n         * the parameters of the Xor node\n         */\n\tparamNum = 0;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\t/* all the Rrd nodes need to be xored together */\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[0];\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[1];\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\t/* any Rud nodes that overlap the failed access need to be\n\t\t * xored in */\n\t\tif (overlappingPDAs[i]) {\n\t\t\tRF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tbcopy((char *) rudNodes[i].params[0].p, (char *) pda, sizeof(RF_PhysDiskAddr_t));\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);\n\t\t\txorNode->params[paramNum++].p = pda;\n\t\t\txorNode->params[paramNum++].p = pda->bufPtr;\n\t\t}\n\t}\n\tRF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));\n\n\t/* install parity pda as last set of params to be xor'd */\n\txorNode->params[paramNum++].p = parityPDA;\n\txorNode->params[paramNum++].p = rpBuf;\n\n\t/*\n         * the last 2 params to the recovery xor node are\n         * the failed PDA and the raidPtr\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = raidPtr;\n\tRF_ASSERT(paramNum == 2 * nXorBufs + 2);\n\n\t/*\n         * The xor node uses results[0] as the target buffer.\n         * Set pointer and zero the buffer. In the kernel, this\n         * may be a user buffer in which case we have to remap it.\n         */\n\txorNode->results[0] = failedPDA->bufPtr;\n\tRF_BZERO(bp, failedPDA->bufPtr, rf_RaidAddressToByte(raidPtr,\n\t\tfailedPDA->numSector));\n\n\t/* connect nodes to form graph */\n\t/* connect the header to the block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the read nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rpNode;\n\trpNode->antecedents[0] = blockNode;\n\trpNode->antType[0] = rf_control;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + i] = &rrdNodes[i];\n\t\trrdNodes[i].antecedents[0] = blockNode;\n\t\trrdNodes[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\trudNodes[i].antecedents[0] = blockNode;\n\t\trudNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the read nodes to the xor node */\n\tRF_ASSERT(xorNode->numAntecedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numSuccedents == 1);\n\trpNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = rpNode;\n\txorNode->antType[0] = rf_trueData;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\trrdNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + i] = &rrdNodes[i];\n\t\txorNode->antType[1 + i] = rf_trueData;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\trudNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\txorNode->antType[1 + nRrdNodes + i] = rf_trueData;\n\t}\n\n\t/* connect the xor node to the commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect the termNode to the commit node */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antType[0] = rf_control;\n\ttermNode->antecedents[0] = commitNode;\n}",
          "includes": [
            "#include \"rf_dagdegrd.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * recFunc)\n{\n\tRF_DagNode_t *nodes, *rudNodes, *rrdNodes, *xorNode, *blockNode;\n\tRF_DagNode_t *commitNode, *rpNode, *termNode;\n\tint     nNodes, nRrdNodes, nRudNodes, nXorBufs, i;\n\tint     j, paramNum;\n\tRF_SectorCount_t sectorsPerSU;\n\tRF_ReconUnitNum_t which_ru;\n\tchar   *overlappingPDAs;/* a temporary array of flags */\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_PhysDiskAddr_t *pda, *parityPDA;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *failedPDA;\n\tRF_RaidLayout_t *layoutPtr;\n\tchar   *rpBuf;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\t/* failedPDA points to the pda within the asm that targets the failed\n\t * disk */\n\tfailedPDA = asmap->failedPDAs[0];\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr,\n\t    asmap->raidAddress, &which_ru);\n\tsectorsPerSU = layoutPtr->sectorsPerStripeUnit;\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating degraded read DAG]\\n\");\n\t}\n\tRF_ASSERT(asmap->numDataFailed == 1);\n\tdag_h->creator = \"DegradedReadDAG\";\n\n\t/*\n         * generate two ASMs identifying the surviving data we need\n         * in order to recover the lost data\n         */\n\n\t/* overlappingPDAs array must be zero'd */\n\tRF_Calloc(overlappingPDAs, asmap->numStripeUnitsAccessed, sizeof(char), (char *));\n\trf_GenerateFailedAccessASMs(raidPtr, asmap, failedPDA, dag_h, new_asm_h, &nXorBufs,\n\t    &rpBuf, overlappingPDAs, allocList);\n\n\t/*\n         * create all the nodes at once\n         *\n         * -1 because no access is generated for the failed pda\n         */\n\tnRudNodes = asmap->numStripeUnitsAccessed - 1;\n\tnRrdNodes = ((new_asm_h[0]) ? new_asm_h[0]->stripeMap->numStripeUnitsAccessed : 0) +\n\t    ((new_asm_h[1]) ? new_asm_h[1]->stripeMap->numStripeUnitsAccessed : 0);\n\tnNodes = 5 + nRudNodes + nRrdNodes;\t/* lock, unlock, xor, Rp, Rud,\n\t\t\t\t\t\t * Rrd */\n\tRF_CallocAndAdd(nodes, nNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *),\n\t    allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti++;\n\tcommitNode = &nodes[i];\n\ti++;\n\txorNode = &nodes[i];\n\ti++;\n\trpNode = &nodes[i];\n\ti++;\n\ttermNode = &nodes[i];\n\ti++;\n\trudNodes = &nodes[i];\n\ti += nRudNodes;\n\trrdNodes = &nodes[i];\n\ti += nRrdNodes;\n\tRF_ASSERT(i == nNodes);\n\n\t/* initialize nodes */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\t/* this dag can not commit until the commit node is reached errors\n\t * prior to the commit point imply the dag has failed */\n\tdag_h->numSuccedents = 1;\n\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nRudNodes + nRrdNodes + 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, recFunc->simple, rf_NullNodeUndoFunc,\n\t    NULL, 1, nRudNodes + nRrdNodes + 1, 2 * nXorBufs + 2, 1, dag_h,\n\t    recFunc->SimpleName, allocList);\n\n\t/* fill in the Rud nodes */\n\tfor (pda = asmap->physInfo, i = 0; i < nRudNodes; i++, pda = pda->next) {\n\t\tif (pda == failedPDA) {\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\trf_InitNode(&rudNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Rud\", allocList);\n\t\tRF_ASSERT(pda);\n\t\trudNodes[i].params[0].p = pda;\n\t\trudNodes[i].params[1].p = pda->bufPtr;\n\t\trudNodes[i].params[2].v = parityStripeID;\n\t\trudNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t}\n\n\t/* fill in the Rrd nodes */\n\ti = 0;\n\tif (new_asm_h[0]) {\n\t\tfor (pda = new_asm_h[0]->stripeMap->physInfo;\n\t\t    i < new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\t\t    i++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i].params[0].p = pda;\n\t\t\trrdNodes[i].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i].params[2].v = parityStripeID;\n\t\t\trrdNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\tif (new_asm_h[1]) {\n\t\tfor (j = 0, pda = new_asm_h[1]->stripeMap->physInfo;\n\t\t    j < new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\t\t    j++, pda = pda->next) {\n\t\t\trf_InitNode(&rrdNodes[i + j], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0,\n\t\t\t    dag_h, \"Rrd\", allocList);\n\t\t\tRF_ASSERT(pda);\n\t\t\trrdNodes[i + j].params[0].p = pda;\n\t\t\trrdNodes[i + j].params[1].p = pda->bufPtr;\n\t\t\trrdNodes[i + j].params[2].v = parityStripeID;\n\t\t\trrdNodes[i + j].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t}\n\t}\n\t/* make a PDA for the parity unit */\n\tRF_MallocAndAdd(parityPDA, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\tparityPDA->row = asmap->parityInfo->row;\n\tparityPDA->col = asmap->parityInfo->col;\n\tparityPDA->startSector = ((asmap->parityInfo->startSector / sectorsPerSU)\n\t    * sectorsPerSU) + (failedPDA->startSector % sectorsPerSU);\n\tparityPDA->numSector = failedPDA->numSector;\n\n\t/* initialize the Rp node */\n\trf_InitNode(rpNode, rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rp \", allocList);\n\trpNode->params[0].p = parityPDA;\n\trpNode->params[1].p = rpBuf;\n\trpNode->params[2].v = parityStripeID;\n\trpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t/*\n         * the last and nastiest step is to assign all\n         * the parameters of the Xor node\n         */\n\tparamNum = 0;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\t/* all the Rrd nodes need to be xored together */\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[0];\n\t\txorNode->params[paramNum++] = rrdNodes[i].params[1];\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\t/* any Rud nodes that overlap the failed access need to be\n\t\t * xored in */\n\t\tif (overlappingPDAs[i]) {\n\t\t\tRF_MallocAndAdd(pda, sizeof(RF_PhysDiskAddr_t), (RF_PhysDiskAddr_t *), allocList);\n\t\t\tbcopy((char *) rudNodes[i].params[0].p, (char *) pda, sizeof(RF_PhysDiskAddr_t));\n\t\t\trf_RangeRestrictPDA(raidPtr, failedPDA, pda, RF_RESTRICT_DOBUFFER, 0);\n\t\t\txorNode->params[paramNum++].p = pda;\n\t\t\txorNode->params[paramNum++].p = pda->bufPtr;\n\t\t}\n\t}\n\tRF_Free(overlappingPDAs, asmap->numStripeUnitsAccessed * sizeof(char));\n\n\t/* install parity pda as last set of params to be xor'd */\n\txorNode->params[paramNum++].p = parityPDA;\n\txorNode->params[paramNum++].p = rpBuf;\n\n\t/*\n         * the last 2 params to the recovery xor node are\n         * the failed PDA and the raidPtr\n         */\n\txorNode->params[paramNum++].p = failedPDA;\n\txorNode->params[paramNum++].p = raidPtr;\n\tRF_ASSERT(paramNum == 2 * nXorBufs + 2);\n\n\t/*\n         * The xor node uses results[0] as the target buffer.\n         * Set pointer and zero the buffer. In the kernel, this\n         * may be a user buffer in which case we have to remap it.\n         */\n\txorNode->results[0] = failedPDA->bufPtr;\n\tRF_BZERO(bp, failedPDA->bufPtr, rf_RaidAddressToByte(raidPtr,\n\t\tfailedPDA->numSector));\n\n\t/* connect nodes to form graph */\n\t/* connect the header to the block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect the block node to the read nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numAntecedents == 1);\n\tblockNode->succedents[0] = rpNode;\n\trpNode->antecedents[0] = blockNode;\n\trpNode->antType[0] = rf_control;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + i] = &rrdNodes[i];\n\t\trrdNodes[i].antecedents[0] = blockNode;\n\t\trrdNodes[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\tblockNode->succedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\trudNodes[i].antecedents[0] = blockNode;\n\t\trudNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the read nodes to the xor node */\n\tRF_ASSERT(xorNode->numAntecedents == (1 + nRrdNodes + nRudNodes));\n\tRF_ASSERT(rpNode->numSuccedents == 1);\n\trpNode->succedents[0] = xorNode;\n\txorNode->antecedents[0] = rpNode;\n\txorNode->antType[0] = rf_trueData;\n\tfor (i = 0; i < nRrdNodes; i++) {\n\t\tRF_ASSERT(rrdNodes[i].numSuccedents == 1);\n\t\trrdNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + i] = &rrdNodes[i];\n\t\txorNode->antType[1 + i] = rf_trueData;\n\t}\n\tfor (i = 0; i < nRudNodes; i++) {\n\t\tRF_ASSERT(rudNodes[i].numSuccedents == 1);\n\t\trudNodes[i].succedents[0] = xorNode;\n\t\txorNode->antecedents[1 + nRrdNodes + i] = &rudNodes[i];\n\t\txorNode->antType[1 + nRrdNodes + i] = rf_trueData;\n\t}\n\n\t/* connect the xor node to the commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect the termNode to the commit node */\n\tRF_ASSERT(commitNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tcommitNode->succedents[0] = termNode;\n\ttermNode->antType[0] = rf_control;\n\ttermNode->antecedents[0] = commitNode;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rf_dagdegrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateRaidFiveDegradedReadDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\trf_CreateDegradedReadDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    &rf_xorRecoveryFuncs);\n}"
  }
]