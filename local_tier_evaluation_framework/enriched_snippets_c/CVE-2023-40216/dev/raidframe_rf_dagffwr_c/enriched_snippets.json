[
  {
    "function_name": "rf_CreateRaidOneWriteDAGFwd",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "1997-2129",
    "snippet": "void \nrf_CreateRaidOneWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *blockNode, *unblockNode, *termNode;\n\tRF_DagNode_t *nodes, *wndNode, *wmirNode;\n\tint     nWndNodes, nWmirNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda, *pdaP;\n\tRF_StripeNum_t parityStripeID;\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID level 1 write DAG]\\n\");\n\t}\n\tnWmirNodes = (asmap->parityInfo->next) ? 2 : 1;\t/* 2 implies access not\n\t\t\t\t\t\t\t * SU aligned */\n\tnWndNodes = (asmap->physInfo->next) ? 2 : 1;\n\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 1)\n\t\tnWndNodes--;\n\tif (asmap->numParityFailed == 1)\n\t\tnWmirNodes--;\n\n\t/* total number of nodes = nWndNodes + nWmirNodes + (block + unblock +\n\t * terminator) */\n\tRF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNode = &nodes[i];\n\ti += nWndNodes;\n\twmirNode = &nodes[i];\n\ti += nWmirNodes;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == (nWndNodes + nWmirNodes + 3));\n\n\t/* this dag can commit immediately */\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the unblock and term nodes */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the wnd nodes */\n\tif (nWndNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tfor (i = 0; i < nWndNodes; i++) {\n\t\t\trf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wpd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twndNode[i].params[0].p = pda;\n\t\t\twndNode[i].params[1].p = pda->bufPtr;\n\t\t\twndNode[i].params[2].v = parityStripeID;\n\t\t\twndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t}\n\t/* initialize the mirror nodes */\n\tif (nWmirNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tpdaP = asmap->parityInfo;\n\t\tfor (i = 0; i < nWmirNodes; i++) {\n\t\t\trf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wsd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twmirNode[i].params[0].p = pdaP;\n\t\t\twmirNode[i].params[1].p = pda->bufPtr;\n\t\t\twmirNode[i].params[2].v = parityStripeID;\n\t\t\twmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t\tpdaP = pdaP->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t\tRF_ASSERT(pdaP == NULL);\n\t}\n\t/* link the header node to the block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* link the block node to the write nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &wndNode[i];\n\t\twndNode[i].antecedents[0] = blockNode;\n\t\twndNode[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numAntecedents == 1);\n\t\tblockNode->succedents[i + nWndNodes] = &wmirNode[i];\n\t\twmirNode[i].antecedents[0] = blockNode;\n\t\twmirNode[i].antType[0] = rf_control;\n\t}\n\n\t/* link the write nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numSuccedents == 1);\n\t\twndNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNode[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numSuccedents == 1);\n\t\twmirNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i + nWndNodes] = &wmirNode[i];\n\t\tunblockNode->antType[i + nWndNodes] = rf_control;\n\t}\n\n\t/* link the unblock node to the term node */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n\n\treturn;\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 2123
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 2122
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numSuccedents == 1"
          ],
          "line": 2121
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wmirNode[i].numSuccedents == 1"
          ],
          "line": 2114
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNode[i].numSuccedents == 1"
          ],
          "line": 2108
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numAntecedents == (nWndNodes + nWmirNodes)"
          ],
          "line": 2106
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wmirNode[i].numAntecedents == 1"
          ],
          "line": 2099
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNode[i].numAntecedents == 1"
          ],
          "line": 2093
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == (nWndNodes + nWmirNodes)"
          ],
          "line": 2091
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 2087
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "dag_h->numSuccedents == 1"
          ],
          "line": 2086
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pdaP == NULL"
          ],
          "line": 2083
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda == NULL"
          ],
          "line": 2082
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 2078
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 2074
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "&wmirNode[i]",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskWriteFunc",
            "rf_DiskWriteUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Wsd\"",
            "allocList"
          ],
          "line": 2073
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda == NULL"
          ],
          "line": 2066
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 2063
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 2059
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == (nWndNodes + nWmirNodes + 3)"
          ],
          "line": 2042
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nWndNodes + nWmirNodes + 3",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 2030
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating RAID level 1 write DAG]\\n\""
          ],
          "line": 2016
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 2013
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateRaidOneWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *blockNode, *unblockNode, *termNode;\n\tRF_DagNode_t *nodes, *wndNode, *wmirNode;\n\tint     nWndNodes, nWmirNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda, *pdaP;\n\tRF_StripeNum_t parityStripeID;\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID level 1 write DAG]\\n\");\n\t}\n\tnWmirNodes = (asmap->parityInfo->next) ? 2 : 1;\t/* 2 implies access not\n\t\t\t\t\t\t\t * SU aligned */\n\tnWndNodes = (asmap->physInfo->next) ? 2 : 1;\n\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 1)\n\t\tnWndNodes--;\n\tif (asmap->numParityFailed == 1)\n\t\tnWmirNodes--;\n\n\t/* total number of nodes = nWndNodes + nWmirNodes + (block + unblock +\n\t * terminator) */\n\tRF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNode = &nodes[i];\n\ti += nWndNodes;\n\twmirNode = &nodes[i];\n\ti += nWmirNodes;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == (nWndNodes + nWmirNodes + 3));\n\n\t/* this dag can commit immediately */\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the unblock and term nodes */\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the wnd nodes */\n\tif (nWndNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tfor (i = 0; i < nWndNodes; i++) {\n\t\t\trf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wpd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twndNode[i].params[0].p = pda;\n\t\t\twndNode[i].params[1].p = pda->bufPtr;\n\t\t\twndNode[i].params[2].v = parityStripeID;\n\t\t\twndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t}\n\t/* initialize the mirror nodes */\n\tif (nWmirNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tpdaP = asmap->parityInfo;\n\t\tfor (i = 0; i < nWmirNodes; i++) {\n\t\t\trf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wsd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twmirNode[i].params[0].p = pdaP;\n\t\t\twmirNode[i].params[1].p = pda->bufPtr;\n\t\t\twmirNode[i].params[2].v = parityStripeID;\n\t\t\twmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t\tpdaP = pdaP->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t\tRF_ASSERT(pdaP == NULL);\n\t}\n\t/* link the header node to the block node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\t/* link the block node to the write nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numAntecedents == 1);\n\t\tblockNode->succedents[i] = &wndNode[i];\n\t\twndNode[i].antecedents[0] = blockNode;\n\t\twndNode[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numAntecedents == 1);\n\t\tblockNode->succedents[i + nWndNodes] = &wmirNode[i];\n\t\twmirNode[i].antecedents[0] = blockNode;\n\t\twmirNode[i].antType[0] = rf_control;\n\t}\n\n\t/* link the write nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numSuccedents == 1);\n\t\twndNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNode[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numSuccedents == 1);\n\t\twmirNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i + nWndNodes] = &wmirNode[i];\n\t\tunblockNode->antType[i + nWndNodes] = rf_control;\n\t}\n\n\t/* link the unblock node to the term node */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n\n\treturn;\n}"
  },
  {
    "function_name": "rf_CommonCreateSmallWriteDAGFwd",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "1483-1977",
    "snippet": "void \nrf_CommonCreateSmallWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;\n\tRF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;\n\tRF_DagNode_t *xorNodes, *qNodes, *blockNode, *nodes;\n\tRF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;\n\tint     i, j, nNodes, totalNumNodes, lu_flag;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     (*qfunc) (RF_DagNode_t *);\n\tint     numDataNodes, numParityNodes;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *name, *qname;\n\tlong    nfaults;\n\n\tnfaults = qfuncs ? 2 : 1;\n\tlu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tpda = asmap->physInfo;\n\tnumDataNodes = asmap->numStripeUnitsAccessed;\n\tnumParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tdag_h->creator = \"SmallWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\tqfunc = NULL;\n\tqname = NULL;\n\n\t/* DAG creation occurs in four steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node (nfaults * nparity) a read\n\t * and write for each parity unit a block node a terminate node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes) + (nfaults * 2 * numParityNodes) + 2;\n\tif (lu_flag)\n\t\ttotalNumNodes += (numDataNodes + (nfaults * numParityNodes));\n\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\twriteParityNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t\tunlockParityNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t} else {\n\t\tunlockDataNodes = unlockParityNodes = NULL;\n\t}\n\tif (nfaults == 2) {\n\t\treadQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\twriteQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tqNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tif (lu_flag) {\n\t\t\tunlockQNodes = &nodes[i];\n\t\t\ti += numParityNodes;\n\t\t} else {\n\t\t\tunlockQNodes = NULL;\n\t\t}\n\t} else {\n\t\treadQNodes = writeQNodes = qNodes = unlockQNodes = NULL;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\tnNodes = numDataNodes + (nfaults * numParityNodes);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminate node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readDataNodes[i].numSuccedents; j++)\n\t\t\treadDataNodes[i].propList[j] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tfor (j = 0; j < readParityNodes[i].numSuccedents; j++)\n\t\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which read old Q (Roq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\trf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Roq\", allocList);\n\t\t\treadQNodes[i].params[0].p = pda;\n\t\t\treadQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old Q */\n\t\t\treadQNodes[i].params[2].v = parityStripeID;\n\t\t\treadQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\t\tfor (j = 0; j < readQNodes[i].numSuccedents; j++)\n\t\t\t\treadQNodes[i].propList[0] = NULL;\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity and Q */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\tif (nfaults == 2) {\n\t\t\t\trf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, qname, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\t\tqNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[2] = readQNodes[i].params[0];\n\t\t\t\tqNodes[i].params[3] = readQNodes[i].params[1];\n\t\t\t\tqNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[6].p = raidPtr;\n\t\t\t\tqNodes[i].results[0] = readQNodes[i].params[1].p;\t/* use old Q buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t\tif (nfaults == 2) {\n\t\t\trf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, qname, allocList);\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Rod */\n\t\t\t\tqNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\t/* and read old q */\n\t\t\tqNodes[0].params[2 * numDataNodes + 0] = readQNodes[0].params[0];\t/* pda */\n\t\t\tqNodes[0].params[2 * numDataNodes + 1] = readQNodes[0].params[1];\t/* buffer pointer */\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Wnd nodes */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\tqNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\t\tqNodes[0].results[0] = readQNodes[0].params[1].p;\n\t\t}\n\t}\n\n\t/* initialize nodes which write new parity (Wnp) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\trf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnp\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twriteParityNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t * filled in by xor node */\n\t\twriteParityNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\twriteParityNodes[i].params[2].v = parityStripeID;\n\t\twriteParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unp\", allocList);\n\t\t\tunlockParityNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new Q (Wnq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnq\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twriteQNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t\t * filled in by xor node */\n\t\t\twriteQNodes[i].params[1].p = qNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\t\twriteQNodes[i].params[2].v = parityStripeID;\n\t\t\twriteQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\t\tif (lu_flag) {\n\t\t\t\t/* initialize node to unlock the disk queue */\n\t\t\t\trf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unq\", allocList);\n\t\t\t\tunlockQNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t\t * desc */\n\t\t\t\tunlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t\t}\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old Q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tblockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];\n\t\t\tRF_ASSERT(readQNodes[i].numAntecedents == 1);\n\t\t\treadQNodes[i].antecedents[0] = blockNode;\n\t\t\treadQNodes[i].antType[0] = rf_control;\n\t\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == ((nfaults * numParityNodes) + 1));\n\t\tRF_ASSERT(writeDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].succedents[0] = &writeDataNodes[i];\n\t\twriteDataNodes[i].antecedents[0] = &readDataNodes[i];\n\t\twriteDataNodes[i].antType[0] = rf_antiData;\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[1 + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numDataNodes; i++)\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\t\treadDataNodes[i].succedents[1 + numParityNodes + j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\t\tqNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\t\treadParityNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old q nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(readQNodes[i].numSuccedents == numParityNodes);\n\t\t\t\treadQNodes[i].succedents[j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];\n\t\t\t\tqNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\t/* connect xor nodes to the write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(writeParityNodes[i].numAntecedents == numParityNodes);\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numSuccedents == numParityNodes);\n\t\t\txorNodes[i].succedents[j] = &writeParityNodes[j];\n\t\t\twriteParityNodes[j].antecedents[i] = &xorNodes[i];\n\t\t\twriteParityNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect q nodes to the write new q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(writeQNodes[i].numAntecedents == numParityNodes);\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numSuccedents == 1);\n\t\t\t\tqNodes[i].succedents[j] = &writeQNodes[j];\n\t\t\t\twriteQNodes[j].antecedents[i] = &qNodes[i];\n\t\t\t\twriteQNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tunlockDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to term node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &writeDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new parity nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockParityNodes[i].numAntecedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = &unlockParityNodes[i];\n\t\t\tunlockParityNodes[i].antecedents[0] = &writeParityNodes[i];\n\t\t\tunlockParityNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockParityNodes[i].numSuccedents == 1);\n\t\t\tunlockParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t} else {\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &writeParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tif (lu_flag) {\n\t\t\t\t/* connect write new Q nodes to unlock nodes */\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numAntecedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = &unlockQNodes[i];\n\t\t\t\tunlockQNodes[i].antecedents[0] = &writeQNodes[i];\n\t\t\t\tunlockQNodes[i].antType[0] = rf_control;\n\n\t\t\t\t/* connect unlock nodes to unblock node */\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numSuccedents == 1);\n\t\t\t\tunlockQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t} else {\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t}\n\t\t}\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeQNodes[i].numSuccedents == 1"
          ],
          "line": 1971
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockQNodes[i].numSuccedents == 1"
          ],
          "line": 1966
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockQNodes[i].numAntecedents == 1"
          ],
          "line": 1960
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeQNodes[i].numSuccedents == 1"
          ],
          "line": 1959
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeParityNodes[i].numSuccedents == 1"
          ],
          "line": 1948
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockParityNodes[i].numSuccedents == 1"
          ],
          "line": 1943
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockParityNodes[i].numAntecedents == 1"
          ],
          "line": 1937
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeParityNodes[i].numSuccedents == 1"
          ],
          "line": 1936
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes))"
          ],
          "line": 1926
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numSuccedents == 1"
          ],
          "line": 1925
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockDataNodes[i].numSuccedents == 1"
          ],
          "line": 1919
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockDataNodes[i].numAntecedents == 1"
          ],
          "line": 1913
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numSuccedents == 1"
          ],
          "line": 1912
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 1908
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes))"
          ],
          "line": 1907
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "qNodes[j].numSuccedents == 1"
          ],
          "line": 1900
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeQNodes[i].numAntecedents == numParityNodes"
          ],
          "line": 1898
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNodes[j].numSuccedents == numParityNodes"
          ],
          "line": 1888
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeParityNodes[i].numAntecedents == numParityNodes"
          ],
          "line": 1886
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readQNodes[i].numSuccedents == numParityNodes"
          ],
          "line": 1877
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readParityNodes[i].numSuccedents == numParityNodes"
          ],
          "line": 1866
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "qNodes[j].numAntecedents == numDataNodes + numParityNodes"
          ],
          "line": 1857
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNodes[j].numAntecedents == numDataNodes + numParityNodes"
          ],
          "line": 1846
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numAntecedents == 1"
          ],
          "line": 1837
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readDataNodes[i].numSuccedents == ((nfaults * numParityNodes) + 1)"
          ],
          "line": 1836
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readQNodes[i].numAntecedents == 1"
          ],
          "line": 1829
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readParityNodes[i].numAntecedents == 1"
          ],
          "line": 1820
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readDataNodes[i].numAntecedents == 1"
          ],
          "line": 1812
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults))"
          ],
          "line": 1809
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "lu_flag",
            "which_ru"
          ],
          "line": 1798
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "&unlockQNodes[i]",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskUnlockFunc",
            "rf_DiskUnlockUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "2",
            "0",
            "dag_h",
            "\"Unq\"",
            "allocList"
          ],
          "line": 1795
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1791
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1784
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "lu_flag",
            "which_ru"
          ],
          "line": 1774
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1767
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1760
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "lu_flag",
            "which_ru"
          ],
          "line": 1657
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1650
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1643
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "lu_flag",
            "0",
            "which_ru"
          ],
          "line": 1634
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_AllocBuffer",
          "args": [
            "raidPtr",
            "dag_h",
            "pda",
            "allocList"
          ],
          "line": 1632
        },
        "resolved": true,
        "details": {
          "function_name": "rf_AllocBuffer",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "263-275",
          "snippet": "void   *\nrf_AllocBuffer(\n    RF_Raid_t * raidPtr,\n    RF_DagHeader_t * dag_h,\n    RF_PhysDiskAddr_t * pda,\n    RF_AllocListElem_t * allocList)\n{\n\tchar   *p;\n\n\tRF_MallocAndAdd(p, pda->numSector << raidPtr->logBytesPerSector,\n\t    (char *), allocList);\n\treturn ((void *) p);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid   *\nrf_AllocBuffer(\n    RF_Raid_t * raidPtr,\n    RF_DagHeader_t * dag_h,\n    RF_PhysDiskAddr_t * pda,\n    RF_AllocListElem_t * allocList)\n{\n\tchar   *p;\n\n\tRF_MallocAndAdd(p, pda->numSector << raidPtr->logBytesPerSector,\n\t    (char *), allocList);\n\treturn ((void *) p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1629
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "lu_flag",
            "0",
            "which_ru"
          ],
          "line": 1619
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1613
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "lu_flag",
            "0",
            "which_ru"
          ],
          "line": 1603
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1597
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == totalNumNodes"
          ],
          "line": 1584
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "totalNumNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 1544
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "numDataNodes > 0"
          ],
          "line": 1518
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating small-write DAG]\\n\""
          ],
          "line": 1517
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 1511
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CommonCreateSmallWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;\n\tRF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;\n\tRF_DagNode_t *xorNodes, *qNodes, *blockNode, *nodes;\n\tRF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;\n\tint     i, j, nNodes, totalNumNodes, lu_flag;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     (*qfunc) (RF_DagNode_t *);\n\tint     numDataNodes, numParityNodes;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *name, *qname;\n\tlong    nfaults;\n\n\tnfaults = qfuncs ? 2 : 1;\n\tlu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tpda = asmap->physInfo;\n\tnumDataNodes = asmap->numStripeUnitsAccessed;\n\tnumParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tdag_h->creator = \"SmallWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\tqfunc = NULL;\n\tqname = NULL;\n\n\t/* DAG creation occurs in four steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node (nfaults * nparity) a read\n\t * and write for each parity unit a block node a terminate node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes) + (nfaults * 2 * numParityNodes) + 2;\n\tif (lu_flag)\n\t\ttotalNumNodes += (numDataNodes + (nfaults * numParityNodes));\n\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\twriteParityNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t\tunlockParityNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t} else {\n\t\tunlockDataNodes = unlockParityNodes = NULL;\n\t}\n\tif (nfaults == 2) {\n\t\treadQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\twriteQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tqNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tif (lu_flag) {\n\t\t\tunlockQNodes = &nodes[i];\n\t\t\ti += numParityNodes;\n\t\t} else {\n\t\t\tunlockQNodes = NULL;\n\t\t}\n\t} else {\n\t\treadQNodes = writeQNodes = qNodes = unlockQNodes = NULL;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\tnNodes = numDataNodes + (nfaults * numParityNodes);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminate node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readDataNodes[i].numSuccedents; j++)\n\t\t\treadDataNodes[i].propList[j] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tfor (j = 0; j < readParityNodes[i].numSuccedents; j++)\n\t\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which read old Q (Roq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\trf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Roq\", allocList);\n\t\t\treadQNodes[i].params[0].p = pda;\n\t\t\treadQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old Q */\n\t\t\treadQNodes[i].params[2].v = parityStripeID;\n\t\t\treadQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\t\tfor (j = 0; j < readQNodes[i].numSuccedents; j++)\n\t\t\t\treadQNodes[i].propList[0] = NULL;\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity and Q */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\tif (nfaults == 2) {\n\t\t\t\trf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, qname, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\t\tqNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[2] = readQNodes[i].params[0];\n\t\t\t\tqNodes[i].params[3] = readQNodes[i].params[1];\n\t\t\t\tqNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[6].p = raidPtr;\n\t\t\t\tqNodes[i].results[0] = readQNodes[i].params[1].p;\t/* use old Q buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t\tif (nfaults == 2) {\n\t\t\trf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, qname, allocList);\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Rod */\n\t\t\t\tqNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\t/* and read old q */\n\t\t\tqNodes[0].params[2 * numDataNodes + 0] = readQNodes[0].params[0];\t/* pda */\n\t\t\tqNodes[0].params[2 * numDataNodes + 1] = readQNodes[0].params[1];\t/* buffer pointer */\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Wnd nodes */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\tqNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\t\tqNodes[0].results[0] = readQNodes[0].params[1].p;\n\t\t}\n\t}\n\n\t/* initialize nodes which write new parity (Wnp) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\trf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnp\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twriteParityNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t * filled in by xor node */\n\t\twriteParityNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\twriteParityNodes[i].params[2].v = parityStripeID;\n\t\twriteParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unp\", allocList);\n\t\t\tunlockParityNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new Q (Wnq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnq\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twriteQNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t\t * filled in by xor node */\n\t\t\twriteQNodes[i].params[1].p = qNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\t\twriteQNodes[i].params[2].v = parityStripeID;\n\t\t\twriteQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\t\tif (lu_flag) {\n\t\t\t\t/* initialize node to unlock the disk queue */\n\t\t\t\trf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unq\", allocList);\n\t\t\t\tunlockQNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t\t * desc */\n\t\t\t\tunlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t\t}\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old Q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tblockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];\n\t\t\tRF_ASSERT(readQNodes[i].numAntecedents == 1);\n\t\t\treadQNodes[i].antecedents[0] = blockNode;\n\t\t\treadQNodes[i].antType[0] = rf_control;\n\t\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == ((nfaults * numParityNodes) + 1));\n\t\tRF_ASSERT(writeDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].succedents[0] = &writeDataNodes[i];\n\t\twriteDataNodes[i].antecedents[0] = &readDataNodes[i];\n\t\twriteDataNodes[i].antType[0] = rf_antiData;\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[1 + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numDataNodes; i++)\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\t\treadDataNodes[i].succedents[1 + numParityNodes + j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\t\tqNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\t\treadParityNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old q nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(readQNodes[i].numSuccedents == numParityNodes);\n\t\t\t\treadQNodes[i].succedents[j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];\n\t\t\t\tqNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\t/* connect xor nodes to the write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(writeParityNodes[i].numAntecedents == numParityNodes);\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numSuccedents == numParityNodes);\n\t\t\txorNodes[i].succedents[j] = &writeParityNodes[j];\n\t\t\twriteParityNodes[j].antecedents[i] = &xorNodes[i];\n\t\t\twriteParityNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect q nodes to the write new q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(writeQNodes[i].numAntecedents == numParityNodes);\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numSuccedents == 1);\n\t\t\t\tqNodes[i].succedents[j] = &writeQNodes[j];\n\t\t\t\twriteQNodes[j].antecedents[i] = &qNodes[i];\n\t\t\t\twriteQNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tunlockDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to term node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &writeDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new parity nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockParityNodes[i].numAntecedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = &unlockParityNodes[i];\n\t\t\tunlockParityNodes[i].antecedents[0] = &writeParityNodes[i];\n\t\t\tunlockParityNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockParityNodes[i].numSuccedents == 1);\n\t\t\tunlockParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t} else {\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &writeParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tif (lu_flag) {\n\t\t\t\t/* connect write new Q nodes to unlock nodes */\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numAntecedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = &unlockQNodes[i];\n\t\t\t\tunlockQNodes[i].antecedents[0] = &writeQNodes[i];\n\t\t\t\tunlockQNodes[i].antType[0] = rf_control;\n\n\t\t\t\t/* connect unlock nodes to unblock node */\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numSuccedents == 1);\n\t\t\t\tunlockQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t} else {\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t}\n\t\t}\n}"
  },
  {
    "function_name": "rf_CommonCreateLargeWriteDAGFwd",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "1215-1447",
    "snippet": "void \nrf_CommonCreateLargeWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *),\n    int allowBufferRecycle)\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;\n\tRF_DagNode_t *wnqNode, *blockNode, *syncNode, *termNode;\n\tint     nWndNodes, nRodNodes, i, nodeNum, asmNum;\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_StripeNum_t parityStripeID;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_PhysDiskAddr_t *pda;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating large-write DAG]\\n\");\n\tdag_h->creator = \"LargeWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0) {\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\t} else {\n\t\trodNodes = NULL;\n\t}\n\n\t/* begin node initialization */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0, dag_h, \"Nil\", allocList);\n\t} else {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h, \"Nil\", allocList);\n\t}\n\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tif (allowBufferRecycle) {\n\t\tfor (i = 0; i < nRodNodes; i++)\n\t\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\t\tbreak;\n\t}\n\tif ((!allowBufferRecycle) || (i == nRodNodes)) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\n\t/* initialize the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = asmap->parityInfo;\n\twnpNode->params[1].p = xorNode->results[0];\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\tif (nfaults == 2) {\n\t\t/* we never try to recycle a buffer for the Q calcuation in\n\t\t * addition to the parity. This would cause two buffers to get\n\t\t * smashed during the P and Q calculation, guaranteeing one\n\t\t * would be wrong. */\n\t\tRF_CallocAndAdd(xorNode->results[1], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\twnqNode->params[0].p = asmap->qInfo;\n\t\twnqNode->params[1].p = xorNode->results[1];\n\t\twnqNode->params[2].v = parityStripeID;\n\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t\t * parity unit */\n\t}\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (nRodNodes > 0) {\n\t\t/* connect the block node to the Rod nodes */\n\t\tRF_ASSERT(blockNode->numSuccedents == nRodNodes);\n\t\tRF_ASSERT(syncNode->numAntecedents == nRodNodes);\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\t\trodNodes[i].antecedents[0] = blockNode;\n\t\t\trodNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect the Rod nodes to the Nil node */\n\t\t\tRF_ASSERT(rodNodes[i].numSuccedents == 1);\n\t\t\trodNodes[i].succedents[0] = syncNode;\n\t\t\tsyncNode->antecedents[i] = &rodNodes[i];\n\t\t\tsyncNode->antType[i] = rf_trueData;\n\t\t}\n\t} else {\n\t\t/* connect the block node to the Nil node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(syncNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = syncNode;\n\t\tsyncNode->antecedents[0] = blockNode;\n\t\tsyncNode->antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Wnd nodes */\n\tRF_ASSERT(syncNode->numSuccedents == (1 + nWndNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Xor node */\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[nWndNodes] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_control;\n\n\t/* connect the xor node to the write parity node */\n\tRF_ASSERT(xorNode->numSuccedents == nfaults);\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\txorNode->succedents[0] = wnpNode;\n\twnpNode->antecedents[0] = xorNode;\n\twnpNode->antType[0] = rf_trueData;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\txorNode->succedents[1] = wnqNode;\n\t\twnqNode->antecedents[0] = xorNode;\n\t\twnqNode->antType[0] = rf_trueData;\n\t}\n\t/* connect the write nodes to the term node */\n\tRF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = termNode;\n\t\ttermNode->antecedents[i] = &wndNodes[i];\n\t\ttermNode->antType[i] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = termNode;\n\ttermNode->antecedents[nWndNodes] = wnpNode;\n\ttermNode->antType[nWndNodes] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\ttermNode->antType[nWndNodes + 1] = rf_control;\n\t}\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnqNode->numSuccedents == 1"
          ],
          "line": 1442
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnpNode->numSuccedents == 1"
          ],
          "line": 1437
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes->numSuccedents == 1"
          ],
          "line": 1432
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 1430
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == nWndNodes + nfaults"
          ],
          "line": 1429
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnqNode->numAntecedents == 1"
          ],
          "line": 1423
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnpNode->numAntecedents == 1"
          ],
          "line": 1418
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numSuccedents == nfaults"
          ],
          "line": 1417
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numAntecedents == 1"
          ],
          "line": 1411
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes->numAntecedents == 1"
          ],
          "line": 1404
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "syncNode->numSuccedents == (1 + nWndNodes)"
          ],
          "line": 1402
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "syncNode->numAntecedents == 1"
          ],
          "line": 1395
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == 1"
          ],
          "line": 1394
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rodNodes[i].numSuccedents == 1"
          ],
          "line": 1387
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rodNodes[i].numAntecedents == 1"
          ],
          "line": 1381
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "syncNode->numAntecedents == nRodNodes"
          ],
          "line": 1379
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == nRodNodes"
          ],
          "line": 1378
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 1373
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->parityInfo->next == NULL"
          ],
          "line": 1366
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1365
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "wnqNode",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskWriteFunc",
            "rf_DiskWriteUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Wnq\"",
            "allocList"
          ],
          "line": 1361
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "xorNode->results[1]",
            "1",
            "rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit)",
            "(void *), allocList"
          ],
          "line": 1360
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "raidPtr->Layout.sectorsPerStripeUnit"
          ],
          "line": 1360
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->parityInfo->next == NULL"
          ],
          "line": 1351
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1350
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "xorNode->results[0]",
            "1",
            "rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit)",
            "(void *), allocList"
          ],
          "line": 1341
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "raidPtr->Layout.sectorsPerStripeUnit"
          ],
          "line": 1341
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1313
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1309
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "nodeNum == nRodNodes"
          ],
          "line": 1303
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1297
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "rodNodes",
            "nRodNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 1272
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_MapUnaccessedPortionOfStripe",
          "args": [
            "raidPtr",
            "layoutPtr",
            "asmap",
            "dag_h",
            "new_asm_h",
            "&nRodNodes",
            "&sosBuffer",
            "&eosBuffer",
            "allocList"
          ],
          "line": 1270
        },
        "resolved": true,
        "details": {
          "function_name": "rf_MapUnaccessedPortionOfStripe",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "799-851",
          "snippet": "void \nrf_MapUnaccessedPortionOfStripe(\n    RF_Raid_t * raidPtr,\n    RF_RaidLayout_t * layoutPtr,/* in: layout information */\n    RF_AccessStripeMap_t * asmap,\t/* in: access stripe map */\n    RF_DagHeader_t * dag_h,\t/* in: header of the dag to create */\n    RF_AccessStripeMapHeader_t ** new_asm_h,\t/* in: ptr to array of 2\n\t\t\t\t\t\t * headers, to be filled in */\n    int *nRodNodes,\t\t/* out: num nodes to be generated to read\n\t\t\t\t * unaccessed data */\n    char **sosBuffer,\t\t/* out: pointers to newly allocated buffer */\n    char **eosBuffer,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidAddr_t sosRaidAddress, eosRaidAddress;\n\tRF_SectorNum_t sosNumSector, eosNumSector;\n\n\tRF_ASSERT(asmap->numStripeUnitsAccessed > (layoutPtr->numDataCol / 2));\n\t/* generate an access map for the region of the array from start of\n\t * stripe to start of access */\n\tnew_asm_h[0] = new_asm_h[1] = NULL;\n\t*nRodNodes = 0;\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->raidAddress)) {\n\t\tsosRaidAddress = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\t\tsosNumSector = asmap->raidAddress - sosRaidAddress;\n\t\tRF_MallocAndAdd(*sosBuffer, rf_RaidAddressToByte(raidPtr, sosNumSector), (char *), allocList);\n\t\tnew_asm_h[0] = rf_MapAccess(raidPtr, sosRaidAddress, sosNumSector, *sosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\t*nRodNodes += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[0]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[0]->stripeMap);\n\t}\n\t/* generate an access map for the region of the array from end of\n\t * access to end of stripe */\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->endRaidAddress)) {\n\t\teosRaidAddress = asmap->endRaidAddress;\n\t\teosNumSector = rf_RaidAddressOfNextStripeBoundary(layoutPtr, eosRaidAddress) - eosRaidAddress;\n\t\tRF_MallocAndAdd(*eosBuffer, rf_RaidAddressToByte(raidPtr, eosNumSector), (char *), allocList);\n\t\tnew_asm_h[1] = rf_MapAccess(raidPtr, eosRaidAddress, eosNumSector, *eosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\t*nRodNodes += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[1]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[1]->stripeMap);\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_MapUnaccessedPortionOfStripe(\n    RF_Raid_t * raidPtr,\n    RF_RaidLayout_t * layoutPtr,/* in: layout information */\n    RF_AccessStripeMap_t * asmap,\t/* in: access stripe map */\n    RF_DagHeader_t * dag_h,\t/* in: header of the dag to create */\n    RF_AccessStripeMapHeader_t ** new_asm_h,\t/* in: ptr to array of 2\n\t\t\t\t\t\t * headers, to be filled in */\n    int *nRodNodes,\t\t/* out: num nodes to be generated to read\n\t\t\t\t * unaccessed data */\n    char **sosBuffer,\t\t/* out: pointers to newly allocated buffer */\n    char **eosBuffer,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidAddr_t sosRaidAddress, eosRaidAddress;\n\tRF_SectorNum_t sosNumSector, eosNumSector;\n\n\tRF_ASSERT(asmap->numStripeUnitsAccessed > (layoutPtr->numDataCol / 2));\n\t/* generate an access map for the region of the array from start of\n\t * stripe to start of access */\n\tnew_asm_h[0] = new_asm_h[1] = NULL;\n\t*nRodNodes = 0;\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->raidAddress)) {\n\t\tsosRaidAddress = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\t\tsosNumSector = asmap->raidAddress - sosRaidAddress;\n\t\tRF_MallocAndAdd(*sosBuffer, rf_RaidAddressToByte(raidPtr, sosNumSector), (char *), allocList);\n\t\tnew_asm_h[0] = rf_MapAccess(raidPtr, sosRaidAddress, sosNumSector, *sosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\t*nRodNodes += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[0]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[0]->stripeMap);\n\t}\n\t/* generate an access map for the region of the array from end of\n\t * access to end of stripe */\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->endRaidAddress)) {\n\t\teosRaidAddress = asmap->endRaidAddress;\n\t\teosNumSector = rf_RaidAddressOfNextStripeBoundary(layoutPtr, eosRaidAddress) - eosRaidAddress;\n\t\tRF_MallocAndAdd(*eosBuffer, rf_RaidAddressToByte(raidPtr, eosNumSector), (char *), allocList);\n\t\tnew_asm_h[1] = rf_MapAccess(raidPtr, eosRaidAddress, eosNumSector, *eosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\t*nRodNodes += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[1]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[1]->stripeMap);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nWndNodes + 4 + nfaults",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 1250
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating large-write DAG]\\n\""
          ],
          "line": 1241
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 1238
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CommonCreateLargeWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *),\n    int allowBufferRecycle)\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;\n\tRF_DagNode_t *wnqNode, *blockNode, *syncNode, *termNode;\n\tint     nWndNodes, nRodNodes, i, nodeNum, asmNum;\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_StripeNum_t parityStripeID;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_PhysDiskAddr_t *pda;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating large-write DAG]\\n\");\n\tdag_h->creator = \"LargeWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0) {\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\t} else {\n\t\trodNodes = NULL;\n\t}\n\n\t/* begin node initialization */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0, dag_h, \"Nil\", allocList);\n\t} else {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h, \"Nil\", allocList);\n\t}\n\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tif (allowBufferRecycle) {\n\t\tfor (i = 0; i < nRodNodes; i++)\n\t\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\t\tbreak;\n\t}\n\tif ((!allowBufferRecycle) || (i == nRodNodes)) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\n\t/* initialize the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = asmap->parityInfo;\n\twnpNode->params[1].p = xorNode->results[0];\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\tif (nfaults == 2) {\n\t\t/* we never try to recycle a buffer for the Q calcuation in\n\t\t * addition to the parity. This would cause two buffers to get\n\t\t * smashed during the P and Q calculation, guaranteeing one\n\t\t * would be wrong. */\n\t\tRF_CallocAndAdd(xorNode->results[1], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\twnqNode->params[0].p = asmap->qInfo;\n\t\twnqNode->params[1].p = xorNode->results[1];\n\t\twnqNode->params[2].v = parityStripeID;\n\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t\t * parity unit */\n\t}\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (nRodNodes > 0) {\n\t\t/* connect the block node to the Rod nodes */\n\t\tRF_ASSERT(blockNode->numSuccedents == nRodNodes);\n\t\tRF_ASSERT(syncNode->numAntecedents == nRodNodes);\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\t\trodNodes[i].antecedents[0] = blockNode;\n\t\t\trodNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect the Rod nodes to the Nil node */\n\t\t\tRF_ASSERT(rodNodes[i].numSuccedents == 1);\n\t\t\trodNodes[i].succedents[0] = syncNode;\n\t\t\tsyncNode->antecedents[i] = &rodNodes[i];\n\t\t\tsyncNode->antType[i] = rf_trueData;\n\t\t}\n\t} else {\n\t\t/* connect the block node to the Nil node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(syncNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = syncNode;\n\t\tsyncNode->antecedents[0] = blockNode;\n\t\tsyncNode->antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Wnd nodes */\n\tRF_ASSERT(syncNode->numSuccedents == (1 + nWndNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Xor node */\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[nWndNodes] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_control;\n\n\t/* connect the xor node to the write parity node */\n\tRF_ASSERT(xorNode->numSuccedents == nfaults);\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\txorNode->succedents[0] = wnpNode;\n\twnpNode->antecedents[0] = xorNode;\n\twnpNode->antType[0] = rf_trueData;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\txorNode->succedents[1] = wnqNode;\n\t\twnqNode->antecedents[0] = xorNode;\n\t\twnqNode->antType[0] = rf_trueData;\n\t}\n\t/* connect the write nodes to the term node */\n\tRF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = termNode;\n\t\ttermNode->antecedents[i] = &wndNodes[i];\n\t\ttermNode->antType[i] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = termNode;\n\ttermNode->antecedents[nWndNodes] = wnpNode;\n\ttermNode->antType[nWndNodes] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\ttermNode->antType[nWndNodes + 1] = rf_control;\n\t}\n}"
  },
  {
    "function_name": "rf_CreateRaidOneWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "1064-1202",
    "snippet": "void \nrf_CreateRaidOneWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *unblockNode, *termNode, *commitNode;\n\tRF_DagNode_t *nodes, *wndNode, *wmirNode;\n\tint     nWndNodes, nWmirNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda, *pdaP;\n\tRF_StripeNum_t parityStripeID;\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID level 1 write DAG]\\n\");\n\t}\n\tdag_h->creator = \"RaidOneWriteDAG\";\n\n\t/* 2 implies access not SU aligned */\n\tnWmirNodes = (asmap->parityInfo->next) ? 2 : 1;\n\tnWndNodes = (asmap->physInfo->next) ? 2 : 1;\n\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 1)\n\t\tnWndNodes--;\n\tif (asmap->numParityFailed == 1)\n\t\tnWmirNodes--;\n\n\t/* total number of nodes = nWndNodes + nWmirNodes + (commit + unblock\n\t * + terminator) */\n\tRF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNode = &nodes[i];\n\ti += nWndNodes;\n\twmirNode = &nodes[i];\n\ti += nWmirNodes;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == (nWndNodes + nWmirNodes + 3));\n\n\t/* this dag can commit immediately */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the commit, unblock, and term nodes */\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the wnd nodes */\n\tif (nWndNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tfor (i = 0; i < nWndNodes; i++) {\n\t\t\trf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wpd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twndNode[i].params[0].p = pda;\n\t\t\twndNode[i].params[1].p = pda->bufPtr;\n\t\t\twndNode[i].params[2].v = parityStripeID;\n\t\t\twndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t}\n\t/* initialize the mirror nodes */\n\tif (nWmirNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tpdaP = asmap->parityInfo;\n\t\tfor (i = 0; i < nWmirNodes; i++) {\n\t\t\trf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wsd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twmirNode[i].params[0].p = pdaP;\n\t\t\twmirNode[i].params[1].p = pda->bufPtr;\n\t\t\twmirNode[i].params[2].v = parityStripeID;\n\t\t\twmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t\tpdaP = pdaP->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t\tRF_ASSERT(pdaP == NULL);\n\t}\n\t/* link the header node to the commit node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 0);\n\tdag_h->succedents[0] = commitNode;\n\n\t/* link the commit node to the write nodes */\n\tRF_ASSERT(commitNode->numSuccedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &wndNode[i];\n\t\twndNode[i].antecedents[0] = commitNode;\n\t\twndNode[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i + nWndNodes] = &wmirNode[i];\n\t\twmirNode[i].antecedents[0] = commitNode;\n\t\twmirNode[i].antType[0] = rf_control;\n\t}\n\n\t/* link the write nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numSuccedents == 1);\n\t\twndNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNode[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numSuccedents == 1);\n\t\twmirNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i + nWndNodes] = &wmirNode[i];\n\t\tunblockNode->antType[i + nWndNodes] = rf_control;\n\t}\n\n\t/* link the unblock node to the term node */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 1198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == 1"
          ],
          "line": 1197
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numSuccedents == 1"
          ],
          "line": 1196
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wmirNode[i].numSuccedents == 1"
          ],
          "line": 1189
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNode[i].numSuccedents == 1"
          ],
          "line": 1183
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unblockNode->numAntecedents == (nWndNodes + nWmirNodes)"
          ],
          "line": 1181
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wmirNode[i].numAntecedents == 1"
          ],
          "line": 1174
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNode[i].numAntecedents == 1"
          ],
          "line": 1168
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numSuccedents == (nWndNodes + nWmirNodes)"
          ],
          "line": 1166
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numAntecedents == 0"
          ],
          "line": 1162
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "dag_h->numSuccedents == 1"
          ],
          "line": 1161
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pdaP == NULL"
          ],
          "line": 1158
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda == NULL"
          ],
          "line": 1157
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1153
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1149
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "&wmirNode[i]",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskWriteFunc",
            "rf_DiskWriteUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Wsd\"",
            "allocList"
          ],
          "line": 1147
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda == NULL"
          ],
          "line": 1140
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 1137
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 1133
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == (nWndNodes + nWmirNodes + 3)"
          ],
          "line": 1112
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nWndNodes + nWmirNodes + 3",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 1099
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating RAID level 1 write DAG]\\n\""
          ],
          "line": 1083
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 1080
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateRaidOneWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_DagNode_t *unblockNode, *termNode, *commitNode;\n\tRF_DagNode_t *nodes, *wndNode, *wmirNode;\n\tint     nWndNodes, nWmirNodes, i;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_PhysDiskAddr_t *pda, *pdaP;\n\tRF_StripeNum_t parityStripeID;\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating RAID level 1 write DAG]\\n\");\n\t}\n\tdag_h->creator = \"RaidOneWriteDAG\";\n\n\t/* 2 implies access not SU aligned */\n\tnWmirNodes = (asmap->parityInfo->next) ? 2 : 1;\n\tnWndNodes = (asmap->physInfo->next) ? 2 : 1;\n\n\t/* alloc the Wnd nodes and the Wmir node */\n\tif (asmap->numDataFailed == 1)\n\t\tnWndNodes--;\n\tif (asmap->numParityFailed == 1)\n\t\tnWmirNodes--;\n\n\t/* total number of nodes = nWndNodes + nWmirNodes + (commit + unblock\n\t * + terminator) */\n\tRF_CallocAndAdd(nodes, nWndNodes + nWmirNodes + 3, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNode = &nodes[i];\n\ti += nWndNodes;\n\twmirNode = &nodes[i];\n\ti += nWmirNodes;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\tunblockNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == (nWndNodes + nWmirNodes + 3));\n\n\t/* this dag can commit immediately */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* initialize the commit, unblock, and term nodes */\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, (nWndNodes + nWmirNodes), 0, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(unblockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, 1, (nWndNodes + nWmirNodes), 0, 0, dag_h, \"Nil\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the wnd nodes */\n\tif (nWndNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tfor (i = 0; i < nWndNodes; i++) {\n\t\t\trf_InitNode(&wndNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wpd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twndNode[i].params[0].p = pda;\n\t\t\twndNode[i].params[1].p = pda->bufPtr;\n\t\t\twndNode[i].params[2].v = parityStripeID;\n\t\t\twndNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t}\n\t/* initialize the mirror nodes */\n\tif (nWmirNodes > 0) {\n\t\tpda = asmap->physInfo;\n\t\tpdaP = asmap->parityInfo;\n\t\tfor (i = 0; i < nWmirNodes; i++) {\n\t\t\trf_InitNode(&wmirNode[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wsd\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twmirNode[i].params[0].p = pdaP;\n\t\t\twmirNode[i].params[1].p = pda->bufPtr;\n\t\t\twmirNode[i].params[2].v = parityStripeID;\n\t\t\twmirNode[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t\tpdaP = pdaP->next;\n\t\t}\n\t\tRF_ASSERT(pda == NULL);\n\t\tRF_ASSERT(pdaP == NULL);\n\t}\n\t/* link the header node to the commit node */\n\tRF_ASSERT(dag_h->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 0);\n\tdag_h->succedents[0] = commitNode;\n\n\t/* link the commit node to the write nodes */\n\tRF_ASSERT(commitNode->numSuccedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &wndNode[i];\n\t\twndNode[i].antecedents[0] = commitNode;\n\t\twndNode[i].antType[0] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i + nWndNodes] = &wmirNode[i];\n\t\twmirNode[i].antecedents[0] = commitNode;\n\t\twmirNode[i].antType[0] = rf_control;\n\t}\n\n\t/* link the write nodes to the unblock node */\n\tRF_ASSERT(unblockNode->numAntecedents == (nWndNodes + nWmirNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNode[i].numSuccedents == 1);\n\t\twndNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i] = &wndNode[i];\n\t\tunblockNode->antType[i] = rf_control;\n\t}\n\tfor (i = 0; i < nWmirNodes; i++) {\n\t\tRF_ASSERT(wmirNode[i].numSuccedents == 1);\n\t\twmirNode[i].succedents[0] = unblockNode;\n\t\tunblockNode->antecedents[i + nWndNodes] = &wmirNode[i];\n\t\tunblockNode->antType[i + nWndNodes] = rf_control;\n\t}\n\n\t/* link the unblock node to the term node */\n\tRF_ASSERT(unblockNode->numSuccedents == 1);\n\tRF_ASSERT(termNode->numAntecedents == 1);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tunblockNode->succedents[0] = termNode;\n\ttermNode->antecedents[0] = unblockNode;\n\ttermNode->antType[0] = rf_control;\n}"
  },
  {
    "function_name": "rf_CommonCreateSmallWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "464-1045",
    "snippet": "void \nrf_CommonCreateSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;\n\tRF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;\n\tRF_DagNode_t *xorNodes, *qNodes, *blockNode, *commitNode, *nodes;\n\tRF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;\n\tint     i, j, nNodes, totalNumNodes, lu_flag;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     (*qfunc) (RF_DagNode_t *);\n\tint     numDataNodes, numParityNodes;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *name, *qname;\n\tlong    nfaults;\n\n\tnfaults = qfuncs ? 2 : 1;\n\tlu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tpda = asmap->physInfo;\n\tnumDataNodes = asmap->numStripeUnitsAccessed;\n\tnumParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating small-write DAG]\\n\");\n\t}\n\tRF_ASSERT(numDataNodes > 0);\n\tdag_h->creator = \"SmallWriteDAG\";\n\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/*\n         * DAG creation occurs in four steps:\n         * 1. count the number of nodes in the DAG\n         * 2. create the nodes\n         * 3. initialize the nodes\n         * 4. connect the nodes\n         */\n\n\t/*\n         * Step 1. compute number of nodes in the graph\n         */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node (nfaults * nparity) a read\n\t * and write for each parity unit a block and commit node (2) a\n\t * terminate node if atomic RMW an unlock node for each data unit,\n\t * redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes)\n\t    + (nfaults * 2 * numParityNodes) + 3;\n\tif (lu_flag) {\n\t\ttotalNumNodes += (numDataNodes + (nfaults * numParityNodes));\n\t}\n\t/*\n         * Step 2. create the nodes\n         */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\twriteParityNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t\tunlockParityNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t} else {\n\t\tunlockDataNodes = unlockParityNodes = NULL;\n\t}\n\tif (nfaults == 2) {\n\t\treadQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\twriteQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tqNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tif (lu_flag) {\n\t\t\tunlockQNodes = &nodes[i];\n\t\t\ti += numParityNodes;\n\t\t} else {\n\t\t\tunlockQNodes = NULL;\n\t\t}\n\t} else {\n\t\treadQNodes = writeQNodes = qNodes = unlockQNodes = NULL;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/*\n         * Step 3. initialize the nodes\n         */\n\t/* initialize block node (Nil) */\n\tnNodes = numDataNodes + (nfaults * numParityNodes);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize commit node (Cmt) */\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nNodes, (nfaults * numParityNodes), 0, 0, dag_h, \"Cmt\", allocList);\n\n\t/* initialize terminate node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, nNodes, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, (nfaults * numParityNodes), 1, 4, 0, dag_h,\n\t\t    \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\t/* physical disk addr desc */\n\t\treadDataNodes[i].params[0].p = pda;\n\t\t/* buffer to hold old data */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr,\n\t\t    dag_h, pda, allocList);\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readDataNodes[i].numSuccedents; j++) {\n\t\t\treadDataNodes[i].propList[j] = NULL;\n\t\t}\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4,\n\t\t    0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\t/* buffer to hold old parity */\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr,\n\t\t    dag_h, pda, allocList);\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readParityNodes[i].numSuccedents; j++) {\n\t\t\treadParityNodes[i].propList[0] = NULL;\n\t\t}\n\t}\n\n\t/* initialize nodes which read old Q (Roq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\trf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Roq\", allocList);\n\t\t\treadQNodes[i].params[0].p = pda;\n\t\t\t/* buffer to hold old Q */\n\t\t\treadQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda,\n\t\t\t    allocList);\n\t\t\treadQNodes[i].params[2].v = parityStripeID;\n\t\t\treadQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    lu_flag, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t\tfor (j = 0; j < readQNodes[i].numSuccedents; j++) {\n\t\t\t\treadQNodes[i].propList[0] = NULL;\n\t\t\t}\n\t\t}\n\t}\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,\n\t\t    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Wnd\", allocList);\n\t\t/* physical disk addr desc */\n\t\twriteDataNodes[i].params[0].p = pda;\n\t\t/* buffer holding new data to be written */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    0, 0, which_ru);\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,\n\t\t\t    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,\n\t\t\t    \"Und\", allocList);\n\t\t\t/* physical disk addr desc */\n\t\t\tunlockDataNodes[i].params[0].p = pda;\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/*\n         * Initialize nodes which compute new parity and Q.\n         */\n\t/*\n         * We use the simple XOR func in the double-XOR case, and when\n         * we're accessing only a portion of one stripe unit. The distinction\n         * between the two is that the regular XOR func assumes that the targbuf\n         * is a full SU in size, and examines the pda associated with the buffer\n         * to decide where within the buffer to XOR the data, whereas\n         * the simple XOR func just XORs the data into the start of the buffer.\n         */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1)\n\t\t&& (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t} else {\n\t\t\tqfunc = NULL;\n\t\t\tqname = NULL;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t} else {\n\t\t\tqfunc = NULL;\n\t\t\tqname = NULL;\n\t\t}\n\t}\n\t/*\n         * Initialize the xor nodes: params are {pda,buf}\n         * from {Rod,Wnd,Rop} nodes, and raidPtr\n         */\n\tif (numParityNodes == 2) {\n\t\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\t/* note: no wakeup func for xor */\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL,\n\t\t\t    1, (numDataNodes + numParityNodes), 7, 1, dag_h, name, allocList);\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\t/* use old parity buf as target buf */\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\n\t\t\tif (nfaults == 2) {\n\t\t\t\t/* note: no wakeup func for qor */\n\t\t\t\trf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,\n\t\t\t\t    (numDataNodes + numParityNodes), 7, 1, dag_h, qname, allocList);\n\t\t\t\tqNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[2] = readQNodes[i].params[0];\n\t\t\t\tqNodes[i].params[3] = readQNodes[i].params[1];\n\t\t\t\tqNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[6].p = raidPtr;\n\t\t\t\t/* use old Q buf as target buf */\n\t\t\t\tqNodes[i].results[0] = readQNodes[i].params[1].p;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, 1,\n\t\t    (numDataNodes + numParityNodes),\n\t\t    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer ptr */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =\t/* pda */\n\t\t\t    writeDataNodes[i].params[0];\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =\t/* buffer ptr */\n\t\t\t    writeDataNodes[i].params[1];\n\t\t}\n\t\t/* xor node needs to get at RAID information */\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t\tif (nfaults == 2) {\n\t\t\trf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,\n\t\t\t    (numDataNodes + numParityNodes),\n\t\t\t    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h,\n\t\t\t    qname, allocList);\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Rod */\n\t\t\t\tqNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer ptr */\n\t\t\t}\n\t\t\t/* and read old q */\n\t\t\tqNodes[0].params[2 * numDataNodes + 0] =\t/* pda */\n\t\t\t    readQNodes[0].params[0];\n\t\t\tqNodes[0].params[2 * numDataNodes + 1] =\t/* buffer ptr */\n\t\t\t    readQNodes[0].params[1];\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Wnd nodes */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =\t/* pda */\n\t\t\t\t    writeDataNodes[i].params[0];\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =\t/* buffer ptr */\n\t\t\t\t    writeDataNodes[i].params[1];\n\t\t\t}\n\t\t\t/* xor node needs to get at RAID information */\n\t\t\tqNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\n\t\t\tqNodes[0].results[0] = readQNodes[0].params[1].p;\n\t\t}\n\t}\n\n\t/* initialize nodes which write new parity (Wnp) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\trf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,\n\t\t    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Wnp\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twriteParityNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t * filled in by xor node */\n\t\twriteParityNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\twriteParityNodes[i].params[2].v = parityStripeID;\n\t\twriteParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    0, 0, which_ru);\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,\n\t\t\t    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,\n\t\t\t    \"Unp\", allocList);\n\t\t\tunlockParityNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new Q (Wnq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,\n\t\t\t    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t\t    \"Wnq\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twriteQNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t\t * filled in by xor node */\n\t\t\twriteQNodes[i].params[1].p = qNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\t\twriteQNodes[i].params[2].v = parityStripeID;\n\t\t\twriteQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    0, 0, which_ru);\n\t\t\tif (lu_flag) {\n\t\t\t\t/* initialize node to unlock the disk queue */\n\t\t\t\trf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,\n\t\t\t\t    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,\n\t\t\t\t    \"Unq\", allocList);\n\t\t\t\tunlockQNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t\t * desc */\n\t\t\t\tunlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t\t    0, lu_flag, which_ru);\n\t\t\t}\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/*\n         * Step 4. connect the nodes.\n         */\n\n\t/* connect header to block node */\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old Q nodes */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tblockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];\n\t\t\tRF_ASSERT(readQNodes[i].numAntecedents == 1);\n\t\t\treadQNodes[i].antecedents[0] = blockNode;\n\t\t\treadQNodes[i].antType[0] = rf_control;\n\t\t}\n\t}\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == (nfaults * numParityNodes));\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to q nodes */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\t\treadDataNodes[i].succedents[numParityNodes + j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\t\tqNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\t\t}\n\t}\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\treadParityNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old q nodes to q nodes */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\treadQNodes[i].succedents[j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];\n\t\t\t\tqNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t\t}\n\t\t}\n\t}\n\t/* connect xor nodes to commit node */\n\tRF_ASSERT(commitNode->numAntecedents == (nfaults * numParityNodes));\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(xorNodes[i].numSuccedents == 1);\n\t\txorNodes[i].succedents[0] = commitNode;\n\t\tcommitNode->antecedents[i] = &xorNodes[i];\n\t\tcommitNode->antType[i] = rf_control;\n\t}\n\n\t/* connect q nodes to commit node */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(qNodes[i].numSuccedents == 1);\n\t\t\tqNodes[i].succedents[0] = commitNode;\n\t\t\tcommitNode->antecedents[i + numParityNodes] = &qNodes[i];\n\t\t\tcommitNode->antType[i + numParityNodes] = rf_control;\n\t\t}\n\t}\n\t/* connect commit node to write nodes */\n\tRF_ASSERT(commitNode->numSuccedents == (numDataNodes + (nfaults * numParityNodes)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(writeDataNodes[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &writeDataNodes[i];\n\t\twriteDataNodes[i].antecedents[0] = commitNode;\n\t\twriteDataNodes[i].antType[0] = rf_trueData;\n\t}\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(writeParityNodes[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i + numDataNodes] = &writeParityNodes[i];\n\t\twriteParityNodes[i].antecedents[0] = commitNode;\n\t\twriteParityNodes[i].antType[0] = rf_trueData;\n\t}\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(writeQNodes[i].numAntecedents == 1);\n\t\t\tcommitNode->succedents[i + numDataNodes + numParityNodes] = &writeQNodes[i];\n\t\t\twriteQNodes[i].antecedents[0] = commitNode;\n\t\t\twriteQNodes[i].antType[0] = rf_trueData;\n\t\t}\n\t}\n\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tunlockDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to term node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &writeDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new parity nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockParityNodes[i].numAntecedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = &unlockParityNodes[i];\n\t\t\tunlockParityNodes[i].antecedents[0] = &writeParityNodes[i];\n\t\t\tunlockParityNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockParityNodes[i].numSuccedents == 1);\n\t\t\tunlockParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t} else {\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &writeParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tif (lu_flag) {\n\t\t\t\t/* connect write new Q nodes to unlock nodes */\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numAntecedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = &unlockQNodes[i];\n\t\t\t\tunlockQNodes[i].antecedents[0] = &writeQNodes[i];\n\t\t\t\tunlockQNodes[i].antType[0] = rf_control;\n\n\t\t\t\t/* connect unlock nodes to unblock node */\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numSuccedents == 1);\n\t\t\t\tunlockQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t} else {\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t}\n\t\t}\n\t}\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeQNodes[i].numSuccedents == 1"
          ],
          "line": 1038
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockQNodes[i].numSuccedents == 1"
          ],
          "line": 1033
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockQNodes[i].numAntecedents == 1"
          ],
          "line": 1027
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeQNodes[i].numSuccedents == 1"
          ],
          "line": 1026
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeParityNodes[i].numSuccedents == 1"
          ],
          "line": 1015
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockParityNodes[i].numSuccedents == 1"
          ],
          "line": 1010
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockParityNodes[i].numAntecedents == 1"
          ],
          "line": 1004
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeParityNodes[i].numSuccedents == 1"
          ],
          "line": 1003
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes))"
          ],
          "line": 993
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numSuccedents == 1"
          ],
          "line": 992
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockDataNodes[i].numSuccedents == 1"
          ],
          "line": 986
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "unlockDataNodes[i].numAntecedents == 1"
          ],
          "line": 980
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numSuccedents == 1"
          ],
          "line": 979
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 975
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes))"
          ],
          "line": 974
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeQNodes[i].numAntecedents == 1"
          ],
          "line": 968
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeParityNodes[i].numAntecedents == 1"
          ],
          "line": 961
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "writeDataNodes[i].numAntecedents == 1"
          ],
          "line": 955
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numSuccedents == (numDataNodes + (nfaults * numParityNodes))"
          ],
          "line": 953
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "qNodes[i].numSuccedents == 1"
          ],
          "line": 946
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNodes[i].numSuccedents == 1"
          ],
          "line": 937
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numAntecedents == (nfaults * numParityNodes)"
          ],
          "line": 935
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readParityNodes[i].numSuccedents == numParityNodes"
          ],
          "line": 926
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readParityNodes[i].numSuccedents == numParityNodes"
          ],
          "line": 915
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "qNodes[j].numAntecedents == numDataNodes + numParityNodes"
          ],
          "line": 906
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNodes[j].numAntecedents == numDataNodes + numParityNodes"
          ],
          "line": 895
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readDataNodes[i].numSuccedents == (nfaults * numParityNodes)"
          ],
          "line": 893
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readQNodes[i].numAntecedents == 1"
          ],
          "line": 886
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readParityNodes[i].numAntecedents == 1"
          ],
          "line": 877
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "readDataNodes[i].numAntecedents == 1"
          ],
          "line": 869
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults))"
          ],
          "line": 866
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "lu_flag",
            "which_ru"
          ],
          "line": 852
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "&unlockQNodes[i]",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskUnlockFunc",
            "rf_DiskUnlockUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "2",
            "0",
            "dag_h",
            "\"Unq\"",
            "allocList"
          ],
          "line": 847
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 843
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 836
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "lu_flag",
            "which_ru"
          ],
          "line": 823
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 814
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 807
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "lu_flag",
            "which_ru"
          ],
          "line": 676
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 667
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 658
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "lu_flag",
            "0",
            "which_ru"
          ],
          "line": 647
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_AllocBuffer",
          "args": [
            "raidPtr",
            "dag_h",
            "pda",
            "allocList"
          ],
          "line": 644
        },
        "resolved": true,
        "details": {
          "function_name": "rf_AllocBuffer",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "263-275",
          "snippet": "void   *\nrf_AllocBuffer(\n    RF_Raid_t * raidPtr,\n    RF_DagHeader_t * dag_h,\n    RF_PhysDiskAddr_t * pda,\n    RF_AllocListElem_t * allocList)\n{\n\tchar   *p;\n\n\tRF_MallocAndAdd(p, pda->numSector << raidPtr->logBytesPerSector,\n\t    (char *), allocList);\n\treturn ((void *) p);\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid   *\nrf_AllocBuffer(\n    RF_Raid_t * raidPtr,\n    RF_DagHeader_t * dag_h,\n    RF_PhysDiskAddr_t * pda,\n    RF_AllocListElem_t * allocList)\n{\n\tchar   *p;\n\n\tRF_MallocAndAdd(p, pda->numSector << raidPtr->logBytesPerSector,\n\t    (char *), allocList);\n\treturn ((void *) p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 639
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "lu_flag",
            "0",
            "which_ru"
          ],
          "line": 627
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 618
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "lu_flag",
            "0",
            "which_ru"
          ],
          "line": 606
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 599
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "i == totalNumNodes"
          ],
          "line": 576
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "totalNumNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 533
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "numDataNodes > 0"
          ],
          "line": 501
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating small-write DAG]\\n\""
          ],
          "line": 499
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "&(raidPtr->Layout)",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 492
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CommonCreateSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;\n\tRF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;\n\tRF_DagNode_t *xorNodes, *qNodes, *blockNode, *commitNode, *nodes;\n\tRF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;\n\tint     i, j, nNodes, totalNumNodes, lu_flag;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     (*qfunc) (RF_DagNode_t *);\n\tint     numDataNodes, numParityNodes;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *name, *qname;\n\tlong    nfaults;\n\n\tnfaults = qfuncs ? 2 : 1;\n\tlu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout),\n\t    asmap->raidAddress, &which_ru);\n\tpda = asmap->physInfo;\n\tnumDataNodes = asmap->numStripeUnitsAccessed;\n\tnumParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating small-write DAG]\\n\");\n\t}\n\tRF_ASSERT(numDataNodes > 0);\n\tdag_h->creator = \"SmallWriteDAG\";\n\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/*\n         * DAG creation occurs in four steps:\n         * 1. count the number of nodes in the DAG\n         * 2. create the nodes\n         * 3. initialize the nodes\n         * 4. connect the nodes\n         */\n\n\t/*\n         * Step 1. compute number of nodes in the graph\n         */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node (nfaults * nparity) a read\n\t * and write for each parity unit a block and commit node (2) a\n\t * terminate node if atomic RMW an unlock node for each data unit,\n\t * redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes)\n\t    + (nfaults * 2 * numParityNodes) + 3;\n\tif (lu_flag) {\n\t\ttotalNumNodes += (numDataNodes + (nfaults * numParityNodes));\n\t}\n\t/*\n         * Step 2. create the nodes\n         */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\twriteParityNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t\tunlockParityNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t} else {\n\t\tunlockDataNodes = unlockParityNodes = NULL;\n\t}\n\tif (nfaults == 2) {\n\t\treadQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\twriteQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tqNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tif (lu_flag) {\n\t\t\tunlockQNodes = &nodes[i];\n\t\t\ti += numParityNodes;\n\t\t} else {\n\t\t\tunlockQNodes = NULL;\n\t\t}\n\t} else {\n\t\treadQNodes = writeQNodes = qNodes = unlockQNodes = NULL;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/*\n         * Step 3. initialize the nodes\n         */\n\t/* initialize block node (Nil) */\n\tnNodes = numDataNodes + (nfaults * numParityNodes);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize commit node (Cmt) */\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t    NULL, nNodes, (nfaults * numParityNodes), 0, 0, dag_h, \"Cmt\", allocList);\n\n\t/* initialize terminate node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t    NULL, 0, nNodes, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t    rf_GenericWakeupFunc, (nfaults * numParityNodes), 1, 4, 0, dag_h,\n\t\t    \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\t/* physical disk addr desc */\n\t\treadDataNodes[i].params[0].p = pda;\n\t\t/* buffer to hold old data */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr,\n\t\t    dag_h, pda, allocList);\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readDataNodes[i].numSuccedents; j++) {\n\t\t\treadDataNodes[i].propList[j] = NULL;\n\t\t}\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4,\n\t\t    0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\t/* buffer to hold old parity */\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr,\n\t\t    dag_h, pda, allocList);\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readParityNodes[i].numSuccedents; j++) {\n\t\t\treadParityNodes[i].propList[0] = NULL;\n\t\t}\n\t}\n\n\t/* initialize nodes which read old Q (Roq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\trf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc,\n\t\t\t    rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Roq\", allocList);\n\t\t\treadQNodes[i].params[0].p = pda;\n\t\t\t/* buffer to hold old Q */\n\t\t\treadQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda,\n\t\t\t    allocList);\n\t\t\treadQNodes[i].params[2].v = parityStripeID;\n\t\t\treadQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    lu_flag, 0, which_ru);\n\t\t\tpda = pda->next;\n\t\t\tfor (j = 0; j < readQNodes[i].numSuccedents; j++) {\n\t\t\t\treadQNodes[i].propList[0] = NULL;\n\t\t\t}\n\t\t}\n\t}\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,\n\t\t    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Wnd\", allocList);\n\t\t/* physical disk addr desc */\n\t\twriteDataNodes[i].params[0].p = pda;\n\t\t/* buffer holding new data to be written */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    0, 0, which_ru);\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,\n\t\t\t    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,\n\t\t\t    \"Und\", allocList);\n\t\t\t/* physical disk addr desc */\n\t\t\tunlockDataNodes[i].params[0].p = pda;\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/*\n         * Initialize nodes which compute new parity and Q.\n         */\n\t/*\n         * We use the simple XOR func in the double-XOR case, and when\n         * we're accessing only a portion of one stripe unit. The distinction\n         * between the two is that the regular XOR func assumes that the targbuf\n         * is a full SU in size, and examines the pda associated with the buffer\n         * to decide where within the buffer to XOR the data, whereas\n         * the simple XOR func just XORs the data into the start of the buffer.\n         */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1)\n\t\t&& (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t} else {\n\t\t\tqfunc = NULL;\n\t\t\tqname = NULL;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t} else {\n\t\t\tqfunc = NULL;\n\t\t\tqname = NULL;\n\t\t}\n\t}\n\t/*\n         * Initialize the xor nodes: params are {pda,buf}\n         * from {Rod,Wnd,Rop} nodes, and raidPtr\n         */\n\tif (numParityNodes == 2) {\n\t\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\t/* note: no wakeup func for xor */\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL,\n\t\t\t    1, (numDataNodes + numParityNodes), 7, 1, dag_h, name, allocList);\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\t/* use old parity buf as target buf */\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\n\t\t\tif (nfaults == 2) {\n\t\t\t\t/* note: no wakeup func for qor */\n\t\t\t\trf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,\n\t\t\t\t    (numDataNodes + numParityNodes), 7, 1, dag_h, qname, allocList);\n\t\t\t\tqNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[2] = readQNodes[i].params[0];\n\t\t\t\tqNodes[i].params[3] = readQNodes[i].params[1];\n\t\t\t\tqNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[6].p = raidPtr;\n\t\t\t\t/* use old Q buf as target buf */\n\t\t\t\tqNodes[i].results[0] = readQNodes[i].params[1].p;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, 1,\n\t\t    (numDataNodes + numParityNodes),\n\t\t    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer ptr */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =\t/* pda */\n\t\t\t    writeDataNodes[i].params[0];\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =\t/* buffer ptr */\n\t\t\t    writeDataNodes[i].params[1];\n\t\t}\n\t\t/* xor node needs to get at RAID information */\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t\tif (nfaults == 2) {\n\t\t\trf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, 1,\n\t\t\t    (numDataNodes + numParityNodes),\n\t\t\t    (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h,\n\t\t\t    qname, allocList);\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Rod */\n\t\t\t\tqNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer ptr */\n\t\t\t}\n\t\t\t/* and read old q */\n\t\t\tqNodes[0].params[2 * numDataNodes + 0] =\t/* pda */\n\t\t\t    readQNodes[0].params[0];\n\t\t\tqNodes[0].params[2 * numDataNodes + 1] =\t/* buffer ptr */\n\t\t\t    readQNodes[0].params[1];\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Wnd nodes */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 0] =\t/* pda */\n\t\t\t\t    writeDataNodes[i].params[0];\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 1] =\t/* buffer ptr */\n\t\t\t\t    writeDataNodes[i].params[1];\n\t\t\t}\n\t\t\t/* xor node needs to get at RAID information */\n\t\t\tqNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\n\t\t\tqNodes[0].results[0] = readQNodes[0].params[1].p;\n\t\t}\n\t}\n\n\t/* initialize nodes which write new parity (Wnp) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\trf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,\n\t\t    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t    \"Wnp\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twriteParityNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t * filled in by xor node */\n\t\twriteParityNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\twriteParityNodes[i].params[2].v = parityStripeID;\n\t\twriteParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t    0, 0, which_ru);\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,\n\t\t\t    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,\n\t\t\t    \"Unp\", allocList);\n\t\t\tunlockParityNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new Q (Wnq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc,\n\t\t\t    rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t\t    \"Wnq\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twriteQNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t\t * filled in by xor node */\n\t\t\twriteQNodes[i].params[1].p = qNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\t\twriteQNodes[i].params[2].v = parityStripeID;\n\t\t\twriteQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t    0, 0, which_ru);\n\t\t\tif (lu_flag) {\n\t\t\t\t/* initialize node to unlock the disk queue */\n\t\t\t\trf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc,\n\t\t\t\t    rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h,\n\t\t\t\t    \"Unq\", allocList);\n\t\t\t\tunlockQNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t\t * desc */\n\t\t\t\tunlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t\t    0, lu_flag, which_ru);\n\t\t\t}\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/*\n         * Step 4. connect the nodes.\n         */\n\n\t/* connect header to block node */\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old Q nodes */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tblockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];\n\t\t\tRF_ASSERT(readQNodes[i].numAntecedents == 1);\n\t\t\treadQNodes[i].antecedents[0] = blockNode;\n\t\t\treadQNodes[i].antType[0] = rf_control;\n\t\t}\n\t}\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == (nfaults * numParityNodes));\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to q nodes */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\t\treadDataNodes[i].succedents[numParityNodes + j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\t\tqNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\t\t}\n\t}\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\treadParityNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old q nodes to q nodes */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\treadQNodes[i].succedents[j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];\n\t\t\t\tqNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t\t}\n\t\t}\n\t}\n\t/* connect xor nodes to commit node */\n\tRF_ASSERT(commitNode->numAntecedents == (nfaults * numParityNodes));\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(xorNodes[i].numSuccedents == 1);\n\t\txorNodes[i].succedents[0] = commitNode;\n\t\tcommitNode->antecedents[i] = &xorNodes[i];\n\t\tcommitNode->antType[i] = rf_control;\n\t}\n\n\t/* connect q nodes to commit node */\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(qNodes[i].numSuccedents == 1);\n\t\t\tqNodes[i].succedents[0] = commitNode;\n\t\t\tcommitNode->antecedents[i + numParityNodes] = &qNodes[i];\n\t\t\tcommitNode->antType[i + numParityNodes] = rf_control;\n\t\t}\n\t}\n\t/* connect commit node to write nodes */\n\tRF_ASSERT(commitNode->numSuccedents == (numDataNodes + (nfaults * numParityNodes)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(writeDataNodes[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &writeDataNodes[i];\n\t\twriteDataNodes[i].antecedents[0] = commitNode;\n\t\twriteDataNodes[i].antType[0] = rf_trueData;\n\t}\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(writeParityNodes[i].numAntecedents == 1);\n\t\tcommitNode->succedents[i + numDataNodes] = &writeParityNodes[i];\n\t\twriteParityNodes[i].antecedents[0] = commitNode;\n\t\twriteParityNodes[i].antType[0] = rf_trueData;\n\t}\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(writeQNodes[i].numAntecedents == 1);\n\t\t\tcommitNode->succedents[i + numDataNodes + numParityNodes] = &writeQNodes[i];\n\t\t\twriteQNodes[i].antecedents[0] = commitNode;\n\t\t\twriteQNodes[i].antType[0] = rf_trueData;\n\t\t}\n\t}\n\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tunlockDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to term node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &writeDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new parity nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockParityNodes[i].numAntecedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = &unlockParityNodes[i];\n\t\t\tunlockParityNodes[i].antecedents[0] = &writeParityNodes[i];\n\t\t\tunlockParityNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockParityNodes[i].numSuccedents == 1);\n\t\t\tunlockParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t} else {\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &writeParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\tif (nfaults == 2) {\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tif (lu_flag) {\n\t\t\t\t/* connect write new Q nodes to unlock nodes */\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numAntecedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = &unlockQNodes[i];\n\t\t\t\tunlockQNodes[i].antecedents[0] = &writeQNodes[i];\n\t\t\t\tunlockQNodes[i].antType[0] = rf_control;\n\n\t\t\t\t/* connect unlock nodes to unblock node */\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numSuccedents == 1);\n\t\t\t\tunlockQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t} else {\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t}\n\t\t}\n\t}\n}"
  },
  {
    "function_name": "rf_CommonCreateLargeWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "170-430",
    "snippet": "void \nrf_CommonCreateLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *),\n    int allowBufferRecycle)\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;\n\tRF_DagNode_t *wnqNode, *blockNode, *commitNode, *termNode;\n\tint     nWndNodes, nRodNodes, i, nodeNum, asmNum;\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_StripeNum_t parityStripeID;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_PhysDiskAddr_t *pda;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,\n\t    &which_ru);\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating large-write DAG]\\n\");\n\t}\n\tdag_h->creator = \"LargeWriteDAG\";\n\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h,\n\t    &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0) {\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t),\n\t\t    (RF_DagNode_t *), allocList);\n\t} else {\n\t\trodNodes = NULL;\n\t}\n\n\t/* begin node initialization */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, nRodNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t} else {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t}\n\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL,\n\t    nWndNodes + nfaults, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL,\n\t    0, nWndNodes + nfaults, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t\t\t    \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t\t    0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,\n\t\t    nRodNodes, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,\n\t\t    \"Xr \", allocList);\n\t} else {\n\t\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,\n\t\t    1, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, \"Xr \", allocList);\n\t}\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\t/* xor node needs to get at RAID information */\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\n\n\t/*\n         * Look for an Rod node that reads a complete SU. If none, alloc a buffer\n         * to receive the parity info. Note that we can't use a new data buffer\n         * because it will not have gotten written when the xor occurs.\n         */\n\tif (allowBufferRecycle) {\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif ((!allowBufferRecycle) || (i == nRodNodes)) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1,\n\t\t    rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),\n\t\t    (void *), allocList);\n\t} else {\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\t}\n\n\t/* initialize the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = asmap->parityInfo;\n\twnpNode->params[1].p = xorNode->results[0];\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t/* parityInfo must describe entire parity unit */\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\n\tif (nfaults == 2) {\n\t\t/*\n\t         * We never try to recycle a buffer for the Q calcuation\n\t         * in addition to the parity. This would cause two buffers\n\t         * to get smashed during the P and Q calculation, guaranteeing\n\t         * one would be wrong.\n\t         */\n\t\tRF_CallocAndAdd(xorNode->results[1], 1,\n\t\t    rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),\n\t\t    (void *), allocList);\n\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\twnqNode->params[0].p = asmap->qInfo;\n\t\twnqNode->params[1].p = xorNode->results[1];\n\t\twnqNode->params[2].v = parityStripeID;\n\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t/* parityInfo must describe entire parity unit */\n\t\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\t}\n\t/*\n         * Connect nodes to form graph.\n         */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (nRodNodes > 0) {\n\t\t/* connect the block node to the Rod nodes */\n\t\tRF_ASSERT(blockNode->numSuccedents == nRodNodes);\n\t\tRF_ASSERT(xorNode->numAntecedents == nRodNodes);\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\t\trodNodes[i].antecedents[0] = blockNode;\n\t\t\trodNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect the Rod nodes to the Xor node */\n\t\t\tRF_ASSERT(rodNodes[i].numSuccedents == 1);\n\t\t\trodNodes[i].succedents[0] = xorNode;\n\t\t\txorNode->antecedents[i] = &rodNodes[i];\n\t\t\txorNode->antType[i] = rf_trueData;\n\t\t}\n\t} else {\n\t\t/* connect the block node to the Xor node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(xorNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = xorNode;\n\t\txorNode->antecedents[0] = blockNode;\n\t\txorNode->antType[0] = rf_control;\n\t}\n\n\t/* connect the xor node to the commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect the commit node to the write nodes */\n\tRF_ASSERT(commitNode->numSuccedents == nWndNodes + nfaults);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = commitNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\tcommitNode->succedents[nWndNodes] = wnpNode;\n\twnpNode->antecedents[0] = commitNode;\n\twnpNode->antType[0] = rf_trueData;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\tcommitNode->succedents[nWndNodes + 1] = wnqNode;\n\t\twnqNode->antecedents[0] = commitNode;\n\t\twnqNode->antType[0] = rf_trueData;\n\t}\n\t/* connect the write nodes to the term node */\n\tRF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = termNode;\n\t\ttermNode->antecedents[i] = &wndNodes[i];\n\t\ttermNode->antType[i] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = termNode;\n\ttermNode->antecedents[nWndNodes] = wnpNode;\n\ttermNode->antType[nWndNodes] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\ttermNode->antType[nWndNodes + 1] = rf_control;\n\t}\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnqNode->numSuccedents == 1"
          ],
          "line": 425
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnpNode->numSuccedents == 1"
          ],
          "line": 420
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes->numSuccedents == 1"
          ],
          "line": 415
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numSuccedents == 0"
          ],
          "line": 413
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "termNode->numAntecedents == nWndNodes + nfaults"
          ],
          "line": 412
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnqNode->numAntecedents == 1"
          ],
          "line": 406
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wnpNode->numAntecedents == 1"
          ],
          "line": 401
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "wndNodes->numAntecedents == 1"
          ],
          "line": 396
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numSuccedents == nWndNodes + nfaults"
          ],
          "line": 394
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "commitNode->numAntecedents == 1"
          ],
          "line": 388
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numSuccedents == 1"
          ],
          "line": 387
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numAntecedents == 1"
          ],
          "line": 380
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == 1"
          ],
          "line": 379
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rodNodes[i].numSuccedents == 1"
          ],
          "line": 372
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "rodNodes[i].numAntecedents == 1"
          ],
          "line": 366
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "xorNode->numAntecedents == nRodNodes"
          ],
          "line": 364
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numSuccedents == nRodNodes"
          ],
          "line": 363
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "blockNode->numAntecedents == 0"
          ],
          "line": 358
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->parityInfo->next == NULL"
          ],
          "line": 351
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 349
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_InitNode",
          "args": [
            "wnqNode",
            "rf_wait",
            "RF_FALSE",
            "rf_DiskWriteFunc",
            "rf_DiskWriteUndoFunc",
            "rf_GenericWakeupFunc",
            "1",
            "1",
            "4",
            "0",
            "dag_h",
            "\"Wnq\"",
            "allocList"
          ],
          "line": 344
        },
        "resolved": true,
        "details": {
          "function_name": "rf_InitNode",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "74-143",
          "snippet": "void \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_RecurPrintDAG(RF_DagNode_t *, int, int);",
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);",
            "static void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_RecurPrintDAG(RF_DagNode_t *, int, int);\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic int \nrf_ValidateBranch(RF_DagNode_t *, int *, int *,\n    RF_DagNode_t **, int);\nstatic void rf_ValidateBranchVisitedBits(RF_DagNode_t *, int, int);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_InitNode(\n    RF_DagNode_t * node,\n    RF_NodeStatus_t initstatus,\n    int commit,\n    int (*doFunc) (RF_DagNode_t * node),\n    int (*undoFunc) (RF_DagNode_t * node),\n    int (*wakeFunc) (RF_DagNode_t * node, int status),\n    int nSucc,\n    int nAnte,\n    int nParam,\n    int nResult,\n    RF_DagHeader_t * hdr,\n    char *name,\n    RF_AllocListElem_t * alist)\n{\n\tvoid  **ptrs;\n\tint     nptrs;\n\n\tif (nAnte > RF_MAX_ANTECEDENTS)\n\t\tRF_PANIC();\n\tnode->status = initstatus;\n\tnode->commitNode = commit;\n\tnode->doFunc = doFunc;\n\tnode->undoFunc = undoFunc;\n\tnode->wakeFunc = wakeFunc;\n\tnode->numParams = nParam;\n\tnode->numResults = nResult;\n\tnode->numAntecedents = nAnte;\n\tnode->numAntDone = 0;\n\tnode->next = NULL;\n\tnode->numSuccedents = nSucc;\n\tnode->name = name;\n\tnode->dagHdr = hdr;\n\tnode->visited = 0;\n\n\t/* allocate all the pointers with one call to malloc */\n\tnptrs = nSucc + nAnte + nResult + nSucc;\n\n\tif (nptrs <= RF_DAG_PTRCACHESIZE) {\n\t\t/*\n\t         * The dag_ptrs field of the node is basically some scribble\n\t         * space to be used here. We could get rid of it, and always\n\t         * allocate the range of pointers, but that's expensive. So,\n\t         * we pick a \"common case\" size for the pointer cache. Hopefully,\n\t         * we'll find that:\n\t         * (1) Generally, nptrs doesn't exceed RF_DAG_PTRCACHESIZE by\n\t         *     only a little bit (least efficient case)\n\t         * (2) Generally, ntprs isn't a lot less than RF_DAG_PTRCACHESIZE\n\t         *     (wasted memory)\n\t         */\n\t\tptrs = (void **) node->dag_ptrs;\n\t} else {\n\t\tRF_CallocAndAdd(ptrs, nptrs, sizeof(void *), (void **), alist);\n\t}\n\tnode->succedents = (nSucc) ? (RF_DagNode_t **) ptrs : NULL;\n\tnode->antecedents = (nAnte) ? (RF_DagNode_t **) (ptrs + nSucc) : NULL;\n\tnode->results = (nResult) ? (void **) (ptrs + nSucc + nAnte) : NULL;\n\tnode->propList = (nSucc) ? (RF_PropHeader_t **) (ptrs + nSucc + nAnte + nResult) : NULL;\n\n\tif (nParam) {\n\t\tif (nParam <= RF_DAG_PARAMCACHESIZE) {\n\t\t\tnode->params = (RF_DagParam_t *) node->dag_params;\n\t\t} else {\n\t\t\tRF_CallocAndAdd(node->params, nParam, sizeof(RF_DagParam_t), (RF_DagParam_t *), alist);\n\t\t}\n\t} else {\n\t\tnode->params = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "xorNode->results[1]",
            "1",
            "rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit)",
            "(void *), allocList"
          ],
          "line": 341
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "raidPtr->Layout.sectorsPerStripeUnit"
          ],
          "line": 342
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "asmap->parityInfo->next == NULL"
          ],
          "line": 332
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 330
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "xorNode->results[0]",
            "1",
            "rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit)",
            "(void *), allocList"
          ],
          "line": 317
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToByte",
          "args": [
            "raidPtr",
            "raidPtr->Layout.sectorsPerStripeUnit"
          ],
          "line": 318
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 280
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "pda != NULL"
          ],
          "line": 276
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_ASSERT",
          "args": [
            "nodeNum == nRodNodes"
          ],
          "line": 269
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CREATE_PARAM3",
          "args": [
            "RF_IO_NORMAL_PRIORITY",
            "0",
            "0",
            "which_ru"
          ],
          "line": 262
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "rodNodes",
            "nRodNodes",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 231
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rf_MapUnaccessedPortionOfStripe",
          "args": [
            "raidPtr",
            "layoutPtr",
            "asmap",
            "dag_h",
            "new_asm_h",
            "&nRodNodes",
            "&sosBuffer",
            "&eosBuffer",
            "allocList"
          ],
          "line": 228
        },
        "resolved": true,
        "details": {
          "function_name": "rf_MapUnaccessedPortionOfStripe",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagutils.c",
          "lines": "799-851",
          "snippet": "void \nrf_MapUnaccessedPortionOfStripe(\n    RF_Raid_t * raidPtr,\n    RF_RaidLayout_t * layoutPtr,/* in: layout information */\n    RF_AccessStripeMap_t * asmap,\t/* in: access stripe map */\n    RF_DagHeader_t * dag_h,\t/* in: header of the dag to create */\n    RF_AccessStripeMapHeader_t ** new_asm_h,\t/* in: ptr to array of 2\n\t\t\t\t\t\t * headers, to be filled in */\n    int *nRodNodes,\t\t/* out: num nodes to be generated to read\n\t\t\t\t * unaccessed data */\n    char **sosBuffer,\t\t/* out: pointers to newly allocated buffer */\n    char **eosBuffer,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidAddr_t sosRaidAddress, eosRaidAddress;\n\tRF_SectorNum_t sosNumSector, eosNumSector;\n\n\tRF_ASSERT(asmap->numStripeUnitsAccessed > (layoutPtr->numDataCol / 2));\n\t/* generate an access map for the region of the array from start of\n\t * stripe to start of access */\n\tnew_asm_h[0] = new_asm_h[1] = NULL;\n\t*nRodNodes = 0;\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->raidAddress)) {\n\t\tsosRaidAddress = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\t\tsosNumSector = asmap->raidAddress - sosRaidAddress;\n\t\tRF_MallocAndAdd(*sosBuffer, rf_RaidAddressToByte(raidPtr, sosNumSector), (char *), allocList);\n\t\tnew_asm_h[0] = rf_MapAccess(raidPtr, sosRaidAddress, sosNumSector, *sosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\t*nRodNodes += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[0]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[0]->stripeMap);\n\t}\n\t/* generate an access map for the region of the array from end of\n\t * access to end of stripe */\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->endRaidAddress)) {\n\t\teosRaidAddress = asmap->endRaidAddress;\n\t\teosNumSector = rf_RaidAddressOfNextStripeBoundary(layoutPtr, eosRaidAddress) - eosRaidAddress;\n\t\tRF_MallocAndAdd(*eosBuffer, rf_RaidAddressToByte(raidPtr, eosNumSector), (char *), allocList);\n\t\tnew_asm_h[1] = rf_MapAccess(raidPtr, eosRaidAddress, eosNumSector, *eosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\t*nRodNodes += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[1]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[1]->stripeMap);\n\t}\n}",
          "includes": [
            "#include \"rf_shutdown.h\"",
            "#include \"rf_map.h\"",
            "#include \"rf_freelist.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\"",
            "#include \"rf_archs.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void rf_PrintDAG(RF_DagHeader_t *);",
            "static void rf_ValidateVisitedBits(RF_DagHeader_t *);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_shutdown.h\"\n#include \"rf_map.h\"\n#include \"rf_freelist.h\"\n#include \"rf_general.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n#include \"rf_archs.h\"\n\nstatic void rf_PrintDAG(RF_DagHeader_t *);\nstatic void rf_ValidateVisitedBits(RF_DagHeader_t *);\n\nvoid \nrf_MapUnaccessedPortionOfStripe(\n    RF_Raid_t * raidPtr,\n    RF_RaidLayout_t * layoutPtr,/* in: layout information */\n    RF_AccessStripeMap_t * asmap,\t/* in: access stripe map */\n    RF_DagHeader_t * dag_h,\t/* in: header of the dag to create */\n    RF_AccessStripeMapHeader_t ** new_asm_h,\t/* in: ptr to array of 2\n\t\t\t\t\t\t * headers, to be filled in */\n    int *nRodNodes,\t\t/* out: num nodes to be generated to read\n\t\t\t\t * unaccessed data */\n    char **sosBuffer,\t\t/* out: pointers to newly allocated buffer */\n    char **eosBuffer,\n    RF_AllocListElem_t * allocList)\n{\n\tRF_RaidAddr_t sosRaidAddress, eosRaidAddress;\n\tRF_SectorNum_t sosNumSector, eosNumSector;\n\n\tRF_ASSERT(asmap->numStripeUnitsAccessed > (layoutPtr->numDataCol / 2));\n\t/* generate an access map for the region of the array from start of\n\t * stripe to start of access */\n\tnew_asm_h[0] = new_asm_h[1] = NULL;\n\t*nRodNodes = 0;\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->raidAddress)) {\n\t\tsosRaidAddress = rf_RaidAddressOfPrevStripeBoundary(layoutPtr, asmap->raidAddress);\n\t\tsosNumSector = asmap->raidAddress - sosRaidAddress;\n\t\tRF_MallocAndAdd(*sosBuffer, rf_RaidAddressToByte(raidPtr, sosNumSector), (char *), allocList);\n\t\tnew_asm_h[0] = rf_MapAccess(raidPtr, sosRaidAddress, sosNumSector, *sosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[0]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[0];\n\t\t*nRodNodes += new_asm_h[0]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[0]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[0]->stripeMap);\n\t}\n\t/* generate an access map for the region of the array from end of\n\t * access to end of stripe */\n\tif (!rf_RaidAddressStripeAligned(layoutPtr, asmap->endRaidAddress)) {\n\t\teosRaidAddress = asmap->endRaidAddress;\n\t\teosNumSector = rf_RaidAddressOfNextStripeBoundary(layoutPtr, eosRaidAddress) - eosRaidAddress;\n\t\tRF_MallocAndAdd(*eosBuffer, rf_RaidAddressToByte(raidPtr, eosNumSector), (char *), allocList);\n\t\tnew_asm_h[1] = rf_MapAccess(raidPtr, eosRaidAddress, eosNumSector, *eosBuffer, RF_DONT_REMAP);\n\t\tnew_asm_h[1]->next = dag_h->asmList;\n\t\tdag_h->asmList = new_asm_h[1];\n\t\t*nRodNodes += new_asm_h[1]->stripeMap->numStripeUnitsAccessed;\n\n\t\tRF_ASSERT(new_asm_h[1]->stripeMap->next == NULL);\n\t\t/* we're totally within one stripe here */\n\t\tif (asmap->flags & RF_ASM_REDIR_LARGE_WRITE)\n\t\t\trf_redirect_asm(raidPtr, new_asm_h[1]->stripeMap);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "RF_CallocAndAdd",
          "args": [
            "nodes",
            "nWndNodes + 4 + nfaults",
            "sizeof(RF_DagNode_t)",
            "(RF_DagNode_t *), allocList"
          ],
          "line": 207
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "printf",
          "args": [
            "\"[Creating large-write DAG]\\n\""
          ],
          "line": 197
        },
        "resolved": true,
        "details": {
          "function_name": "rf_debug_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_debugprint.c",
          "lines": "82-108",
          "snippet": "void \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}",
          "includes": [
            "#include <sys/param.h>",
            "#include \"rf_options.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_debugprint.h\"",
            "#include \"rf_threadstuff.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [
            "#define BUFMASK  (BUFSIZE-1)"
          ],
          "globals_used": [
            "static struct RF_Entry_s rf_debugprint_buf[BUFSIZE];",
            "static int rf_debugprint_index = 0;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <sys/param.h>\n#include \"rf_options.h\"\n#include \"rf_general.h\"\n#include \"rf_debugprint.h\"\n#include \"rf_threadstuff.h\"\n#include \"rf_types.h\"\n\n#define BUFMASK  (BUFSIZE-1)\n\nstatic struct RF_Entry_s rf_debugprint_buf[BUFSIZE];\nstatic int rf_debugprint_index = 0;\n\nvoid \nrf_debug_printf(s, a1, a2, a3, a4, a5, a6, a7, a8)\n\tchar   *s;\n\tvoid   *a1, *a2, *a3, *a4, *a5, *a6, *a7, *a8;\n{\n\tint     idx;\n\n\tif (rf_debugPrintUseBuffer) {\n\n\t\tRF_LOCK_MUTEX(rf_debug_print_mutex);\n\t\tidx = rf_debugprint_index;\n\t\trf_debugprint_index = (rf_debugprint_index + 1) & BUFMASK;\n\t\tRF_UNLOCK_MUTEX(rf_debug_print_mutex);\n\n\t\trf_debugprint_buf[idx].cstring = s;\n\t\trf_debugprint_buf[idx].a1 = a1;\n\t\trf_debugprint_buf[idx].a2 = a2;\n\t\trf_debugprint_buf[idx].a3 = a3;\n\t\trf_debugprint_buf[idx].a4 = a4;\n\t\trf_debugprint_buf[idx].a5 = a5;\n\t\trf_debugprint_buf[idx].a6 = a6;\n\t\trf_debugprint_buf[idx].a7 = a7;\n\t\trf_debugprint_buf[idx].a8 = a8;\n\t} else {\n\t\tprintf(s, a1, a2, a3, a4, a5, a6, a7, a8);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rf_RaidAddressToParityStripeID",
          "args": [
            "layoutPtr",
            "asmap->raidAddress",
            "&which_ru"
          ],
          "line": 193
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CommonCreateLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *),\n    int allowBufferRecycle)\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;\n\tRF_DagNode_t *wnqNode, *blockNode, *commitNode, *termNode;\n\tint     nWndNodes, nRodNodes, i, nodeNum, asmNum;\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_StripeNum_t parityStripeID;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_PhysDiskAddr_t *pda;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(layoutPtr, asmap->raidAddress,\n\t    &which_ru);\n\n\tif (rf_dagDebug) {\n\t\tprintf(\"[Creating large-write DAG]\\n\");\n\t}\n\tdag_h->creator = \"LargeWriteDAG\";\n\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h,\n\t    &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0) {\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t),\n\t\t    (RF_DagNode_t *), allocList);\n\t} else {\n\t\trodNodes = NULL;\n\t}\n\n\t/* begin node initialization */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, nRodNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t} else {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t}\n\n\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL,\n\t    nWndNodes + nfaults, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL,\n\t    0, nWndNodes + nfaults, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc,\n\t\t\t\t    rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h,\n\t\t\t\t    \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY,\n\t\t\t\t    0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,\n\t\t    nRodNodes, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h,\n\t\t    \"Xr \", allocList);\n\t} else {\n\t\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1,\n\t\t    1, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, \"Xr \", allocList);\n\t}\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\t/* xor node needs to get at RAID information */\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\n\n\t/*\n         * Look for an Rod node that reads a complete SU. If none, alloc a buffer\n         * to receive the parity info. Note that we can't use a new data buffer\n         * because it will not have gotten written when the xor occurs.\n         */\n\tif (allowBufferRecycle) {\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif ((!allowBufferRecycle) || (i == nRodNodes)) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1,\n\t\t    rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),\n\t\t    (void *), allocList);\n\t} else {\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\t}\n\n\t/* initialize the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = asmap->parityInfo;\n\twnpNode->params[1].p = xorNode->results[0];\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t/* parityInfo must describe entire parity unit */\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\n\tif (nfaults == 2) {\n\t\t/*\n\t         * We never try to recycle a buffer for the Q calcuation\n\t         * in addition to the parity. This would cause two buffers\n\t         * to get smashed during the P and Q calculation, guaranteeing\n\t         * one would be wrong.\n\t         */\n\t\tRF_CallocAndAdd(xorNode->results[1], 1,\n\t\t    rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit),\n\t\t    (void *), allocList);\n\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc,\n\t\t    rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\twnqNode->params[0].p = asmap->qInfo;\n\t\twnqNode->params[1].p = xorNode->results[1];\n\t\twnqNode->params[2].v = parityStripeID;\n\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t/* parityInfo must describe entire parity unit */\n\t\tRF_ASSERT(asmap->parityInfo->next == NULL);\n\t}\n\t/*\n         * Connect nodes to form graph.\n         */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (nRodNodes > 0) {\n\t\t/* connect the block node to the Rod nodes */\n\t\tRF_ASSERT(blockNode->numSuccedents == nRodNodes);\n\t\tRF_ASSERT(xorNode->numAntecedents == nRodNodes);\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\t\trodNodes[i].antecedents[0] = blockNode;\n\t\t\trodNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect the Rod nodes to the Xor node */\n\t\t\tRF_ASSERT(rodNodes[i].numSuccedents == 1);\n\t\t\trodNodes[i].succedents[0] = xorNode;\n\t\t\txorNode->antecedents[i] = &rodNodes[i];\n\t\t\txorNode->antType[i] = rf_trueData;\n\t\t}\n\t} else {\n\t\t/* connect the block node to the Xor node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(xorNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = xorNode;\n\t\txorNode->antecedents[0] = blockNode;\n\t\txorNode->antType[0] = rf_control;\n\t}\n\n\t/* connect the xor node to the commit node */\n\tRF_ASSERT(xorNode->numSuccedents == 1);\n\tRF_ASSERT(commitNode->numAntecedents == 1);\n\txorNode->succedents[0] = commitNode;\n\tcommitNode->antecedents[0] = xorNode;\n\tcommitNode->antType[0] = rf_control;\n\n\t/* connect the commit node to the write nodes */\n\tRF_ASSERT(commitNode->numSuccedents == nWndNodes + nfaults);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tcommitNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = commitNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\tcommitNode->succedents[nWndNodes] = wnpNode;\n\twnpNode->antecedents[0] = commitNode;\n\twnpNode->antType[0] = rf_trueData;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\tcommitNode->succedents[nWndNodes + 1] = wnqNode;\n\t\twnqNode->antecedents[0] = commitNode;\n\t\twnqNode->antType[0] = rf_trueData;\n\t}\n\t/* connect the write nodes to the term node */\n\tRF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = termNode;\n\t\ttermNode->antecedents[i] = &wndNodes[i];\n\t\ttermNode->antType[i] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = termNode;\n\ttermNode->antecedents[nWndNodes] = wnpNode;\n\ttermNode->antType[nWndNodes] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\ttermNode->antType[nWndNodes + 1] = rf_control;\n\t}\n}"
  },
  {
    "function_name": "rf_CreateLargeWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "118-130",
    "snippet": "void \nrf_CreateLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\t/* \"normal\" rollaway */\n\trf_CommonCreateLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    1, rf_RegularXorFunc, RF_TRUE);\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CommonCreateLargeWriteDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList",
            "1",
            "rf_RegularXorFunc",
            "RF_TRUE"
          ],
          "line": 128
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CommonCreateLargeWriteDAGFwd",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
          "lines": "1215-1447",
          "snippet": "void \nrf_CommonCreateLargeWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *),\n    int allowBufferRecycle)\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;\n\tRF_DagNode_t *wnqNode, *blockNode, *syncNode, *termNode;\n\tint     nWndNodes, nRodNodes, i, nodeNum, asmNum;\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_StripeNum_t parityStripeID;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_PhysDiskAddr_t *pda;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating large-write DAG]\\n\");\n\tdag_h->creator = \"LargeWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0) {\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\t} else {\n\t\trodNodes = NULL;\n\t}\n\n\t/* begin node initialization */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0, dag_h, \"Nil\", allocList);\n\t} else {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h, \"Nil\", allocList);\n\t}\n\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tif (allowBufferRecycle) {\n\t\tfor (i = 0; i < nRodNodes; i++)\n\t\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\t\tbreak;\n\t}\n\tif ((!allowBufferRecycle) || (i == nRodNodes)) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\n\t/* initialize the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = asmap->parityInfo;\n\twnpNode->params[1].p = xorNode->results[0];\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\tif (nfaults == 2) {\n\t\t/* we never try to recycle a buffer for the Q calcuation in\n\t\t * addition to the parity. This would cause two buffers to get\n\t\t * smashed during the P and Q calculation, guaranteeing one\n\t\t * would be wrong. */\n\t\tRF_CallocAndAdd(xorNode->results[1], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\twnqNode->params[0].p = asmap->qInfo;\n\t\twnqNode->params[1].p = xorNode->results[1];\n\t\twnqNode->params[2].v = parityStripeID;\n\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t\t * parity unit */\n\t}\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (nRodNodes > 0) {\n\t\t/* connect the block node to the Rod nodes */\n\t\tRF_ASSERT(blockNode->numSuccedents == nRodNodes);\n\t\tRF_ASSERT(syncNode->numAntecedents == nRodNodes);\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\t\trodNodes[i].antecedents[0] = blockNode;\n\t\t\trodNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect the Rod nodes to the Nil node */\n\t\t\tRF_ASSERT(rodNodes[i].numSuccedents == 1);\n\t\t\trodNodes[i].succedents[0] = syncNode;\n\t\t\tsyncNode->antecedents[i] = &rodNodes[i];\n\t\t\tsyncNode->antType[i] = rf_trueData;\n\t\t}\n\t} else {\n\t\t/* connect the block node to the Nil node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(syncNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = syncNode;\n\t\tsyncNode->antecedents[0] = blockNode;\n\t\tsyncNode->antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Wnd nodes */\n\tRF_ASSERT(syncNode->numSuccedents == (1 + nWndNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Xor node */\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[nWndNodes] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_control;\n\n\t/* connect the xor node to the write parity node */\n\tRF_ASSERT(xorNode->numSuccedents == nfaults);\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\txorNode->succedents[0] = wnpNode;\n\twnpNode->antecedents[0] = xorNode;\n\twnpNode->antType[0] = rf_trueData;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\txorNode->succedents[1] = wnqNode;\n\t\twnqNode->antecedents[0] = xorNode;\n\t\twnqNode->antType[0] = rf_trueData;\n\t}\n\t/* connect the write nodes to the term node */\n\tRF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = termNode;\n\t\ttermNode->antecedents[i] = &wndNodes[i];\n\t\ttermNode->antType[i] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = termNode;\n\ttermNode->antecedents[nWndNodes] = wnpNode;\n\ttermNode->antType[nWndNodes] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\ttermNode->antType[nWndNodes + 1] = rf_control;\n\t}\n}",
          "includes": [
            "#include \"rf_dagffwr.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_dagffrd.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CommonCreateLargeWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    int nfaults,\n    int (*redFunc) (RF_DagNode_t *),\n    int allowBufferRecycle)\n{\n\tRF_DagNode_t *nodes, *wndNodes, *rodNodes, *xorNode, *wnpNode;\n\tRF_DagNode_t *wnqNode, *blockNode, *syncNode, *termNode;\n\tint     nWndNodes, nRodNodes, i, nodeNum, asmNum;\n\tRF_AccessStripeMapHeader_t *new_asm_h[2];\n\tRF_StripeNum_t parityStripeID;\n\tchar   *sosBuffer, *eosBuffer;\n\tRF_ReconUnitNum_t which_ru;\n\tRF_RaidLayout_t *layoutPtr;\n\tRF_PhysDiskAddr_t *pda;\n\n\tlayoutPtr = &(raidPtr->Layout);\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating large-write DAG]\\n\");\n\tdag_h->creator = \"LargeWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/* alloc the nodes: Wnd, xor, commit, block, term, and  Wnp */\n\tnWndNodes = asmap->numStripeUnitsAccessed;\n\tRF_CallocAndAdd(nodes, nWndNodes + 4 + nfaults, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\twndNodes = &nodes[i];\n\ti += nWndNodes;\n\txorNode = &nodes[i];\n\ti += 1;\n\twnpNode = &nodes[i];\n\ti += 1;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tsyncNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (nfaults == 2) {\n\t\twnqNode = &nodes[i];\n\t\ti += 1;\n\t} else {\n\t\twnqNode = NULL;\n\t}\n\trf_MapUnaccessedPortionOfStripe(raidPtr, layoutPtr, asmap, dag_h, new_asm_h, &nRodNodes, &sosBuffer, &eosBuffer, allocList);\n\tif (nRodNodes > 0) {\n\t\tRF_CallocAndAdd(rodNodes, nRodNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\t} else {\n\t\trodNodes = NULL;\n\t}\n\n\t/* begin node initialization */\n\tif (nRodNodes > 0) {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nRodNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, nRodNodes, 0, 0, dag_h, \"Nil\", allocList);\n\t} else {\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(syncNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nWndNodes + 1, 1, 0, 0, dag_h, \"Nil\", allocList);\n\t}\n\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nWndNodes + nfaults, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize the Rod nodes */\n\tfor (nodeNum = asmNum = 0; asmNum < 2; asmNum++) {\n\t\tif (new_asm_h[asmNum]) {\n\t\t\tpda = new_asm_h[asmNum]->stripeMap->physInfo;\n\t\t\twhile (pda) {\n\t\t\t\trf_InitNode(&rodNodes[nodeNum], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\t\t\trodNodes[nodeNum].params[0].p = pda;\n\t\t\t\trodNodes[nodeNum].params[1].p = pda->bufPtr;\n\t\t\t\trodNodes[nodeNum].params[2].v = parityStripeID;\n\t\t\t\trodNodes[nodeNum].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\t\t\tnodeNum++;\n\t\t\t\tpda = pda->next;\n\t\t\t}\n\t\t}\n\t}\n\tRF_ASSERT(nodeNum == nRodNodes);\n\n\t/* initialize the wnd nodes */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\trf_InitNode(&wndNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twndNodes[i].params[0].p = pda;\n\t\twndNodes[i].params[1].p = pda->bufPtr;\n\t\twndNodes[i].params[2].v = parityStripeID;\n\t\twndNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize the redundancy node */\n\trf_InitNode(xorNode, rf_wait, RF_FALSE, redFunc, rf_NullNodeUndoFunc, NULL, 1, nfaults, 2 * (nWndNodes + nRodNodes) + 1, nfaults, dag_h, \"Xr \", allocList);\n\txorNode->flags |= RF_DAGNODE_FLAG_YIELD;\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\txorNode->params[2 * i + 0] = wndNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * i + 1] = wndNodes[i].params[1];\t/* buf ptr */\n\t}\n\tfor (i = 0; i < nRodNodes; i++) {\n\t\txorNode->params[2 * (nWndNodes + i) + 0] = rodNodes[i].params[0];\t/* pda */\n\t\txorNode->params[2 * (nWndNodes + i) + 1] = rodNodes[i].params[1];\t/* buf ptr */\n\t}\n\txorNode->params[2 * (nWndNodes + nRodNodes)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t * at RAID information */\n\n\t/* look for an Rod node that reads a complete SU.  If none, alloc a\n\t * buffer to receive the parity info. Note that we can't use a new\n\t * data buffer because it will not have gotten written when the xor\n\t * occurs. */\n\tif (allowBufferRecycle) {\n\t\tfor (i = 0; i < nRodNodes; i++)\n\t\t\tif (((RF_PhysDiskAddr_t *) rodNodes[i].params[0].p)->numSector == raidPtr->Layout.sectorsPerStripeUnit)\n\t\t\t\tbreak;\n\t}\n\tif ((!allowBufferRecycle) || (i == nRodNodes)) {\n\t\tRF_CallocAndAdd(xorNode->results[0], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t} else\n\t\txorNode->results[0] = rodNodes[i].params[1].p;\n\n\t/* initialize the Wnp node */\n\trf_InitNode(wnpNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnp\", allocList);\n\twnpNode->params[0].p = asmap->parityInfo;\n\twnpNode->params[1].p = xorNode->results[0];\n\twnpNode->params[2].v = parityStripeID;\n\twnpNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t * parity unit */\n\n\tif (nfaults == 2) {\n\t\t/* we never try to recycle a buffer for the Q calcuation in\n\t\t * addition to the parity. This would cause two buffers to get\n\t\t * smashed during the P and Q calculation, guaranteeing one\n\t\t * would be wrong. */\n\t\tRF_CallocAndAdd(xorNode->results[1], 1, rf_RaidAddressToByte(raidPtr, raidPtr->Layout.sectorsPerStripeUnit), (void *), allocList);\n\t\trf_InitNode(wnqNode, rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnq\", allocList);\n\t\twnqNode->params[0].p = asmap->qInfo;\n\t\twnqNode->params[1].p = xorNode->results[1];\n\t\twnqNode->params[2].v = parityStripeID;\n\t\twnqNode->params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\t\tRF_ASSERT(asmap->parityInfo->next == NULL);\t/* parityInfo must\n\t\t\t\t\t\t\t\t * describe entire\n\t\t\t\t\t\t\t\t * parity unit */\n\t}\n\t/* connect nodes to form graph */\n\n\t/* connect dag header to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (nRodNodes > 0) {\n\t\t/* connect the block node to the Rod nodes */\n\t\tRF_ASSERT(blockNode->numSuccedents == nRodNodes);\n\t\tRF_ASSERT(syncNode->numAntecedents == nRodNodes);\n\t\tfor (i = 0; i < nRodNodes; i++) {\n\t\t\tRF_ASSERT(rodNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &rodNodes[i];\n\t\t\trodNodes[i].antecedents[0] = blockNode;\n\t\t\trodNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect the Rod nodes to the Nil node */\n\t\t\tRF_ASSERT(rodNodes[i].numSuccedents == 1);\n\t\t\trodNodes[i].succedents[0] = syncNode;\n\t\t\tsyncNode->antecedents[i] = &rodNodes[i];\n\t\t\tsyncNode->antType[i] = rf_trueData;\n\t\t}\n\t} else {\n\t\t/* connect the block node to the Nil node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(syncNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = syncNode;\n\t\tsyncNode->antecedents[0] = blockNode;\n\t\tsyncNode->antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Wnd nodes */\n\tRF_ASSERT(syncNode->numSuccedents == (1 + nWndNodes));\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numAntecedents == 1);\n\t\tsyncNode->succedents[i] = &wndNodes[i];\n\t\twndNodes[i].antecedents[0] = syncNode;\n\t\twndNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect the sync node to the Xor node */\n\tRF_ASSERT(xorNode->numAntecedents == 1);\n\tsyncNode->succedents[nWndNodes] = xorNode;\n\txorNode->antecedents[0] = syncNode;\n\txorNode->antType[0] = rf_control;\n\n\t/* connect the xor node to the write parity node */\n\tRF_ASSERT(xorNode->numSuccedents == nfaults);\n\tRF_ASSERT(wnpNode->numAntecedents == 1);\n\txorNode->succedents[0] = wnpNode;\n\twnpNode->antecedents[0] = xorNode;\n\twnpNode->antType[0] = rf_trueData;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numAntecedents == 1);\n\t\txorNode->succedents[1] = wnqNode;\n\t\twnqNode->antecedents[0] = xorNode;\n\t\twnqNode->antType[0] = rf_trueData;\n\t}\n\t/* connect the write nodes to the term node */\n\tRF_ASSERT(termNode->numAntecedents == nWndNodes + nfaults);\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < nWndNodes; i++) {\n\t\tRF_ASSERT(wndNodes->numSuccedents == 1);\n\t\twndNodes[i].succedents[0] = termNode;\n\t\ttermNode->antecedents[i] = &wndNodes[i];\n\t\ttermNode->antType[i] = rf_control;\n\t}\n\tRF_ASSERT(wnpNode->numSuccedents == 1);\n\twnpNode->succedents[0] = termNode;\n\ttermNode->antecedents[nWndNodes] = wnpNode;\n\ttermNode->antType[nWndNodes] = rf_control;\n\tif (nfaults == 2) {\n\t\tRF_ASSERT(wnqNode->numSuccedents == 1);\n\t\twnqNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[nWndNodes + 1] = wnqNode;\n\t\ttermNode->antType[nWndNodes + 1] = rf_control;\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateLargeWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\t/* \"normal\" rollaway */\n\trf_CommonCreateLargeWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    1, rf_RegularXorFunc, RF_TRUE);\n}"
  },
  {
    "function_name": "rf_CreateSmallWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "104-116",
    "snippet": "void \nrf_CreateSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\t/* \"normal\" rollaway */\n\trf_CommonCreateSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    &rf_xorFuncs, NULL);\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CommonCreateSmallWriteDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList",
            "&rf_xorFuncs",
            "NULL"
          ],
          "line": 114
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CommonCreateSmallWriteDAGFwd",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
          "lines": "1483-1977",
          "snippet": "void \nrf_CommonCreateSmallWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;\n\tRF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;\n\tRF_DagNode_t *xorNodes, *qNodes, *blockNode, *nodes;\n\tRF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;\n\tint     i, j, nNodes, totalNumNodes, lu_flag;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     (*qfunc) (RF_DagNode_t *);\n\tint     numDataNodes, numParityNodes;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *name, *qname;\n\tlong    nfaults;\n\n\tnfaults = qfuncs ? 2 : 1;\n\tlu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tpda = asmap->physInfo;\n\tnumDataNodes = asmap->numStripeUnitsAccessed;\n\tnumParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tdag_h->creator = \"SmallWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\tqfunc = NULL;\n\tqname = NULL;\n\n\t/* DAG creation occurs in four steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node (nfaults * nparity) a read\n\t * and write for each parity unit a block node a terminate node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes) + (nfaults * 2 * numParityNodes) + 2;\n\tif (lu_flag)\n\t\ttotalNumNodes += (numDataNodes + (nfaults * numParityNodes));\n\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\twriteParityNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t\tunlockParityNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t} else {\n\t\tunlockDataNodes = unlockParityNodes = NULL;\n\t}\n\tif (nfaults == 2) {\n\t\treadQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\twriteQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tqNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tif (lu_flag) {\n\t\t\tunlockQNodes = &nodes[i];\n\t\t\ti += numParityNodes;\n\t\t} else {\n\t\t\tunlockQNodes = NULL;\n\t\t}\n\t} else {\n\t\treadQNodes = writeQNodes = qNodes = unlockQNodes = NULL;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\tnNodes = numDataNodes + (nfaults * numParityNodes);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminate node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readDataNodes[i].numSuccedents; j++)\n\t\t\treadDataNodes[i].propList[j] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tfor (j = 0; j < readParityNodes[i].numSuccedents; j++)\n\t\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which read old Q (Roq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\trf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Roq\", allocList);\n\t\t\treadQNodes[i].params[0].p = pda;\n\t\t\treadQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old Q */\n\t\t\treadQNodes[i].params[2].v = parityStripeID;\n\t\t\treadQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\t\tfor (j = 0; j < readQNodes[i].numSuccedents; j++)\n\t\t\t\treadQNodes[i].propList[0] = NULL;\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity and Q */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\tif (nfaults == 2) {\n\t\t\t\trf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, qname, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\t\tqNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[2] = readQNodes[i].params[0];\n\t\t\t\tqNodes[i].params[3] = readQNodes[i].params[1];\n\t\t\t\tqNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[6].p = raidPtr;\n\t\t\t\tqNodes[i].results[0] = readQNodes[i].params[1].p;\t/* use old Q buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t\tif (nfaults == 2) {\n\t\t\trf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, qname, allocList);\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Rod */\n\t\t\t\tqNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\t/* and read old q */\n\t\t\tqNodes[0].params[2 * numDataNodes + 0] = readQNodes[0].params[0];\t/* pda */\n\t\t\tqNodes[0].params[2 * numDataNodes + 1] = readQNodes[0].params[1];\t/* buffer pointer */\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Wnd nodes */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\tqNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\t\tqNodes[0].results[0] = readQNodes[0].params[1].p;\n\t\t}\n\t}\n\n\t/* initialize nodes which write new parity (Wnp) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\trf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnp\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twriteParityNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t * filled in by xor node */\n\t\twriteParityNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\twriteParityNodes[i].params[2].v = parityStripeID;\n\t\twriteParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unp\", allocList);\n\t\t\tunlockParityNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new Q (Wnq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnq\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twriteQNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t\t * filled in by xor node */\n\t\t\twriteQNodes[i].params[1].p = qNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\t\twriteQNodes[i].params[2].v = parityStripeID;\n\t\t\twriteQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\t\tif (lu_flag) {\n\t\t\t\t/* initialize node to unlock the disk queue */\n\t\t\t\trf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unq\", allocList);\n\t\t\t\tunlockQNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t\t * desc */\n\t\t\t\tunlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t\t}\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old Q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tblockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];\n\t\t\tRF_ASSERT(readQNodes[i].numAntecedents == 1);\n\t\t\treadQNodes[i].antecedents[0] = blockNode;\n\t\t\treadQNodes[i].antType[0] = rf_control;\n\t\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == ((nfaults * numParityNodes) + 1));\n\t\tRF_ASSERT(writeDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].succedents[0] = &writeDataNodes[i];\n\t\twriteDataNodes[i].antecedents[0] = &readDataNodes[i];\n\t\twriteDataNodes[i].antType[0] = rf_antiData;\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[1 + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numDataNodes; i++)\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\t\treadDataNodes[i].succedents[1 + numParityNodes + j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\t\tqNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\t\treadParityNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old q nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(readQNodes[i].numSuccedents == numParityNodes);\n\t\t\t\treadQNodes[i].succedents[j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];\n\t\t\t\tqNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\t/* connect xor nodes to the write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(writeParityNodes[i].numAntecedents == numParityNodes);\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numSuccedents == numParityNodes);\n\t\t\txorNodes[i].succedents[j] = &writeParityNodes[j];\n\t\t\twriteParityNodes[j].antecedents[i] = &xorNodes[i];\n\t\t\twriteParityNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect q nodes to the write new q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(writeQNodes[i].numAntecedents == numParityNodes);\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numSuccedents == 1);\n\t\t\t\tqNodes[i].succedents[j] = &writeQNodes[j];\n\t\t\t\twriteQNodes[j].antecedents[i] = &qNodes[i];\n\t\t\t\twriteQNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tunlockDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to term node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &writeDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new parity nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockParityNodes[i].numAntecedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = &unlockParityNodes[i];\n\t\t\tunlockParityNodes[i].antecedents[0] = &writeParityNodes[i];\n\t\t\tunlockParityNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockParityNodes[i].numSuccedents == 1);\n\t\t\tunlockParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t} else {\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &writeParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tif (lu_flag) {\n\t\t\t\t/* connect write new Q nodes to unlock nodes */\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numAntecedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = &unlockQNodes[i];\n\t\t\t\tunlockQNodes[i].antecedents[0] = &writeQNodes[i];\n\t\t\t\tunlockQNodes[i].antType[0] = rf_control;\n\n\t\t\t\t/* connect unlock nodes to unblock node */\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numSuccedents == 1);\n\t\t\t\tunlockQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t} else {\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t}\n\t\t}\n}",
          "includes": [
            "#include \"rf_dagffwr.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_dagffrd.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CommonCreateSmallWriteDAGFwd(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_RedFuncs_t * pfuncs,\n    RF_RedFuncs_t * qfuncs)\n{\n\tRF_DagNode_t *readDataNodes, *readParityNodes, *readQNodes, *termNode;\n\tRF_DagNode_t *unlockDataNodes, *unlockParityNodes, *unlockQNodes;\n\tRF_DagNode_t *xorNodes, *qNodes, *blockNode, *nodes;\n\tRF_DagNode_t *writeDataNodes, *writeParityNodes, *writeQNodes;\n\tint     i, j, nNodes, totalNumNodes, lu_flag;\n\tRF_ReconUnitNum_t which_ru;\n\tint     (*func) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     (*qfunc) (RF_DagNode_t *);\n\tint     numDataNodes, numParityNodes;\n\tRF_StripeNum_t parityStripeID;\n\tRF_PhysDiskAddr_t *pda;\n\tchar   *name, *qname;\n\tlong    nfaults;\n\n\tnfaults = qfuncs ? 2 : 1;\n\tlu_flag = (rf_enableAtomicRMW) ? 1 : 0;\t/* lock/unlock flag */\n\n\tparityStripeID = rf_RaidAddressToParityStripeID(&(raidPtr->Layout), asmap->raidAddress, &which_ru);\n\tpda = asmap->physInfo;\n\tnumDataNodes = asmap->numStripeUnitsAccessed;\n\tnumParityNodes = (asmap->parityInfo->next) ? 2 : 1;\n\n\tif (rf_dagDebug)\n\t\tprintf(\"[Creating small-write DAG]\\n\");\n\tRF_ASSERT(numDataNodes > 0);\n\tdag_h->creator = \"SmallWriteDAGFwd\";\n\n\tdag_h->numCommitNodes = 0;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\tqfunc = NULL;\n\tqname = NULL;\n\n\t/* DAG creation occurs in four steps: 1. count the number of nodes in\n\t * the DAG 2. create the nodes 3. initialize the nodes 4. connect the\n\t * nodes */\n\n\t/* Step 1. compute number of nodes in the graph */\n\n\t/* number of nodes: a read and write for each data unit a redundancy\n\t * computation node for each parity node (nfaults * nparity) a read\n\t * and write for each parity unit a block node a terminate node if\n\t * atomic RMW an unlock node for each data unit, redundancy unit */\n\ttotalNumNodes = (2 * numDataNodes) + (nfaults * numParityNodes) + (nfaults * 2 * numParityNodes) + 2;\n\tif (lu_flag)\n\t\ttotalNumNodes += (numDataNodes + (nfaults * numParityNodes));\n\n\n\t/* Step 2. create the nodes */\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t), (RF_DagNode_t *), allocList);\n\ti = 0;\n\tblockNode = &nodes[i];\n\ti += 1;\n\treadDataNodes = &nodes[i];\n\ti += numDataNodes;\n\treadParityNodes = &nodes[i];\n\ti += numParityNodes;\n\twriteDataNodes = &nodes[i];\n\ti += numDataNodes;\n\twriteParityNodes = &nodes[i];\n\ti += numParityNodes;\n\txorNodes = &nodes[i];\n\ti += numParityNodes;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tif (lu_flag) {\n\t\tunlockDataNodes = &nodes[i];\n\t\ti += numDataNodes;\n\t\tunlockParityNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t} else {\n\t\tunlockDataNodes = unlockParityNodes = NULL;\n\t}\n\tif (nfaults == 2) {\n\t\treadQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\twriteQNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tqNodes = &nodes[i];\n\t\ti += numParityNodes;\n\t\tif (lu_flag) {\n\t\t\tunlockQNodes = &nodes[i];\n\t\t\ti += numParityNodes;\n\t\t} else {\n\t\t\tunlockQNodes = NULL;\n\t\t}\n\t} else {\n\t\treadQNodes = writeQNodes = qNodes = unlockQNodes = NULL;\n\t}\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* Step 3. initialize the nodes */\n\t/* initialize block node (Nil) */\n\tnNodes = numDataNodes + (nfaults * numParityNodes);\n\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc, NULL, nNodes, 0, 0, 0, dag_h, \"Nil\", allocList);\n\n\t/* initialize terminate node (Trm) */\n\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc, NULL, 0, nNodes, 0, 0, dag_h, \"Trm\", allocList);\n\n\t/* initialize nodes which read old data (Rod) */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\trf_InitNode(&readDataNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, (numParityNodes * nfaults) + 1, 1, 4, 0, dag_h, \"Rod\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\treadDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\treadDataNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t * data */\n\t\treadDataNodes[i].params[2].v = parityStripeID;\n\t\treadDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tpda = pda->next;\n\t\tfor (j = 0; j < readDataNodes[i].numSuccedents; j++)\n\t\t\treadDataNodes[i].propList[j] = NULL;\n\t}\n\n\t/* initialize nodes which read old parity (Rop) */\n\tpda = asmap->parityInfo;\n\ti = 0;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&readParityNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Rop\", allocList);\n\t\treadParityNodes[i].params[0].p = pda;\n\t\treadParityNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old\n\t\t\t\t\t\t\t\t\t\t\t\t\t * parity */\n\t\treadParityNodes[i].params[2].v = parityStripeID;\n\t\treadParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\tfor (j = 0; j < readParityNodes[i].numSuccedents; j++)\n\t\t\treadParityNodes[i].propList[0] = NULL;\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which read old Q (Roq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\trf_InitNode(&readQNodes[i], rf_wait, RF_FALSE, rf_DiskReadFunc, rf_DiskReadUndoFunc, rf_GenericWakeupFunc, numParityNodes, 1, 4, 0, dag_h, \"Roq\", allocList);\n\t\t\treadQNodes[i].params[0].p = pda;\n\t\t\treadQNodes[i].params[1].p = rf_AllocBuffer(raidPtr, dag_h, pda, allocList);\t/* buffer to hold old Q */\n\t\t\treadQNodes[i].params[2].v = parityStripeID;\n\t\t\treadQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, lu_flag, 0, which_ru);\n\t\t\tfor (j = 0; j < readQNodes[i].numSuccedents; j++)\n\t\t\t\treadQNodes[i].propList[0] = NULL;\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* initialize nodes which write new data (Wnd) */\n\tpda = asmap->physInfo;\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&writeDataNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, 1, 4, 0, dag_h, \"Wnd\", allocList);\n\t\twriteDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t * desc */\n\t\twriteDataNodes[i].params[1].p = pda->bufPtr;\t/* buffer holding new\n\t\t\t\t\t\t\t\t * data to be written */\n\t\twriteDataNodes[i].params[2].v = parityStripeID;\n\t\twriteDataNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockDataNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Und\", allocList);\n\t\t\tunlockDataNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockDataNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\n\t/* initialize nodes which compute new parity and Q */\n\t/* we use the simple XOR func in the double-XOR case, and when we're\n\t * accessing only a portion of one stripe unit. the distinction\n\t * between the two is that the regular XOR func assumes that the\n\t * targbuf is a full SU in size, and examines the pda associated with\n\t * the buffer to decide where within the buffer to XOR the data,\n\t * whereas the simple XOR func just XORs the data into the start of\n\t * the buffer. */\n\tif ((numParityNodes == 2) || ((numDataNodes == 1) && (asmap->totalSectorsAccessed < raidPtr->Layout.sectorsPerStripeUnit))) {\n\t\tfunc = pfuncs->simple;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->SimpleName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->simple;\n\t\t\tqname = qfuncs->SimpleName;\n\t\t}\n\t} else {\n\t\tfunc = pfuncs->regular;\n\t\tundoFunc = rf_NullNodeUndoFunc;\n\t\tname = pfuncs->RegularName;\n\t\tif (qfuncs) {\n\t\t\tqfunc = qfuncs->regular;\n\t\t\tqname = qfuncs->RegularName;\n\t\t}\n\t}\n\t/* initialize the xor nodes: params are {pda,buf} from {Rod,Wnd,Rop}\n\t * nodes, and raidPtr  */\n\tif (numParityNodes == 2) {\t/* double-xor case */\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&xorNodes[i], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, name, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\txorNodes[i].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\t\txorNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\txorNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\txorNodes[i].params[2] = readParityNodes[i].params[0];\n\t\t\txorNodes[i].params[3] = readParityNodes[i].params[1];\n\t\t\txorNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\txorNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\txorNodes[i].params[6].p = raidPtr;\n\t\t\txorNodes[i].results[0] = readParityNodes[i].params[1].p;\t/* use old parity buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\tif (nfaults == 2) {\n\t\t\t\trf_InitNode(&qNodes[i], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, 7, 1, dag_h, qname, allocList);\t/* no wakeup func for\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t * xor */\n\t\t\t\tqNodes[i].params[0] = readDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[1] = readDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[2] = readQNodes[i].params[0];\n\t\t\t\tqNodes[i].params[3] = readQNodes[i].params[1];\n\t\t\t\tqNodes[i].params[4] = writeDataNodes[i].params[0];\n\t\t\t\tqNodes[i].params[5] = writeDataNodes[i].params[1];\n\t\t\t\tqNodes[i].params[6].p = raidPtr;\n\t\t\t\tqNodes[i].results[0] = readQNodes[i].params[1].p;\t/* use old Q buf as\n\t\t\t\t\t\t\t\t\t\t\t * target buf */\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* there is only one xor node in this case */\n\t\trf_InitNode(&xorNodes[0], rf_wait, RF_FALSE, func, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, name, allocList);\n\t\txorNodes[0].flags |= RF_DAGNODE_FLAG_YIELD;\n\t\tfor (i = 0; i < numDataNodes + 1; i++) {\n\t\t\t/* set up params related to Rod and Rop nodes */\n\t\t\txorNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t/* set up params related to Wnd and Wnp nodes */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\txorNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t}\n\t\txorNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\txorNodes[0].results[0] = readParityNodes[0].params[1].p;\n\t\tif (nfaults == 2) {\n\t\t\trf_InitNode(&qNodes[0], rf_wait, RF_FALSE, qfunc, undoFunc, NULL, numParityNodes, numParityNodes + numDataNodes, (2 * (numDataNodes + numDataNodes + 1) + 1), 1, dag_h, qname, allocList);\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Rod */\n\t\t\t\tqNodes[0].params[2 * i + 0] = readDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * i + 1] = readDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\t/* and read old q */\n\t\t\tqNodes[0].params[2 * numDataNodes + 0] = readQNodes[0].params[0];\t/* pda */\n\t\t\tqNodes[0].params[2 * numDataNodes + 1] = readQNodes[0].params[1];\t/* buffer pointer */\n\t\t\tfor (i = 0; i < numDataNodes; i++) {\n\t\t\t\t/* set up params related to Wnd nodes */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 0] = writeDataNodes[i].params[0];\t/* pda */\n\t\t\t\tqNodes[0].params[2 * (numDataNodes + 1 + i) + 1] = writeDataNodes[i].params[1];\t/* buffer pointer */\n\t\t\t}\n\t\t\tqNodes[0].params[2 * (numDataNodes + numDataNodes + 1)].p = raidPtr;\t/* xor node needs to get\n\t\t\t\t\t\t\t\t\t\t\t\t * at RAID information */\n\t\t\tqNodes[0].results[0] = readQNodes[0].params[1].p;\n\t\t}\n\t}\n\n\t/* initialize nodes which write new parity (Wnp) */\n\tpda = asmap->parityInfo;\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\trf_InitNode(&writeParityNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnp\", allocList);\n\t\tRF_ASSERT(pda != NULL);\n\t\twriteParityNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t * filled in by xor node */\n\t\twriteParityNodes[i].params[1].p = xorNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\twriteParityNodes[i].params[2].v = parityStripeID;\n\t\twriteParityNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\tif (lu_flag) {\n\t\t\t/* initialize node to unlock the disk queue */\n\t\t\trf_InitNode(&unlockParityNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unp\", allocList);\n\t\t\tunlockParityNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t * desc */\n\t\t\tunlockParityNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t}\n\t\tpda = pda->next;\n\t}\n\n\t/* initialize nodes which write new Q (Wnq) */\n\tif (nfaults == 2) {\n\t\tpda = asmap->qInfo;\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\trf_InitNode(&writeQNodes[i], rf_wait, RF_FALSE, rf_DiskWriteFunc, rf_DiskWriteUndoFunc, rf_GenericWakeupFunc, 1, numParityNodes, 4, 0, dag_h, \"Wnq\", allocList);\n\t\t\tRF_ASSERT(pda != NULL);\n\t\t\twriteQNodes[i].params[0].p = pda;\t/* param 1 (bufPtr)\n\t\t\t\t\t\t\t\t * filled in by xor node */\n\t\t\twriteQNodes[i].params[1].p = qNodes[i].results[0];\t/* buffer pointer for\n\t\t\t\t\t\t\t\t\t\t * parity write\n\t\t\t\t\t\t\t\t\t\t * operation */\n\t\t\twriteQNodes[i].params[2].v = parityStripeID;\n\t\t\twriteQNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, which_ru);\n\n\t\t\tif (lu_flag) {\n\t\t\t\t/* initialize node to unlock the disk queue */\n\t\t\t\trf_InitNode(&unlockQNodes[i], rf_wait, RF_FALSE, rf_DiskUnlockFunc, rf_DiskUnlockUndoFunc, rf_GenericWakeupFunc, 1, 1, 2, 0, dag_h, \"Unq\", allocList);\n\t\t\t\tunlockQNodes[i].params[0].p = pda;\t/* physical disk addr\n\t\t\t\t\t\t\t\t\t * desc */\n\t\t\t\tunlockQNodes[i].params[1].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, lu_flag, which_ru);\n\t\t\t}\n\t\t\tpda = pda->next;\n\t\t}\n\t}\n\t/* Step 4. connect the nodes */\n\n\t/* connect header to block node */\n\tdag_h->succedents[0] = blockNode;\n\n\t/* connect block node to read old data nodes */\n\tRF_ASSERT(blockNode->numSuccedents == (numDataNodes + (numParityNodes * nfaults)));\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tblockNode->succedents[i] = &readDataNodes[i];\n\t\tRF_ASSERT(readDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].antecedents[0] = blockNode;\n\t\treadDataNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tblockNode->succedents[numDataNodes + i] = &readParityNodes[i];\n\t\tRF_ASSERT(readParityNodes[i].numAntecedents == 1);\n\t\treadParityNodes[i].antecedents[0] = blockNode;\n\t\treadParityNodes[i].antType[0] = rf_control;\n\t}\n\n\t/* connect block node to read old Q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tblockNode->succedents[numDataNodes + numParityNodes + i] = &readQNodes[i];\n\t\t\tRF_ASSERT(readQNodes[i].numAntecedents == 1);\n\t\t\treadQNodes[i].antecedents[0] = blockNode;\n\t\t\treadQNodes[i].antType[0] = rf_control;\n\t\t}\n\n\t/* connect read old data nodes to write new data nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tRF_ASSERT(readDataNodes[i].numSuccedents == ((nfaults * numParityNodes) + 1));\n\t\tRF_ASSERT(writeDataNodes[i].numAntecedents == 1);\n\t\treadDataNodes[i].succedents[0] = &writeDataNodes[i];\n\t\twriteDataNodes[i].antecedents[0] = &readDataNodes[i];\n\t\twriteDataNodes[i].antType[0] = rf_antiData;\n\t}\n\n\t/* connect read old data nodes to xor nodes */\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\treadDataNodes[i].succedents[1 + j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\txorNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old data nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numDataNodes; i++)\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numAntecedents == numDataNodes + numParityNodes);\n\t\t\t\treadDataNodes[i].succedents[1 + numParityNodes + j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[i] = &readDataNodes[i];\n\t\t\t\tqNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\n\t/* connect read old parity nodes to xor nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(readParityNodes[i].numSuccedents == numParityNodes);\n\t\t\treadParityNodes[i].succedents[j] = &xorNodes[j];\n\t\t\txorNodes[j].antecedents[numDataNodes + i] = &readParityNodes[i];\n\t\t\txorNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect read old q nodes to q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(readQNodes[i].numSuccedents == numParityNodes);\n\t\t\t\treadQNodes[i].succedents[j] = &qNodes[j];\n\t\t\t\tqNodes[j].antecedents[numDataNodes + i] = &readQNodes[i];\n\t\t\t\tqNodes[j].antType[numDataNodes + i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\t/* connect xor nodes to the write new parity nodes */\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tRF_ASSERT(writeParityNodes[i].numAntecedents == numParityNodes);\n\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\tRF_ASSERT(xorNodes[j].numSuccedents == numParityNodes);\n\t\t\txorNodes[i].succedents[j] = &writeParityNodes[j];\n\t\t\twriteParityNodes[j].antecedents[i] = &xorNodes[i];\n\t\t\twriteParityNodes[j].antType[i] = rf_trueData;\n\t\t}\n\t}\n\n\t/* connect q nodes to the write new q nodes */\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tRF_ASSERT(writeQNodes[i].numAntecedents == numParityNodes);\n\t\t\tfor (j = 0; j < numParityNodes; j++) {\n\t\t\t\tRF_ASSERT(qNodes[j].numSuccedents == 1);\n\t\t\t\tqNodes[i].succedents[j] = &writeQNodes[j];\n\t\t\t\twriteQNodes[j].antecedents[i] = &qNodes[i];\n\t\t\t\twriteQNodes[j].antType[i] = rf_trueData;\n\t\t\t}\n\t\t}\n\n\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\tRF_ASSERT(termNode->numSuccedents == 0);\n\tfor (i = 0; i < numDataNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new data nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockDataNodes[i].numAntecedents == 1);\n\t\t\twriteDataNodes[i].succedents[0] = &unlockDataNodes[i];\n\t\t\tunlockDataNodes[i].antecedents[0] = &writeDataNodes[i];\n\t\t\tunlockDataNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockDataNodes[i].numSuccedents == 1);\n\t\t\tunlockDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &unlockDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t} else {\n\t\t\t/* connect write new data nodes to term node */\n\t\t\tRF_ASSERT(writeDataNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(termNode->numAntecedents == (numDataNodes + (nfaults * numParityNodes)));\n\t\t\twriteDataNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &writeDataNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n\n\tfor (i = 0; i < numParityNodes; i++) {\n\t\tif (lu_flag) {\n\t\t\t/* connect write new parity nodes to unlock nodes */\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\tRF_ASSERT(unlockParityNodes[i].numAntecedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = &unlockParityNodes[i];\n\t\t\tunlockParityNodes[i].antecedents[0] = &writeParityNodes[i];\n\t\t\tunlockParityNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect unlock nodes to term node */\n\t\t\tRF_ASSERT(unlockParityNodes[i].numSuccedents == 1);\n\t\t\tunlockParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &unlockParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t} else {\n\t\t\tRF_ASSERT(writeParityNodes[i].numSuccedents == 1);\n\t\t\twriteParityNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[numDataNodes + i] = &writeParityNodes[i];\n\t\t\ttermNode->antType[numDataNodes + i] = rf_control;\n\t\t}\n\t}\n\n\tif (nfaults == 2)\n\t\tfor (i = 0; i < numParityNodes; i++) {\n\t\t\tif (lu_flag) {\n\t\t\t\t/* connect write new Q nodes to unlock nodes */\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numAntecedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = &unlockQNodes[i];\n\t\t\t\tunlockQNodes[i].antecedents[0] = &writeQNodes[i];\n\t\t\t\tunlockQNodes[i].antType[0] = rf_control;\n\n\t\t\t\t/* connect unlock nodes to unblock node */\n\t\t\t\tRF_ASSERT(unlockQNodes[i].numSuccedents == 1);\n\t\t\t\tunlockQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &unlockQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t} else {\n\t\t\t\tRF_ASSERT(writeQNodes[i].numSuccedents == 1);\n\t\t\t\twriteQNodes[i].succedents[0] = termNode;\n\t\t\t\ttermNode->antecedents[numDataNodes + numParityNodes + i] = &writeQNodes[i];\n\t\t\t\ttermNode->antType[numDataNodes + numParityNodes + i] = rf_control;\n\t\t\t}\n\t\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateSmallWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList)\n{\n\t/* \"normal\" rollaway */\n\trf_CommonCreateSmallWriteDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    &rf_xorFuncs, NULL);\n}"
  },
  {
    "function_name": "rf_CreateRAID0WriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "90-102",
    "snippet": "void \nrf_CreateRAID0WriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\trf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    RF_IO_TYPE_WRITE);\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CreateNonredundantDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList",
            "RF_IO_TYPE_WRITE"
          ],
          "line": 100
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CreateNonredundantDAG",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffrd.c",
          "lines": "127-288",
          "snippet": "void \nrf_CreateNonredundantDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\tRF_DagNode_t *nodes, *diskNodes, *blockNode, *commitNode, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     (*doFunc) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     i, n, totalNumNodes;\n\tchar   *name;\n\n\tn = asmap->numStripeUnitsAccessed;\n\tdag_h->creator = \"NonredundantDAG\";\n\n\tRF_ASSERT(RF_IO_IS_R_OR_W(type));\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\tdoFunc = rf_DiskReadFunc;\n\t\tundoFunc = rf_DiskReadUndoFunc;\n\t\tname = \"R  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant read DAG]\\n\");\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\tdoFunc = rf_DiskWriteFunc;\n\t\tundoFunc = rf_DiskWriteUndoFunc;\n\t\tname = \"W  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant write DAG]\\n\");\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\t/*\n         * For reads, the dag can not commit until the block node is reached.\n         * for writes, the dag commits immediately.\n         */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/*\n         * Node count:\n         * 1 block node\n         * n data reads (or writes)\n         * 1 commit node\n         * 1 terminator node\n         */\n\tRF_ASSERT(n > 0);\n\ttotalNumNodes = n + 3;\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tdiskNodes = &nodes[i];\n\ti += n;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* initialize nodes */\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, n, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, n, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&diskNodes[i], rf_wait, RF_FALSE, doFunc, undoFunc, rf_GenericWakeupFunc,\n\t\t    1, 1, 4, 0, dag_h, name, allocList);\n\t\tdiskNodes[i].params[0].p = pda;\n\t\tdiskNodes[i].params[1].p = pda->bufPtr;\n\t\t/* parity stripe id is not necessary */\n\t\tdiskNodes[i].params[2].v = 0;\n\t\tdiskNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, 0);\n\t\tpda = pda->next;\n\t}\n\n\t/*\n         * Connect nodes.\n         */\n\n\t/* connect hdr to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (type == RF_IO_TYPE_READ) {\n\t\t/* connecting a nonredundant read DAG */\n\t\tRF_ASSERT(blockNode->numSuccedents == n);\n\t\tRF_ASSERT(commitNode->numAntecedents == n);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect block node to each read node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = blockNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each read node to the commit node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = commitNode;\n\t\t\tcommitNode->antecedents[i] = &diskNodes[i];\n\t\t\tcommitNode->antType[i] = rf_control;\n\t\t}\n\t\t/* connect the commit node to the term node */\n\t\tRF_ASSERT(commitNode->numSuccedents == 1);\n\t\tRF_ASSERT(termNode->numAntecedents == 1);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tcommitNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[0] = commitNode;\n\t\ttermNode->antType[0] = rf_control;\n\t} else {\n\t\t/* connecting a nonredundant write DAG */\n\t\t/* connect the block node to the commit node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(commitNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = commitNode;\n\t\tcommitNode->antecedents[0] = blockNode;\n\t\tcommitNode->antType[0] = rf_control;\n\n\t\tRF_ASSERT(commitNode->numSuccedents == n);\n\t\tRF_ASSERT(termNode->numAntecedents == n);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect the commit node to each write node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tcommitNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = commitNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each write node to the term node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &diskNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"rf_dagffrd.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_dagffrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateNonredundantDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\tRF_DagNode_t *nodes, *diskNodes, *blockNode, *commitNode, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     (*doFunc) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     i, n, totalNumNodes;\n\tchar   *name;\n\n\tn = asmap->numStripeUnitsAccessed;\n\tdag_h->creator = \"NonredundantDAG\";\n\n\tRF_ASSERT(RF_IO_IS_R_OR_W(type));\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\tdoFunc = rf_DiskReadFunc;\n\t\tundoFunc = rf_DiskReadUndoFunc;\n\t\tname = \"R  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant read DAG]\\n\");\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\tdoFunc = rf_DiskWriteFunc;\n\t\tundoFunc = rf_DiskWriteUndoFunc;\n\t\tname = \"W  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant write DAG]\\n\");\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\t/*\n         * For reads, the dag can not commit until the block node is reached.\n         * for writes, the dag commits immediately.\n         */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/*\n         * Node count:\n         * 1 block node\n         * n data reads (or writes)\n         * 1 commit node\n         * 1 terminator node\n         */\n\tRF_ASSERT(n > 0);\n\ttotalNumNodes = n + 3;\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tdiskNodes = &nodes[i];\n\ti += n;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* initialize nodes */\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, n, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, n, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&diskNodes[i], rf_wait, RF_FALSE, doFunc, undoFunc, rf_GenericWakeupFunc,\n\t\t    1, 1, 4, 0, dag_h, name, allocList);\n\t\tdiskNodes[i].params[0].p = pda;\n\t\tdiskNodes[i].params[1].p = pda->bufPtr;\n\t\t/* parity stripe id is not necessary */\n\t\tdiskNodes[i].params[2].v = 0;\n\t\tdiskNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, 0);\n\t\tpda = pda->next;\n\t}\n\n\t/*\n         * Connect nodes.\n         */\n\n\t/* connect hdr to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (type == RF_IO_TYPE_READ) {\n\t\t/* connecting a nonredundant read DAG */\n\t\tRF_ASSERT(blockNode->numSuccedents == n);\n\t\tRF_ASSERT(commitNode->numAntecedents == n);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect block node to each read node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = blockNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each read node to the commit node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = commitNode;\n\t\t\tcommitNode->antecedents[i] = &diskNodes[i];\n\t\t\tcommitNode->antType[i] = rf_control;\n\t\t}\n\t\t/* connect the commit node to the term node */\n\t\tRF_ASSERT(commitNode->numSuccedents == 1);\n\t\tRF_ASSERT(termNode->numAntecedents == 1);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tcommitNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[0] = commitNode;\n\t\ttermNode->antType[0] = rf_control;\n\t} else {\n\t\t/* connecting a nonredundant write DAG */\n\t\t/* connect the block node to the commit node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(commitNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = commitNode;\n\t\tcommitNode->antecedents[0] = blockNode;\n\t\tcommitNode->antType[0] = rf_control;\n\n\t\tRF_ASSERT(commitNode->numSuccedents == n);\n\t\tRF_ASSERT(termNode->numAntecedents == n);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect the commit node to each write node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tcommitNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = commitNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each write node to the term node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &diskNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateRAID0WriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\trf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    RF_IO_TYPE_WRITE);\n}"
  },
  {
    "function_name": "rf_CreateNonRedundantWriteDAG",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffwr.c",
    "lines": "76-88",
    "snippet": "void \nrf_CreateNonRedundantWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\trf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    RF_IO_TYPE_WRITE);\n}",
    "includes": [
      "#include \"rf_dagffwr.h\"",
      "#include \"rf_general.h\"",
      "#include \"rf_memchunk.h\"",
      "#include \"rf_dagffrd.h\"",
      "#include \"rf_debugMem.h\"",
      "#include \"rf_dagfuncs.h\"",
      "#include \"rf_dagutils.h\"",
      "#include \"rf_dag.h\"",
      "#include \"rf_raid.h\"",
      "#include \"rf_types.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rf_CreateNonredundantDAG",
          "args": [
            "raidPtr",
            "asmap",
            "dag_h",
            "bp",
            "flags",
            "allocList",
            "RF_IO_TYPE_WRITE"
          ],
          "line": 86
        },
        "resolved": true,
        "details": {
          "function_name": "rf_CreateNonredundantDAG",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c_ICV/CVE-2023-40216/repo/sys/dev/raidframe/rf_dagffrd.c",
          "lines": "127-288",
          "snippet": "void \nrf_CreateNonredundantDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\tRF_DagNode_t *nodes, *diskNodes, *blockNode, *commitNode, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     (*doFunc) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     i, n, totalNumNodes;\n\tchar   *name;\n\n\tn = asmap->numStripeUnitsAccessed;\n\tdag_h->creator = \"NonredundantDAG\";\n\n\tRF_ASSERT(RF_IO_IS_R_OR_W(type));\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\tdoFunc = rf_DiskReadFunc;\n\t\tundoFunc = rf_DiskReadUndoFunc;\n\t\tname = \"R  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant read DAG]\\n\");\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\tdoFunc = rf_DiskWriteFunc;\n\t\tundoFunc = rf_DiskWriteUndoFunc;\n\t\tname = \"W  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant write DAG]\\n\");\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\t/*\n         * For reads, the dag can not commit until the block node is reached.\n         * for writes, the dag commits immediately.\n         */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/*\n         * Node count:\n         * 1 block node\n         * n data reads (or writes)\n         * 1 commit node\n         * 1 terminator node\n         */\n\tRF_ASSERT(n > 0);\n\ttotalNumNodes = n + 3;\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tdiskNodes = &nodes[i];\n\ti += n;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* initialize nodes */\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, n, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, n, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&diskNodes[i], rf_wait, RF_FALSE, doFunc, undoFunc, rf_GenericWakeupFunc,\n\t\t    1, 1, 4, 0, dag_h, name, allocList);\n\t\tdiskNodes[i].params[0].p = pda;\n\t\tdiskNodes[i].params[1].p = pda->bufPtr;\n\t\t/* parity stripe id is not necessary */\n\t\tdiskNodes[i].params[2].v = 0;\n\t\tdiskNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, 0);\n\t\tpda = pda->next;\n\t}\n\n\t/*\n         * Connect nodes.\n         */\n\n\t/* connect hdr to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (type == RF_IO_TYPE_READ) {\n\t\t/* connecting a nonredundant read DAG */\n\t\tRF_ASSERT(blockNode->numSuccedents == n);\n\t\tRF_ASSERT(commitNode->numAntecedents == n);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect block node to each read node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = blockNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each read node to the commit node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = commitNode;\n\t\t\tcommitNode->antecedents[i] = &diskNodes[i];\n\t\t\tcommitNode->antType[i] = rf_control;\n\t\t}\n\t\t/* connect the commit node to the term node */\n\t\tRF_ASSERT(commitNode->numSuccedents == 1);\n\t\tRF_ASSERT(termNode->numAntecedents == 1);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tcommitNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[0] = commitNode;\n\t\ttermNode->antType[0] = rf_control;\n\t} else {\n\t\t/* connecting a nonredundant write DAG */\n\t\t/* connect the block node to the commit node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(commitNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = commitNode;\n\t\tcommitNode->antecedents[0] = blockNode;\n\t\tcommitNode->antType[0] = rf_control;\n\n\t\tRF_ASSERT(commitNode->numSuccedents == n);\n\t\tRF_ASSERT(termNode->numAntecedents == n);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect the commit node to each write node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tcommitNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = commitNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each write node to the term node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &diskNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"rf_dagffrd.h\"",
            "#include \"rf_general.h\"",
            "#include \"rf_memchunk.h\"",
            "#include \"rf_debugMem.h\"",
            "#include \"rf_dagfuncs.h\"",
            "#include \"rf_dagutils.h\"",
            "#include \"rf_dag.h\"",
            "#include \"rf_raid.h\"",
            "#include \"rf_types.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rf_dagffrd.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateNonredundantDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\tRF_DagNode_t *nodes, *diskNodes, *blockNode, *commitNode, *termNode;\n\tRF_PhysDiskAddr_t *pda = asmap->physInfo;\n\tint     (*doFunc) (RF_DagNode_t *), (*undoFunc) (RF_DagNode_t *);\n\tint     i, n, totalNumNodes;\n\tchar   *name;\n\n\tn = asmap->numStripeUnitsAccessed;\n\tdag_h->creator = \"NonredundantDAG\";\n\n\tRF_ASSERT(RF_IO_IS_R_OR_W(type));\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\tdoFunc = rf_DiskReadFunc;\n\t\tundoFunc = rf_DiskReadUndoFunc;\n\t\tname = \"R  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant read DAG]\\n\");\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\tdoFunc = rf_DiskWriteFunc;\n\t\tundoFunc = rf_DiskWriteUndoFunc;\n\t\tname = \"W  \";\n\t\tif (rf_dagDebug)\n\t\t\tprintf(\"[Creating non-redundant write DAG]\\n\");\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\t/*\n         * For reads, the dag can not commit until the block node is reached.\n         * for writes, the dag commits immediately.\n         */\n\tdag_h->numCommitNodes = 1;\n\tdag_h->numCommits = 0;\n\tdag_h->numSuccedents = 1;\n\n\t/*\n         * Node count:\n         * 1 block node\n         * n data reads (or writes)\n         * 1 commit node\n         * 1 terminator node\n         */\n\tRF_ASSERT(n > 0);\n\ttotalNumNodes = n + 3;\n\tRF_CallocAndAdd(nodes, totalNumNodes, sizeof(RF_DagNode_t),\n\t    (RF_DagNode_t *), allocList);\n\ti = 0;\n\tdiskNodes = &nodes[i];\n\ti += n;\n\tblockNode = &nodes[i];\n\ti += 1;\n\tcommitNode = &nodes[i];\n\ti += 1;\n\ttermNode = &nodes[i];\n\ti += 1;\n\tRF_ASSERT(i == totalNumNodes);\n\n\t/* initialize nodes */\n\tswitch (type) {\n\tcase RF_IO_TYPE_READ:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, n, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, 1, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tcase RF_IO_TYPE_WRITE:\n\t\trf_InitNode(blockNode, rf_wait, RF_FALSE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, 1, 0, 0, 0, dag_h, \"Nil\", allocList);\n\t\trf_InitNode(commitNode, rf_wait, RF_TRUE, rf_NullNodeFunc, rf_NullNodeUndoFunc,\n\t\t    NULL, n, 1, 0, 0, dag_h, \"Cmt\", allocList);\n\t\trf_InitNode(termNode, rf_wait, RF_FALSE, rf_TerminateFunc, rf_TerminateUndoFunc,\n\t\t    NULL, 0, n, 0, 0, dag_h, \"Trm\", allocList);\n\t\tbreak;\n\tdefault:\n\t\tRF_PANIC();\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\t\tRF_ASSERT(pda != NULL);\n\t\trf_InitNode(&diskNodes[i], rf_wait, RF_FALSE, doFunc, undoFunc, rf_GenericWakeupFunc,\n\t\t    1, 1, 4, 0, dag_h, name, allocList);\n\t\tdiskNodes[i].params[0].p = pda;\n\t\tdiskNodes[i].params[1].p = pda->bufPtr;\n\t\t/* parity stripe id is not necessary */\n\t\tdiskNodes[i].params[2].v = 0;\n\t\tdiskNodes[i].params[3].v = RF_CREATE_PARAM3(RF_IO_NORMAL_PRIORITY, 0, 0, 0);\n\t\tpda = pda->next;\n\t}\n\n\t/*\n         * Connect nodes.\n         */\n\n\t/* connect hdr to block node */\n\tRF_ASSERT(blockNode->numAntecedents == 0);\n\tdag_h->succedents[0] = blockNode;\n\n\tif (type == RF_IO_TYPE_READ) {\n\t\t/* connecting a nonredundant read DAG */\n\t\tRF_ASSERT(blockNode->numSuccedents == n);\n\t\tRF_ASSERT(commitNode->numAntecedents == n);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect block node to each read node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tblockNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = blockNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each read node to the commit node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = commitNode;\n\t\t\tcommitNode->antecedents[i] = &diskNodes[i];\n\t\t\tcommitNode->antType[i] = rf_control;\n\t\t}\n\t\t/* connect the commit node to the term node */\n\t\tRF_ASSERT(commitNode->numSuccedents == 1);\n\t\tRF_ASSERT(termNode->numAntecedents == 1);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tcommitNode->succedents[0] = termNode;\n\t\ttermNode->antecedents[0] = commitNode;\n\t\ttermNode->antType[0] = rf_control;\n\t} else {\n\t\t/* connecting a nonredundant write DAG */\n\t\t/* connect the block node to the commit node */\n\t\tRF_ASSERT(blockNode->numSuccedents == 1);\n\t\tRF_ASSERT(commitNode->numAntecedents == 1);\n\t\tblockNode->succedents[0] = commitNode;\n\t\tcommitNode->antecedents[0] = blockNode;\n\t\tcommitNode->antType[0] = rf_control;\n\n\t\tRF_ASSERT(commitNode->numSuccedents == n);\n\t\tRF_ASSERT(termNode->numAntecedents == n);\n\t\tRF_ASSERT(termNode->numSuccedents == 0);\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t/* connect the commit node to each write node */\n\t\t\tRF_ASSERT(diskNodes[i].numAntecedents == 1);\n\t\t\tcommitNode->succedents[i] = &diskNodes[i];\n\t\t\tdiskNodes[i].antecedents[0] = commitNode;\n\t\t\tdiskNodes[i].antType[0] = rf_control;\n\n\t\t\t/* connect each write node to the term node */\n\t\t\tRF_ASSERT(diskNodes[i].numSuccedents == 1);\n\t\t\tdiskNodes[i].succedents[0] = termNode;\n\t\t\ttermNode->antecedents[i] = &diskNodes[i];\n\t\t\ttermNode->antType[i] = rf_control;\n\t\t}\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rf_dagffwr.h\"\n#include \"rf_general.h\"\n#include \"rf_memchunk.h\"\n#include \"rf_dagffrd.h\"\n#include \"rf_debugMem.h\"\n#include \"rf_dagfuncs.h\"\n#include \"rf_dagutils.h\"\n#include \"rf_dag.h\"\n#include \"rf_raid.h\"\n#include \"rf_types.h\"\n\nvoid \nrf_CreateNonRedundantWriteDAG(\n    RF_Raid_t * raidPtr,\n    RF_AccessStripeMap_t * asmap,\n    RF_DagHeader_t * dag_h,\n    void *bp,\n    RF_RaidAccessFlags_t flags,\n    RF_AllocListElem_t * allocList,\n    RF_IoType_t type)\n{\n\trf_CreateNonredundantDAG(raidPtr, asmap, dag_h, bp, flags, allocList,\n\t    RF_IO_TYPE_WRITE);\n}"
  }
]