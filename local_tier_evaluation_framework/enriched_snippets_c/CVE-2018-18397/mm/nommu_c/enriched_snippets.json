[
  {
    "function_name": "init_admin_reserve",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1939-1947",
    "snippet": "static int __meminit init_admin_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "min",
          "args": [
            "free_kbytes / 32",
            "1UL << 13"
          ],
          "line": 1945
        },
        "resolved": true,
        "details": {
          "function_name": "si_meminfo_node",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/page_alloc.c",
          "lines": "4761-4790",
          "snippet": "void si_meminfo_node(struct sysinfo *val, int nid)\n{\n\tint zone_type;\t\t/* needs to be signed */\n\tunsigned long managed_pages = 0;\n\tunsigned long managed_highpages = 0;\n\tunsigned long free_highpages = 0;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages += pgdat->node_zones[zone_type].managed_pages;\n\tval->totalram = managed_pages;\n\tval->sharedram = node_page_state(pgdat, NR_SHMEM);\n\tval->freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);\n#ifdef CONFIG_HIGHMEM\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\n\t\tif (is_highmem(zone)) {\n\t\t\tmanaged_highpages += zone->managed_pages;\n\t\t\tfree_highpages += zone_page_state(zone, NR_FREE_PAGES);\n\t\t}\n\t}\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#else\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#endif\n\tval->mem_unit = PAGE_SIZE;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/div64.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/sections.h>",
            "#include <linux/psi.h>",
            "#include <linux/nmi.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/kthread.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/migrate.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/prefetch.h>",
            "#include <trace/events/oom.h>",
            "#include <trace/events/kmem.h>",
            "#include <linux/compaction.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/pfn.h>",
            "#include <linux/sort.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/memremap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/topology.h>",
            "#include <linux/oom.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/slab.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/suspend.h>",
            "#include <linux/module.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kernel.h>",
            "#include <linux/compiler.h>",
            "#include <linux/memblock.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/stddef.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline\nstruct",
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/div64.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <linux/psi.h>\n#include <linux/nmi.h>\n#include <linux/lockdep.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/kthread.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/migrate.h>\n#include <linux/mm_inline.h>\n#include <linux/prefetch.h>\n#include <trace/events/oom.h>\n#include <trace/events/kmem.h>\n#include <linux/compaction.h>\n#include <linux/kmemleak.h>\n#include <linux/debugobjects.h>\n#include <linux/page_ext.h>\n#include <linux/page-isolation.h>\n#include <linux/fault-inject.h>\n#include <linux/backing-dev.h>\n#include <linux/pfn.h>\n#include <linux/sort.h>\n#include <linux/stop_machine.h>\n#include <linux/memremap.h>\n#include <linux/mempolicy.h>\n#include <linux/vmstat.h>\n#include <linux/vmalloc.h>\n#include <linux/nodemask.h>\n#include <linux/memory_hotplug.h>\n#include <linux/cpuset.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/topology.h>\n#include <linux/oom.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/suspend.h>\n#include <linux/module.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/memblock.h>\n#include <linux/jiffies.h>\n#include <linux/pagemap.h>\n#include <linux/interrupt.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n\nstatic __always_inline\nstruct;\nstatic __always_inline struct;\n\nvoid si_meminfo_node(struct sysinfo *val, int nid)\n{\n\tint zone_type;\t\t/* needs to be signed */\n\tunsigned long managed_pages = 0;\n\tunsigned long managed_highpages = 0;\n\tunsigned long free_highpages = 0;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages += pgdat->node_zones[zone_type].managed_pages;\n\tval->totalram = managed_pages;\n\tval->sharedram = node_page_state(pgdat, NR_SHMEM);\n\tval->freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);\n#ifdef CONFIG_HIGHMEM\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\n\t\tif (is_highmem(zone)) {\n\t\t\tmanaged_highpages += zone->managed_pages;\n\t\t\tfree_highpages += zone_page_state(zone, NR_FREE_PAGES);\n\t\t}\n\t}\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#else\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#endif\n\tval->mem_unit = PAGE_SIZE;\n}"
        }
      },
      {
        "call_info": {
          "callee": "global_zone_page_state",
          "args": [
            "NR_FREE_PAGES"
          ],
          "line": 1943
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int __meminit init_admin_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);\n\treturn 0;\n}"
  },
  {
    "function_name": "init_user_reserve",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1918-1926",
    "snippet": "static int __meminit init_user_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "min",
          "args": [
            "free_kbytes / 32",
            "1UL << 17"
          ],
          "line": 1924
        },
        "resolved": true,
        "details": {
          "function_name": "si_meminfo_node",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/page_alloc.c",
          "lines": "4761-4790",
          "snippet": "void si_meminfo_node(struct sysinfo *val, int nid)\n{\n\tint zone_type;\t\t/* needs to be signed */\n\tunsigned long managed_pages = 0;\n\tunsigned long managed_highpages = 0;\n\tunsigned long free_highpages = 0;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages += pgdat->node_zones[zone_type].managed_pages;\n\tval->totalram = managed_pages;\n\tval->sharedram = node_page_state(pgdat, NR_SHMEM);\n\tval->freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);\n#ifdef CONFIG_HIGHMEM\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\n\t\tif (is_highmem(zone)) {\n\t\t\tmanaged_highpages += zone->managed_pages;\n\t\t\tfree_highpages += zone_page_state(zone, NR_FREE_PAGES);\n\t\t}\n\t}\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#else\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#endif\n\tval->mem_unit = PAGE_SIZE;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/div64.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/sections.h>",
            "#include <linux/psi.h>",
            "#include <linux/nmi.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/kthread.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/migrate.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/prefetch.h>",
            "#include <trace/events/oom.h>",
            "#include <trace/events/kmem.h>",
            "#include <linux/compaction.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/pfn.h>",
            "#include <linux/sort.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/memremap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/topology.h>",
            "#include <linux/oom.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/slab.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/suspend.h>",
            "#include <linux/module.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kernel.h>",
            "#include <linux/compiler.h>",
            "#include <linux/memblock.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/stddef.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline\nstruct",
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/div64.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <linux/psi.h>\n#include <linux/nmi.h>\n#include <linux/lockdep.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/kthread.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/migrate.h>\n#include <linux/mm_inline.h>\n#include <linux/prefetch.h>\n#include <trace/events/oom.h>\n#include <trace/events/kmem.h>\n#include <linux/compaction.h>\n#include <linux/kmemleak.h>\n#include <linux/debugobjects.h>\n#include <linux/page_ext.h>\n#include <linux/page-isolation.h>\n#include <linux/fault-inject.h>\n#include <linux/backing-dev.h>\n#include <linux/pfn.h>\n#include <linux/sort.h>\n#include <linux/stop_machine.h>\n#include <linux/memremap.h>\n#include <linux/mempolicy.h>\n#include <linux/vmstat.h>\n#include <linux/vmalloc.h>\n#include <linux/nodemask.h>\n#include <linux/memory_hotplug.h>\n#include <linux/cpuset.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/topology.h>\n#include <linux/oom.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/suspend.h>\n#include <linux/module.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/memblock.h>\n#include <linux/jiffies.h>\n#include <linux/pagemap.h>\n#include <linux/interrupt.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n\nstatic __always_inline\nstruct;\nstatic __always_inline struct;\n\nvoid si_meminfo_node(struct sysinfo *val, int nid)\n{\n\tint zone_type;\t\t/* needs to be signed */\n\tunsigned long managed_pages = 0;\n\tunsigned long managed_highpages = 0;\n\tunsigned long free_highpages = 0;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages += pgdat->node_zones[zone_type].managed_pages;\n\tval->totalram = managed_pages;\n\tval->sharedram = node_page_state(pgdat, NR_SHMEM);\n\tval->freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);\n#ifdef CONFIG_HIGHMEM\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\n\t\tif (is_highmem(zone)) {\n\t\t\tmanaged_highpages += zone->managed_pages;\n\t\t\tfree_highpages += zone_page_state(zone, NR_FREE_PAGES);\n\t\t}\n\t}\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#else\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#endif\n\tval->mem_unit = PAGE_SIZE;\n}"
        }
      },
      {
        "call_info": {
          "callee": "global_zone_page_state",
          "args": [
            "NR_FREE_PAGES"
          ],
          "line": 1922
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int __meminit init_user_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);\n\n\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);\n\treturn 0;\n}"
  },
  {
    "function_name": "nommu_shrink_inode_mappings",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1857-1906",
    "snippet": "int nommu_shrink_inode_mappings(struct inode *inode, size_t size,\n\t\t\t\tsize_t newsize)\n{\n\tstruct vm_area_struct *vma;\n\tstruct vm_region *region;\n\tpgoff_t low, high;\n\tsize_t r_size, r_top;\n\n\tlow = newsize >> PAGE_SHIFT;\n\thigh = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\n\tdown_write(&nommu_region_sem);\n\ti_mmap_lock_read(inode->i_mapping);\n\n\t/* search for VMAs that fall within the dead zone */\n\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, low, high) {\n\t\t/* found one - only interested if it's shared out of the page\n\t\t * cache */\n\t\tif (vma->vm_flags & VM_SHARED) {\n\t\t\ti_mmap_unlock_read(inode->i_mapping);\n\t\t\tup_write(&nommu_region_sem);\n\t\t\treturn -ETXTBSY; /* not quite true, but near enough */\n\t\t}\n\t}\n\n\t/* reduce any regions that overlap the dead zone - if in existence,\n\t * these will be pointed to by VMAs that don't overlap the dead zone\n\t *\n\t * we don't check for any regions that start beyond the EOF as there\n\t * shouldn't be any\n\t */\n\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, 0, ULONG_MAX) {\n\t\tif (!(vma->vm_flags & VM_SHARED))\n\t\t\tcontinue;\n\n\t\tregion = vma->vm_region;\n\t\tr_size = region->vm_top - region->vm_start;\n\t\tr_top = (region->vm_pgoff << PAGE_SHIFT) + r_size;\n\n\t\tif (r_top > newsize) {\n\t\t\tregion->vm_top -= r_top - newsize;\n\t\t\tif (region->vm_end > region->vm_top)\n\t\t\t\tregion->vm_end = region->vm_top;\n\t\t}\n\t}\n\n\ti_mmap_unlock_read(inode->i_mapping);\n\tup_write(&nommu_region_sem);\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1904
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "i_mmap_unlock_read",
          "args": [
            "inode->i_mapping"
          ],
          "line": 1903
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vma_interval_tree_foreach",
          "args": [
            "vma",
            "&inode->i_mapping->i_mmap",
            "0",
            "ULONG_MAX"
          ],
          "line": 1888
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1877
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "i_mmap_unlock_read",
          "args": [
            "inode->i_mapping"
          ],
          "line": 1876
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vma_interval_tree_foreach",
          "args": [
            "vma",
            "&inode->i_mapping->i_mmap",
            "low",
            "high"
          ],
          "line": 1872
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "i_mmap_lock_read",
          "args": [
            "inode->i_mapping"
          ],
          "line": 1869
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1868
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint nommu_shrink_inode_mappings(struct inode *inode, size_t size,\n\t\t\t\tsize_t newsize)\n{\n\tstruct vm_area_struct *vma;\n\tstruct vm_region *region;\n\tpgoff_t low, high;\n\tsize_t r_size, r_top;\n\n\tlow = newsize >> PAGE_SHIFT;\n\thigh = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\n\tdown_write(&nommu_region_sem);\n\ti_mmap_lock_read(inode->i_mapping);\n\n\t/* search for VMAs that fall within the dead zone */\n\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, low, high) {\n\t\t/* found one - only interested if it's shared out of the page\n\t\t * cache */\n\t\tif (vma->vm_flags & VM_SHARED) {\n\t\t\ti_mmap_unlock_read(inode->i_mapping);\n\t\t\tup_write(&nommu_region_sem);\n\t\t\treturn -ETXTBSY; /* not quite true, but near enough */\n\t\t}\n\t}\n\n\t/* reduce any regions that overlap the dead zone - if in existence,\n\t * these will be pointed to by VMAs that don't overlap the dead zone\n\t *\n\t * we don't check for any regions that start beyond the EOF as there\n\t * shouldn't be any\n\t */\n\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, 0, ULONG_MAX) {\n\t\tif (!(vma->vm_flags & VM_SHARED))\n\t\t\tcontinue;\n\n\t\tregion = vma->vm_region;\n\t\tr_size = region->vm_top - region->vm_start;\n\t\tr_top = (region->vm_pgoff << PAGE_SHIFT) + r_size;\n\n\t\tif (r_top > newsize) {\n\t\t\tregion->vm_top -= r_top - newsize;\n\t\t\tif (region->vm_end > region->vm_top)\n\t\t\t\tregion->vm_end = region->vm_top;\n\t\t}\n\t}\n\n\ti_mmap_unlock_read(inode->i_mapping);\n\tup_write(&nommu_region_sem);\n\treturn 0;\n}"
  },
  {
    "function_name": "access_process_vm",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1827-1843",
    "snippet": "int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len,\n\t\tunsigned int gup_flags)\n{\n\tstruct mm_struct *mm;\n\n\tif (addr + len < addr)\n\t\treturn 0;\n\n\tmm = get_task_mm(tsk);\n\tif (!mm)\n\t\treturn 0;\n\n\tlen = __access_remote_vm(tsk, mm, addr, buf, len, gup_flags);\n\n\tmmput(mm);\n\treturn len;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mmput",
          "args": [
            "mm"
          ],
          "line": 1841
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__access_remote_vm",
          "args": [
            "tsk",
            "mm",
            "addr",
            "buf",
            "len",
            "gup_flags"
          ],
          "line": 1839
        },
        "resolved": true,
        "details": {
          "function_name": "__access_remote_vm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1774-1805",
          "snippet": "int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long addr, void *buf, int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tdown_read(&mm->mmap_sem);\n\n\t/* the access must start within one of the target process's mappings */\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t/* don't overrun this mapping */\n\t\tif (addr + len >= vma->vm_end)\n\t\t\tlen = vma->vm_end - addr;\n\n\t\t/* only read or write mappings where it is permitted */\n\t\tif (write && vma->vm_flags & VM_MAYWRITE)\n\t\t\tcopy_to_user_page(vma, NULL, addr,\n\t\t\t\t\t (void *) addr, buf, len);\n\t\telse if (!write && vma->vm_flags & VM_MAYREAD)\n\t\t\tcopy_from_user_page(vma, NULL, addr,\n\t\t\t\t\t    buf, (void *) addr, len);\n\t\telse\n\t\t\tlen = 0;\n\t} else {\n\t\tlen = 0;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\n\treturn len;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long addr, void *buf, int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tdown_read(&mm->mmap_sem);\n\n\t/* the access must start within one of the target process's mappings */\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t/* don't overrun this mapping */\n\t\tif (addr + len >= vma->vm_end)\n\t\t\tlen = vma->vm_end - addr;\n\n\t\t/* only read or write mappings where it is permitted */\n\t\tif (write && vma->vm_flags & VM_MAYWRITE)\n\t\t\tcopy_to_user_page(vma, NULL, addr,\n\t\t\t\t\t (void *) addr, buf, len);\n\t\telse if (!write && vma->vm_flags & VM_MAYREAD)\n\t\t\tcopy_from_user_page(vma, NULL, addr,\n\t\t\t\t\t    buf, (void *) addr, len);\n\t\telse\n\t\t\tlen = 0;\n\t} else {\n\t\tlen = 0;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\n\treturn len;\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_mm",
          "args": [
            "tsk"
          ],
          "line": 1835
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len,\n\t\tunsigned int gup_flags)\n{\n\tstruct mm_struct *mm;\n\n\tif (addr + len < addr)\n\t\treturn 0;\n\n\tmm = get_task_mm(tsk);\n\tif (!mm)\n\t\treturn 0;\n\n\tlen = __access_remote_vm(tsk, mm, addr, buf, len, gup_flags);\n\n\tmmput(mm);\n\treturn len;\n}"
  },
  {
    "function_name": "access_remote_vm",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1817-1821",
    "snippet": "int access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags)\n{\n\treturn __access_remote_vm(NULL, mm, addr, buf, len, gup_flags);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__access_remote_vm",
          "args": [
            "NULL",
            "mm",
            "addr",
            "buf",
            "len",
            "gup_flags"
          ],
          "line": 1820
        },
        "resolved": true,
        "details": {
          "function_name": "__access_remote_vm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1774-1805",
          "snippet": "int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long addr, void *buf, int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tdown_read(&mm->mmap_sem);\n\n\t/* the access must start within one of the target process's mappings */\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t/* don't overrun this mapping */\n\t\tif (addr + len >= vma->vm_end)\n\t\t\tlen = vma->vm_end - addr;\n\n\t\t/* only read or write mappings where it is permitted */\n\t\tif (write && vma->vm_flags & VM_MAYWRITE)\n\t\t\tcopy_to_user_page(vma, NULL, addr,\n\t\t\t\t\t (void *) addr, buf, len);\n\t\telse if (!write && vma->vm_flags & VM_MAYREAD)\n\t\t\tcopy_from_user_page(vma, NULL, addr,\n\t\t\t\t\t    buf, (void *) addr, len);\n\t\telse\n\t\t\tlen = 0;\n\t} else {\n\t\tlen = 0;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\n\treturn len;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long addr, void *buf, int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tdown_read(&mm->mmap_sem);\n\n\t/* the access must start within one of the target process's mappings */\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t/* don't overrun this mapping */\n\t\tif (addr + len >= vma->vm_end)\n\t\t\tlen = vma->vm_end - addr;\n\n\t\t/* only read or write mappings where it is permitted */\n\t\tif (write && vma->vm_flags & VM_MAYWRITE)\n\t\t\tcopy_to_user_page(vma, NULL, addr,\n\t\t\t\t\t (void *) addr, buf, len);\n\t\telse if (!write && vma->vm_flags & VM_MAYREAD)\n\t\t\tcopy_from_user_page(vma, NULL, addr,\n\t\t\t\t\t    buf, (void *) addr, len);\n\t\telse\n\t\t\tlen = 0;\n\t} else {\n\t\tlen = 0;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\n\treturn len;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags)\n{\n\treturn __access_remote_vm(NULL, mm, addr, buf, len, gup_flags);\n}"
  },
  {
    "function_name": "__access_remote_vm",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1774-1805",
    "snippet": "int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long addr, void *buf, int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tdown_read(&mm->mmap_sem);\n\n\t/* the access must start within one of the target process's mappings */\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t/* don't overrun this mapping */\n\t\tif (addr + len >= vma->vm_end)\n\t\t\tlen = vma->vm_end - addr;\n\n\t\t/* only read or write mappings where it is permitted */\n\t\tif (write && vma->vm_flags & VM_MAYWRITE)\n\t\t\tcopy_to_user_page(vma, NULL, addr,\n\t\t\t\t\t (void *) addr, buf, len);\n\t\telse if (!write && vma->vm_flags & VM_MAYREAD)\n\t\t\tcopy_from_user_page(vma, NULL, addr,\n\t\t\t\t\t    buf, (void *) addr, len);\n\t\telse\n\t\t\tlen = 0;\n\t} else {\n\t\tlen = 0;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\n\treturn len;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "up_read",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 1802
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "copy_from_user_page",
          "args": [
            "vma",
            "NULL",
            "addr",
            "buf",
            "(void *) addr",
            "len"
          ],
          "line": 1794
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "copy_to_user_page",
          "args": [
            "vma",
            "NULL",
            "addr",
            "(void *) addr",
            "buf",
            "len"
          ],
          "line": 1791
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_vma",
          "args": [
            "mm",
            "addr"
          ],
          "line": 1783
        },
        "resolved": true,
        "details": {
          "function_name": "find_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "775-796",
          "snippet": "struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "down_read",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 1780
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long addr, void *buf, int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tdown_read(&mm->mmap_sem);\n\n\t/* the access must start within one of the target process's mappings */\n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t/* don't overrun this mapping */\n\t\tif (addr + len >= vma->vm_end)\n\t\t\tlen = vma->vm_end - addr;\n\n\t\t/* only read or write mappings where it is permitted */\n\t\tif (write && vma->vm_flags & VM_MAYWRITE)\n\t\t\tcopy_to_user_page(vma, NULL, addr,\n\t\t\t\t\t (void *) addr, buf, len);\n\t\telse if (!write && vma->vm_flags & VM_MAYREAD)\n\t\t\tcopy_from_user_page(vma, NULL, addr,\n\t\t\t\t\t    buf, (void *) addr, len);\n\t\telse\n\t\t\tlen = 0;\n\t} else {\n\t\tlen = 0;\n\t}\n\n\tup_read(&mm->mmap_sem);\n\n\treturn len;\n}"
  },
  {
    "function_name": "filemap_map_pages",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1767-1771",
    "snippet": "void filemap_map_pages(struct vm_fault *vmf,\n\t\tpgoff_t start_pgoff, pgoff_t end_pgoff)\n{\n\tBUG();\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 1770
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid filemap_map_pages(struct vm_fault *vmf,\n\t\tpgoff_t start_pgoff, pgoff_t end_pgoff)\n{\n\tBUG();\n}"
  },
  {
    "function_name": "filemap_fault",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1760-1764",
    "snippet": "vm_fault_t filemap_fault(struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 1762
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvm_fault_t filemap_fault(struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}"
  },
  {
    "function_name": "arch_get_unmapped_area",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1754-1758",
    "snippet": "unsigned long arch_get_unmapped_area(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long pgoff, unsigned long flags)\n{\n\treturn -ENOMEM;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nunsigned long arch_get_unmapped_area(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long pgoff, unsigned long flags)\n{\n\treturn -ENOMEM;\n}"
  },
  {
    "function_name": "remap_vmalloc_range",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1739-1751",
    "snippet": "int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,\n\t\t\tunsigned long pgoff)\n{\n\tunsigned int size = vma->vm_end - vma->vm_start;\n\n\tif (!(vma->vm_flags & VM_USERMAP))\n\t\treturn -EINVAL;\n\n\tvma->vm_start = (unsigned long)(addr + (pgoff << PAGE_SHIFT));\n\tvma->vm_end = vma->vm_start + size;\n\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint remap_vmalloc_range(struct vm_area_struct *vma, void *addr,\n\t\t\tunsigned long pgoff)\n{\n\tunsigned int size = vma->vm_end - vma->vm_start;\n\n\tif (!(vma->vm_flags & VM_USERMAP))\n\t\treturn -EINVAL;\n\n\tvma->vm_start = (unsigned long)(addr + (pgoff << PAGE_SHIFT));\n\tvma->vm_end = vma->vm_start + size;\n\n\treturn 0;\n}"
  },
  {
    "function_name": "vm_iomap_memory",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1729-1736",
    "snippet": "int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)\n{\n\tunsigned long pfn = start >> PAGE_SHIFT;\n\tunsigned long vm_len = vma->vm_end - vma->vm_start;\n\n\tpfn += vma->vm_pgoff;\n\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "io_remap_pfn_range",
          "args": [
            "vma",
            "vma->vm_start",
            "pfn",
            "vm_len",
            "vma->vm_page_prot"
          ],
          "line": 1735
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)\n{\n\tunsigned long pfn = start >> PAGE_SHIFT;\n\tunsigned long vm_len = vma->vm_end - vma->vm_start;\n\n\tpfn += vma->vm_pgoff;\n\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);\n}"
  },
  {
    "function_name": "remap_pfn_range",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1718-1726",
    "snippet": "int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,\n\t\tunsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tif (addr != (pfn << PAGE_SHIFT))\n\t\treturn -EINVAL;\n\n\tvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,\n\t\tunsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tif (addr != (pfn << PAGE_SHIFT))\n\t\treturn -EINVAL;\n\n\tvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;\n\treturn 0;\n}"
  },
  {
    "function_name": "follow_page",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1712-1716",
    "snippet": "struct page *follow_page(struct vm_area_struct *vma, unsigned long address,\n\t\t\t unsigned int foll_flags)\n{\n\treturn NULL;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct page *follow_page(struct vm_area_struct *vma, unsigned long address,\n\t\t\t unsigned int foll_flags)\n{\n\treturn NULL;\n}"
  },
  {
    "function_name": "do_mremap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1664-1698",
    "snippet": "static unsigned long do_mremap(unsigned long addr,\n\t\t\tunsigned long old_len, unsigned long new_len,\n\t\t\tunsigned long flags, unsigned long new_addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* insanity checks first */\n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\tif (old_len == 0 || new_len == 0)\n\t\treturn (unsigned long) -EINVAL;\n\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\tif (flags & MREMAP_FIXED && new_addr != addr)\n\t\treturn (unsigned long) -EINVAL;\n\n\tvma = find_vma_exact(current->mm, addr, old_len);\n\tif (!vma)\n\t\treturn (unsigned long) -EINVAL;\n\n\tif (vma->vm_end != vma->vm_start + old_len)\n\t\treturn (unsigned long) -EFAULT;\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn (unsigned long) -EPERM;\n\n\tif (new_len > vma->vm_region->vm_end - vma->vm_region->vm_start)\n\t\treturn (unsigned long) -ENOMEM;\n\n\t/* all checks complete - do it */\n\tvma->vm_end = vma->vm_start + new_len;\n\treturn vma->vm_start;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "find_vma_exact",
          "args": [
            "current->mm",
            "addr",
            "old_len"
          ],
          "line": 1682
        },
        "resolved": true,
        "details": {
          "function_name": "find_vma_exact",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "821-847",
          "snippet": "static struct vm_area_struct *find_vma_exact(struct mm_struct *mm,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     unsigned long len)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end = addr + len;\n\n\t/* check the cache first */\n\tvma = vmacache_find_exact(mm, addr, end);\n\tif (vma)\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start < addr)\n\t\t\tcontinue;\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end == end) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct vm_area_struct *find_vma_exact(struct mm_struct *mm,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     unsigned long len)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end = addr + len;\n\n\t/* check the cache first */\n\tvma = vmacache_find_exact(mm, addr, end);\n\tif (vma)\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start < addr)\n\t\t\tcontinue;\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end == end) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "offset_in_page",
          "args": [
            "addr"
          ],
          "line": 1676
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "PAGE_ALIGN",
          "args": [
            "new_len"
          ],
          "line": 1672
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "PAGE_ALIGN",
          "args": [
            "old_len"
          ],
          "line": 1671
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic unsigned long do_mremap(unsigned long addr,\n\t\t\tunsigned long old_len, unsigned long new_len,\n\t\t\tunsigned long flags, unsigned long new_addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* insanity checks first */\n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\tif (old_len == 0 || new_len == 0)\n\t\treturn (unsigned long) -EINVAL;\n\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\tif (flags & MREMAP_FIXED && new_addr != addr)\n\t\treturn (unsigned long) -EINVAL;\n\n\tvma = find_vma_exact(current->mm, addr, old_len);\n\tif (!vma)\n\t\treturn (unsigned long) -EINVAL;\n\n\tif (vma->vm_end != vma->vm_start + old_len)\n\t\treturn (unsigned long) -EFAULT;\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn (unsigned long) -EPERM;\n\n\tif (new_len > vma->vm_region->vm_end - vma->vm_region->vm_start)\n\t\treturn (unsigned long) -ENOMEM;\n\n\t/* all checks complete - do it */\n\tvma->vm_end = vma->vm_start + new_len;\n\treturn vma->vm_start;\n}"
  },
  {
    "function_name": "vm_brk",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1649-1652",
    "snippet": "int vm_brk(unsigned long addr, unsigned long len)\n{\n\treturn -ENOMEM;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint vm_brk(unsigned long addr, unsigned long len)\n{\n\treturn -ENOMEM;\n}"
  },
  {
    "function_name": "exit_mmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1632-1647",
    "snippet": "void exit_mmap(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\n\tif (!mm)\n\t\treturn;\n\n\tmm->total_vm = 0;\n\n\twhile ((vma = mm->mmap)) {\n\t\tmm->mmap = vma->vm_next;\n\t\tdelete_vma_from_mm(vma);\n\t\tdelete_vma(mm, vma);\n\t\tcond_resched();\n\t}\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cond_resched",
          "args": [],
          "line": 1645
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "delete_vma",
          "args": [
            "mm",
            "vma"
          ],
          "line": 1644
        },
        "resolved": true,
        "details": {
          "function_name": "delete_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "761-769",
          "snippet": "static void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tput_nommu_region(vma->vm_region);\n\tvm_area_free(vma);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tput_nommu_region(vma->vm_region);\n\tvm_area_free(vma);\n}"
        }
      },
      {
        "call_info": {
          "callee": "delete_vma_from_mm",
          "args": [
            "vma"
          ],
          "line": 1643
        },
        "resolved": true,
        "details": {
          "function_name": "delete_vma_from_mm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "719-756",
          "snippet": "static void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid exit_mmap(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\n\tif (!mm)\n\t\treturn;\n\n\tmm->total_vm = 0;\n\n\twhile ((vma = mm->mmap)) {\n\t\tmm->mmap = vma->vm_next;\n\t\tdelete_vma_from_mm(vma);\n\t\tdelete_vma(mm, vma);\n\t\tcond_resched();\n\t}\n}"
  },
  {
    "function_name": "vm_munmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1612-1621",
    "snippet": "int vm_munmap(unsigned long addr, size_t len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tdown_write(&mm->mmap_sem);\n\tret = do_munmap(mm, addr, len, NULL);\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 1619
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "do_munmap",
          "args": [
            "mm",
            "addr",
            "len",
            "NULL"
          ],
          "line": 1618
        },
        "resolved": true,
        "details": {
          "function_name": "do_munmap",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1552-1609",
          "snippet": "int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end;\n\tint ret;\n\n\tlen = PAGE_ALIGN(len);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\tend = start + len;\n\n\t/* find the first potentially overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\tstatic int limit;\n\t\tif (limit < 5) {\n\t\t\tpr_warn(\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\",\n\t\t\t\t\tcurrent->pid, current->comm,\n\t\t\t\t\tstart, start + len - 1);\n\t\t\tlimit++;\n\t\t}\n\t\treturn -EINVAL;\n\t}\n\n\t/* we're allowed to split an anonymous VMA but not a file-backed one */\n\tif (vma->vm_file) {\n\t\tdo {\n\t\t\tif (start > vma->vm_start)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (end == vma->vm_end)\n\t\t\t\tgoto erase_whole_vma;\n\t\t\tvma = vma->vm_next;\n\t\t} while (vma);\n\t\treturn -EINVAL;\n\t} else {\n\t\t/* the chunk must be a subset of the VMA found */\n\t\tif (start == vma->vm_start && end == vma->vm_end)\n\t\t\tgoto erase_whole_vma;\n\t\tif (start < vma->vm_start || end > vma->vm_end)\n\t\t\treturn -EINVAL;\n\t\tif (offset_in_page(start))\n\t\t\treturn -EINVAL;\n\t\tif (end != vma->vm_end && offset_in_page(end))\n\t\t\treturn -EINVAL;\n\t\tif (start != vma->vm_start && end != vma->vm_end) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\t\treturn shrink_vma(mm, vma, start, end);\n\t}\n\nerase_whole_vma:\n\tdelete_vma_from_mm(vma);\n\tdelete_vma(mm, vma);\n\treturn 0;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end;\n\tint ret;\n\n\tlen = PAGE_ALIGN(len);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\tend = start + len;\n\n\t/* find the first potentially overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\tstatic int limit;\n\t\tif (limit < 5) {\n\t\t\tpr_warn(\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\",\n\t\t\t\t\tcurrent->pid, current->comm,\n\t\t\t\t\tstart, start + len - 1);\n\t\t\tlimit++;\n\t\t}\n\t\treturn -EINVAL;\n\t}\n\n\t/* we're allowed to split an anonymous VMA but not a file-backed one */\n\tif (vma->vm_file) {\n\t\tdo {\n\t\t\tif (start > vma->vm_start)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (end == vma->vm_end)\n\t\t\t\tgoto erase_whole_vma;\n\t\t\tvma = vma->vm_next;\n\t\t} while (vma);\n\t\treturn -EINVAL;\n\t} else {\n\t\t/* the chunk must be a subset of the VMA found */\n\t\tif (start == vma->vm_start && end == vma->vm_end)\n\t\t\tgoto erase_whole_vma;\n\t\tif (start < vma->vm_start || end > vma->vm_end)\n\t\t\treturn -EINVAL;\n\t\tif (offset_in_page(start))\n\t\t\treturn -EINVAL;\n\t\tif (end != vma->vm_end && offset_in_page(end))\n\t\t\treturn -EINVAL;\n\t\tif (start != vma->vm_start && end != vma->vm_end) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\t\treturn shrink_vma(mm, vma, start, end);\n\t}\n\nerase_whole_vma:\n\tdelete_vma_from_mm(vma);\n\tdelete_vma(mm, vma);\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 1617
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint vm_munmap(unsigned long addr, size_t len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tdown_write(&mm->mmap_sem);\n\tret = do_munmap(mm, addr, len, NULL);\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}"
  },
  {
    "function_name": "do_munmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1552-1609",
    "snippet": "int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end;\n\tint ret;\n\n\tlen = PAGE_ALIGN(len);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\tend = start + len;\n\n\t/* find the first potentially overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\tstatic int limit;\n\t\tif (limit < 5) {\n\t\t\tpr_warn(\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\",\n\t\t\t\t\tcurrent->pid, current->comm,\n\t\t\t\t\tstart, start + len - 1);\n\t\t\tlimit++;\n\t\t}\n\t\treturn -EINVAL;\n\t}\n\n\t/* we're allowed to split an anonymous VMA but not a file-backed one */\n\tif (vma->vm_file) {\n\t\tdo {\n\t\t\tif (start > vma->vm_start)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (end == vma->vm_end)\n\t\t\t\tgoto erase_whole_vma;\n\t\t\tvma = vma->vm_next;\n\t\t} while (vma);\n\t\treturn -EINVAL;\n\t} else {\n\t\t/* the chunk must be a subset of the VMA found */\n\t\tif (start == vma->vm_start && end == vma->vm_end)\n\t\t\tgoto erase_whole_vma;\n\t\tif (start < vma->vm_start || end > vma->vm_end)\n\t\t\treturn -EINVAL;\n\t\tif (offset_in_page(start))\n\t\t\treturn -EINVAL;\n\t\tif (end != vma->vm_end && offset_in_page(end))\n\t\t\treturn -EINVAL;\n\t\tif (start != vma->vm_start && end != vma->vm_end) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\t\treturn shrink_vma(mm, vma, start, end);\n\t}\n\nerase_whole_vma:\n\tdelete_vma_from_mm(vma);\n\tdelete_vma(mm, vma);\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "delete_vma",
          "args": [
            "mm",
            "vma"
          ],
          "line": 1607
        },
        "resolved": true,
        "details": {
          "function_name": "delete_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "761-769",
          "snippet": "static void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tput_nommu_region(vma->vm_region);\n\tvm_area_free(vma);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tput_nommu_region(vma->vm_region);\n\tvm_area_free(vma);\n}"
        }
      },
      {
        "call_info": {
          "callee": "delete_vma_from_mm",
          "args": [
            "vma"
          ],
          "line": 1606
        },
        "resolved": true,
        "details": {
          "function_name": "delete_vma_from_mm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "719-756",
          "snippet": "static void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}"
        }
      },
      {
        "call_info": {
          "callee": "shrink_vma",
          "args": [
            "mm",
            "vma",
            "start",
            "end"
          ],
          "line": 1602
        },
        "resolved": true,
        "details": {
          "function_name": "shrink_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1513-1545",
          "snippet": "static int shrink_vma(struct mm_struct *mm,\n\t\t      struct vm_area_struct *vma,\n\t\t      unsigned long from, unsigned long to)\n{\n\tstruct vm_region *region;\n\n\t/* adjust the VMA's pointers, which may reposition it in the MM's tree\n\t * and list */\n\tdelete_vma_from_mm(vma);\n\tif (from > vma->vm_start)\n\t\tvma->vm_end = from;\n\telse\n\t\tvma->vm_start = to;\n\tadd_vma_to_mm(mm, vma);\n\n\t/* cut the backing region down to size */\n\tregion = vma->vm_region;\n\tBUG_ON(region->vm_usage != 1);\n\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(region);\n\tif (from > region->vm_start) {\n\t\tto = region->vm_top;\n\t\tregion->vm_top = region->vm_end = from;\n\t} else {\n\t\tregion->vm_start = to;\n\t}\n\tadd_nommu_region(region);\n\tup_write(&nommu_region_sem);\n\n\tfree_page_series(from, to);\n\treturn 0;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int shrink_vma(struct mm_struct *mm,\n\t\t      struct vm_area_struct *vma,\n\t\t      unsigned long from, unsigned long to)\n{\n\tstruct vm_region *region;\n\n\t/* adjust the VMA's pointers, which may reposition it in the MM's tree\n\t * and list */\n\tdelete_vma_from_mm(vma);\n\tif (from > vma->vm_start)\n\t\tvma->vm_end = from;\n\telse\n\t\tvma->vm_start = to;\n\tadd_vma_to_mm(mm, vma);\n\n\t/* cut the backing region down to size */\n\tregion = vma->vm_region;\n\tBUG_ON(region->vm_usage != 1);\n\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(region);\n\tif (from > region->vm_start) {\n\t\tto = region->vm_top;\n\t\tregion->vm_top = region->vm_end = from;\n\t} else {\n\t\tregion->vm_start = to;\n\t}\n\tadd_nommu_region(region);\n\tup_write(&nommu_region_sem);\n\n\tfree_page_series(from, to);\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "split_vma",
          "args": [
            "mm",
            "vma",
            "start",
            "1"
          ],
          "line": 1598
        },
        "resolved": true,
        "details": {
          "function_name": "split_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1450-1507",
          "snippet": "int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tstruct vm_area_struct *new;\n\tstruct vm_region *region;\n\tunsigned long npages;\n\n\t/* we're only permitted to split anonymous regions (these should have\n\t * only a single usage on the region) */\n\tif (vma->vm_file)\n\t\treturn -ENOMEM;\n\n\tif (mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tregion = kmem_cache_alloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tnew = vm_area_dup(vma);\n\tif (!new) {\n\t\tkmem_cache_free(vm_region_jar, region);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* most fields are the same, copy all, and then fixup */\n\t*region = *vma->vm_region;\n\tnew->vm_region = region;\n\n\tnpages = (addr - vma->vm_start) >> PAGE_SHIFT;\n\n\tif (new_below) {\n\t\tregion->vm_top = region->vm_end = new->vm_end = addr;\n\t} else {\n\t\tregion->vm_start = new->vm_start = addr;\n\t\tregion->vm_pgoff = new->vm_pgoff += npages;\n\t}\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tdelete_vma_from_mm(vma);\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(vma->vm_region);\n\tif (new_below) {\n\t\tvma->vm_region->vm_start = vma->vm_start = addr;\n\t\tvma->vm_region->vm_pgoff = vma->vm_pgoff += npages;\n\t} else {\n\t\tvma->vm_region->vm_end = vma->vm_end = addr;\n\t\tvma->vm_region->vm_top = addr;\n\t}\n\tadd_nommu_region(vma->vm_region);\n\tadd_nommu_region(new->vm_region);\n\tup_write(&nommu_region_sem);\n\tadd_vma_to_mm(mm, vma);\n\tadd_vma_to_mm(mm, new);\n\treturn 0;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static struct kmem_cache *vm_region_jar;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct kmem_cache *vm_region_jar;\n\nint split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tstruct vm_area_struct *new;\n\tstruct vm_region *region;\n\tunsigned long npages;\n\n\t/* we're only permitted to split anonymous regions (these should have\n\t * only a single usage on the region) */\n\tif (vma->vm_file)\n\t\treturn -ENOMEM;\n\n\tif (mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tregion = kmem_cache_alloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tnew = vm_area_dup(vma);\n\tif (!new) {\n\t\tkmem_cache_free(vm_region_jar, region);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* most fields are the same, copy all, and then fixup */\n\t*region = *vma->vm_region;\n\tnew->vm_region = region;\n\n\tnpages = (addr - vma->vm_start) >> PAGE_SHIFT;\n\n\tif (new_below) {\n\t\tregion->vm_top = region->vm_end = new->vm_end = addr;\n\t} else {\n\t\tregion->vm_start = new->vm_start = addr;\n\t\tregion->vm_pgoff = new->vm_pgoff += npages;\n\t}\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tdelete_vma_from_mm(vma);\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(vma->vm_region);\n\tif (new_below) {\n\t\tvma->vm_region->vm_start = vma->vm_start = addr;\n\t\tvma->vm_region->vm_pgoff = vma->vm_pgoff += npages;\n\t} else {\n\t\tvma->vm_region->vm_end = vma->vm_end = addr;\n\t\tvma->vm_region->vm_top = addr;\n\t}\n\tadd_nommu_region(vma->vm_region);\n\tadd_nommu_region(new->vm_region);\n\tup_write(&nommu_region_sem);\n\tadd_vma_to_mm(mm, vma);\n\tadd_vma_to_mm(mm, new);\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "offset_in_page",
          "args": [
            "end"
          ],
          "line": 1595
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "offset_in_page",
          "args": [
            "start"
          ],
          "line": 1593
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\"",
            "current->pid",
            "current->comm",
            "start",
            "start + len - 1"
          ],
          "line": 1569
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_vma",
          "args": [
            "mm",
            "start"
          ],
          "line": 1565
        },
        "resolved": true,
        "details": {
          "function_name": "find_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "775-796",
          "snippet": "struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "PAGE_ALIGN",
          "args": [
            "len"
          ],
          "line": 1558
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end;\n\tint ret;\n\n\tlen = PAGE_ALIGN(len);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\tend = start + len;\n\n\t/* find the first potentially overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\tstatic int limit;\n\t\tif (limit < 5) {\n\t\t\tpr_warn(\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\",\n\t\t\t\t\tcurrent->pid, current->comm,\n\t\t\t\t\tstart, start + len - 1);\n\t\t\tlimit++;\n\t\t}\n\t\treturn -EINVAL;\n\t}\n\n\t/* we're allowed to split an anonymous VMA but not a file-backed one */\n\tif (vma->vm_file) {\n\t\tdo {\n\t\t\tif (start > vma->vm_start)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (end == vma->vm_end)\n\t\t\t\tgoto erase_whole_vma;\n\t\t\tvma = vma->vm_next;\n\t\t} while (vma);\n\t\treturn -EINVAL;\n\t} else {\n\t\t/* the chunk must be a subset of the VMA found */\n\t\tif (start == vma->vm_start && end == vma->vm_end)\n\t\t\tgoto erase_whole_vma;\n\t\tif (start < vma->vm_start || end > vma->vm_end)\n\t\t\treturn -EINVAL;\n\t\tif (offset_in_page(start))\n\t\t\treturn -EINVAL;\n\t\tif (end != vma->vm_end && offset_in_page(end))\n\t\t\treturn -EINVAL;\n\t\tif (start != vma->vm_start && end != vma->vm_end) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\t\treturn shrink_vma(mm, vma, start, end);\n\t}\n\nerase_whole_vma:\n\tdelete_vma_from_mm(vma);\n\tdelete_vma(mm, vma);\n\treturn 0;\n}"
  },
  {
    "function_name": "shrink_vma",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1513-1545",
    "snippet": "static int shrink_vma(struct mm_struct *mm,\n\t\t      struct vm_area_struct *vma,\n\t\t      unsigned long from, unsigned long to)\n{\n\tstruct vm_region *region;\n\n\t/* adjust the VMA's pointers, which may reposition it in the MM's tree\n\t * and list */\n\tdelete_vma_from_mm(vma);\n\tif (from > vma->vm_start)\n\t\tvma->vm_end = from;\n\telse\n\t\tvma->vm_start = to;\n\tadd_vma_to_mm(mm, vma);\n\n\t/* cut the backing region down to size */\n\tregion = vma->vm_region;\n\tBUG_ON(region->vm_usage != 1);\n\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(region);\n\tif (from > region->vm_start) {\n\t\tto = region->vm_top;\n\t\tregion->vm_top = region->vm_end = from;\n\t} else {\n\t\tregion->vm_start = to;\n\t}\n\tadd_nommu_region(region);\n\tup_write(&nommu_region_sem);\n\n\tfree_page_series(from, to);\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "free_page_series",
          "args": [
            "from",
            "to"
          ],
          "line": 1543
        },
        "resolved": true,
        "details": {
          "function_name": "free_page_series",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "601-609",
          "snippet": "static void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "atomic_long_t mmap_pages_allocated;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\natomic_long_t mmap_pages_allocated;\n\nstatic void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1541
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "add_nommu_region",
          "args": [
            "region"
          ],
          "line": 1540
        },
        "resolved": true,
        "details": {
          "function_name": "add_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "558-584",
          "snippet": "static void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "struct rb_root nommu_region_tree = RB_ROOT;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}"
        }
      },
      {
        "call_info": {
          "callee": "delete_nommu_region",
          "args": [
            "region"
          ],
          "line": 1533
        },
        "resolved": true,
        "details": {
          "function_name": "delete_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "589-596",
          "snippet": "static void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "struct rb_root nommu_region_tree = RB_ROOT;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}"
        }
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1532
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "region->vm_usage != 1"
          ],
          "line": 1530
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "add_vma_to_mm",
          "args": [
            "mm",
            "vma"
          ],
          "line": 1526
        },
        "resolved": true,
        "details": {
          "function_name": "add_vma_to_mm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "655-714",
          "snippet": "static void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}"
        }
      },
      {
        "call_info": {
          "callee": "delete_vma_from_mm",
          "args": [
            "vma"
          ],
          "line": 1521
        },
        "resolved": true,
        "details": {
          "function_name": "delete_vma_from_mm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "719-756",
          "snippet": "static void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int shrink_vma(struct mm_struct *mm,\n\t\t      struct vm_area_struct *vma,\n\t\t      unsigned long from, unsigned long to)\n{\n\tstruct vm_region *region;\n\n\t/* adjust the VMA's pointers, which may reposition it in the MM's tree\n\t * and list */\n\tdelete_vma_from_mm(vma);\n\tif (from > vma->vm_start)\n\t\tvma->vm_end = from;\n\telse\n\t\tvma->vm_start = to;\n\tadd_vma_to_mm(mm, vma);\n\n\t/* cut the backing region down to size */\n\tregion = vma->vm_region;\n\tBUG_ON(region->vm_usage != 1);\n\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(region);\n\tif (from > region->vm_start) {\n\t\tto = region->vm_top;\n\t\tregion->vm_top = region->vm_end = from;\n\t} else {\n\t\tregion->vm_start = to;\n\t}\n\tadd_nommu_region(region);\n\tup_write(&nommu_region_sem);\n\n\tfree_page_series(from, to);\n\treturn 0;\n}"
  },
  {
    "function_name": "split_vma",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1450-1507",
    "snippet": "int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tstruct vm_area_struct *new;\n\tstruct vm_region *region;\n\tunsigned long npages;\n\n\t/* we're only permitted to split anonymous regions (these should have\n\t * only a single usage on the region) */\n\tif (vma->vm_file)\n\t\treturn -ENOMEM;\n\n\tif (mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tregion = kmem_cache_alloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tnew = vm_area_dup(vma);\n\tif (!new) {\n\t\tkmem_cache_free(vm_region_jar, region);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* most fields are the same, copy all, and then fixup */\n\t*region = *vma->vm_region;\n\tnew->vm_region = region;\n\n\tnpages = (addr - vma->vm_start) >> PAGE_SHIFT;\n\n\tif (new_below) {\n\t\tregion->vm_top = region->vm_end = new->vm_end = addr;\n\t} else {\n\t\tregion->vm_start = new->vm_start = addr;\n\t\tregion->vm_pgoff = new->vm_pgoff += npages;\n\t}\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tdelete_vma_from_mm(vma);\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(vma->vm_region);\n\tif (new_below) {\n\t\tvma->vm_region->vm_start = vma->vm_start = addr;\n\t\tvma->vm_region->vm_pgoff = vma->vm_pgoff += npages;\n\t} else {\n\t\tvma->vm_region->vm_end = vma->vm_end = addr;\n\t\tvma->vm_region->vm_top = addr;\n\t}\n\tadd_nommu_region(vma->vm_region);\n\tadd_nommu_region(new->vm_region);\n\tup_write(&nommu_region_sem);\n\tadd_vma_to_mm(mm, vma);\n\tadd_vma_to_mm(mm, new);\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static struct kmem_cache *vm_region_jar;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "add_vma_to_mm",
          "args": [
            "mm",
            "new"
          ],
          "line": 1505
        },
        "resolved": true,
        "details": {
          "function_name": "add_vma_to_mm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "655-714",
          "snippet": "static void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}"
        }
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1503
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "add_nommu_region",
          "args": [
            "new->vm_region"
          ],
          "line": 1502
        },
        "resolved": true,
        "details": {
          "function_name": "add_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "558-584",
          "snippet": "static void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "struct rb_root nommu_region_tree = RB_ROOT;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}"
        }
      },
      {
        "call_info": {
          "callee": "delete_nommu_region",
          "args": [
            "vma->vm_region"
          ],
          "line": 1493
        },
        "resolved": true,
        "details": {
          "function_name": "delete_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "589-596",
          "snippet": "static void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "struct rb_root nommu_region_tree = RB_ROOT;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}"
        }
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1492
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "delete_vma_from_mm",
          "args": [
            "vma"
          ],
          "line": 1491
        },
        "resolved": true,
        "details": {
          "function_name": "delete_vma_from_mm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "719-756",
          "snippet": "static void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}"
        }
      },
      {
        "call_info": {
          "callee": "new->vm_ops->open",
          "args": [
            "new"
          ],
          "line": 1489
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kmem_cache_free",
          "args": [
            "vm_region_jar",
            "region"
          ],
          "line": 1471
        },
        "resolved": true,
        "details": {
          "function_name": "kmem_cache_free",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/slab.c",
          "lines": "3749-3764",
          "snippet": "void kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}",
          "includes": [
            "#include\t\"slab.h\"",
            "#include\t\"internal.h\"",
            "#include <trace/events/kmem.h>",
            "#include\t<asm/page.h>",
            "#include\t<asm/tlbflush.h>",
            "#include\t<asm/cacheflush.h>",
            "#include\t<net/sock.h>",
            "#include\t<linux/sched/task_stack.h>",
            "#include\t<linux/prefetch.h>",
            "#include\t<linux/memory.h>",
            "#include\t<linux/debugobjects.h>",
            "#include\t<linux/reciprocal_div.h>",
            "#include\t<linux/rtmutex.h>",
            "#include\t<linux/fault-inject.h>",
            "#include\t<linux/mutex.h>",
            "#include\t<linux/mempolicy.h>",
            "#include\t<linux/kmemleak.h>",
            "#include\t<linux/nodemask.h>",
            "#include\t<linux/uaccess.h>",
            "#include\t<linux/string.h>",
            "#include\t<linux/rcupdate.h>",
            "#include\t<linux/module.h>",
            "#include\t<linux/sysctl.h>",
            "#include\t<linux/cpu.h>",
            "#include\t<linux/kallsyms.h>",
            "#include\t<linux/notifier.h>",
            "#include\t<linux/seq_file.h>",
            "#include\t<linux/proc_fs.h>",
            "#include\t<linux/cpuset.h>",
            "#include\t<linux/compiler.h>",
            "#include\t<linux/init.h>",
            "#include\t<linux/interrupt.h>",
            "#include\t<linux/cache.h>",
            "#include\t<linux/swap.h>",
            "#include\t<linux/poison.h>",
            "#include\t<linux/mm.h>",
            "#include\t<linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static noinline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include\t\"slab.h\"\n#include\t\"internal.h\"\n#include <trace/events/kmem.h>\n#include\t<asm/page.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/cacheflush.h>\n#include\t<net/sock.h>\n#include\t<linux/sched/task_stack.h>\n#include\t<linux/prefetch.h>\n#include\t<linux/memory.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/mutex.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/string.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/module.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/cpu.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/notifier.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/compiler.h>\n#include\t<linux/init.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/cache.h>\n#include\t<linux/swap.h>\n#include\t<linux/poison.h>\n#include\t<linux/mm.h>\n#include\t<linux/slab.h>\n\nstatic noinline struct;\n\nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}"
        }
      },
      {
        "call_info": {
          "callee": "vm_area_dup",
          "args": [
            "vma"
          ],
          "line": 1469
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kmem_cache_alloc",
          "args": [
            "vm_region_jar",
            "GFP_KERNEL"
          ],
          "line": 1465
        },
        "resolved": true,
        "details": {
          "function_name": "kmem_cache_alloc",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/slab.c",
          "lines": "3550-3559",
          "snippet": "void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tvoid *ret = slab_alloc(cachep, flags, _RET_IP_);\n\n\tkasan_slab_alloc(cachep, ret, flags);\n\ttrace_kmem_cache_alloc(_RET_IP_, ret,\n\t\t\t       cachep->object_size, cachep->size, flags);\n\n\treturn ret;\n}",
          "includes": [
            "#include\t\"slab.h\"",
            "#include\t\"internal.h\"",
            "#include <trace/events/kmem.h>",
            "#include\t<asm/page.h>",
            "#include\t<asm/tlbflush.h>",
            "#include\t<asm/cacheflush.h>",
            "#include\t<net/sock.h>",
            "#include\t<linux/sched/task_stack.h>",
            "#include\t<linux/prefetch.h>",
            "#include\t<linux/memory.h>",
            "#include\t<linux/debugobjects.h>",
            "#include\t<linux/reciprocal_div.h>",
            "#include\t<linux/rtmutex.h>",
            "#include\t<linux/fault-inject.h>",
            "#include\t<linux/mutex.h>",
            "#include\t<linux/mempolicy.h>",
            "#include\t<linux/kmemleak.h>",
            "#include\t<linux/nodemask.h>",
            "#include\t<linux/uaccess.h>",
            "#include\t<linux/string.h>",
            "#include\t<linux/rcupdate.h>",
            "#include\t<linux/module.h>",
            "#include\t<linux/sysctl.h>",
            "#include\t<linux/cpu.h>",
            "#include\t<linux/kallsyms.h>",
            "#include\t<linux/notifier.h>",
            "#include\t<linux/seq_file.h>",
            "#include\t<linux/proc_fs.h>",
            "#include\t<linux/cpuset.h>",
            "#include\t<linux/compiler.h>",
            "#include\t<linux/init.h>",
            "#include\t<linux/interrupt.h>",
            "#include\t<linux/cache.h>",
            "#include\t<linux/swap.h>",
            "#include\t<linux/poison.h>",
            "#include\t<linux/mm.h>",
            "#include\t<linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static noinline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include\t\"slab.h\"\n#include\t\"internal.h\"\n#include <trace/events/kmem.h>\n#include\t<asm/page.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/cacheflush.h>\n#include\t<net/sock.h>\n#include\t<linux/sched/task_stack.h>\n#include\t<linux/prefetch.h>\n#include\t<linux/memory.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/mutex.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/string.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/module.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/cpu.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/notifier.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/compiler.h>\n#include\t<linux/init.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/cache.h>\n#include\t<linux/swap.h>\n#include\t<linux/poison.h>\n#include\t<linux/mm.h>\n#include\t<linux/slab.h>\n\nstatic noinline struct;\n\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tvoid *ret = slab_alloc(cachep, flags, _RET_IP_);\n\n\tkasan_slab_alloc(cachep, ret, flags);\n\ttrace_kmem_cache_alloc(_RET_IP_, ret,\n\t\t\t       cachep->object_size, cachep->size, flags);\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct kmem_cache *vm_region_jar;\n\nint split_vma(struct mm_struct *mm, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tstruct vm_area_struct *new;\n\tstruct vm_region *region;\n\tunsigned long npages;\n\n\t/* we're only permitted to split anonymous regions (these should have\n\t * only a single usage on the region) */\n\tif (vma->vm_file)\n\t\treturn -ENOMEM;\n\n\tif (mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tregion = kmem_cache_alloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tnew = vm_area_dup(vma);\n\tif (!new) {\n\t\tkmem_cache_free(vm_region_jar, region);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* most fields are the same, copy all, and then fixup */\n\t*region = *vma->vm_region;\n\tnew->vm_region = region;\n\n\tnpages = (addr - vma->vm_start) >> PAGE_SHIFT;\n\n\tif (new_below) {\n\t\tregion->vm_top = region->vm_end = new->vm_end = addr;\n\t} else {\n\t\tregion->vm_start = new->vm_start = addr;\n\t\tregion->vm_pgoff = new->vm_pgoff += npages;\n\t}\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tdelete_vma_from_mm(vma);\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(vma->vm_region);\n\tif (new_below) {\n\t\tvma->vm_region->vm_start = vma->vm_start = addr;\n\t\tvma->vm_region->vm_pgoff = vma->vm_pgoff += npages;\n\t} else {\n\t\tvma->vm_region->vm_end = vma->vm_end = addr;\n\t\tvma->vm_region->vm_top = addr;\n\t}\n\tadd_nommu_region(vma->vm_region);\n\tadd_nommu_region(new->vm_region);\n\tup_write(&nommu_region_sem);\n\tadd_vma_to_mm(mm, vma);\n\tadd_vma_to_mm(mm, new);\n\treturn 0;\n}"
  },
  {
    "function_name": "ksys_mmap_pgoff",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1391-1413",
    "snippet": "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval = -EBADF;\n\n\taudit_mmap_fd(fd, flags);\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\tgoto out;\n\t}\n\n\tflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\n\n\tif (file)\n\t\tfput(file);\nout:\n\treturn retval;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "fput",
          "args": [
            "file"
          ],
          "line": 1410
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vm_mmap_pgoff",
          "args": [
            "file",
            "addr",
            "len",
            "prot",
            "flags",
            "pgoff"
          ],
          "line": 1407
        },
        "resolved": true,
        "details": {
          "function_name": "vm_mmap_pgoff",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/util.c",
          "lines": "337-358",
          "snippet": "unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long pgoff)\n{\n\tunsigned long ret;\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long populate;\n\tLIST_HEAD(uf);\n\n\tret = security_mmap_file(file, prot, flag);\n\tif (!ret) {\n\t\tif (down_write_killable(&mm->mmap_sem))\n\t\t\treturn -EINTR;\n\t\tret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,\n\t\t\t\t    &populate, &uf);\n\t\tup_write(&mm->mmap_sem);\n\t\tuserfaultfd_unmap_complete(mm, &uf);\n\t\tif (populate)\n\t\t\tmm_populate(ret, populate);\n\t}\n\treturn ret;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <linux/uaccess.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/mman.h>",
            "#include <linux/swapops.h>",
            "#include <linux/swap.h>",
            "#include <linux/security.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched.h>",
            "#include <linux/err.h>",
            "#include <linux/export.h>",
            "#include <linux/compiler.h>",
            "#include <linux/string.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <linux/uaccess.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/vmalloc.h>\n#include <linux/hugetlb.h>\n#include <linux/mman.h>\n#include <linux/swapops.h>\n#include <linux/swap.h>\n#include <linux/security.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/mm.h>\n#include <linux/sched.h>\n#include <linux/err.h>\n#include <linux/export.h>\n#include <linux/compiler.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nunsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long pgoff)\n{\n\tunsigned long ret;\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long populate;\n\tLIST_HEAD(uf);\n\n\tret = security_mmap_file(file, prot, flag);\n\tif (!ret) {\n\t\tif (down_write_killable(&mm->mmap_sem))\n\t\t\treturn -EINTR;\n\t\tret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,\n\t\t\t\t    &populate, &uf);\n\t\tup_write(&mm->mmap_sem);\n\t\tuserfaultfd_unmap_complete(mm, &uf);\n\t\tif (populate)\n\t\t\tmm_populate(ret, populate);\n\t}\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "fget",
          "args": [
            "fd"
          ],
          "line": 1400
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "audit_mmap_fd",
          "args": [
            "fd",
            "flags"
          ],
          "line": 1398
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nunsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval = -EBADF;\n\n\taudit_mmap_fd(fd, flags);\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\tgoto out;\n\t}\n\n\tflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\n\n\tif (file)\n\t\tfput(file);\nout:\n\treturn retval;\n}"
  },
  {
    "function_name": "do_mmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1167-1389",
    "snippet": "unsigned long do_mmap(struct file *file,\n\t\t\tunsigned long addr,\n\t\t\tunsigned long len,\n\t\t\tunsigned long prot,\n\t\t\tunsigned long flags,\n\t\t\tvm_flags_t vm_flags,\n\t\t\tunsigned long pgoff,\n\t\t\tunsigned long *populate,\n\t\t\tstruct list_head *uf)\n{\n\tstruct vm_area_struct *vma;\n\tstruct vm_region *region;\n\tstruct rb_node *rb;\n\tunsigned long capabilities, result;\n\tint ret;\n\n\t*populate = 0;\n\n\t/* decide whether we should attempt the mapping, and if so what sort of\n\t * mapping */\n\tret = validate_mmap_request(file, addr, len, prot, flags, pgoff,\n\t\t\t\t    &capabilities);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* we ignore the address hint */\n\taddr = 0;\n\tlen = PAGE_ALIGN(len);\n\n\t/* we've determined that we can make the mapping, now translate what we\n\t * now know into VMA flags */\n\tvm_flags |= determine_vm_flags(file, prot, flags, capabilities);\n\n\t/* we're going to need to record the mapping */\n\tregion = kmem_cache_zalloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\tgoto error_getting_region;\n\n\tvma = vm_area_alloc(current->mm);\n\tif (!vma)\n\t\tgoto error_getting_vma;\n\n\tregion->vm_usage = 1;\n\tregion->vm_flags = vm_flags;\n\tregion->vm_pgoff = pgoff;\n\n\tvma->vm_flags = vm_flags;\n\tvma->vm_pgoff = pgoff;\n\n\tif (file) {\n\t\tregion->vm_file = get_file(file);\n\t\tvma->vm_file = get_file(file);\n\t}\n\n\tdown_write(&nommu_region_sem);\n\n\t/* if we want to share, we need to check for regions created by other\n\t * mmap() calls that overlap with our proposed mapping\n\t * - we can only share with a superset match on most regular files\n\t * - shared mappings on character devices and memory backed files are\n\t *   permitted to overlap inexactly as far as we are concerned for in\n\t *   these cases, sharing is handled in the driver or filesystem rather\n\t *   than here\n\t */\n\tif (vm_flags & VM_MAYSHARE) {\n\t\tstruct vm_region *pregion;\n\t\tunsigned long pglen, rpglen, pgend, rpgend, start;\n\n\t\tpglen = (len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tpgend = pgoff + pglen;\n\n\t\tfor (rb = rb_first(&nommu_region_tree); rb; rb = rb_next(rb)) {\n\t\t\tpregion = rb_entry(rb, struct vm_region, vm_rb);\n\n\t\t\tif (!(pregion->vm_flags & VM_MAYSHARE))\n\t\t\t\tcontinue;\n\n\t\t\t/* search for overlapping mappings on the same file */\n\t\t\tif (file_inode(pregion->vm_file) !=\n\t\t\t    file_inode(file))\n\t\t\t\tcontinue;\n\n\t\t\tif (pregion->vm_pgoff >= pgend)\n\t\t\t\tcontinue;\n\n\t\t\trpglen = pregion->vm_end - pregion->vm_start;\n\t\t\trpglen = (rpglen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\t\trpgend = pregion->vm_pgoff + rpglen;\n\t\t\tif (pgoff >= rpgend)\n\t\t\t\tcontinue;\n\n\t\t\t/* handle inexactly overlapping matches between\n\t\t\t * mappings */\n\t\t\tif ((pregion->vm_pgoff != pgoff || rpglen != pglen) &&\n\t\t\t    !(pgoff >= pregion->vm_pgoff && pgend <= rpgend)) {\n\t\t\t\t/* new mapping is not a subset of the region */\n\t\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\t\tgoto sharing_violation;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* we've found a region we can share */\n\t\t\tpregion->vm_usage++;\n\t\t\tvma->vm_region = pregion;\n\t\t\tstart = pregion->vm_start;\n\t\t\tstart += (pgoff - pregion->vm_pgoff) << PAGE_SHIFT;\n\t\t\tvma->vm_start = start;\n\t\t\tvma->vm_end = start + len;\n\n\t\t\tif (pregion->vm_flags & VM_MAPPED_COPY)\n\t\t\t\tvma->vm_flags |= VM_MAPPED_COPY;\n\t\t\telse {\n\t\t\t\tret = do_mmap_shared_file(vma);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tvma->vm_region = NULL;\n\t\t\t\t\tvma->vm_start = 0;\n\t\t\t\t\tvma->vm_end = 0;\n\t\t\t\t\tpregion->vm_usage--;\n\t\t\t\t\tpregion = NULL;\n\t\t\t\t\tgoto error_just_free;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfput(region->vm_file);\n\t\t\tkmem_cache_free(vm_region_jar, region);\n\t\t\tregion = pregion;\n\t\t\tresult = start;\n\t\t\tgoto share;\n\t\t}\n\n\t\t/* obtain the address at which to make a shared mapping\n\t\t * - this is the hook for quasi-memory character devices to\n\t\t *   tell us the location of a shared mapping\n\t\t */\n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\taddr = file->f_op->get_unmapped_area(file, addr, len,\n\t\t\t\t\t\t\t     pgoff, flags);\n\t\t\tif (IS_ERR_VALUE(addr)) {\n\t\t\t\tret = addr;\n\t\t\t\tif (ret != -ENOSYS)\n\t\t\t\t\tgoto error_just_free;\n\n\t\t\t\t/* the driver refused to tell us where to site\n\t\t\t\t * the mapping so we'll have to attempt to copy\n\t\t\t\t * it */\n\t\t\t\tret = -ENODEV;\n\t\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\t\tgoto error_just_free;\n\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t} else {\n\t\t\t\tvma->vm_start = region->vm_start = addr;\n\t\t\t\tvma->vm_end = region->vm_end = addr + len;\n\t\t\t}\n\t\t}\n\t}\n\n\tvma->vm_region = region;\n\n\t/* set up the mapping\n\t * - the region is filled in if NOMMU_MAP_DIRECT is still set\n\t */\n\tif (file && vma->vm_flags & VM_SHARED)\n\t\tret = do_mmap_shared_file(vma);\n\telse\n\t\tret = do_mmap_private(vma, region, len, capabilities);\n\tif (ret < 0)\n\t\tgoto error_just_free;\n\tadd_nommu_region(region);\n\n\t/* clear anonymous mappings that don't ask for uninitialized data */\n\tif (!vma->vm_file && !(flags & MAP_UNINITIALIZED))\n\t\tmemset((void *)region->vm_start, 0,\n\t\t       region->vm_end - region->vm_start);\n\n\t/* okay... we have a mapping; now we have to register it */\n\tresult = vma->vm_start;\n\n\tcurrent->mm->total_vm += len >> PAGE_SHIFT;\n\nshare:\n\tadd_vma_to_mm(current->mm, vma);\n\n\t/* we flush the region from the icache only when the first executable\n\t * mapping of it is made  */\n\tif (vma->vm_flags & VM_EXEC && !region->vm_icache_flushed) {\n\t\tflush_icache_range(region->vm_start, region->vm_end);\n\t\tregion->vm_icache_flushed = true;\n\t}\n\n\tup_write(&nommu_region_sem);\n\n\treturn result;\n\nerror_just_free:\n\tup_write(&nommu_region_sem);\nerror:\n\tif (region->vm_file)\n\t\tfput(region->vm_file);\n\tkmem_cache_free(vm_region_jar, region);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tvm_area_free(vma);\n\treturn ret;\n\nsharing_violation:\n\tup_write(&nommu_region_sem);\n\tpr_warn(\"Attempt to share mismatched mappings\\n\");\n\tret = -EINVAL;\n\tgoto error;\n\nerror_getting_vma:\n\tkmem_cache_free(vm_region_jar, region);\n\tpr_warn(\"Allocation of vma for %lu byte allocation from process %d failed\\n\",\n\t\t\tlen, current->pid);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n\nerror_getting_region:\n\tpr_warn(\"Allocation of vm region for %lu byte allocation from process %d failed\\n\",\n\t\t\tlen, current->pid);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static struct kmem_cache *vm_region_jar;",
      "struct rb_root nommu_region_tree = RB_ROOT;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "show_free_areas",
          "args": [
            "0",
            "NULL"
          ],
          "line": 1387
        },
        "resolved": true,
        "details": {
          "function_name": "show_free_areas",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/page_alloc.c",
          "lines": "4851-8225",
          "snippet": "void show_free_areas(unsigned int filter, nodemask_t *nodemask)\n{\n\tunsigned long free_pcp = 0;\n\tint cpu;\n\tstruct zone *zone;\n\tpg_data_t *pgdat;\n\n\tfor_each_populated_zone(zone) {\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\t}\n\n\tprintk(\"active_anon:%lu inactive_anon:%lu isolated_anon:%lu\\n\"\n\t\t\" active_file:%lu inactive_file:%lu isolated_file:%lu\\n\"\n\t\t\" unevictable:%lu dirty:%lu writeback:%lu unstable:%lu\\n\"\n\t\t\" slab_reclaimable:%lu slab_unreclaimable:%lu\\n\"\n\t\t\" mapped:%lu shmem:%lu pagetables:%lu bounce:%lu\\n\"\n\t\t\" free:%lu free_pcp:%lu free_cma:%lu\\n\",\n\t\tglobal_node_page_state(NR_ACTIVE_ANON),\n\t\tglobal_node_page_state(NR_INACTIVE_ANON),\n\t\tglobal_node_page_state(NR_ISOLATED_ANON),\n\t\tglobal_node_page_state(NR_ACTIVE_FILE),\n\t\tglobal_node_page_state(NR_INACTIVE_FILE),\n\t\tglobal_node_page_state(NR_ISOLATED_FILE),\n\t\tglobal_node_page_state(NR_UNEVICTABLE),\n\t\tglobal_node_page_state(NR_FILE_DIRTY),\n\t\tglobal_node_page_state(NR_WRITEBACK),\n\t\tglobal_node_page_state(NR_UNSTABLE_NFS),\n\t\tglobal_node_page_state(NR_SLAB_RECLAIMABLE),\n\t\tglobal_node_page_state(NR_SLAB_UNRECLAIMABLE),\n\t\tglobal_node_page_state(NR_FILE_MAPPED),\n\t\tglobal_node_page_state(NR_SHMEM),\n\t\tglobal_zone_page_state(NR_PAGETABLE),\n\t\tglobal_zone_page_state(NR_BOUNCE),\n\t\tglobal_zone_page_state(NR_FREE_PAGES),\n\t\tfree_pcp,\n\t\tglobal_zone_page_state(NR_FREE_CMA_PAGES));\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tif (show_mem_node_skip(filter, pgdat->node_id, nodemask))\n\t\t\tcontinue;\n\n\t\tprintk(\"Node %d\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" isolated(anon):%lukB\"\n\t\t\t\" isolated(file):%lukB\"\n\t\t\t\" mapped:%lukB\"\n\t\t\t\" dirty:%lukB\"\n\t\t\t\" writeback:%lukB\"\n\t\t\t\" shmem:%lukB\"\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\t\" shmem_thp: %lukB\"\n\t\t\t\" shmem_pmdmapped: %lukB\"\n\t\t\t\" anon_thp: %lukB\"\n#endif\n\t\t\t\" writeback_tmp:%lukB\"\n\t\t\t\" unstable:%lukB\"\n\t\t\t\" all_unreclaimable? %s\"\n\t\t\t\"\\n\",\n\t\t\tpgdat->node_id,\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_UNEVICTABLE)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_MAPPED)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_DIRTY)),\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK)),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM)),\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_THPS) * HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_PMDMAPPED)\n\t\t\t\t\t* HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_ANON_THPS) * HPAGE_PMD_NR),\n#endif\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK_TEMP)),\n\t\t\tK(node_page_state(pgdat, NR_UNSTABLE_NFS)),\n\t\t\tpgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ?\n\t\t\t\t\"yes\" : \"no\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tint i;\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfree_pcp = 0;\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT\n\t\t\t\"%s\"\n\t\t\t\" free:%lukB\"\n\t\t\t\" min:%lukB\"\n\t\t\t\" low:%lukB\"\n\t\t\t\" high:%lukB\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" writepending:%lukB\"\n\t\t\t\" present:%lukB\"\n\t\t\t\" managed:%lukB\"\n\t\t\t\" mlocked:%lukB\"\n\t\t\t\" kernel_stack:%lukB\"\n\t\t\t\" pagetables:%lukB\"\n\t\t\t\" bounce:%lukB\"\n\t\t\t\" free_pcp:%lukB\"\n\t\t\t\" local_pcp:%ukB\"\n\t\t\t\" free_cma:%lukB\"\n\t\t\t\"\\n\",\n\t\t\tzone->name,\n\t\t\tK(zone_page_state(zone, NR_FREE_PAGES)),\n\t\t\tK(min_wmark_pages(zone)),\n\t\t\tK(low_wmark_pages(zone)),\n\t\t\tK(high_wmark_pages(zone)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_UNEVICTABLE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_WRITE_PENDING)),\n\t\t\tK(zone->present_pages),\n\t\t\tK(zone->managed_pages),\n\t\t\tK(zone_page_state(zone, NR_MLOCK)),\n\t\t\tzone_page_state(zone, NR_KERNEL_STACK_KB),\n\t\t\tK(zone_page_state(zone, NR_PAGETABLE)),\n\t\t\tK(zone_page_state(zone, NR_BOUNCE)),\n\t\t\tK(free_pcp),\n\t\t\tK(this_cpu_read(zone->pageset->pcp.count)),\n\t\t\tK(zone_page_state(zone, NR_FREE_CMA_PAGES)));\n\t\tprintk(\"lowmem_reserve[]:\");\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++)\n\t\t\tprintk(KERN_CONT \" %ld\", zone->lowmem_reserve[i]);\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int order;\n\t\tunsigned long nr[MAX_ORDER], flags, total = 0;\n\t\tunsigned char types[MAX_ORDER];\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT \"%s: \", zone->name);\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tstruct free_area *area = &zone->free_area[order];\n\t\t\tint type;\n\n\t\t\tnr[order] = area->nr_free;\n\t\t\ttotal += nr[order] << order;\n\n\t\t\ttypes[order] = 0;\n\t\t\tfor (type = 0; type < MIGRATE_TYPES; type++) {\n\t\t\t\tif (!list_empty(&area->free_list[type]))\n\t\t\t\t\ttypes[order] |= 1 << type;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n\t\t\t       nr[order], K(1UL) << order);\n\t\t\tif (nr[order])\n\t\t\t\tshow_migration_types(types[order]);\n\t\t}\n\t\tprintk(KERN_CONT \"= %lukB\\n\", K(total));\n\t}\n\n\thugetlb_show_meminfo();\n\n\tprintk(\"%ld total pagecache pages\\n\", global_node_page_state(NR_FILE_PAGES));\n\n\tshow_swap_cache_info();\n}\n\nstatic void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)\n{\n\tzoneref->zone = zone;\n\tzoneref->zone_idx = zone_idx(zone);\n}\n\n/*\n * Builds allocation fallback zone lists.\n *\n * Add all populated zones of a node to the zonelist.\n */\nstatic int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)\n{\n\tstruct zone *zone;\n\tenum zone_type zone_type = MAX_NR_ZONES;\n\tint nr_zones = 0;\n\n\tdo {\n\t\tzone_type--;\n\t\tzone = pgdat->node_zones + zone_type;\n\t\tif (managed_zone(zone)) {\n\t\t\tzoneref_set_zone(zone, &zonerefs[nr_zones++]);\n\t\t\tcheck_highest_zone(zone_type);\n\t\t}\n\t} while (zone_type);\n\n\treturn nr_zones;\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int __parse_numa_zonelist_order(char *s)\n{\n\t/*\n\t * We used to support different zonlists modes but they turned\n\t * out to be just not useful. Let's keep the warning in place\n\t * if somebody still use the cmd line parameter so that we do\n\t * not fail it silently\n\t */\n\tif (!(*s == 'd' || *s == 'D' || *s == 'n' || *s == 'N')) {\n\t\tpr_warn(\"Ignoring unsupported numa_zonelist_order value:  %s\\n\", s);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic __init int setup_numa_zonelist_order(char *s)\n{\n\tif (!s)\n\t\treturn 0;\n\n\treturn __parse_numa_zonelist_order(s);\n}\nearly_param(\"numa_zonelist_order\", setup_numa_zonelist_order);\n\nchar numa_zonelist_order[] = \"Node\";\n\n/*\n * sysctl handler for numa_zonelist_order\n */\nint numa_zonelist_order_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *length,\n\t\tloff_t *ppos)\n{\n\tchar *str;\n\tint ret;\n\n\tif (!write)\n\t\treturn proc_dostring(table, write, buffer, length, ppos);\n\tstr = memdup_user_nul(buffer, 16);\n\tif (IS_ERR(str))\n\t\treturn PTR_ERR(str);\n\n\tret = __parse_numa_zonelist_order(str);\n\tkfree(str);\n\treturn ret;\n}\n\n\n#define MAX_NODE_LOAD (nr_online_nodes)\nstatic int node_load[MAX_NUMNODES];\n\n/**\n * find_next_best_node - find the next node that should appear in a given node's fallback list\n * @node: node whose fallback list we're appending\n * @used_node_mask: nodemask_t of already used nodes\n *\n * We use a number of factors to determine which is the next node that should\n * appear on a given node's fallback list.  The node should not have appeared\n * already in @node's fallback list, and it should be the next closest node\n * according to the distance array (which contains arbitrary distance values\n * from each node to each node in the system), and should also prefer nodes\n * with no CPUs, since presumably they'll have very little allocation pressure\n * on them otherwise.\n * It returns -1 if no node is found.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_node_mask)\n{\n\tint n, val;\n\tint min_val = INT_MAX;\n\tint best_node = NUMA_NO_NODE;\n\tconst struct cpumask *tmp = cpumask_of_node(0);\n\n\t/* Use the local node if we haven't already */\n\tif (!node_isset(node, *used_node_mask)) {\n\t\tnode_set(node, *used_node_mask);\n\t\treturn node;\n\t}\n\n\tfor_each_node_state(n, N_MEMORY) {\n\n\t\t/* Don't want a node to appear more than once */\n\t\tif (node_isset(n, *used_node_mask))\n\t\t\tcontinue;\n\n\t\t/* Use the distance array to find the distance */\n\t\tval = node_distance(node, n);\n\n\t\t/* Penalize nodes under us (\"prefer the next node\") */\n\t\tval += (n < node);\n\n\t\t/* Give preference to headless and unused nodes */\n\t\ttmp = cpumask_of_node(n);\n\t\tif (!cpumask_empty(tmp))\n\t\t\tval += PENALTY_FOR_NODE_WITH_CPUS;\n\n\t\t/* Slight preference for less loaded node */\n\t\tval *= (MAX_NODE_LOAD*MAX_NUMNODES);\n\t\tval += node_load[n];\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node >= 0)\n\t\tnode_set(best_node, *used_node_mask);\n\n\treturn best_node;\n}\n\n\n/*\n * Build zonelists ordered by node and zones within node.\n * This results in maximum locality--normal zone overflows into local\n * DMA zone, if any--but risks exhausting DMA zone.\n */\nstatic void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order,\n\t\tunsigned nr_nodes)\n{\n\tstruct zoneref *zonerefs;\n\tint i;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\n\tfor (i = 0; i < nr_nodes; i++) {\n\t\tint nr_zones;\n\n\t\tpg_data_t *node = NODE_DATA(node_order[i]);\n\n\t\tnr_zones = build_zonerefs_node(node, zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build gfp_thisnode zonelists\n */\nstatic void build_thisnode_zonelists(pg_data_t *pgdat)\n{\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build zonelists ordered by zone and nodes within zones.\n * This results in conserving DMA zone[s] until all Normal memory is\n * exhausted, but results in overflowing to remote node while memory\n * may still exist in local DMA zone.\n */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tstatic int node_order[MAX_NUMNODES];\n\tint node, load, nr_nodes = 0;\n\tnodemask_t used_mask;\n\tint local_node, prev_node;\n\n\t/* NUMA-aware ordering of nodes */\n\tlocal_node = pgdat->node_id;\n\tload = nr_online_nodes;\n\tprev_node = local_node;\n\tnodes_clear(used_mask);\n\n\tmemset(node_order, 0, sizeof(node_order));\n\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n\t\t/*\n\t\t * We don't want to pressure a particular node.\n\t\t * So adding penalty to the first node in same\n\t\t * distance group to make it round-robin.\n\t\t */\n\t\tif (node_distance(local_node, node) !=\n\t\t    node_distance(local_node, prev_node))\n\t\t\tnode_load[node] = load;\n\n\t\tnode_order[nr_nodes++] = node;\n\t\tprev_node = node;\n\t\tload--;\n\t}\n\n\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n\tbuild_thisnode_zonelists(pgdat);\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n/*\n * Return node id of node used for \"local\" allocations.\n * I.e., first node id of first zone in arg node's generic zonelist.\n * Used for initializing percpu 'numa_mem', which is used primarily\n * for kernel allocations, so use GFP_KERNEL flags to locate zonelist.\n */\nint local_memory_node(int node)\n{\n\tstruct zoneref *z;\n\n\tz = first_zones_zonelist(node_zonelist(node, GFP_KERNEL),\n\t\t\t\t   gfp_zone(GFP_KERNEL),\n\t\t\t\t   NULL);\n\treturn zone_to_nid(z->zone);\n}\n#endif\n\nstatic void setup_min_unmapped_ratio(void);\nstatic void setup_min_slab_ratio(void);\n#else\t/* CONFIG_NUMA */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tint node, local_node;\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tlocal_node = pgdat->node_id;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\n\t/*\n\t * Now we build the zonelist so that it contains the zones\n\t * of all the other nodes.\n\t * We don't want to pressure a particular node, so when\n\t * building the zones for node N, we make sure that the\n\t * zones coming right after the local ones are those from\n\t * node N+1 (modulo N)\n\t */\n\tfor (node = local_node + 1; node < MAX_NUMNODES; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tfor (node = 0; node < local_node; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n#endif\t/* CONFIG_NUMA */\n\n/*\n * Boot pageset table. One per cpu which is going to be used for all\n * zones and all nodes. The parameters will be set in such a way\n * that an item put on a list will immediately be handed over to\n * the buddy list. This is safe since pageset manipulation is done\n * with interrupts disabled.\n *\n * The boot_pagesets must be kept even after bootup is complete for\n * unused processors and/or zones. They do play a role for bootstrapping\n * hotplugged processors.\n *\n * zoneinfo_show() and maybe other functions do\n * not check if the processor is online before following the pageset pointer.\n * Other parts of the kernel may not check if the zone is available.\n */\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch);\nstatic DEFINE_PER_CPU(struct per_cpu_pageset, boot_pageset);\nstatic DEFINE_PER_CPU(struct per_cpu_nodestat, boot_nodestats);\n\nstatic void __build_all_zonelists(void *data)\n{\n\tint nid;\n\tint __maybe_unused cpu;\n\tpg_data_t *self = data;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\n#ifdef CONFIG_NUMA\n\tmemset(node_load, 0, sizeof(node_load));\n#endif\n\n\t/*\n\t * This node is hotadded and no memory is yet present.   So just\n\t * building zonelists is fine - no need to touch other nodes.\n\t */\n\tif (self && !node_online(self->node_id)) {\n\t\tbuild_zonelists(self);\n\t} else {\n\t\tfor_each_online_node(nid) {\n\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\t\t\tbuild_zonelists(pgdat);\n\t\t}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n\t\t/*\n\t\t * We now know the \"local memory node\" for each node--\n\t\t * i.e., the node of the first zone in the generic zonelist.\n\t\t * Set up numa_mem percpu variable for on-line cpus.  During\n\t\t * boot, only the boot cpu should be on-line;  we'll init the\n\t\t * secondary cpus' numa_mem as they come on-line.  During\n\t\t * node/memory hotplug, we'll fixup all on-line cpus.\n\t\t */\n\t\tfor_each_online_cpu(cpu)\n\t\t\tset_cpu_numa_mem(cpu, local_memory_node(cpu_to_node(cpu)));\n#endif\n\t}\n\n\tspin_unlock(&lock);\n}\n\nstatic noinline void __init\nbuild_all_zonelists_init(void)\n{\n\tint cpu;\n\n\t__build_all_zonelists(NULL);\n\n\t/*\n\t * Initialize the boot_pagesets that are going to be used\n\t * for bootstrapping processors. The real pagesets for\n\t * each zone will be allocated later when the per cpu\n\t * allocator is available.\n\t *\n\t * boot_pagesets are used also for bootstrapping offline\n\t * cpus if the system is already booted because the pagesets\n\t * are needed to initialize allocators on a specific cpu too.\n\t * F.e. the percpu allocator needs the page allocator which\n\t * needs the percpu allocator in order to allocate its pagesets\n\t * (a chicken-egg dilemma).\n\t */\n\tfor_each_possible_cpu(cpu)\n\t\tsetup_pageset(&per_cpu(boot_pageset, cpu), 0);\n\n\tmminit_verify_zonelist();\n\tcpuset_init_current_mems_allowed();\n}\n\n/*\n * unless system_state == SYSTEM_BOOTING.\n *\n * __ref due to call of __init annotated helper build_all_zonelists_init\n * [protected by SYSTEM_BOOTING].\n */\nvoid __ref build_all_zonelists(pg_data_t *pgdat)\n{\n\tif (system_state == SYSTEM_BOOTING) {\n\t\tbuild_all_zonelists_init();\n\t} else {\n\t\t__build_all_zonelists(pgdat);\n\t\t/* cpuset refresh routine should be here */\n\t}\n\tvm_total_pages = nr_free_pagecache_pages();\n\t/*\n\t * Disable grouping by mobility if the number of pages in the\n\t * system is too low to allow the mechanism to work. It would be\n\t * more accurate, but expensive to check per-zone. This check is\n\t * made on memory-hotadd so a system can start with mobility\n\t * disabled and enable it later\n\t */\n\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))\n\t\tpage_group_by_mobility_disabled = 1;\n\telse\n\t\tpage_group_by_mobility_disabled = 0;\n\n\tpr_info(\"Built %i zonelists, mobility grouping %s.  Total pages: %ld\\n\",\n\t\tnr_online_nodes,\n\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",\n\t\tvm_total_pages);\n#ifdef CONFIG_NUMA\n\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);\n#endif\n}\n\n/* If zone is ZONE_MOVABLE but memory is mirrored, it is an overlapped init */\nstatic bool __meminit\noverlap_memmap_init(unsigned long zone, unsigned long *pfn)\n{\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tstatic struct memblock_region *r;\n\n\tif (mirrored_kernelcore && zone == ZONE_MOVABLE) {\n\t\tif (!r || *pfn >= memblock_region_memory_end_pfn(r)) {\n\t\t\tfor_each_memblock(memory, r) {\n\t\t\t\tif (*pfn < memblock_region_memory_end_pfn(r))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (*pfn >= memblock_region_memory_base_pfn(r) &&\n\t\t    memblock_is_mirror(r)) {\n\t\t\t*pfn = memblock_region_memory_end_pfn(r);\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\treturn false;\n}\n\n/*\n * Initially all pages are reserved - free ones are freed\n * up by memblock_free_all() once the early boot process is\n * done. Non-atomic initialization, single-pass.\n */\nvoid __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,\n\t\tunsigned long start_pfn, enum memmap_context context,\n\t\tstruct vmem_altmap *altmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct page *page;\n\n\tif (highest_memmap_pfn < end_pfn - 1)\n\t\thighest_memmap_pfn = end_pfn - 1;\n\n#ifdef CONFIG_ZONE_DEVICE\n\t/*\n\t * Honor reservation requested by the driver for this ZONE_DEVICE\n\t * memory. We limit the total number of pages to initialize to just\n\t * those that might contain the memory mapping. We will defer the\n\t * ZONE_DEVICE page initialization until after we have released\n\t * the hotplug lock.\n\t */\n\tif (zone == ZONE_DEVICE) {\n\t\tif (!altmap)\n\t\t\treturn;\n\n\t\tif (start_pfn == altmap->base_pfn)\n\t\t\tstart_pfn += altmap->reserve;\n\t\tend_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t}\n#endif\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\t/*\n\t\t * There can be holes in boot-time mem_map[]s handed to this\n\t\t * function.  They do not exist on hotplugged memory.\n\t\t */\n\t\tif (context == MEMMAP_EARLY) {\n\t\t\tif (!early_pfn_valid(pfn))\n\t\t\t\tcontinue;\n\t\t\tif (!early_pfn_in_nid(pfn, nid))\n\t\t\t\tcontinue;\n\t\t\tif (overlap_memmap_init(zone, &pfn))\n\t\t\t\tcontinue;\n\t\t\tif (defer_init(nid, pfn, end_pfn))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tpage = pfn_to_page(pfn);\n\t\t__init_single_page(page, pfn, zone, nid);\n\t\tif (context == MEMMAP_HOTPLUG)\n\t\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nvoid __ref memmap_init_zone_device(struct zone *zone,\n\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t   unsigned long size,\n\t\t\t\t   struct dev_pagemap *pgmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tunsigned long zone_idx = zone_idx(zone);\n\tunsigned long start = jiffies;\n\tint nid = pgdat->node_id;\n\n\tif (WARN_ON_ONCE(!pgmap || !is_dev_zone(zone)))\n\t\treturn;\n\n\t/*\n\t * The call to memmap_init_zone should have already taken care\n\t * of the pages reserved for the memmap, so we can just jump to\n\t * the end of that region and start processing the device pages.\n\t */\n\tif (pgmap->altmap_valid) {\n\t\tstruct vmem_altmap *altmap = &pgmap->altmap;\n\n\t\tstart_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t\tsize = end_pfn - start_pfn;\n\t}\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t__init_single_page(page, pfn, zone_idx, nid);\n\n\t\t/*\n\t\t * Mark page reserved as it will need to wait for onlining\n\t\t * phase for it to be fully associated with a zone.\n\t\t *\n\t\t * We can use the non-atomic __set_bit operation for setting\n\t\t * the flag as we are still initializing the pages.\n\t\t */\n\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * ZONE_DEVICE pages union ->lru with a ->pgmap back\n\t\t * pointer and hmm_data.  It is a bug if a ZONE_DEVICE\n\t\t * page is ever freed or placed on a driver-private list.\n\t\t */\n\t\tpage->pgmap = pgmap;\n\t\tpage->hmm_data = 0;\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t *\n\t\t * Please note that MEMMAP_HOTPLUG path doesn't clear memmap\n\t\t * because this is done early in sparse_add_one_section\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tpr_info(\"%s initialised, %lu pages in %ums\\n\", dev_name(pgmap->dev),\n\t\tsize, jiffies_to_msecs(jiffies - start));\n}\n\n#endif\nstatic void __meminit zone_init_free_lists(struct zone *zone)\n{\n\tunsigned int order, t;\n\tfor_each_migratetype_order(order, t) {\n\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);\n\t\tzone->free_area[order].nr_free = 0;\n\t}\n}\n\nvoid __meminit __weak memmap_init(unsigned long size, int nid,\n\t\t\t\t  unsigned long zone, unsigned long start_pfn)\n{\n\tmemmap_init_zone(size, nid, zone, start_pfn, MEMMAP_EARLY, NULL);\n}\n\nstatic int zone_batchsize(struct zone *zone)\n{\n#ifdef CONFIG_MMU\n\tint batch;\n\n\t/*\n\t * The per-cpu-pages pools are set to around 1000th of the\n\t * size of the zone.\n\t */\n\tbatch = zone->managed_pages / 1024;\n\t/* But no more than a meg. */\n\tif (batch * PAGE_SIZE > 1024 * 1024)\n\t\tbatch = (1024 * 1024) / PAGE_SIZE;\n\tbatch /= 4;\t\t/* We effectively *= 4 below */\n\tif (batch < 1)\n\t\tbatch = 1;\n\n\t/*\n\t * Clamp the batch to a 2^n - 1 value. Having a power\n\t * of 2 value was found to be more likely to have\n\t * suboptimal cache aliasing properties in some cases.\n\t *\n\t * For example if 2 tasks are alternately allocating\n\t * batches of pages, one task can end up with a lot\n\t * of pages of one half of the possible page colors\n\t * and the other with pages of the other colors.\n\t */\n\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;\n\n\treturn batch;\n\n#else\n\t/* The deferral and batching of frees should be suppressed under NOMMU\n\t * conditions.\n\t *\n\t * The problem is that NOMMU needs to be able to allocate large chunks\n\t * of contiguous memory as there's no hardware page translation to\n\t * assemble apparent contiguous memory from discontiguous pages.\n\t *\n\t * Queueing large contiguous runs of pages for batching, however,\n\t * causes the pages to actually be freed in smaller chunks.  As there\n\t * can be a significant delay between the individual batches being\n\t * recycled, this leads to the once large chunks of space being\n\t * fragmented and becoming unavailable for high-order allocations.\n\t */\n\treturn 0;\n#endif\n}\n\n/*\n * pcp->high and pcp->batch values are related and dependent on one another:\n * ->batch must never be higher then ->high.\n * The following function updates them in a safe manner without read side\n * locking.\n *\n * Any new users of pcp->batch and pcp->high should ensure they can cope with\n * those fields changing asynchronously (acording the the above rule).\n *\n * mutex_is_locked(&pcp_batch_high_lock) required when calling this function\n * outside of boot time (or some other assurance that no concurrent updaters\n * exist).\n */\nstatic void pageset_update(struct per_cpu_pages *pcp, unsigned long high,\n\t\tunsigned long batch)\n{\n       /* start with a fail safe value for batch */\n\tpcp->batch = 1;\n\tsmp_wmb();\n\n       /* Update high, then batch, in order */\n\tpcp->high = high;\n\tsmp_wmb();\n\n\tpcp->batch = batch;\n}\n\n/* a companion to pageset_set_high() */\nstatic void pageset_set_batch(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_update(&p->pcp, 6 * batch, max(1UL, 1 * batch));\n}\n\nstatic void pageset_init(struct per_cpu_pageset *p)\n{\n\tstruct per_cpu_pages *pcp;\n\tint migratetype;\n\n\tmemset(p, 0, sizeof(*p));\n\n\tpcp = &p->pcp;\n\tpcp->count = 0;\n\tfor (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)\n\t\tINIT_LIST_HEAD(&pcp->lists[migratetype]);\n}\n\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_init(p);\n\tpageset_set_batch(p, batch);\n}\n\n/*\n * pageset_set_high() sets the high water mark for hot per_cpu_pagelist\n * to the value high for the pageset p.\n */\nstatic void pageset_set_high(struct per_cpu_pageset *p,\n\t\t\t\tunsigned long high)\n{\n\tunsigned long batch = max(1UL, high / 4);\n\tif ((high / 4) > (PAGE_SHIFT * 8))\n\t\tbatch = PAGE_SHIFT * 8;\n\n\tpageset_update(&p->pcp, high, batch);\n}\n\nstatic void pageset_set_high_and_batch(struct zone *zone,\n\t\t\t\t       struct per_cpu_pageset *pcp)\n{\n\tif (percpu_pagelist_fraction)\n\t\tpageset_set_high(pcp,\n\t\t\t(zone->managed_pages /\n\t\t\t\tpercpu_pagelist_fraction));\n\telse\n\t\tpageset_set_batch(pcp, zone_batchsize(zone));\n}\n\nstatic void __meminit zone_pageset_init(struct zone *zone, int cpu)\n{\n\tstruct per_cpu_pageset *pcp = per_cpu_ptr(zone->pageset, cpu);\n\n\tpageset_init(pcp);\n\tpageset_set_high_and_batch(zone, pcp);\n}\n\nvoid __meminit setup_zone_pageset(struct zone *zone)\n{\n\tint cpu;\n\tzone->pageset = alloc_percpu(struct per_cpu_pageset);\n\tfor_each_possible_cpu(cpu)\n\t\tzone_pageset_init(zone, cpu);\n}\n\n/*\n * Allocate per cpu pagesets and initialize them.\n * Before this call only boot pagesets were available.\n */\nvoid __init setup_per_cpu_pageset(void)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone)\n\t\tsetup_zone_pageset(zone);\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->per_cpu_nodestats =\n\t\t\talloc_percpu(struct per_cpu_nodestat);\n}\n\nstatic __meminit void zone_pcp_init(struct zone *zone)\n{\n\t/*\n\t * per cpu subsystem is not up at this point. The following code\n\t * relies on the ability of the linker to provide the\n\t * offset of a (static) per cpu variable into the per cpu area.\n\t */\n\tzone->pageset = &boot_pageset;\n\n\tif (populated_zone(zone))\n\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages, LIFO batch:%u\\n\",\n\t\t\tzone->name, zone->present_pages,\n\t\t\t\t\t zone_batchsize(zone));\n}\n\nvoid __meminit init_currently_empty_zone(struct zone *zone,\n\t\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\t\tunsigned long size)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tint zone_idx = zone_idx(zone) + 1;\n\n\tif (zone_idx > pgdat->nr_zones)\n\t\tpgdat->nr_zones = zone_idx;\n\n\tzone->zone_start_pfn = zone_start_pfn;\n\n\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",\n\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",\n\t\t\tpgdat->node_id,\n\t\t\t(unsigned long)zone_idx(zone),\n\t\t\tzone_start_pfn, (zone_start_pfn + size));\n\n\tzone_init_free_lists(zone);\n\tzone->initialized = 1;\n}\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n#ifndef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID\n\n/*\n * Required by SPARSEMEM. Given a PFN, return what node the PFN is on.\n */\nint __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint nid;\n\n\tif (state->last_start <= pfn && pfn < state->last_end)\n\t\treturn state->last_nid;\n\n\tnid = memblock_search_pfn_nid(pfn, &start_pfn, &end_pfn);\n\tif (nid != -1) {\n\t\tstate->last_start = start_pfn;\n\t\tstate->last_end = end_pfn;\n\t\tstate->last_nid = nid;\n\t}\n\n\treturn nid;\n}\n#endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */\n\n/**\n * free_bootmem_with_active_regions - Call memblock_free_early_nid for each active range\n * @nid: The node to free memory on. If MAX_NUMNODES, all nodes are freed.\n * @max_low_pfn: The highest PFN that will be passed to memblock_free_early_nid\n *\n * If an architecture guarantees that all ranges registered contain no holes\n * and may be freed, this this function may be used instead of calling\n * memblock_free_early_nid() manually.\n */\nvoid __init free_bootmem_with_active_regions(int nid, unsigned long max_low_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid) {\n\t\tstart_pfn = min(start_pfn, max_low_pfn);\n\t\tend_pfn = min(end_pfn, max_low_pfn);\n\n\t\tif (start_pfn < end_pfn)\n\t\t\tmemblock_free_early_nid(PFN_PHYS(start_pfn),\n\t\t\t\t\t(end_pfn - start_pfn) << PAGE_SHIFT,\n\t\t\t\t\tthis_nid);\n\t}\n}\n\n/**\n * sparse_memory_present_with_active_regions - Call memory_present for each active range\n * @nid: The node to call memory_present for. If MAX_NUMNODES, all nodes will be used.\n *\n * If an architecture guarantees that all ranges registered contain no holes and may\n * be freed, this function may be used instead of calling memory_present() manually.\n */\nvoid __init sparse_memory_present_with_active_regions(int nid)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid)\n\t\tmemory_present(this_nid, start_pfn, end_pfn);\n}\n\n/**\n * get_pfn_range_for_nid - Return the start and end page frames for a node\n * @nid: The nid to return the range for. If MAX_NUMNODES, the min and max PFN are returned.\n * @start_pfn: Passed by reference. On return, it will have the node start_pfn.\n * @end_pfn: Passed by reference. On return, it will have the node end_pfn.\n *\n * It returns the start and end page frame of a node based on information\n * provided by memblock_set_node(). If called for a node\n * with no available memory, a warning is printed and the start and end\n * PFNs will be 0.\n */\nvoid __meminit get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn)\n{\n\tunsigned long this_start_pfn, this_end_pfn;\n\tint i;\n\n\t*start_pfn = -1UL;\n\t*end_pfn = 0;\n\n\tfor_each_mem_pfn_range(i, nid, &this_start_pfn, &this_end_pfn, NULL) {\n\t\t*start_pfn = min(*start_pfn, this_start_pfn);\n\t\t*end_pfn = max(*end_pfn, this_end_pfn);\n\t}\n\n\tif (*start_pfn == -1UL)\n\t\t*start_pfn = 0;\n}\n\n/*\n * This finds a zone that can be used for ZONE_MOVABLE pages. The\n * assumption is made that zones within a node are ordered in monotonic\n * increasing memory addresses so that the \"highest\" populated zone is used\n */\nstatic void __init find_usable_zone_for_movable(void)\n{\n\tint zone_index;\n\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {\n\t\tif (zone_index == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tif (arch_zone_highest_possible_pfn[zone_index] >\n\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])\n\t\t\tbreak;\n\t}\n\n\tVM_BUG_ON(zone_index == -1);\n\tmovable_zone = zone_index;\n}\n\n/*\n * The zone ranges provided by the architecture do not include ZONE_MOVABLE\n * because it is sized independent of architecture. Unlike the other zones,\n * the starting point for ZONE_MOVABLE is not fixed. It may be different\n * in each node depending on the size of each node and how evenly kernelcore\n * is distributed. This helper function adjusts the zone ranges\n * provided by the architecture for a given node by using the end of the\n * highest usable zone for ZONE_MOVABLE. This preserves the assumption that\n * zones within a node are in order of monotonic increases memory addresses\n */\nstatic void __meminit adjust_zone_range_for_zone_movable(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\t/* Only adjust if ZONE_MOVABLE is on this node */\n\tif (zone_movable_pfn[nid]) {\n\t\t/* Size ZONE_MOVABLE */\n\t\tif (zone_type == ZONE_MOVABLE) {\n\t\t\t*zone_start_pfn = zone_movable_pfn[nid];\n\t\t\t*zone_end_pfn = min(node_end_pfn,\n\t\t\t\tarch_zone_highest_possible_pfn[movable_zone]);\n\n\t\t/* Adjust for ZONE_MOVABLE starting within this range */\n\t\t} else if (!mirrored_kernelcore &&\n\t\t\t*zone_start_pfn < zone_movable_pfn[nid] &&\n\t\t\t*zone_end_pfn > zone_movable_pfn[nid]) {\n\t\t\t*zone_end_pfn = zone_movable_pfn[nid];\n\n\t\t/* Check if this whole range is within ZONE_MOVABLE */\n\t\t} else if (*zone_start_pfn >= zone_movable_pfn[nid])\n\t\t\t*zone_start_pfn = *zone_end_pfn;\n\t}\n}\n\n/*\n * Return the number of pages a zone spans in a node, including holes\n * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()\n */\nstatic unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\t/* Get the start and end of the zone */\n\t*zone_start_pfn = arch_zone_lowest_possible_pfn[zone_type];\n\t*zone_end_pfn = arch_zone_highest_possible_pfn[zone_type];\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t/* Check that this node has pages within the zone's required range */\n\tif (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)\n\t\treturn 0;\n\n\t/* Move the zone boundaries inside the node if necessary */\n\t*zone_end_pfn = min(*zone_end_pfn, node_end_pfn);\n\t*zone_start_pfn = max(*zone_start_pfn, node_start_pfn);\n\n\t/* Return the spanned pages */\n\treturn *zone_end_pfn - *zone_start_pfn;\n}\n\n/*\n * Return the number of holes in a range on a node. If nid is MAX_NUMNODES,\n * then all holes in the requested range will be accounted for.\n */\nunsigned long __meminit __absent_pages_in_range(int nid,\n\t\t\t\tunsigned long range_start_pfn,\n\t\t\t\tunsigned long range_end_pfn)\n{\n\tunsigned long nr_absent = range_end_pfn - range_start_pfn;\n\tunsigned long start_pfn, end_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);\n\t\tend_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);\n\t\tnr_absent -= end_pfn - start_pfn;\n\t}\n\treturn nr_absent;\n}\n\n/**\n * absent_pages_in_range - Return number of page frames in holes within a range\n * @start_pfn: The start PFN to start searching for holes\n * @end_pfn: The end PFN to stop searching for holes\n *\n * It returns the number of pages frames in memory holes within a range.\n */\nunsigned long __init absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\treturn __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);\n}\n\n/* Return the number of page frames in holes in a zone on a node */\nstatic unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\tunsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type];\n\tunsigned long zone_high = arch_zone_highest_possible_pfn[zone_type];\n\tunsigned long zone_start_pfn, zone_end_pfn;\n\tunsigned long nr_absent;\n\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\tzone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);\n\tzone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);\n\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t&zone_start_pfn, &zone_end_pfn);\n\tnr_absent = __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn);\n\n\t/*\n\t * ZONE_MOVABLE handling.\n\t * Treat pages to be ZONE_MOVABLE in ZONE_NORMAL as absent pages\n\t * and vice versa.\n\t */\n\tif (mirrored_kernelcore && zone_movable_pfn[nid]) {\n\t\tunsigned long start_pfn, end_pfn;\n\t\tstruct memblock_region *r;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tstart_pfn = clamp(memblock_region_memory_base_pfn(r),\n\t\t\t\t\t  zone_start_pfn, zone_end_pfn);\n\t\t\tend_pfn = clamp(memblock_region_memory_end_pfn(r),\n\t\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t\t\tif (zone_type == ZONE_MOVABLE &&\n\t\t\t    memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\n\t\t\tif (zone_type == ZONE_NORMAL &&\n\t\t\t    !memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\t\t}\n\t}\n\n\treturn nr_absent;\n}\n\n#else /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\nstatic inline unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *zones_size)\n{\n\tunsigned int zone;\n\n\t*zone_start_pfn = node_start_pfn;\n\tfor (zone = 0; zone < zone_type; zone++)\n\t\t*zone_start_pfn += zones_size[zone];\n\n\t*zone_end_pfn = *zone_start_pfn + zones_size[zone_type];\n\n\treturn zones_size[zone_type];\n}\n\nstatic inline unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tif (!zholes_size)\n\t\treturn 0;\n\n\treturn zholes_size[zone_type];\n}\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nstatic void __meminit calculate_node_totalpages(struct pglist_data *pgdat,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zones_size,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tunsigned long realtotalpages = 0, totalpages = 0;\n\tenum zone_type i;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\tunsigned long zone_start_pfn, zone_end_pfn;\n\t\tunsigned long size, real_size;\n\n\t\tsize = zone_spanned_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn,\n\t\t\t\t\t\t  node_end_pfn,\n\t\t\t\t\t\t  &zone_start_pfn,\n\t\t\t\t\t\t  &zone_end_pfn,\n\t\t\t\t\t\t  zones_size);\n\t\treal_size = size - zone_absent_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn, node_end_pfn,\n\t\t\t\t\t\t  zholes_size);\n\t\tif (size)\n\t\t\tzone->zone_start_pfn = zone_start_pfn;\n\t\telse\n\t\t\tzone->zone_start_pfn = 0;\n\t\tzone->spanned_pages = size;\n\t\tzone->present_pages = real_size;\n\n\t\ttotalpages += size;\n\t\trealtotalpages += real_size;\n\t}\n\n\tpgdat->node_spanned_pages = totalpages;\n\tpgdat->node_present_pages = realtotalpages;\n\tprintk(KERN_DEBUG \"On node %d totalpages: %lu\\n\", pgdat->node_id,\n\t\t\t\t\t\t\trealtotalpages);\n}\n\n#ifndef CONFIG_SPARSEMEM\n/*\n * Calculate the size of the zone->blockflags rounded to an unsigned long\n * Start by making sure zonesize is a multiple of pageblock_order by rounding\n * up. Then use 1 NR_PAGEBLOCK_BITS worth of bits per pageblock, finally\n * round what is now in bits to nearest long in bits, then return it in\n * bytes.\n */\nstatic unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)\n{\n\tunsigned long usemapsize;\n\n\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);\n\tusemapsize = roundup(zonesize, pageblock_nr_pages);\n\tusemapsize = usemapsize >> pageblock_order;\n\tusemapsize *= NR_PAGEBLOCK_BITS;\n\tusemapsize = roundup(usemapsize, 8 * sizeof(unsigned long));\n\n\treturn usemapsize / 8;\n}\n\nstatic void __ref setup_usemap(struct pglist_data *pgdat,\n\t\t\t\tstruct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\tunsigned long zonesize)\n{\n\tunsigned long usemapsize = usemap_size(zone_start_pfn, zonesize);\n\tzone->pageblock_flags = NULL;\n\tif (usemapsize)\n\t\tzone->pageblock_flags =\n\t\t\tmemblock_alloc_node_nopanic(usemapsize,\n\t\t\t\t\t\t\t pgdat->node_id);\n}\n#else\nstatic inline void setup_usemap(struct pglist_data *pgdat, struct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn, unsigned long zonesize) {}\n#endif /* CONFIG_SPARSEMEM */\n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\n\n/* Initialise the number of pages represented by NR_PAGEBLOCK_BITS */\nvoid __init set_pageblock_order(void)\n{\n\tunsigned int order;\n\n\t/* Check that pageblock_nr_pages has not already been setup */\n\tif (pageblock_order)\n\t\treturn;\n\n\tif (HPAGE_SHIFT > PAGE_SHIFT)\n\t\torder = HUGETLB_PAGE_ORDER;\n\telse\n\t\torder = MAX_ORDER - 1;\n\n\t/*\n\t * Assume the largest contiguous order of interest is a huge page.\n\t * This value may be variable depending on boot parameters on IA64 and\n\t * powerpc.\n\t */\n\tpageblock_order = order;\n}\n#else /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\n/*\n * When CONFIG_HUGETLB_PAGE_SIZE_VARIABLE is not set, set_pageblock_order()\n * is unused as pageblock_order is set at compile-time. See\n * include/linux/pageblock-flags.h for the values of pageblock_order based on\n * the kernel config\n */\nvoid __init set_pageblock_order(void)\n{\n}\n\n#endif /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\nstatic unsigned long __init calc_memmap_size(unsigned long spanned_pages,\n\t\t\t\t\t\tunsigned long present_pages)\n{\n\tunsigned long pages = spanned_pages;\n\n\t/*\n\t * Provide a more accurate estimation if there are holes within\n\t * the zone and SPARSEMEM is in use. If there are holes within the\n\t * zone, each populated memory region may cost us one or two extra\n\t * memmap pages due to alignment because memmap pages for each\n\t * populated regions may not be naturally aligned on page boundary.\n\t * So the (present_pages >> 4) heuristic is a tradeoff for that.\n\t */\n\tif (spanned_pages > present_pages + (present_pages >> 4) &&\n\t    IS_ENABLED(CONFIG_SPARSEMEM))\n\t\tpages = present_pages;\n\n\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat)\n{\n\tspin_lock_init(&pgdat->split_queue_lock);\n\tINIT_LIST_HEAD(&pgdat->split_queue);\n\tpgdat->split_queue_len = 0;\n}\n#else\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat) {}\n#endif\n\n#ifdef CONFIG_COMPACTION\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat)\n{\n\tinit_waitqueue_head(&pgdat->kcompactd_wait);\n}\n#else\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n#endif\n\nstatic void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n{\n\tpgdat_resize_init(pgdat);\n\n\tpgdat_init_split_queue(pgdat);\n\tpgdat_init_kcompactd(pgdat);\n\n\tinit_waitqueue_head(&pgdat->kswapd_wait);\n\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\n\n\tpgdat_page_ext_init(pgdat);\n\tspin_lock_init(&pgdat->lru_lock);\n\tlruvec_init(node_lruvec(pgdat));\n}\n\nstatic void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,\n\t\t\t\t\t\t\tunsigned long remaining_pages)\n{\n\tzone->managed_pages = remaining_pages;\n\tzone_set_nid(zone, nid);\n\tzone->name = zone_names[idx];\n\tzone->zone_pgdat = NODE_DATA(nid);\n\tspin_lock_init(&zone->lock);\n\tzone_seqlock_init(zone);\n\tzone_pcp_init(zone);\n}\n\n/*\n * Set up the zone data structures\n * - init pgdat internals\n * - init all zones belonging to this node\n *\n * NOTE: this function is only called during memory hotplug\n */\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid __ref free_area_init_core_hotplug(int nid)\n{\n\tenum zone_type z;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tpgdat_init_internals(pgdat);\n\tfor (z = 0; z < MAX_NR_ZONES; z++)\n\t\tzone_init_internals(&pgdat->node_zones[z], z, nid, 0);\n}\n#endif\n\n/*\n * Set up the zone data structures:\n *   - mark all pages reserved\n *   - mark all memory queues empty\n *   - clear the memory bitmaps\n *\n * NOTE: pgdat should get zeroed by caller.\n * NOTE: this function is only called during early init.\n */\nstatic void __init free_area_init_core(struct pglist_data *pgdat)\n{\n\tenum zone_type j;\n\tint nid = pgdat->node_id;\n\n\tpgdat_init_internals(pgdat);\n\tpgdat->per_cpu_nodestats = &boot_nodestats;\n\n\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\tunsigned long size, freesize, memmap_pages;\n\t\tunsigned long zone_start_pfn = zone->zone_start_pfn;\n\n\t\tsize = zone->spanned_pages;\n\t\tfreesize = zone->present_pages;\n\n\t\t/*\n\t\t * Adjust freesize so that it accounts for how much memory\n\t\t * is used by this zone for memmap. This affects the watermark\n\t\t * and per-cpu initialisations\n\t\t */\n\t\tmemmap_pages = calc_memmap_size(size, freesize);\n\t\tif (!is_highmem_idx(j)) {\n\t\t\tif (freesize >= memmap_pages) {\n\t\t\t\tfreesize -= memmap_pages;\n\t\t\t\tif (memmap_pages)\n\t\t\t\t\tprintk(KERN_DEBUG\n\t\t\t\t\t       \"  %s zone: %lu pages used for memmap\\n\",\n\t\t\t\t\t       zone_names[j], memmap_pages);\n\t\t\t} else\n\t\t\t\tpr_warn(\"  %s zone: %lu pages exceeds freesize %lu\\n\",\n\t\t\t\t\tzone_names[j], memmap_pages, freesize);\n\t\t}\n\n\t\t/* Account for reserved pages */\n\t\tif (j == 0 && freesize > dma_reserve) {\n\t\t\tfreesize -= dma_reserve;\n\t\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages reserved\\n\",\n\t\t\t\t\tzone_names[0], dma_reserve);\n\t\t}\n\n\t\tif (!is_highmem_idx(j))\n\t\t\tnr_kernel_pages += freesize;\n\t\t/* Charge for highmem memmap if there are enough kernel pages */\n\t\telse if (nr_kernel_pages > memmap_pages * 2)\n\t\t\tnr_kernel_pages -= memmap_pages;\n\t\tnr_all_pages += freesize;\n\n\t\t/*\n\t\t * Set an approximate value for lowmem here, it will be adjusted\n\t\t * when the bootmem allocator frees pages into the buddy system.\n\t\t * And all highmem pages will be managed by the buddy system.\n\t\t */\n\t\tzone_init_internals(zone, j, nid, freesize);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\n\t\tset_pageblock_order();\n\t\tsetup_usemap(pgdat, zone, zone_start_pfn, size);\n\t\tinit_currently_empty_zone(zone, zone_start_pfn, size);\n\t\tmemmap_init(size, nid, j, zone_start_pfn);\n\t}\n}\n\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat)\n{\n\tunsigned long __maybe_unused start = 0;\n\tunsigned long __maybe_unused offset = 0;\n\n\t/* Skip empty nodes */\n\tif (!pgdat->node_spanned_pages)\n\t\treturn;\n\n\tstart = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);\n\toffset = pgdat->node_start_pfn - start;\n\t/* ia64 gets its own node_mem_map, before this, without bootmem */\n\tif (!pgdat->node_mem_map) {\n\t\tunsigned long size, end;\n\t\tstruct page *map;\n\n\t\t/*\n\t\t * The zone's endpoints aren't required to be MAX_ORDER\n\t\t * aligned but the node_mem_map endpoints must be in order\n\t\t * for the buddy allocator to function correctly.\n\t\t */\n\t\tend = pgdat_end_pfn(pgdat);\n\t\tend = ALIGN(end, MAX_ORDER_NR_PAGES);\n\t\tsize =  (end - start) * sizeof(struct page);\n\t\tmap = memblock_alloc_node_nopanic(size, pgdat->node_id);\n\t\tpgdat->node_mem_map = map + offset;\n\t}\n\tpr_debug(\"%s: node %d, pgdat %08lx, node_mem_map %08lx\\n\",\n\t\t\t\t__func__, pgdat->node_id, (unsigned long)pgdat,\n\t\t\t\t(unsigned long)pgdat->node_mem_map);\n#ifndef CONFIG_NEED_MULTIPLE_NODES\n\t/*\n\t * With no DISCONTIG, the global mem_map is just set as node 0's\n\t */\n\tif (pgdat == NODE_DATA(0)) {\n\t\tmem_map = NODE_DATA(0)->node_mem_map;\n#if defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) || defined(CONFIG_FLATMEM)\n\t\tif (page_to_pfn(mem_map) != pgdat->node_start_pfn)\n\t\t\tmem_map -= offset;\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\t}\n#endif\n}\n#else\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }\n#endif /* CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat)\n{\n\t/*\n\t * We start only with one section of pages, more pages are added as\n\t * needed until the rest of deferred pages are initialized.\n\t */\n\tpgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,\n\t\t\t\t\t\tpgdat->node_spanned_pages);\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n}\n#else\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}\n#endif\n\nvoid __init free_area_init_node(int nid, unsigned long *zones_size,\n\t\t\t\t   unsigned long node_start_pfn,\n\t\t\t\t   unsigned long *zholes_size)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tunsigned long start_pfn = 0;\n\tunsigned long end_pfn = 0;\n\n\t/* pg_data_t should be reset to zero when it's allocated */\n\tWARN_ON(pgdat->nr_zones || pgdat->kswapd_classzone_idx);\n\n\tpgdat->node_id = nid;\n\tpgdat->node_start_pfn = node_start_pfn;\n\tpgdat->per_cpu_nodestats = NULL;\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);\n\tpr_info(\"Initmem setup node %d [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\tend_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);\n#else\n\tstart_pfn = node_start_pfn;\n#endif\n\tcalculate_node_totalpages(pgdat, start_pfn, end_pfn,\n\t\t\t\t  zones_size, zholes_size);\n\n\talloc_node_mem_map(pgdat);\n\tpgdat_set_deferred_range(pgdat);\n\n\tfree_area_init_core(pgdat);\n}\n\n#if !defined(CONFIG_FLAT_NODE_MEM_MAP)\n/*\n * Zero all valid struct pages in range [spfn, epfn), return number of struct\n * pages zeroed\n */\nstatic u64 zero_pfn_range(unsigned long spfn, unsigned long epfn)\n{\n\tunsigned long pfn;\n\tu64 pgcnt = 0;\n\n\tfor (pfn = spfn; pfn < epfn; pfn++) {\n\t\tif (!pfn_valid(ALIGN_DOWN(pfn, pageblock_nr_pages))) {\n\t\t\tpfn = ALIGN_DOWN(pfn, pageblock_nr_pages)\n\t\t\t\t+ pageblock_nr_pages - 1;\n\t\t\tcontinue;\n\t\t}\n\t\tmm_zero_struct_page(pfn_to_page(pfn));\n\t\tpgcnt++;\n\t}\n\n\treturn pgcnt;\n}\n\n/*\n * Only struct pages that are backed by physical memory are zeroed and\n * initialized by going through __init_single_page(). But, there are some\n * struct pages which are reserved in memblock allocator and their fields\n * may be accessed (for example page_to_pfn() on some configuration accesses\n * flags). We must explicitly zero those struct pages.\n *\n * This function also addresses a similar issue where struct pages are left\n * uninitialized because the physical address range is not covered by\n * memblock.memory or memblock.reserved. That could happen when memblock\n * layout is manually configured via memmap=.\n */\nvoid __init zero_resv_unavail(void)\n{\n\tphys_addr_t start, end;\n\tu64 i, pgcnt;\n\tphys_addr_t next = 0;\n\n\t/*\n\t * Loop through unavailable ranges not covered by memblock.memory.\n\t */\n\tpgcnt = 0;\n\tfor_each_mem_range(i, &memblock.memory, NULL,\n\t\t\tNUMA_NO_NODE, MEMBLOCK_NONE, &start, &end, NULL) {\n\t\tif (next < start)\n\t\t\tpgcnt += zero_pfn_range(PFN_DOWN(next), PFN_UP(start));\n\t\tnext = end;\n\t}\n\tpgcnt += zero_pfn_range(PFN_DOWN(next), max_pfn);\n\n\t/*\n\t * Struct pages that do not have backing memory. This could be because\n\t * firmware is using some of this memory, or for some other reasons.\n\t */\n\tif (pgcnt)\n\t\tpr_info(\"Zeroed struct page in unavailable ranges: %lld pages\", pgcnt);\n}\n#endif /* !CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\n#if MAX_NUMNODES > 1\n/*\n * Figure out the number of possible node ids.\n */\nvoid __init setup_nr_node_ids(void)\n{\n\tunsigned int highest;\n\n\thighest = find_last_bit(node_possible_map.bits, MAX_NUMNODES);\n\tnr_node_ids = highest + 1;\n}\n#endif\n\n/**\n * node_map_pfn_alignment - determine the maximum internode alignment\n *\n * This function should be called after node map is populated and sorted.\n * It calculates the maximum power of two alignment which can distinguish\n * all the nodes.\n *\n * For example, if all nodes are 1GiB and aligned to 1GiB, the return value\n * would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the\n * nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is\n * shifted, 1GiB is enough and this function will indicate so.\n *\n * This is used to test whether pfn -> nid mapping of the chosen memory\n * model has fine enough granularity to avoid incorrect mapping for the\n * populated node map.\n *\n * Returns the determined alignment in pfn's.  0 if there is no alignment\n * requirement (single node).\n */\nunsigned long __init node_map_pfn_alignment(void)\n{\n\tunsigned long accl_mask = 0, last_end = 0;\n\tunsigned long start, end, mask;\n\tint last_nid = -1;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid) {\n\t\tif (!start || last_nid < 0 || last_nid == nid) {\n\t\t\tlast_nid = nid;\n\t\t\tlast_end = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Start with a mask granular enough to pin-point to the\n\t\t * start pfn and tick off bits one-by-one until it becomes\n\t\t * too coarse to separate the current node from the last.\n\t\t */\n\t\tmask = ~((1 << __ffs(start)) - 1);\n\t\twhile (mask && last_end <= (start & (mask << 1)))\n\t\t\tmask <<= 1;\n\n\t\t/* accumulate all internode masks */\n\t\taccl_mask |= mask;\n\t}\n\n\t/* convert mask to number of pages */\n\treturn ~accl_mask + 1;\n}\n\n/* Find the lowest pfn for a node */\nstatic unsigned long __init find_min_pfn_for_node(int nid)\n{\n\tunsigned long min_pfn = ULONG_MAX;\n\tunsigned long start_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, NULL, NULL)\n\t\tmin_pfn = min(min_pfn, start_pfn);\n\n\tif (min_pfn == ULONG_MAX) {\n\t\tpr_warn(\"Could not find start_pfn for node %d\\n\", nid);\n\t\treturn 0;\n\t}\n\n\treturn min_pfn;\n}\n\n/**\n * find_min_pfn_with_active_regions - Find the minimum PFN registered\n *\n * It returns the minimum PFN based on information provided via\n * memblock_set_node().\n */\nunsigned long __init find_min_pfn_with_active_regions(void)\n{\n\treturn find_min_pfn_for_node(MAX_NUMNODES);\n}\n\n/*\n * early_calculate_totalpages()\n * Sum pages in active regions for movable zone.\n * Populate N_MEMORY for calculating usable_nodes.\n */\nstatic unsigned long __init early_calculate_totalpages(void)\n{\n\tunsigned long totalpages = 0;\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tunsigned long pages = end_pfn - start_pfn;\n\n\t\ttotalpages += pages;\n\t\tif (pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t}\n\treturn totalpages;\n}\n\n/*\n * Find the PFN the Movable zone begins in each node. Kernel memory\n * is spread evenly between nodes as long as the nodes have enough\n * memory. When they don't, some nodes will have more kernelcore than\n * others\n */\nstatic void __init find_zone_movable_pfns_for_nodes(void)\n{\n\tint i, nid;\n\tunsigned long usable_startpfn;\n\tunsigned long kernelcore_node, kernelcore_remaining;\n\t/* save the state before borrow the nodemask */\n\tnodemask_t saved_node_state = node_states[N_MEMORY];\n\tunsigned long totalpages = early_calculate_totalpages();\n\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);\n\tstruct memblock_region *r;\n\n\t/* Need to find movable_zone earlier when movable_node is specified. */\n\tfind_usable_zone_for_movable();\n\n\t/*\n\t * If movable_node is specified, ignore kernelcore and movablecore\n\t * options.\n\t */\n\tif (movable_node_is_enabled()) {\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (!memblock_is_hotpluggable(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = PFN_DOWN(r->base);\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=mirror is specified, ignore movablecore option\n\t */\n\tif (mirrored_kernelcore) {\n\t\tbool mem_below_4gb_not_mirrored = false;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (memblock_is_mirror(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);\n\n\t\t\tif (usable_startpfn < 0x100000) {\n\t\t\t\tmem_below_4gb_not_mirrored = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tif (mem_below_4gb_not_mirrored)\n\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\");\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=nn% or movablecore=nn% was specified, calculate the\n\t * amount of necessary memory.\n\t */\n\tif (required_kernelcore_percent)\n\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /\n\t\t\t\t       10000UL;\n\tif (required_movablecore_percent)\n\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /\n\t\t\t\t\t10000UL;\n\n\t/*\n\t * If movablecore= was specified, calculate what size of\n\t * kernelcore that corresponds so that memory usable for\n\t * any allocation type is evenly spread. If both kernelcore\n\t * and movablecore are specified, then the value of kernelcore\n\t * will be used for required_kernelcore if it's greater than\n\t * what movablecore would have allowed.\n\t */\n\tif (required_movablecore) {\n\t\tunsigned long corepages;\n\n\t\t/*\n\t\t * Round-up so that ZONE_MOVABLE is at least as large as what\n\t\t * was requested by the user\n\t\t */\n\t\trequired_movablecore =\n\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);\n\t\trequired_movablecore = min(totalpages, required_movablecore);\n\t\tcorepages = totalpages - required_movablecore;\n\n\t\trequired_kernelcore = max(required_kernelcore, corepages);\n\t}\n\n\t/*\n\t * If kernelcore was not specified or kernelcore size is larger\n\t * than totalpages, there is no ZONE_MOVABLE.\n\t */\n\tif (!required_kernelcore || required_kernelcore >= totalpages)\n\t\tgoto out;\n\n\t/* usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */\n\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];\n\nrestart:\n\t/* Spread kernelcore memory as evenly as possible throughout nodes */\n\tkernelcore_node = required_kernelcore / usable_nodes;\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\t/*\n\t\t * Recalculate kernelcore_node if the division per node\n\t\t * now exceeds what is necessary to satisfy the requested\n\t\t * amount of memory for the kernel\n\t\t */\n\t\tif (required_kernelcore < kernelcore_node)\n\t\t\tkernelcore_node = required_kernelcore / usable_nodes;\n\n\t\t/*\n\t\t * As the map is walked, we track how much memory is usable\n\t\t * by the kernel using kernelcore_remaining. When it is\n\t\t * 0, the rest of the node is usable by ZONE_MOVABLE\n\t\t */\n\t\tkernelcore_remaining = kernelcore_node;\n\n\t\t/* Go through each range of PFNs within this node */\n\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\t\tunsigned long size_pages;\n\n\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);\n\t\t\tif (start_pfn >= end_pfn)\n\t\t\t\tcontinue;\n\n\t\t\t/* Account for what is only usable for kernelcore */\n\t\t\tif (start_pfn < usable_startpfn) {\n\t\t\t\tunsigned long kernel_pages;\n\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)\n\t\t\t\t\t\t\t\t- start_pfn;\n\n\t\t\t\tkernelcore_remaining -= min(kernel_pages,\n\t\t\t\t\t\t\tkernelcore_remaining);\n\t\t\t\trequired_kernelcore -= min(kernel_pages,\n\t\t\t\t\t\t\trequired_kernelcore);\n\n\t\t\t\t/* Continue if range is now fully accounted */\n\t\t\t\tif (end_pfn <= usable_startpfn) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Push zone_movable_pfn to the end so\n\t\t\t\t\t * that if we have to rebalance\n\t\t\t\t\t * kernelcore across nodes, we will\n\t\t\t\t\t * not double account here\n\t\t\t\t\t */\n\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstart_pfn = usable_startpfn;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * The usable PFN range for ZONE_MOVABLE is from\n\t\t\t * start_pfn->end_pfn. Calculate size_pages as the\n\t\t\t * number of pages used as kernelcore\n\t\t\t */\n\t\t\tsize_pages = end_pfn - start_pfn;\n\t\t\tif (size_pages > kernelcore_remaining)\n\t\t\t\tsize_pages = kernelcore_remaining;\n\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;\n\n\t\t\t/*\n\t\t\t * Some kernelcore has been met, update counts and\n\t\t\t * break if the kernelcore for this node has been\n\t\t\t * satisfied\n\t\t\t */\n\t\t\trequired_kernelcore -= min(required_kernelcore,\n\t\t\t\t\t\t\t\tsize_pages);\n\t\t\tkernelcore_remaining -= size_pages;\n\t\t\tif (!kernelcore_remaining)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * If there is still required_kernelcore, we do another pass with one\n\t * less node in the count. This will push zone_movable_pfn[nid] further\n\t * along on the nodes that still have memory until kernelcore is\n\t * satisfied\n\t */\n\tusable_nodes--;\n\tif (usable_nodes && required_kernelcore > usable_nodes)\n\t\tgoto restart;\n\nout2:\n\t/* Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */\n\tfor (nid = 0; nid < MAX_NUMNODES; nid++)\n\t\tzone_movable_pfn[nid] =\n\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);\n\nout:\n\t/* restore the node_state */\n\tnode_states[N_MEMORY] = saved_node_state;\n}\n\n/* Any regular or high memory on that node ? */\nstatic void check_for_memory(pg_data_t *pgdat, int nid)\n{\n\tenum zone_type zone_type;\n\n\tfor (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\t\tif (populated_zone(zone)) {\n\t\t\tif (IS_ENABLED(CONFIG_HIGHMEM))\n\t\t\t\tnode_set_state(nid, N_HIGH_MEMORY);\n\t\t\tif (zone_type <= ZONE_NORMAL)\n\t\t\t\tnode_set_state(nid, N_NORMAL_MEMORY);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/**\n * free_area_init_nodes - Initialise all pg_data_t and zone data\n * @max_zone_pfn: an array of max PFNs for each zone\n *\n * This will call free_area_init_node() for each active node in the system.\n * Using the page ranges provided by memblock_set_node(), the size of each\n * zone in each node and their holes is calculated. If the maximum PFN\n * between two adjacent zones match, it is assumed that the zone is empty.\n * For example, if arch_max_dma_pfn == arch_max_dma32_pfn, it is assumed\n * that arch_max_dma32_pfn has no pages. It is also assumed that a zone\n * starts where the previous one ended. For example, ZONE_DMA32 starts\n * at arch_max_dma_pfn.\n */\nvoid __init free_area_init_nodes(unsigned long *max_zone_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\t/* Record where the zone boundaries are */\n\tmemset(arch_zone_lowest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_lowest_possible_pfn));\n\tmemset(arch_zone_highest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_highest_possible_pfn));\n\n\tstart_pfn = find_min_pfn_with_active_regions();\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tend_pfn = max(max_zone_pfn[i], start_pfn);\n\t\tarch_zone_lowest_possible_pfn[i] = start_pfn;\n\t\tarch_zone_highest_possible_pfn[i] = end_pfn;\n\n\t\tstart_pfn = end_pfn;\n\t}\n\n\t/* Find the PFNs that ZONE_MOVABLE begins at in each node */\n\tmemset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));\n\tfind_zone_movable_pfns_for_nodes();\n\n\t/* Print out the zone ranges */\n\tpr_info(\"Zone ranges:\\n\");\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\t\tpr_info(\"  %-8s \", zone_names[i]);\n\t\tif (arch_zone_lowest_possible_pfn[i] ==\n\t\t\t\tarch_zone_highest_possible_pfn[i])\n\t\t\tpr_cont(\"empty\\n\");\n\t\telse\n\t\t\tpr_cont(\"[mem %#018Lx-%#018Lx]\\n\",\n\t\t\t\t(u64)arch_zone_lowest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT,\n\t\t\t\t((u64)arch_zone_highest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT) - 1);\n\t}\n\n\t/* Print out the PFNs ZONE_MOVABLE begins at in each node */\n\tpr_info(\"Movable zone start for each node\\n\");\n\tfor (i = 0; i < MAX_NUMNODES; i++) {\n\t\tif (zone_movable_pfn[i])\n\t\t\tpr_info(\"  Node %d: %#018Lx\\n\", i,\n\t\t\t       (u64)zone_movable_pfn[i] << PAGE_SHIFT);\n\t}\n\n\t/* Print out the early node map */\n\tpr_info(\"Early memory node ranges\\n\");\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid)\n\t\tpr_info(\"  node %3d: [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\t\t((u64)end_pfn << PAGE_SHIFT) - 1);\n\n\t/* Initialise every node */\n\tmminit_verify_pageflags_layout();\n\tsetup_nr_node_ids();\n\tzero_resv_unavail();\n\tfor_each_online_node(nid) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tfree_area_init_node(nid, NULL,\n\t\t\t\tfind_min_pfn_for_node(nid), NULL);\n\n\t\t/* Any memory on that node */\n\t\tif (pgdat->node_present_pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t\tcheck_for_memory(pgdat, nid);\n\t}\n}\n\nstatic int __init cmdline_parse_core(char *p, unsigned long *core,\n\t\t\t\t     unsigned long *percent)\n{\n\tunsigned long long coremem;\n\tchar *endptr;\n\n\tif (!p)\n\t\treturn -EINVAL;\n\n\t/* Value may be a percentage of total memory, otherwise bytes */\n\tcoremem = simple_strtoull(p, &endptr, 0);\n\tif (*endptr == '%') {\n\t\t/* Paranoid check for percent values greater than 100 */\n\t\tWARN_ON(coremem > 100);\n\n\t\t*percent = coremem;\n\t} else {\n\t\tcoremem = memparse(p, &p);\n\t\t/* Paranoid check that UL is enough for the coremem value */\n\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);\n\n\t\t*core = coremem >> PAGE_SHIFT;\n\t\t*percent = 0UL;\n\t}\n\treturn 0;\n}\n\n/*\n * kernelcore=size sets the amount of memory for use for allocations that\n * cannot be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_kernelcore(char *p)\n{\n\t/* parse kernelcore=mirror */\n\tif (parse_option_str(p, \"mirror\")) {\n\t\tmirrored_kernelcore = true;\n\t\treturn 0;\n\t}\n\n\treturn cmdline_parse_core(p, &required_kernelcore,\n\t\t\t\t  &required_kernelcore_percent);\n}\n\n/*\n * movablecore=size sets the amount of memory for use for allocations that\n * can be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_movablecore(char *p)\n{\n\treturn cmdline_parse_core(p, &required_movablecore,\n\t\t\t\t  &required_movablecore_percent);\n}\n\nearly_param(\"kernelcore\", cmdline_parse_kernelcore);\nearly_param(\"movablecore\", cmdline_parse_movablecore);\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nvoid adjust_managed_page_count(struct page *page, long count)\n{\n\tspin_lock(&managed_page_count_lock);\n\tpage_zone(page)->managed_pages += count;\n\ttotalram_pages += count;\n#ifdef CONFIG_HIGHMEM\n\tif (PageHighMem(page))\n\t\ttotalhigh_pages += count;\n#endif\n\tspin_unlock(&managed_page_count_lock);\n}\nEXPORT_SYMBOL(adjust_managed_page_count);\n\nunsigned long free_reserved_area(void *start, void *end, int poison, char *s)\n{\n\tvoid *pos;\n\tunsigned long pages = 0;\n\n\tstart = (void *)PAGE_ALIGN((unsigned long)start);\n\tend = (void *)((unsigned long)end & PAGE_MASK);\n\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {\n\t\tstruct page *page = virt_to_page(pos);\n\t\tvoid *direct_map_addr;\n\n\t\t/*\n\t\t * 'direct_map_addr' might be different from 'pos'\n\t\t * because some architectures' virt_to_page()\n\t\t * work with aliases.  Getting the direct map\n\t\t * address ensures that we get a _writeable_\n\t\t * alias for the memset().\n\t\t */\n\t\tdirect_map_addr = page_address(page);\n\t\tif ((unsigned int)poison <= 0xFF)\n\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);\n\n\t\tfree_reserved_page(page);\n\t}\n\n\tif (pages && s)\n\t\tpr_info(\"Freeing %s memory: %ldK\\n\",\n\t\t\ts, pages << (PAGE_SHIFT - 10));\n\n\treturn pages;\n}\nEXPORT_SYMBOL(free_reserved_area);\n\n#ifdef\tCONFIG_HIGHMEM\nvoid free_highmem_page(struct page *page)\n{\n\t__free_reserved_page(page);\n\ttotalram_pages++;\n\tpage_zone(page)->managed_pages++;\n\ttotalhigh_pages++;\n}\n#endif\n\n\nvoid __init mem_init_print_info(const char *str)\n{\n\tunsigned long physpages, codesize, datasize, rosize, bss_size;\n\tunsigned long init_code_size, init_data_size;\n\n\tphyspages = get_num_physpages();\n\tcodesize = _etext - _stext;\n\tdatasize = _edata - _sdata;\n\trosize = __end_rodata - __start_rodata;\n\tbss_size = __bss_stop - __bss_start;\n\tinit_data_size = __init_end - __init_begin;\n\tinit_code_size = _einittext - _sinittext;\n\n\t/*\n\t * Detect special cases and adjust section sizes accordingly:\n\t * 1) .init.* may be embedded into .data sections\n\t * 2) .init.text.* may be out of [__init_begin, __init_end],\n\t *    please refer to arch/tile/kernel/vmlinux.lds.S.\n\t * 3) .rodata.* may be embedded into .text or .data sections.\n\t */\n#define adj_init_size(start, end, size, pos, adj) \\\n\tdo { \\\n\t\tif (start <= pos && pos < end && size > adj) \\\n\t\t\tsize -= adj; \\\n\t} while (0)\n\n\tadj_init_size(__init_begin, __init_end, init_data_size,\n\t\t     _sinittext, init_code_size);\n\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);\n\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);\n\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);\n\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);\n\n#undef\tadj_init_size\n\n\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"\n#ifdef\tCONFIG_HIGHMEM\n\t\t\", %luK highmem\"\n#endif\n\t\t\"%s%s)\\n\",\n\t\tnr_free_pages() << (PAGE_SHIFT - 10),\n\t\tphyspages << (PAGE_SHIFT - 10),\n\t\tcodesize >> 10, datasize >> 10, rosize >> 10,\n\t\t(init_data_size + init_code_size) >> 10, bss_size >> 10,\n\t\t(physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT - 10),\n\t\ttotalcma_pages << (PAGE_SHIFT - 10),\n#ifdef\tCONFIG_HIGHMEM\n\t\ttotalhigh_pages << (PAGE_SHIFT - 10),\n#endif\n\t\tstr ? \", \" : \"\", str ? str : \"\");\n}\n\n/**\n * set_dma_reserve - set the specified number of pages reserved in the first zone\n * @new_dma_reserve: The number of pages to mark reserved\n *\n * The per-cpu batchsize and zone watermarks are determined by managed_pages.\n * In the DMA zone, a significant percentage may be consumed by kernel image\n * and other unfreeable allocations which can skew the watermarks badly. This\n * function may optionally be used to account for unfreeable pages in the\n * first zone (e.g., ZONE_DMA). The effect will be lower watermarks and\n * smaller per-cpu batchsize.\n */\nvoid __init set_dma_reserve(unsigned long new_dma_reserve)\n{\n\tdma_reserve = new_dma_reserve;\n}\n\nvoid __init free_area_init(unsigned long *zones_size)\n{\n\tzero_resv_unavail();\n\tfree_area_init_node(0, zones_size,\n\t\t\t__pa(PAGE_OFFSET) >> PAGE_SHIFT, NULL);\n}\n\nstatic int page_alloc_cpu_dead(unsigned int cpu)\n{\n\n\tlru_add_drain_cpu(cpu);\n\tdrain_pages(cpu);\n\n\t/*\n\t * Spill the event counters of the dead processor\n\t * into the current processors event counters.\n\t * This artificially elevates the count of the current\n\t * processor.\n\t */\n\tvm_events_fold_cpu(cpu);\n\n\t/*\n\t * Zero the differential counters of the dead processor\n\t * so that the vm statistics are consistent.\n\t *\n\t * This is only okay since the processor is dead and cannot\n\t * race with what we are doing.\n\t */\n\tcpu_vm_stats_fold(cpu);\n\treturn 0;\n}\n\nvoid __init page_alloc_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC_DEAD,\n\t\t\t\t\t\"mm/page_alloc:dead\", NULL,\n\t\t\t\t\tpage_alloc_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n\n/*\n * calculate_totalreserve_pages - called when sysctl_lowmem_reserve_ratio\n *\tor min_free_kbytes changes.\n */\nstatic void calculate_totalreserve_pages(void)\n{\n\tstruct pglist_data *pgdat;\n\tunsigned long reserve_pages = 0;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\n\t\tpgdat->totalreserve_pages = 0;\n\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\t\tlong max = 0;\n\n\t\t\t/* Find valid and maximum lowmem_reserve in the zone */\n\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {\n\t\t\t\tif (zone->lowmem_reserve[j] > max)\n\t\t\t\t\tmax = zone->lowmem_reserve[j];\n\t\t\t}\n\n\t\t\t/* we treat the high watermark as reserved pages. */\n\t\t\tmax += high_wmark_pages(zone);\n\n\t\t\tif (max > zone->managed_pages)\n\t\t\t\tmax = zone->managed_pages;\n\n\t\t\tpgdat->totalreserve_pages += max;\n\n\t\t\treserve_pages += max;\n\t\t}\n\t}\n\ttotalreserve_pages = reserve_pages;\n}\n\n/*\n * setup_per_zone_lowmem_reserve - called whenever\n *\tsysctl_lowmem_reserve_ratio changes.  Ensures that each zone\n *\thas a correct pages reserved value, so an adequate number of\n *\tpages are left in the zone after a successful __alloc_pages().\n */\nstatic void setup_per_zone_lowmem_reserve(void)\n{\n\tstruct pglist_data *pgdat;\n\tenum zone_type j, idx;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\t\tunsigned long managed_pages = zone->managed_pages;\n\n\t\t\tzone->lowmem_reserve[j] = 0;\n\n\t\t\tidx = j;\n\t\t\twhile (idx) {\n\t\t\t\tstruct zone *lower_zone;\n\n\t\t\t\tidx--;\n\t\t\t\tlower_zone = pgdat->node_zones + idx;\n\n\t\t\t\tif (sysctl_lowmem_reserve_ratio[idx] < 1) {\n\t\t\t\t\tsysctl_lowmem_reserve_ratio[idx] = 0;\n\t\t\t\t\tlower_zone->lowmem_reserve[j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\tlower_zone->lowmem_reserve[j] =\n\t\t\t\t\t\tmanaged_pages / sysctl_lowmem_reserve_ratio[idx];\n\t\t\t\t}\n\t\t\t\tmanaged_pages += lower_zone->managed_pages;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\nstatic void __setup_per_zone_wmarks(void)\n{\n\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);\n\tunsigned long lowmem_pages = 0;\n\tstruct zone *zone;\n\tunsigned long flags;\n\n\t/* Calculate total number of !ZONE_HIGHMEM pages */\n\tfor_each_zone(zone) {\n\t\tif (!is_highmem(zone))\n\t\t\tlowmem_pages += zone->managed_pages;\n\t}\n\n\tfor_each_zone(zone) {\n\t\tu64 tmp;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\ttmp = (u64)pages_min * zone->managed_pages;\n\t\tdo_div(tmp, lowmem_pages);\n\t\tif (is_highmem(zone)) {\n\t\t\t/*\n\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't\n\t\t\t * need highmem pages, so cap pages_min to a small\n\t\t\t * value here.\n\t\t\t *\n\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)\n\t\t\t * deltas control asynch page reclaim, and so should\n\t\t\t * not be capped for highmem.\n\t\t\t */\n\t\t\tunsigned long min_pages;\n\n\t\t\tmin_pages = zone->managed_pages / 1024;\n\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);\n\t\t\tzone->watermark[WMARK_MIN] = min_pages;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If it's a lowmem zone, reserve a number of pages\n\t\t\t * proportionate to the zone's size.\n\t\t\t */\n\t\t\tzone->watermark[WMARK_MIN] = tmp;\n\t\t}\n\n\t\t/*\n\t\t * Set the kswapd watermarks distance according to the\n\t\t * scale factor in proportion to available memory, but\n\t\t * ensure a minimum size on small systems.\n\t\t */\n\t\ttmp = max_t(u64, tmp >> 2,\n\t\t\t    mult_frac(zone->managed_pages,\n\t\t\t\t      watermark_scale_factor, 10000));\n\n\t\tzone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n\t\tzone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;\n\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\n/**\n * setup_per_zone_wmarks - called when min_free_kbytes changes\n * or when memory is hot-{added|removed}\n *\n * Ensures that the watermark[min,low,high] values for each zone are set\n * correctly with respect to min_free_kbytes.\n */\nvoid setup_per_zone_wmarks(void)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\t__setup_per_zone_wmarks();\n\tspin_unlock(&lock);\n}\n\n/*\n * Initialise min_free_kbytes.\n *\n * For small machines we want it small (128k min).  For large machines\n * we want it large (64MB max).  But it is not linear, because network\n * bandwidth does not increase linearly with machine size.  We use\n *\n *\tmin_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:\n *\tmin_free_kbytes = sqrt(lowmem_kbytes * 16)\n *\n * which yields\n *\n * 16MB:\t512k\n * 32MB:\t724k\n * 64MB:\t1024k\n * 128MB:\t1448k\n * 256MB:\t2048k\n * 512MB:\t2896k\n * 1024MB:\t4096k\n * 2048MB:\t5792k\n * 4096MB:\t8192k\n * 8192MB:\t11584k\n * 16384MB:\t16384k\n */\nint __meminit init_per_zone_wmark_min(void)\n{\n\tunsigned long lowmem_kbytes;\n\tint new_min_free_kbytes;\n\n\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);\n\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);\n\n\tif (new_min_free_kbytes > user_min_free_kbytes) {\n\t\tmin_free_kbytes = new_min_free_kbytes;\n\t\tif (min_free_kbytes < 128)\n\t\t\tmin_free_kbytes = 128;\n\t\tif (min_free_kbytes > 65536)\n\t\t\tmin_free_kbytes = 65536;\n\t} else {\n\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",\n\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);\n\t}\n\tsetup_per_zone_wmarks();\n\trefresh_zone_stat_thresholds();\n\tsetup_per_zone_lowmem_reserve();\n\n#ifdef CONFIG_NUMA\n\tsetup_min_unmapped_ratio();\n\tsetup_min_slab_ratio();\n#endif\n\n\treturn 0;\n}\ncore_initcall(init_per_zone_wmark_min)\n\n/*\n * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so\n *\tthat we can call two helper functions whenever min_free_kbytes\n *\tchanges.\n */\nint min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write) {\n\t\tuser_min_free_kbytes = min_free_kbytes;\n\t\tsetup_per_zone_wmarks();\n\t}\n\treturn 0;\n}\n\nint watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write)\n\t\tsetup_per_zone_wmarks();\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic void setup_min_unmapped_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_unmapped_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_unmapped_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_unmapped_ratio) / 100;\n}\n\n\nint sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_unmapped_ratio();\n\n\treturn 0;\n}\n\nstatic void setup_min_slab_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_slab_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_slab_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_slab_ratio) / 100;\n}\n\nint sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_slab_ratio();\n\n\treturn 0;\n}\n#endif\n\n/*\n * lowmem_reserve_ratio_sysctl_handler - just a wrapper around\n *\tproc_dointvec() so that we can call setup_per_zone_lowmem_reserve()\n *\twhenever sysctl_lowmem_reserve_ratio changes.\n *\n * The reserve ratio obviously has absolutely no relation with the\n * minimum watermarks. The lowmem reserve ratio can only make sense\n * if in function of the boot time zone sizes.\n */\nint lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tproc_dointvec_minmax(table, write, buffer, length, ppos);\n\tsetup_per_zone_lowmem_reserve();\n\treturn 0;\n}\n\n/*\n * percpu_pagelist_fraction - changes the pcp->high for each zone on each\n * cpu.  It is the fraction of total pages in each zone that a hot per cpu\n * pagelist can have before it gets flushed back to buddy allocator.\n */\nint percpu_pagelist_fraction_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct zone *zone;\n\tint old_percpu_pagelist_fraction;\n\tint ret;\n\n\tmutex_lock(&pcp_batch_high_lock);\n\told_percpu_pagelist_fraction = percpu_pagelist_fraction;\n\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (!write || ret < 0)\n\t\tgoto out;\n\n\t/* Sanity checking to avoid pcp imbalance */\n\tif (percpu_pagelist_fraction &&\n\t    percpu_pagelist_fraction < MIN_PERCPU_PAGELIST_FRACTION) {\n\t\tpercpu_pagelist_fraction = old_percpu_pagelist_fraction;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* No change? */\n\tif (percpu_pagelist_fraction == old_percpu_pagelist_fraction)\n\t\tgoto out;\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int cpu;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tpageset_set_high_and_batch(zone,\n\t\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\t}\nout:\n\tmutex_unlock(&pcp_batch_high_lock);\n\treturn ret;\n}\n\n#ifdef CONFIG_NUMA\nint hashdist = HASHDIST_DEFAULT;\n\nstatic int __init set_hashdist(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\thashdist = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"hashdist=\", set_hashdist);\n#endif\n\n#ifndef __HAVE_ARCH_RESERVED_KERNEL_PAGES\n/*\n * Returns the number of pages that arch has reserved but\n * is not known to alloc_large_system_hash().\n */\nstatic unsigned long __init arch_reserved_kernel_pages(void)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Adaptive scale is meant to reduce sizes of hash tables on large memory\n * machines. As memory size is increased the scale is also increased but at\n * slower pace.  Starting from ADAPT_SCALE_BASE (64G), every time memory\n * quadruples the scale is increased by one, which means the size of hash table\n * only doubles, instead of quadrupling as well.\n * Because 32-bit systems cannot have large physical memory, where this scaling\n * makes sense, it is disabled on such platforms.\n */\n#if __BITS_PER_LONG > 32\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#endif\n\n/*\n * allocate a large system hash table from bootmem\n * - it is assumed that the hash table must contain an exact power-of-2\n *   quantity of entries\n * - limit is the number of hash buckets, not the total allocation size\n */\nvoid *__init alloc_large_system_hash(const char *tablename,\n\t\t\t\t     unsigned long bucketsize,\n\t\t\t\t     unsigned long numentries,\n\t\t\t\t     int scale,\n\t\t\t\t     int flags,\n\t\t\t\t     unsigned int *_hash_shift,\n\t\t\t\t     unsigned int *_hash_mask,\n\t\t\t\t     unsigned long low_limit,\n\t\t\t\t     unsigned long high_limit)\n{\n\tunsigned long long max = high_limit;\n\tunsigned long log2qty, size;\n\tvoid *table = NULL;\n\tgfp_t gfp_flags;\n\n\t/* allow the kernel cmdline to have a say */\n\tif (!numentries) {\n\t\t/* round applicable memory size up to nearest megabyte */\n\t\tnumentries = nr_kernel_pages;\n\t\tnumentries -= arch_reserved_kernel_pages();\n\n\t\t/* It isn't necessary when PAGE_SIZE >= 1MB */\n\t\tif (PAGE_SHIFT < 20)\n\t\t\tnumentries = round_up(numentries, (1<<20)/PAGE_SIZE);\n\n#if __BITS_PER_LONG > 32\n\t\tif (!high_limit) {\n\t\t\tunsigned long adapt;\n\n\t\t\tfor (adapt = ADAPT_SCALE_NPAGES; adapt < numentries;\n\t\t\t     adapt <<= ADAPT_SCALE_SHIFT)\n\t\t\t\tscale++;\n\t\t}\n#endif\n\n\t\t/* limit to 1 bucket per 2^scale bytes of low memory */\n\t\tif (scale > PAGE_SHIFT)\n\t\t\tnumentries >>= (scale - PAGE_SHIFT);\n\t\telse\n\t\t\tnumentries <<= (PAGE_SHIFT - scale);\n\n\t\t/* Make sure we've got at least a 0-order allocation.. */\n\t\tif (unlikely(flags & HASH_SMALL)) {\n\t\t\t/* Makes no sense without HASH_EARLY */\n\t\t\tWARN_ON(!(flags & HASH_EARLY));\n\t\t\tif (!(numentries >> *_hash_shift)) {\n\t\t\t\tnumentries = 1UL << *_hash_shift;\n\t\t\t\tBUG_ON(!numentries);\n\t\t\t}\n\t\t} else if (unlikely((numentries * bucketsize) < PAGE_SIZE))\n\t\t\tnumentries = PAGE_SIZE / bucketsize;\n\t}\n\tnumentries = roundup_pow_of_two(numentries);\n\n\t/* limit allocation size to 1/16 total memory by default */\n\tif (max == 0) {\n\t\tmax = ((unsigned long long)nr_all_pages << PAGE_SHIFT) >> 4;\n\t\tdo_div(max, bucketsize);\n\t}\n\tmax = min(max, 0x80000000ULL);\n\n\tif (numentries < low_limit)\n\t\tnumentries = low_limit;\n\tif (numentries > max)\n\t\tnumentries = max;\n\n\tlog2qty = ilog2(numentries);\n\n\tgfp_flags = (flags & HASH_ZERO) ? GFP_ATOMIC | __GFP_ZERO : GFP_ATOMIC;\n\tdo {\n\t\tsize = bucketsize << log2qty;\n\t\tif (flags & HASH_EARLY) {\n\t\t\tif (flags & HASH_ZERO)\n\t\t\t\ttable = memblock_alloc_nopanic(size,\n\t\t\t\t\t\t\t       SMP_CACHE_BYTES);\n\t\t\telse\n\t\t\t\ttable = memblock_alloc_raw(size,\n\t\t\t\t\t\t\t   SMP_CACHE_BYTES);\n\t\t} else if (hashdist) {\n\t\t\ttable = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\t\t} else {\n\t\t\t/*\n\t\t\t * If bucketsize is not a power-of-two, we may free\n\t\t\t * some pages at the end of hash table which\n\t\t\t * alloc_pages_exact() automatically does\n\t\t\t */\n\t\t\tif (get_order(size) < MAX_ORDER) {\n\t\t\t\ttable = alloc_pages_exact(size, gfp_flags);\n\t\t\t\tkmemleak_alloc(table, size, 1, gfp_flags);\n\t\t\t}\n\t\t}\n\t} while (!table && size > PAGE_SIZE && --log2qty);\n\n\tif (!table)\n\t\tpanic(\"Failed to allocate %s hash table\\n\", tablename);\n\n\tpr_info(\"%s hash table entries: %ld (order: %d, %lu bytes)\\n\",\n\t\ttablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size);\n\n\tif (_hash_shift)\n\t\t*_hash_shift = log2qty;\n\tif (_hash_mask)\n\t\t*_hash_mask = (1 << log2qty) - 1;\n\n\treturn table;\n}\n\n/*\n * This function checks whether pageblock includes unmovable pages or not.\n * If @count is not zero, it is okay to include less @count unmovable pages\n *\n * PageLRU check without isolation or lru_lock could race so that\n * MIGRATE_MOVABLE block might include unmovable pages. And __PageMovable\n * check without lock_page also may miss some movable non-lru pages at\n * race condition. So you can't expect this function should be exact.\n */\nbool has_unmovable_pages(struct zone *zone, struct page *page, int count,\n\t\t\t int migratetype,\n\t\t\t bool skip_hwpoisoned_pages)\n{\n\tunsigned long pfn, iter, found;\n\n\t/*\n\t * TODO we could make this much more efficient by not checking every\n\t * page in the range if we know all of them are in MOVABLE_ZONE and\n\t * that the movable zone guarantees that pages are migratable but\n\t * the later is not the case right now unfortunatelly. E.g. movablecore\n\t * can still lead to having bootmem allocations in zone_movable.\n\t */\n\n\t/*\n\t * CMA allocations (alloc_contig_range) really need to mark isolate\n\t * CMA pageblocks even when they are not movable in fact so consider\n\t * them movable here.\n\t */\n\tif (is_migrate_cma(migratetype) &&\n\t\t\tis_migrate_cma(get_pageblock_migratetype(page)))\n\t\treturn false;\n\n\tpfn = page_to_pfn(page);\n\tfor (found = 0, iter = 0; iter < pageblock_nr_pages; iter++) {\n\t\tunsigned long check = pfn + iter;\n\n\t\tif (!pfn_valid_within(check))\n\t\t\tcontinue;\n\n\t\tpage = pfn_to_page(check);\n\n\t\tif (PageReserved(page))\n\t\t\tgoto unmovable;\n\n\t\t/*\n\t\t * If the zone is movable and we have ruled out all reserved\n\t\t * pages then it should be reasonably safe to assume the rest\n\t\t * is movable.\n\t\t */\n\t\tif (zone_idx(zone) == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Hugepages are not in LRU lists, but they're movable.\n\t\t * We need not scan over tail pages bacause we don't\n\t\t * handle each tail page individually in migration.\n\t\t */\n\t\tif (PageHuge(page)) {\n\n\t\t\tif (!hugepage_migration_supported(page_hstate(page)))\n\t\t\t\tgoto unmovable;\n\n\t\t\titer = round_up(iter + 1, 1<<compound_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We can't use page_count without pin a page\n\t\t * because another CPU can free compound page.\n\t\t * This check already skips compound tails of THP\n\t\t * because their page->_refcount is zero at all time.\n\t\t */\n\t\tif (!page_ref_count(page)) {\n\t\t\tif (PageBuddy(page))\n\t\t\t\titer += (1 << page_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (skip_hwpoisoned_pages && PageHWPoison(page))\n\t\t\tcontinue;\n\n\t\tif (__PageMovable(page))\n\t\t\tcontinue;\n\n\t\tif (!PageLRU(page))\n\t\t\tfound++;\n\t\t/*\n\t\t * If there are RECLAIMABLE pages, we need to check\n\t\t * it.  But now, memory offline itself doesn't call\n\t\t * shrink_node_slabs() and it still to be fixed.\n\t\t */\n\t\t/*\n\t\t * If the page is not RAM, page_count()should be 0.\n\t\t * we don't need more check. This is an _used_ not-movable page.\n\t\t *\n\t\t * The problematic thing here is PG_reserved pages. PG_reserved\n\t\t * is set to both of a memory hole page and a _used_ kernel\n\t\t * page at boot.\n\t\t */\n\t\tif (found > count)\n\t\t\tgoto unmovable;\n\t}\n\treturn false;\nunmovable:\n\tWARN_ON_ONCE(zone_idx(zone) == ZONE_MOVABLE);\n\treturn true;\n}\n\n#if (defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || defined(CONFIG_CMA)\n\nstatic unsigned long pfn_max_align_down(unsigned long pfn)\n{\n\treturn pfn & ~(max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t     pageblock_nr_pages) - 1);\n}\n\nstatic unsigned long pfn_max_align_up(unsigned long pfn)\n{\n\treturn ALIGN(pfn, max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t\tpageblock_nr_pages));\n}\n\n/* [start, end) must belong to a single zone. */\nstatic int __alloc_contig_migrate_range(struct compact_control *cc,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\t/* This function is based on compact_zone() from compaction.c. */\n\tunsigned long nr_reclaimed;\n\tunsigned long pfn = start;\n\tunsigned int tries = 0;\n\tint ret = 0;\n\n\tmigrate_prep();\n\n\twhile (pfn < end || !list_empty(&cc->migratepages)) {\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (list_empty(&cc->migratepages)) {\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tpfn = isolate_migratepages_range(cc, pfn, end);\n\t\t\tif (!pfn) {\n\t\t\t\tret = -EINTR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttries = 0;\n\t\t} else if (++tries == 5) {\n\t\t\tret = ret < 0 ? ret : -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\tnr_reclaimed = reclaim_clean_pages_from_list(cc->zone,\n\t\t\t\t\t\t\t&cc->migratepages);\n\t\tcc->nr_migratepages -= nr_reclaimed;\n\n\t\tret = migrate_pages(&cc->migratepages, alloc_migrate_target,\n\t\t\t\t    NULL, 0, cc->mode, MR_CONTIG_RANGE);\n\t}\n\tif (ret < 0) {\n\t\tputback_movable_pages(&cc->migratepages);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n/**\n * alloc_contig_range() -- tries to allocate given range of pages\n * @start:\tstart PFN to allocate\n * @end:\tone-past-the-last PFN to allocate\n * @migratetype:\tmigratetype of the underlaying pageblocks (either\n *\t\t\t#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks\n *\t\t\tin range must have the same migratetype and it must\n *\t\t\tbe either of the two.\n * @gfp_mask:\tGFP mask to use during compaction\n *\n * The PFN range does not have to be pageblock or MAX_ORDER_NR_PAGES\n * aligned.  The PFN range must belong to a single zone.\n *\n * The first thing this routine does is attempt to MIGRATE_ISOLATE all\n * pageblocks in the range.  Once isolated, the pageblocks should not\n * be modified by others.\n *\n * Returns zero on success or negative error code.  On success all\n * pages which PFN is in [start, end) are allocated for the caller and\n * need to be freed with free_contig_range().\n */\nint alloc_contig_range(unsigned long start, unsigned long end,\n\t\t       unsigned migratetype, gfp_t gfp_mask)\n{\n\tunsigned long outer_start, outer_end;\n\tunsigned int order;\n\tint ret = 0;\n\n\tstruct compact_control cc = {\n\t\t.nr_migratepages = 0,\n\t\t.order = -1,\n\t\t.zone = page_zone(pfn_to_page(start)),\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.no_set_skip_hint = true,\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t};\n\tINIT_LIST_HEAD(&cc.migratepages);\n\n\t/*\n\t * What we do here is we mark all pageblocks in range as\n\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may\n\t * have different sizes, and due to the way page allocator\n\t * work, we align the range to biggest of the two pages so\n\t * that page allocator won't try to merge buddies from\n\t * different pageblocks and change MIGRATE_ISOLATE to some\n\t * other migration type.\n\t *\n\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we\n\t * migrate the pages from an unaligned range (ie. pages that\n\t * we are interested in).  This will put all the pages in\n\t * range back to page allocator as MIGRATE_ISOLATE.\n\t *\n\t * When this is done, we take the pages in range from page\n\t * allocator removing them from the buddy system.  This way\n\t * page allocator will never consider using them.\n\t *\n\t * This lets us mark the pageblocks back as\n\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the\n\t * aligned range but not in the unaligned, original range are\n\t * put back to page allocator so that buddy can use them.\n\t */\n\n\tret = start_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\t       pfn_max_align_up(end), migratetype,\n\t\t\t\t       false);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * In case of -EBUSY, we'd like to know which page causes problem.\n\t * So, just fall through. test_pages_isolated() has a tracepoint\n\t * which will report the busy page.\n\t *\n\t * It is possible that busy pages could become available before\n\t * the call to test_pages_isolated, and the range will actually be\n\t * allocated.  So, if we fall through be sure to clear ret so that\n\t * -EBUSY is not accidentally used or returned to caller.\n\t */\n\tret = __alloc_contig_migrate_range(&cc, start, end);\n\tif (ret && ret != -EBUSY)\n\t\tgoto done;\n\tret =0;\n\n\t/*\n\t * Pages from [start, end) are within a MAX_ORDER_NR_PAGES\n\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's\n\t * more, all pages in [start, end) are free in page allocator.\n\t * What we are going to do is to allocate all pages from\n\t * [start, end) (that is remove them from page allocator).\n\t *\n\t * The only problem is that pages at the beginning and at the\n\t * end of interesting range may be not aligned with pages that\n\t * page allocator holds, ie. they can be part of higher order\n\t * pages.  Because of this, we reserve the bigger range and\n\t * once this is done free the pages we are not interested in.\n\t *\n\t * We don't have to hold zone->lock here because the pages are\n\t * isolated thus they won't get removed from buddy.\n\t */\n\n\tlru_add_drain_all();\n\tdrain_all_pages(cc.zone);\n\n\torder = 0;\n\touter_start = start;\n\twhile (!PageBuddy(pfn_to_page(outer_start))) {\n\t\tif (++order >= MAX_ORDER) {\n\t\t\touter_start = start;\n\t\t\tbreak;\n\t\t}\n\t\touter_start &= ~0UL << order;\n\t}\n\n\tif (outer_start != start) {\n\t\torder = page_order(pfn_to_page(outer_start));\n\n\t\t/*\n\t\t * outer_start page could be small order buddy page and\n\t\t * it doesn't include start page. Adjust outer_start\n\t\t * in this case to report failed page properly\n\t\t * on tracepoint in test_pages_isolated()\n\t\t */\n\t\tif (outer_start + (1UL << order) <= start)\n\t\t\touter_start = start;\n\t}\n\n\t/* Make sure the range is really isolated. */\n\tif (test_pages_isolated(outer_start, end, false)) {\n\t\tpr_info_ratelimited(\"%s: [%lx, %lx) PFNs busy\\n\",\n\t\t\t__func__, outer_start, end);\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Grab isolated pages from freelists. */\n\touter_end = isolate_freepages_range(&cc, outer_start, end);\n\tif (!outer_end) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Free head and tail (if any) */\n\tif (start != outer_start)\n\t\tfree_contig_range(outer_start, start - outer_start);\n\tif (end != outer_end)\n\t\tfree_contig_range(end, outer_end - end);\n\ndone:\n\tundo_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\tpfn_max_align_up(end), migratetype);\n\treturn ret;\n}\n\nvoid free_contig_range(unsigned long pfn, unsigned nr_pages)\n{\n\tunsigned int count = 0;\n\n\tfor (; nr_pages--; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tcount += page_count(page) != 1;\n\t\t__free_page(page);\n\t}\n\tWARN(count != 0, \"%d pages are still in use!\\n\", count);\n}\n#endif\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n/*\n * The zone indicated has a new number of managed_pages; batch sizes and percpu\n * page high values need to be recalulated.\n */\nvoid __meminit zone_pcp_update(struct zone *zone)\n{\n\tunsigned cpu;\n\tmutex_lock(&pcp_batch_high_lock);\n\tfor_each_possible_cpu(cpu)\n\t\tpageset_set_high_and_batch(zone,\n\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n#endif\n\nvoid zone_pcp_reset(struct zone *zone)\n{\n\tunsigned long flags;\n\tint cpu;\n\tstruct per_cpu_pageset *pset;\n\n\t/* avoid races with drain_pages()  */\n\tlocal_irq_save(flags);\n\tif (zone->pageset != &boot_pageset) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpset = per_cpu_ptr(zone->pageset, cpu);\n\t\t\tdrain_zonestat(zone, pset);\n\t\t}\n\t\tfree_percpu(zone->pageset);\n\t\tzone->pageset = &boot_pageset;\n\t}\n\tlocal_irq_restore(flags);\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n/*\n * All pages in the range must be in a single zone and isolated\n * before calling this.\n */\nvoid\n__offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tstruct page *page;\n\tstruct zone *zone;\n\tunsigned int order, i;\n\tunsigned long pfn;\n\tunsigned long flags;\n\t/* find the first valid pfn */\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++)\n\t\tif (pfn_valid(pfn))\n\t\t\tbreak;\n\tif (pfn == end_pfn)\n\t\treturn;\n\toffline_mem_sections(pfn, end_pfn);\n\tzone = page_zone(pfn_to_page(pfn));\n\tspin_lock_irqsave(&zone->lock, flags);\n\tpfn = start_pfn;\n\twhile (pfn < end_pfn) {\n\t\tif (!pfn_valid(pfn)) {\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\t\tpage = pfn_to_page(pfn);\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (unlikely(!PageBuddy(page) && PageHWPoison(page))) {\n\t\t\tpfn++;\n\t\t\tSetPageReserved(page);\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(page_count(page));\n\t\tBUG_ON(!PageBuddy(page));\n\t\torder = page_order(page);\n#ifdef CONFIG_DEBUG_VM\n\t\tpr_info(\"remove from free list %lx %d %lx\\n\",\n\t\t\tpfn, 1 << order, end_pfn);\n#endif\n\t\tlist_del(&page->lru);\n\t\trmv_page_order(page);\n\t\tzone->free_area[order].nr_free--;\n\t\tfor (i = 0; i < (1 << order); i++)\n\t\t\tSetPageReserved((page+i));\n\t\tpfn += (1 << order);\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n#endif\n\nbool is_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn order < MAX_ORDER;\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n/*\n * Set PG_hwpoison flag if a given page is confirmed to be a free page.  This\n * test is performed under the zone lock to prevent a race against page\n * allocation.\n */\nbool set_hwpoison_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\tbool hwpoisoned = false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order) {\n\t\t\tif (!TestSetPageHWPoison(page))\n\t\t\t\thwpoisoned = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn hwpoisoned;\n}\n#endif",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/div64.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/sections.h>",
            "#include <linux/psi.h>",
            "#include <linux/nmi.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/kthread.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/migrate.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/prefetch.h>",
            "#include <trace/events/oom.h>",
            "#include <trace/events/kmem.h>",
            "#include <linux/compaction.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/pfn.h>",
            "#include <linux/sort.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/memremap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/topology.h>",
            "#include <linux/oom.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/slab.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/suspend.h>",
            "#include <linux/module.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kernel.h>",
            "#include <linux/compiler.h>",
            "#include <linux/memblock.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/stddef.h>"
          ],
          "macros_used": [
            "#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)",
            "#define ADAPT_SCALE_SHIFT\t2",
            "#define ADAPT_SCALE_BASE\t(64ul << 30)",
            "#define MAX_NODE_LOAD (nr_online_nodes)",
            "#define MIN_PERCPU_PAGELIST_FRACTION\t(8)"
          ],
          "globals_used": [
            "static DEFINE_MUTEX(pcp_batch_high_lock);",
            "nodemask_t node_states[NR_NODE_STATES] __read_mostly = {\n\t[N_POSSIBLE] = NODE_MASK_ALL,\n\t[N_ONLINE] = { { [0] = 1UL } },\n#ifndef CONFIG_NUMA\n\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },\n#ifdef CONFIG_HIGHMEM\n\t[N_HIGH_MEMORY] = { { [0] = 1UL } },\n#endif\n\t[N_MEMORY] = { { [0] = 1UL } },\n\t[N_CPU] = { { [0] = 1UL } },\n#endif\t/* NUMA */\n};",
            "static DEFINE_SPINLOCK(managed_page_count_lock);",
            "int percpu_pagelist_fraction;",
            "static void __free_pages_ok(struct page *page, unsigned int order);",
            "int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t[ZONE_DMA] = 256,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t[ZONE_DMA32] = 256,\n#endif\n\t[ZONE_NORMAL] = 32,\n#ifdef CONFIG_HIGHMEM\n\t[ZONE_HIGHMEM] = 0,\n#endif\n\t[ZONE_MOVABLE] = 0,\n};",
            "static char * const zone_names[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t \"DMA\",\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t \"DMA32\",\n#endif\n\t \"Normal\",\n#ifdef CONFIG_HIGHMEM\n\t \"HighMem\",\n#endif\n\t \"Movable\",\n#ifdef CONFIG_ZONE_DEVICE\n\t \"Device\",\n#endif\n};",
            "int min_free_kbytes = 1024;",
            "int user_min_free_kbytes = -1;",
            "int watermark_scale_factor = 10;",
            "int page_group_by_mobility_disabled",
            "static __always_inline\nstruct",
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/div64.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <linux/psi.h>\n#include <linux/nmi.h>\n#include <linux/lockdep.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/kthread.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/migrate.h>\n#include <linux/mm_inline.h>\n#include <linux/prefetch.h>\n#include <trace/events/oom.h>\n#include <trace/events/kmem.h>\n#include <linux/compaction.h>\n#include <linux/kmemleak.h>\n#include <linux/debugobjects.h>\n#include <linux/page_ext.h>\n#include <linux/page-isolation.h>\n#include <linux/fault-inject.h>\n#include <linux/backing-dev.h>\n#include <linux/pfn.h>\n#include <linux/sort.h>\n#include <linux/stop_machine.h>\n#include <linux/memremap.h>\n#include <linux/mempolicy.h>\n#include <linux/vmstat.h>\n#include <linux/vmalloc.h>\n#include <linux/nodemask.h>\n#include <linux/memory_hotplug.h>\n#include <linux/cpuset.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/topology.h>\n#include <linux/oom.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/suspend.h>\n#include <linux/module.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/memblock.h>\n#include <linux/jiffies.h>\n#include <linux/pagemap.h>\n#include <linux/interrupt.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define MAX_NODE_LOAD (nr_online_nodes)\n#define MIN_PERCPU_PAGELIST_FRACTION\t(8)\n\nstatic DEFINE_MUTEX(pcp_batch_high_lock);\nnodemask_t node_states[NR_NODE_STATES] __read_mostly = {\n\t[N_POSSIBLE] = NODE_MASK_ALL,\n\t[N_ONLINE] = { { [0] = 1UL } },\n#ifndef CONFIG_NUMA\n\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },\n#ifdef CONFIG_HIGHMEM\n\t[N_HIGH_MEMORY] = { { [0] = 1UL } },\n#endif\n\t[N_MEMORY] = { { [0] = 1UL } },\n\t[N_CPU] = { { [0] = 1UL } },\n#endif\t/* NUMA */\n};\nstatic DEFINE_SPINLOCK(managed_page_count_lock);\nint percpu_pagelist_fraction;\nstatic void __free_pages_ok(struct page *page, unsigned int order);\nint sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t[ZONE_DMA] = 256,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t[ZONE_DMA32] = 256,\n#endif\n\t[ZONE_NORMAL] = 32,\n#ifdef CONFIG_HIGHMEM\n\t[ZONE_HIGHMEM] = 0,\n#endif\n\t[ZONE_MOVABLE] = 0,\n};\nstatic char * const zone_names[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t \"DMA\",\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t \"DMA32\",\n#endif\n\t \"Normal\",\n#ifdef CONFIG_HIGHMEM\n\t \"HighMem\",\n#endif\n\t \"Movable\",\n#ifdef CONFIG_ZONE_DEVICE\n\t \"Device\",\n#endif\n};\nint min_free_kbytes = 1024;\nint user_min_free_kbytes = -1;\nint watermark_scale_factor = 10;\nint page_group_by_mobility_disabled;\nstatic __always_inline\nstruct;\nstatic __always_inline struct;\n\nvoid show_free_areas(unsigned int filter, nodemask_t *nodemask)\n{\n\tunsigned long free_pcp = 0;\n\tint cpu;\n\tstruct zone *zone;\n\tpg_data_t *pgdat;\n\n\tfor_each_populated_zone(zone) {\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\t}\n\n\tprintk(\"active_anon:%lu inactive_anon:%lu isolated_anon:%lu\\n\"\n\t\t\" active_file:%lu inactive_file:%lu isolated_file:%lu\\n\"\n\t\t\" unevictable:%lu dirty:%lu writeback:%lu unstable:%lu\\n\"\n\t\t\" slab_reclaimable:%lu slab_unreclaimable:%lu\\n\"\n\t\t\" mapped:%lu shmem:%lu pagetables:%lu bounce:%lu\\n\"\n\t\t\" free:%lu free_pcp:%lu free_cma:%lu\\n\",\n\t\tglobal_node_page_state(NR_ACTIVE_ANON),\n\t\tglobal_node_page_state(NR_INACTIVE_ANON),\n\t\tglobal_node_page_state(NR_ISOLATED_ANON),\n\t\tglobal_node_page_state(NR_ACTIVE_FILE),\n\t\tglobal_node_page_state(NR_INACTIVE_FILE),\n\t\tglobal_node_page_state(NR_ISOLATED_FILE),\n\t\tglobal_node_page_state(NR_UNEVICTABLE),\n\t\tglobal_node_page_state(NR_FILE_DIRTY),\n\t\tglobal_node_page_state(NR_WRITEBACK),\n\t\tglobal_node_page_state(NR_UNSTABLE_NFS),\n\t\tglobal_node_page_state(NR_SLAB_RECLAIMABLE),\n\t\tglobal_node_page_state(NR_SLAB_UNRECLAIMABLE),\n\t\tglobal_node_page_state(NR_FILE_MAPPED),\n\t\tglobal_node_page_state(NR_SHMEM),\n\t\tglobal_zone_page_state(NR_PAGETABLE),\n\t\tglobal_zone_page_state(NR_BOUNCE),\n\t\tglobal_zone_page_state(NR_FREE_PAGES),\n\t\tfree_pcp,\n\t\tglobal_zone_page_state(NR_FREE_CMA_PAGES));\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tif (show_mem_node_skip(filter, pgdat->node_id, nodemask))\n\t\t\tcontinue;\n\n\t\tprintk(\"Node %d\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" isolated(anon):%lukB\"\n\t\t\t\" isolated(file):%lukB\"\n\t\t\t\" mapped:%lukB\"\n\t\t\t\" dirty:%lukB\"\n\t\t\t\" writeback:%lukB\"\n\t\t\t\" shmem:%lukB\"\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\t\" shmem_thp: %lukB\"\n\t\t\t\" shmem_pmdmapped: %lukB\"\n\t\t\t\" anon_thp: %lukB\"\n#endif\n\t\t\t\" writeback_tmp:%lukB\"\n\t\t\t\" unstable:%lukB\"\n\t\t\t\" all_unreclaimable? %s\"\n\t\t\t\"\\n\",\n\t\t\tpgdat->node_id,\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_UNEVICTABLE)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_MAPPED)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_DIRTY)),\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK)),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM)),\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_THPS) * HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_PMDMAPPED)\n\t\t\t\t\t* HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_ANON_THPS) * HPAGE_PMD_NR),\n#endif\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK_TEMP)),\n\t\t\tK(node_page_state(pgdat, NR_UNSTABLE_NFS)),\n\t\t\tpgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ?\n\t\t\t\t\"yes\" : \"no\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tint i;\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfree_pcp = 0;\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT\n\t\t\t\"%s\"\n\t\t\t\" free:%lukB\"\n\t\t\t\" min:%lukB\"\n\t\t\t\" low:%lukB\"\n\t\t\t\" high:%lukB\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" writepending:%lukB\"\n\t\t\t\" present:%lukB\"\n\t\t\t\" managed:%lukB\"\n\t\t\t\" mlocked:%lukB\"\n\t\t\t\" kernel_stack:%lukB\"\n\t\t\t\" pagetables:%lukB\"\n\t\t\t\" bounce:%lukB\"\n\t\t\t\" free_pcp:%lukB\"\n\t\t\t\" local_pcp:%ukB\"\n\t\t\t\" free_cma:%lukB\"\n\t\t\t\"\\n\",\n\t\t\tzone->name,\n\t\t\tK(zone_page_state(zone, NR_FREE_PAGES)),\n\t\t\tK(min_wmark_pages(zone)),\n\t\t\tK(low_wmark_pages(zone)),\n\t\t\tK(high_wmark_pages(zone)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_UNEVICTABLE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_WRITE_PENDING)),\n\t\t\tK(zone->present_pages),\n\t\t\tK(zone->managed_pages),\n\t\t\tK(zone_page_state(zone, NR_MLOCK)),\n\t\t\tzone_page_state(zone, NR_KERNEL_STACK_KB),\n\t\t\tK(zone_page_state(zone, NR_PAGETABLE)),\n\t\t\tK(zone_page_state(zone, NR_BOUNCE)),\n\t\t\tK(free_pcp),\n\t\t\tK(this_cpu_read(zone->pageset->pcp.count)),\n\t\t\tK(zone_page_state(zone, NR_FREE_CMA_PAGES)));\n\t\tprintk(\"lowmem_reserve[]:\");\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++)\n\t\t\tprintk(KERN_CONT \" %ld\", zone->lowmem_reserve[i]);\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int order;\n\t\tunsigned long nr[MAX_ORDER], flags, total = 0;\n\t\tunsigned char types[MAX_ORDER];\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT \"%s: \", zone->name);\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tstruct free_area *area = &zone->free_area[order];\n\t\t\tint type;\n\n\t\t\tnr[order] = area->nr_free;\n\t\t\ttotal += nr[order] << order;\n\n\t\t\ttypes[order] = 0;\n\t\t\tfor (type = 0; type < MIGRATE_TYPES; type++) {\n\t\t\t\tif (!list_empty(&area->free_list[type]))\n\t\t\t\t\ttypes[order] |= 1 << type;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n\t\t\t       nr[order], K(1UL) << order);\n\t\t\tif (nr[order])\n\t\t\t\tshow_migration_types(types[order]);\n\t\t}\n\t\tprintk(KERN_CONT \"= %lukB\\n\", K(total));\n\t}\n\n\thugetlb_show_meminfo();\n\n\tprintk(\"%ld total pagecache pages\\n\", global_node_page_state(NR_FILE_PAGES));\n\n\tshow_swap_cache_info();\n}\n\nstatic void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)\n{\n\tzoneref->zone = zone;\n\tzoneref->zone_idx = zone_idx(zone);\n}\n\n/*\n * Builds allocation fallback zone lists.\n *\n * Add all populated zones of a node to the zonelist.\n */\nstatic int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)\n{\n\tstruct zone *zone;\n\tenum zone_type zone_type = MAX_NR_ZONES;\n\tint nr_zones = 0;\n\n\tdo {\n\t\tzone_type--;\n\t\tzone = pgdat->node_zones + zone_type;\n\t\tif (managed_zone(zone)) {\n\t\t\tzoneref_set_zone(zone, &zonerefs[nr_zones++]);\n\t\t\tcheck_highest_zone(zone_type);\n\t\t}\n\t} while (zone_type);\n\n\treturn nr_zones;\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int __parse_numa_zonelist_order(char *s)\n{\n\t/*\n\t * We used to support different zonlists modes but they turned\n\t * out to be just not useful. Let's keep the warning in place\n\t * if somebody still use the cmd line parameter so that we do\n\t * not fail it silently\n\t */\n\tif (!(*s == 'd' || *s == 'D' || *s == 'n' || *s == 'N')) {\n\t\tpr_warn(\"Ignoring unsupported numa_zonelist_order value:  %s\\n\", s);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic __init int setup_numa_zonelist_order(char *s)\n{\n\tif (!s)\n\t\treturn 0;\n\n\treturn __parse_numa_zonelist_order(s);\n}\nearly_param(\"numa_zonelist_order\", setup_numa_zonelist_order);\n\nchar numa_zonelist_order[] = \"Node\";\n\n/*\n * sysctl handler for numa_zonelist_order\n */\nint numa_zonelist_order_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *length,\n\t\tloff_t *ppos)\n{\n\tchar *str;\n\tint ret;\n\n\tif (!write)\n\t\treturn proc_dostring(table, write, buffer, length, ppos);\n\tstr = memdup_user_nul(buffer, 16);\n\tif (IS_ERR(str))\n\t\treturn PTR_ERR(str);\n\n\tret = __parse_numa_zonelist_order(str);\n\tkfree(str);\n\treturn ret;\n}\n\n\n#define MAX_NODE_LOAD (nr_online_nodes)\nstatic int node_load[MAX_NUMNODES];\n\n/**\n * find_next_best_node - find the next node that should appear in a given node's fallback list\n * @node: node whose fallback list we're appending\n * @used_node_mask: nodemask_t of already used nodes\n *\n * We use a number of factors to determine which is the next node that should\n * appear on a given node's fallback list.  The node should not have appeared\n * already in @node's fallback list, and it should be the next closest node\n * according to the distance array (which contains arbitrary distance values\n * from each node to each node in the system), and should also prefer nodes\n * with no CPUs, since presumably they'll have very little allocation pressure\n * on them otherwise.\n * It returns -1 if no node is found.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_node_mask)\n{\n\tint n, val;\n\tint min_val = INT_MAX;\n\tint best_node = NUMA_NO_NODE;\n\tconst struct cpumask *tmp = cpumask_of_node(0);\n\n\t/* Use the local node if we haven't already */\n\tif (!node_isset(node, *used_node_mask)) {\n\t\tnode_set(node, *used_node_mask);\n\t\treturn node;\n\t}\n\n\tfor_each_node_state(n, N_MEMORY) {\n\n\t\t/* Don't want a node to appear more than once */\n\t\tif (node_isset(n, *used_node_mask))\n\t\t\tcontinue;\n\n\t\t/* Use the distance array to find the distance */\n\t\tval = node_distance(node, n);\n\n\t\t/* Penalize nodes under us (\"prefer the next node\") */\n\t\tval += (n < node);\n\n\t\t/* Give preference to headless and unused nodes */\n\t\ttmp = cpumask_of_node(n);\n\t\tif (!cpumask_empty(tmp))\n\t\t\tval += PENALTY_FOR_NODE_WITH_CPUS;\n\n\t\t/* Slight preference for less loaded node */\n\t\tval *= (MAX_NODE_LOAD*MAX_NUMNODES);\n\t\tval += node_load[n];\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node >= 0)\n\t\tnode_set(best_node, *used_node_mask);\n\n\treturn best_node;\n}\n\n\n/*\n * Build zonelists ordered by node and zones within node.\n * This results in maximum locality--normal zone overflows into local\n * DMA zone, if any--but risks exhausting DMA zone.\n */\nstatic void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order,\n\t\tunsigned nr_nodes)\n{\n\tstruct zoneref *zonerefs;\n\tint i;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\n\tfor (i = 0; i < nr_nodes; i++) {\n\t\tint nr_zones;\n\n\t\tpg_data_t *node = NODE_DATA(node_order[i]);\n\n\t\tnr_zones = build_zonerefs_node(node, zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build gfp_thisnode zonelists\n */\nstatic void build_thisnode_zonelists(pg_data_t *pgdat)\n{\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build zonelists ordered by zone and nodes within zones.\n * This results in conserving DMA zone[s] until all Normal memory is\n * exhausted, but results in overflowing to remote node while memory\n * may still exist in local DMA zone.\n */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tstatic int node_order[MAX_NUMNODES];\n\tint node, load, nr_nodes = 0;\n\tnodemask_t used_mask;\n\tint local_node, prev_node;\n\n\t/* NUMA-aware ordering of nodes */\n\tlocal_node = pgdat->node_id;\n\tload = nr_online_nodes;\n\tprev_node = local_node;\n\tnodes_clear(used_mask);\n\n\tmemset(node_order, 0, sizeof(node_order));\n\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n\t\t/*\n\t\t * We don't want to pressure a particular node.\n\t\t * So adding penalty to the first node in same\n\t\t * distance group to make it round-robin.\n\t\t */\n\t\tif (node_distance(local_node, node) !=\n\t\t    node_distance(local_node, prev_node))\n\t\t\tnode_load[node] = load;\n\n\t\tnode_order[nr_nodes++] = node;\n\t\tprev_node = node;\n\t\tload--;\n\t}\n\n\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n\tbuild_thisnode_zonelists(pgdat);\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n/*\n * Return node id of node used for \"local\" allocations.\n * I.e., first node id of first zone in arg node's generic zonelist.\n * Used for initializing percpu 'numa_mem', which is used primarily\n * for kernel allocations, so use GFP_KERNEL flags to locate zonelist.\n */\nint local_memory_node(int node)\n{\n\tstruct zoneref *z;\n\n\tz = first_zones_zonelist(node_zonelist(node, GFP_KERNEL),\n\t\t\t\t   gfp_zone(GFP_KERNEL),\n\t\t\t\t   NULL);\n\treturn zone_to_nid(z->zone);\n}\n#endif\n\nstatic void setup_min_unmapped_ratio(void);\nstatic void setup_min_slab_ratio(void);\n#else\t/* CONFIG_NUMA */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tint node, local_node;\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tlocal_node = pgdat->node_id;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\n\t/*\n\t * Now we build the zonelist so that it contains the zones\n\t * of all the other nodes.\n\t * We don't want to pressure a particular node, so when\n\t * building the zones for node N, we make sure that the\n\t * zones coming right after the local ones are those from\n\t * node N+1 (modulo N)\n\t */\n\tfor (node = local_node + 1; node < MAX_NUMNODES; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tfor (node = 0; node < local_node; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n#endif\t/* CONFIG_NUMA */\n\n/*\n * Boot pageset table. One per cpu which is going to be used for all\n * zones and all nodes. The parameters will be set in such a way\n * that an item put on a list will immediately be handed over to\n * the buddy list. This is safe since pageset manipulation is done\n * with interrupts disabled.\n *\n * The boot_pagesets must be kept even after bootup is complete for\n * unused processors and/or zones. They do play a role for bootstrapping\n * hotplugged processors.\n *\n * zoneinfo_show() and maybe other functions do\n * not check if the processor is online before following the pageset pointer.\n * Other parts of the kernel may not check if the zone is available.\n */\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch);\nstatic DEFINE_PER_CPU(struct per_cpu_pageset, boot_pageset);\nstatic DEFINE_PER_CPU(struct per_cpu_nodestat, boot_nodestats);\n\nstatic void __build_all_zonelists(void *data)\n{\n\tint nid;\n\tint __maybe_unused cpu;\n\tpg_data_t *self = data;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\n#ifdef CONFIG_NUMA\n\tmemset(node_load, 0, sizeof(node_load));\n#endif\n\n\t/*\n\t * This node is hotadded and no memory is yet present.   So just\n\t * building zonelists is fine - no need to touch other nodes.\n\t */\n\tif (self && !node_online(self->node_id)) {\n\t\tbuild_zonelists(self);\n\t} else {\n\t\tfor_each_online_node(nid) {\n\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\t\t\tbuild_zonelists(pgdat);\n\t\t}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n\t\t/*\n\t\t * We now know the \"local memory node\" for each node--\n\t\t * i.e., the node of the first zone in the generic zonelist.\n\t\t * Set up numa_mem percpu variable for on-line cpus.  During\n\t\t * boot, only the boot cpu should be on-line;  we'll init the\n\t\t * secondary cpus' numa_mem as they come on-line.  During\n\t\t * node/memory hotplug, we'll fixup all on-line cpus.\n\t\t */\n\t\tfor_each_online_cpu(cpu)\n\t\t\tset_cpu_numa_mem(cpu, local_memory_node(cpu_to_node(cpu)));\n#endif\n\t}\n\n\tspin_unlock(&lock);\n}\n\nstatic noinline void __init\nbuild_all_zonelists_init(void)\n{\n\tint cpu;\n\n\t__build_all_zonelists(NULL);\n\n\t/*\n\t * Initialize the boot_pagesets that are going to be used\n\t * for bootstrapping processors. The real pagesets for\n\t * each zone will be allocated later when the per cpu\n\t * allocator is available.\n\t *\n\t * boot_pagesets are used also for bootstrapping offline\n\t * cpus if the system is already booted because the pagesets\n\t * are needed to initialize allocators on a specific cpu too.\n\t * F.e. the percpu allocator needs the page allocator which\n\t * needs the percpu allocator in order to allocate its pagesets\n\t * (a chicken-egg dilemma).\n\t */\n\tfor_each_possible_cpu(cpu)\n\t\tsetup_pageset(&per_cpu(boot_pageset, cpu), 0);\n\n\tmminit_verify_zonelist();\n\tcpuset_init_current_mems_allowed();\n}\n\n/*\n * unless system_state == SYSTEM_BOOTING.\n *\n * __ref due to call of __init annotated helper build_all_zonelists_init\n * [protected by SYSTEM_BOOTING].\n */\nvoid __ref build_all_zonelists(pg_data_t *pgdat)\n{\n\tif (system_state == SYSTEM_BOOTING) {\n\t\tbuild_all_zonelists_init();\n\t} else {\n\t\t__build_all_zonelists(pgdat);\n\t\t/* cpuset refresh routine should be here */\n\t}\n\tvm_total_pages = nr_free_pagecache_pages();\n\t/*\n\t * Disable grouping by mobility if the number of pages in the\n\t * system is too low to allow the mechanism to work. It would be\n\t * more accurate, but expensive to check per-zone. This check is\n\t * made on memory-hotadd so a system can start with mobility\n\t * disabled and enable it later\n\t */\n\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))\n\t\tpage_group_by_mobility_disabled = 1;\n\telse\n\t\tpage_group_by_mobility_disabled = 0;\n\n\tpr_info(\"Built %i zonelists, mobility grouping %s.  Total pages: %ld\\n\",\n\t\tnr_online_nodes,\n\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",\n\t\tvm_total_pages);\n#ifdef CONFIG_NUMA\n\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);\n#endif\n}\n\n/* If zone is ZONE_MOVABLE but memory is mirrored, it is an overlapped init */\nstatic bool __meminit\noverlap_memmap_init(unsigned long zone, unsigned long *pfn)\n{\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tstatic struct memblock_region *r;\n\n\tif (mirrored_kernelcore && zone == ZONE_MOVABLE) {\n\t\tif (!r || *pfn >= memblock_region_memory_end_pfn(r)) {\n\t\t\tfor_each_memblock(memory, r) {\n\t\t\t\tif (*pfn < memblock_region_memory_end_pfn(r))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (*pfn >= memblock_region_memory_base_pfn(r) &&\n\t\t    memblock_is_mirror(r)) {\n\t\t\t*pfn = memblock_region_memory_end_pfn(r);\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\treturn false;\n}\n\n/*\n * Initially all pages are reserved - free ones are freed\n * up by memblock_free_all() once the early boot process is\n * done. Non-atomic initialization, single-pass.\n */\nvoid __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,\n\t\tunsigned long start_pfn, enum memmap_context context,\n\t\tstruct vmem_altmap *altmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct page *page;\n\n\tif (highest_memmap_pfn < end_pfn - 1)\n\t\thighest_memmap_pfn = end_pfn - 1;\n\n#ifdef CONFIG_ZONE_DEVICE\n\t/*\n\t * Honor reservation requested by the driver for this ZONE_DEVICE\n\t * memory. We limit the total number of pages to initialize to just\n\t * those that might contain the memory mapping. We will defer the\n\t * ZONE_DEVICE page initialization until after we have released\n\t * the hotplug lock.\n\t */\n\tif (zone == ZONE_DEVICE) {\n\t\tif (!altmap)\n\t\t\treturn;\n\n\t\tif (start_pfn == altmap->base_pfn)\n\t\t\tstart_pfn += altmap->reserve;\n\t\tend_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t}\n#endif\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\t/*\n\t\t * There can be holes in boot-time mem_map[]s handed to this\n\t\t * function.  They do not exist on hotplugged memory.\n\t\t */\n\t\tif (context == MEMMAP_EARLY) {\n\t\t\tif (!early_pfn_valid(pfn))\n\t\t\t\tcontinue;\n\t\t\tif (!early_pfn_in_nid(pfn, nid))\n\t\t\t\tcontinue;\n\t\t\tif (overlap_memmap_init(zone, &pfn))\n\t\t\t\tcontinue;\n\t\t\tif (defer_init(nid, pfn, end_pfn))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tpage = pfn_to_page(pfn);\n\t\t__init_single_page(page, pfn, zone, nid);\n\t\tif (context == MEMMAP_HOTPLUG)\n\t\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nvoid __ref memmap_init_zone_device(struct zone *zone,\n\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t   unsigned long size,\n\t\t\t\t   struct dev_pagemap *pgmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tunsigned long zone_idx = zone_idx(zone);\n\tunsigned long start = jiffies;\n\tint nid = pgdat->node_id;\n\n\tif (WARN_ON_ONCE(!pgmap || !is_dev_zone(zone)))\n\t\treturn;\n\n\t/*\n\t * The call to memmap_init_zone should have already taken care\n\t * of the pages reserved for the memmap, so we can just jump to\n\t * the end of that region and start processing the device pages.\n\t */\n\tif (pgmap->altmap_valid) {\n\t\tstruct vmem_altmap *altmap = &pgmap->altmap;\n\n\t\tstart_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t\tsize = end_pfn - start_pfn;\n\t}\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t__init_single_page(page, pfn, zone_idx, nid);\n\n\t\t/*\n\t\t * Mark page reserved as it will need to wait for onlining\n\t\t * phase for it to be fully associated with a zone.\n\t\t *\n\t\t * We can use the non-atomic __set_bit operation for setting\n\t\t * the flag as we are still initializing the pages.\n\t\t */\n\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * ZONE_DEVICE pages union ->lru with a ->pgmap back\n\t\t * pointer and hmm_data.  It is a bug if a ZONE_DEVICE\n\t\t * page is ever freed or placed on a driver-private list.\n\t\t */\n\t\tpage->pgmap = pgmap;\n\t\tpage->hmm_data = 0;\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t *\n\t\t * Please note that MEMMAP_HOTPLUG path doesn't clear memmap\n\t\t * because this is done early in sparse_add_one_section\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tpr_info(\"%s initialised, %lu pages in %ums\\n\", dev_name(pgmap->dev),\n\t\tsize, jiffies_to_msecs(jiffies - start));\n}\n\n#endif\nstatic void __meminit zone_init_free_lists(struct zone *zone)\n{\n\tunsigned int order, t;\n\tfor_each_migratetype_order(order, t) {\n\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);\n\t\tzone->free_area[order].nr_free = 0;\n\t}\n}\n\nvoid __meminit __weak memmap_init(unsigned long size, int nid,\n\t\t\t\t  unsigned long zone, unsigned long start_pfn)\n{\n\tmemmap_init_zone(size, nid, zone, start_pfn, MEMMAP_EARLY, NULL);\n}\n\nstatic int zone_batchsize(struct zone *zone)\n{\n#ifdef CONFIG_MMU\n\tint batch;\n\n\t/*\n\t * The per-cpu-pages pools are set to around 1000th of the\n\t * size of the zone.\n\t */\n\tbatch = zone->managed_pages / 1024;\n\t/* But no more than a meg. */\n\tif (batch * PAGE_SIZE > 1024 * 1024)\n\t\tbatch = (1024 * 1024) / PAGE_SIZE;\n\tbatch /= 4;\t\t/* We effectively *= 4 below */\n\tif (batch < 1)\n\t\tbatch = 1;\n\n\t/*\n\t * Clamp the batch to a 2^n - 1 value. Having a power\n\t * of 2 value was found to be more likely to have\n\t * suboptimal cache aliasing properties in some cases.\n\t *\n\t * For example if 2 tasks are alternately allocating\n\t * batches of pages, one task can end up with a lot\n\t * of pages of one half of the possible page colors\n\t * and the other with pages of the other colors.\n\t */\n\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;\n\n\treturn batch;\n\n#else\n\t/* The deferral and batching of frees should be suppressed under NOMMU\n\t * conditions.\n\t *\n\t * The problem is that NOMMU needs to be able to allocate large chunks\n\t * of contiguous memory as there's no hardware page translation to\n\t * assemble apparent contiguous memory from discontiguous pages.\n\t *\n\t * Queueing large contiguous runs of pages for batching, however,\n\t * causes the pages to actually be freed in smaller chunks.  As there\n\t * can be a significant delay between the individual batches being\n\t * recycled, this leads to the once large chunks of space being\n\t * fragmented and becoming unavailable for high-order allocations.\n\t */\n\treturn 0;\n#endif\n}\n\n/*\n * pcp->high and pcp->batch values are related and dependent on one another:\n * ->batch must never be higher then ->high.\n * The following function updates them in a safe manner without read side\n * locking.\n *\n * Any new users of pcp->batch and pcp->high should ensure they can cope with\n * those fields changing asynchronously (acording the the above rule).\n *\n * mutex_is_locked(&pcp_batch_high_lock) required when calling this function\n * outside of boot time (or some other assurance that no concurrent updaters\n * exist).\n */\nstatic void pageset_update(struct per_cpu_pages *pcp, unsigned long high,\n\t\tunsigned long batch)\n{\n       /* start with a fail safe value for batch */\n\tpcp->batch = 1;\n\tsmp_wmb();\n\n       /* Update high, then batch, in order */\n\tpcp->high = high;\n\tsmp_wmb();\n\n\tpcp->batch = batch;\n}\n\n/* a companion to pageset_set_high() */\nstatic void pageset_set_batch(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_update(&p->pcp, 6 * batch, max(1UL, 1 * batch));\n}\n\nstatic void pageset_init(struct per_cpu_pageset *p)\n{\n\tstruct per_cpu_pages *pcp;\n\tint migratetype;\n\n\tmemset(p, 0, sizeof(*p));\n\n\tpcp = &p->pcp;\n\tpcp->count = 0;\n\tfor (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)\n\t\tINIT_LIST_HEAD(&pcp->lists[migratetype]);\n}\n\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_init(p);\n\tpageset_set_batch(p, batch);\n}\n\n/*\n * pageset_set_high() sets the high water mark for hot per_cpu_pagelist\n * to the value high for the pageset p.\n */\nstatic void pageset_set_high(struct per_cpu_pageset *p,\n\t\t\t\tunsigned long high)\n{\n\tunsigned long batch = max(1UL, high / 4);\n\tif ((high / 4) > (PAGE_SHIFT * 8))\n\t\tbatch = PAGE_SHIFT * 8;\n\n\tpageset_update(&p->pcp, high, batch);\n}\n\nstatic void pageset_set_high_and_batch(struct zone *zone,\n\t\t\t\t       struct per_cpu_pageset *pcp)\n{\n\tif (percpu_pagelist_fraction)\n\t\tpageset_set_high(pcp,\n\t\t\t(zone->managed_pages /\n\t\t\t\tpercpu_pagelist_fraction));\n\telse\n\t\tpageset_set_batch(pcp, zone_batchsize(zone));\n}\n\nstatic void __meminit zone_pageset_init(struct zone *zone, int cpu)\n{\n\tstruct per_cpu_pageset *pcp = per_cpu_ptr(zone->pageset, cpu);\n\n\tpageset_init(pcp);\n\tpageset_set_high_and_batch(zone, pcp);\n}\n\nvoid __meminit setup_zone_pageset(struct zone *zone)\n{\n\tint cpu;\n\tzone->pageset = alloc_percpu(struct per_cpu_pageset);\n\tfor_each_possible_cpu(cpu)\n\t\tzone_pageset_init(zone, cpu);\n}\n\n/*\n * Allocate per cpu pagesets and initialize them.\n * Before this call only boot pagesets were available.\n */\nvoid __init setup_per_cpu_pageset(void)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone)\n\t\tsetup_zone_pageset(zone);\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->per_cpu_nodestats =\n\t\t\talloc_percpu(struct per_cpu_nodestat);\n}\n\nstatic __meminit void zone_pcp_init(struct zone *zone)\n{\n\t/*\n\t * per cpu subsystem is not up at this point. The following code\n\t * relies on the ability of the linker to provide the\n\t * offset of a (static) per cpu variable into the per cpu area.\n\t */\n\tzone->pageset = &boot_pageset;\n\n\tif (populated_zone(zone))\n\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages, LIFO batch:%u\\n\",\n\t\t\tzone->name, zone->present_pages,\n\t\t\t\t\t zone_batchsize(zone));\n}\n\nvoid __meminit init_currently_empty_zone(struct zone *zone,\n\t\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\t\tunsigned long size)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tint zone_idx = zone_idx(zone) + 1;\n\n\tif (zone_idx > pgdat->nr_zones)\n\t\tpgdat->nr_zones = zone_idx;\n\n\tzone->zone_start_pfn = zone_start_pfn;\n\n\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",\n\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",\n\t\t\tpgdat->node_id,\n\t\t\t(unsigned long)zone_idx(zone),\n\t\t\tzone_start_pfn, (zone_start_pfn + size));\n\n\tzone_init_free_lists(zone);\n\tzone->initialized = 1;\n}\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n#ifndef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID\n\n/*\n * Required by SPARSEMEM. Given a PFN, return what node the PFN is on.\n */\nint __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint nid;\n\n\tif (state->last_start <= pfn && pfn < state->last_end)\n\t\treturn state->last_nid;\n\n\tnid = memblock_search_pfn_nid(pfn, &start_pfn, &end_pfn);\n\tif (nid != -1) {\n\t\tstate->last_start = start_pfn;\n\t\tstate->last_end = end_pfn;\n\t\tstate->last_nid = nid;\n\t}\n\n\treturn nid;\n}\n#endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */\n\n/**\n * free_bootmem_with_active_regions - Call memblock_free_early_nid for each active range\n * @nid: The node to free memory on. If MAX_NUMNODES, all nodes are freed.\n * @max_low_pfn: The highest PFN that will be passed to memblock_free_early_nid\n *\n * If an architecture guarantees that all ranges registered contain no holes\n * and may be freed, this this function may be used instead of calling\n * memblock_free_early_nid() manually.\n */\nvoid __init free_bootmem_with_active_regions(int nid, unsigned long max_low_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid) {\n\t\tstart_pfn = min(start_pfn, max_low_pfn);\n\t\tend_pfn = min(end_pfn, max_low_pfn);\n\n\t\tif (start_pfn < end_pfn)\n\t\t\tmemblock_free_early_nid(PFN_PHYS(start_pfn),\n\t\t\t\t\t(end_pfn - start_pfn) << PAGE_SHIFT,\n\t\t\t\t\tthis_nid);\n\t}\n}\n\n/**\n * sparse_memory_present_with_active_regions - Call memory_present for each active range\n * @nid: The node to call memory_present for. If MAX_NUMNODES, all nodes will be used.\n *\n * If an architecture guarantees that all ranges registered contain no holes and may\n * be freed, this function may be used instead of calling memory_present() manually.\n */\nvoid __init sparse_memory_present_with_active_regions(int nid)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid)\n\t\tmemory_present(this_nid, start_pfn, end_pfn);\n}\n\n/**\n * get_pfn_range_for_nid - Return the start and end page frames for a node\n * @nid: The nid to return the range for. If MAX_NUMNODES, the min and max PFN are returned.\n * @start_pfn: Passed by reference. On return, it will have the node start_pfn.\n * @end_pfn: Passed by reference. On return, it will have the node end_pfn.\n *\n * It returns the start and end page frame of a node based on information\n * provided by memblock_set_node(). If called for a node\n * with no available memory, a warning is printed and the start and end\n * PFNs will be 0.\n */\nvoid __meminit get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn)\n{\n\tunsigned long this_start_pfn, this_end_pfn;\n\tint i;\n\n\t*start_pfn = -1UL;\n\t*end_pfn = 0;\n\n\tfor_each_mem_pfn_range(i, nid, &this_start_pfn, &this_end_pfn, NULL) {\n\t\t*start_pfn = min(*start_pfn, this_start_pfn);\n\t\t*end_pfn = max(*end_pfn, this_end_pfn);\n\t}\n\n\tif (*start_pfn == -1UL)\n\t\t*start_pfn = 0;\n}\n\n/*\n * This finds a zone that can be used for ZONE_MOVABLE pages. The\n * assumption is made that zones within a node are ordered in monotonic\n * increasing memory addresses so that the \"highest\" populated zone is used\n */\nstatic void __init find_usable_zone_for_movable(void)\n{\n\tint zone_index;\n\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {\n\t\tif (zone_index == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tif (arch_zone_highest_possible_pfn[zone_index] >\n\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])\n\t\t\tbreak;\n\t}\n\n\tVM_BUG_ON(zone_index == -1);\n\tmovable_zone = zone_index;\n}\n\n/*\n * The zone ranges provided by the architecture do not include ZONE_MOVABLE\n * because it is sized independent of architecture. Unlike the other zones,\n * the starting point for ZONE_MOVABLE is not fixed. It may be different\n * in each node depending on the size of each node and how evenly kernelcore\n * is distributed. This helper function adjusts the zone ranges\n * provided by the architecture for a given node by using the end of the\n * highest usable zone for ZONE_MOVABLE. This preserves the assumption that\n * zones within a node are in order of monotonic increases memory addresses\n */\nstatic void __meminit adjust_zone_range_for_zone_movable(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\t/* Only adjust if ZONE_MOVABLE is on this node */\n\tif (zone_movable_pfn[nid]) {\n\t\t/* Size ZONE_MOVABLE */\n\t\tif (zone_type == ZONE_MOVABLE) {\n\t\t\t*zone_start_pfn = zone_movable_pfn[nid];\n\t\t\t*zone_end_pfn = min(node_end_pfn,\n\t\t\t\tarch_zone_highest_possible_pfn[movable_zone]);\n\n\t\t/* Adjust for ZONE_MOVABLE starting within this range */\n\t\t} else if (!mirrored_kernelcore &&\n\t\t\t*zone_start_pfn < zone_movable_pfn[nid] &&\n\t\t\t*zone_end_pfn > zone_movable_pfn[nid]) {\n\t\t\t*zone_end_pfn = zone_movable_pfn[nid];\n\n\t\t/* Check if this whole range is within ZONE_MOVABLE */\n\t\t} else if (*zone_start_pfn >= zone_movable_pfn[nid])\n\t\t\t*zone_start_pfn = *zone_end_pfn;\n\t}\n}\n\n/*\n * Return the number of pages a zone spans in a node, including holes\n * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()\n */\nstatic unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\t/* Get the start and end of the zone */\n\t*zone_start_pfn = arch_zone_lowest_possible_pfn[zone_type];\n\t*zone_end_pfn = arch_zone_highest_possible_pfn[zone_type];\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t/* Check that this node has pages within the zone's required range */\n\tif (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)\n\t\treturn 0;\n\n\t/* Move the zone boundaries inside the node if necessary */\n\t*zone_end_pfn = min(*zone_end_pfn, node_end_pfn);\n\t*zone_start_pfn = max(*zone_start_pfn, node_start_pfn);\n\n\t/* Return the spanned pages */\n\treturn *zone_end_pfn - *zone_start_pfn;\n}\n\n/*\n * Return the number of holes in a range on a node. If nid is MAX_NUMNODES,\n * then all holes in the requested range will be accounted for.\n */\nunsigned long __meminit __absent_pages_in_range(int nid,\n\t\t\t\tunsigned long range_start_pfn,\n\t\t\t\tunsigned long range_end_pfn)\n{\n\tunsigned long nr_absent = range_end_pfn - range_start_pfn;\n\tunsigned long start_pfn, end_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);\n\t\tend_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);\n\t\tnr_absent -= end_pfn - start_pfn;\n\t}\n\treturn nr_absent;\n}\n\n/**\n * absent_pages_in_range - Return number of page frames in holes within a range\n * @start_pfn: The start PFN to start searching for holes\n * @end_pfn: The end PFN to stop searching for holes\n *\n * It returns the number of pages frames in memory holes within a range.\n */\nunsigned long __init absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\treturn __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);\n}\n\n/* Return the number of page frames in holes in a zone on a node */\nstatic unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\tunsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type];\n\tunsigned long zone_high = arch_zone_highest_possible_pfn[zone_type];\n\tunsigned long zone_start_pfn, zone_end_pfn;\n\tunsigned long nr_absent;\n\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\tzone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);\n\tzone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);\n\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t&zone_start_pfn, &zone_end_pfn);\n\tnr_absent = __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn);\n\n\t/*\n\t * ZONE_MOVABLE handling.\n\t * Treat pages to be ZONE_MOVABLE in ZONE_NORMAL as absent pages\n\t * and vice versa.\n\t */\n\tif (mirrored_kernelcore && zone_movable_pfn[nid]) {\n\t\tunsigned long start_pfn, end_pfn;\n\t\tstruct memblock_region *r;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tstart_pfn = clamp(memblock_region_memory_base_pfn(r),\n\t\t\t\t\t  zone_start_pfn, zone_end_pfn);\n\t\t\tend_pfn = clamp(memblock_region_memory_end_pfn(r),\n\t\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t\t\tif (zone_type == ZONE_MOVABLE &&\n\t\t\t    memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\n\t\t\tif (zone_type == ZONE_NORMAL &&\n\t\t\t    !memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\t\t}\n\t}\n\n\treturn nr_absent;\n}\n\n#else /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\nstatic inline unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *zones_size)\n{\n\tunsigned int zone;\n\n\t*zone_start_pfn = node_start_pfn;\n\tfor (zone = 0; zone < zone_type; zone++)\n\t\t*zone_start_pfn += zones_size[zone];\n\n\t*zone_end_pfn = *zone_start_pfn + zones_size[zone_type];\n\n\treturn zones_size[zone_type];\n}\n\nstatic inline unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tif (!zholes_size)\n\t\treturn 0;\n\n\treturn zholes_size[zone_type];\n}\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nstatic void __meminit calculate_node_totalpages(struct pglist_data *pgdat,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zones_size,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tunsigned long realtotalpages = 0, totalpages = 0;\n\tenum zone_type i;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\tunsigned long zone_start_pfn, zone_end_pfn;\n\t\tunsigned long size, real_size;\n\n\t\tsize = zone_spanned_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn,\n\t\t\t\t\t\t  node_end_pfn,\n\t\t\t\t\t\t  &zone_start_pfn,\n\t\t\t\t\t\t  &zone_end_pfn,\n\t\t\t\t\t\t  zones_size);\n\t\treal_size = size - zone_absent_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn, node_end_pfn,\n\t\t\t\t\t\t  zholes_size);\n\t\tif (size)\n\t\t\tzone->zone_start_pfn = zone_start_pfn;\n\t\telse\n\t\t\tzone->zone_start_pfn = 0;\n\t\tzone->spanned_pages = size;\n\t\tzone->present_pages = real_size;\n\n\t\ttotalpages += size;\n\t\trealtotalpages += real_size;\n\t}\n\n\tpgdat->node_spanned_pages = totalpages;\n\tpgdat->node_present_pages = realtotalpages;\n\tprintk(KERN_DEBUG \"On node %d totalpages: %lu\\n\", pgdat->node_id,\n\t\t\t\t\t\t\trealtotalpages);\n}\n\n#ifndef CONFIG_SPARSEMEM\n/*\n * Calculate the size of the zone->blockflags rounded to an unsigned long\n * Start by making sure zonesize is a multiple of pageblock_order by rounding\n * up. Then use 1 NR_PAGEBLOCK_BITS worth of bits per pageblock, finally\n * round what is now in bits to nearest long in bits, then return it in\n * bytes.\n */\nstatic unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)\n{\n\tunsigned long usemapsize;\n\n\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);\n\tusemapsize = roundup(zonesize, pageblock_nr_pages);\n\tusemapsize = usemapsize >> pageblock_order;\n\tusemapsize *= NR_PAGEBLOCK_BITS;\n\tusemapsize = roundup(usemapsize, 8 * sizeof(unsigned long));\n\n\treturn usemapsize / 8;\n}\n\nstatic void __ref setup_usemap(struct pglist_data *pgdat,\n\t\t\t\tstruct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\tunsigned long zonesize)\n{\n\tunsigned long usemapsize = usemap_size(zone_start_pfn, zonesize);\n\tzone->pageblock_flags = NULL;\n\tif (usemapsize)\n\t\tzone->pageblock_flags =\n\t\t\tmemblock_alloc_node_nopanic(usemapsize,\n\t\t\t\t\t\t\t pgdat->node_id);\n}\n#else\nstatic inline void setup_usemap(struct pglist_data *pgdat, struct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn, unsigned long zonesize) {}\n#endif /* CONFIG_SPARSEMEM */\n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\n\n/* Initialise the number of pages represented by NR_PAGEBLOCK_BITS */\nvoid __init set_pageblock_order(void)\n{\n\tunsigned int order;\n\n\t/* Check that pageblock_nr_pages has not already been setup */\n\tif (pageblock_order)\n\t\treturn;\n\n\tif (HPAGE_SHIFT > PAGE_SHIFT)\n\t\torder = HUGETLB_PAGE_ORDER;\n\telse\n\t\torder = MAX_ORDER - 1;\n\n\t/*\n\t * Assume the largest contiguous order of interest is a huge page.\n\t * This value may be variable depending on boot parameters on IA64 and\n\t * powerpc.\n\t */\n\tpageblock_order = order;\n}\n#else /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\n/*\n * When CONFIG_HUGETLB_PAGE_SIZE_VARIABLE is not set, set_pageblock_order()\n * is unused as pageblock_order is set at compile-time. See\n * include/linux/pageblock-flags.h for the values of pageblock_order based on\n * the kernel config\n */\nvoid __init set_pageblock_order(void)\n{\n}\n\n#endif /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\nstatic unsigned long __init calc_memmap_size(unsigned long spanned_pages,\n\t\t\t\t\t\tunsigned long present_pages)\n{\n\tunsigned long pages = spanned_pages;\n\n\t/*\n\t * Provide a more accurate estimation if there are holes within\n\t * the zone and SPARSEMEM is in use. If there are holes within the\n\t * zone, each populated memory region may cost us one or two extra\n\t * memmap pages due to alignment because memmap pages for each\n\t * populated regions may not be naturally aligned on page boundary.\n\t * So the (present_pages >> 4) heuristic is a tradeoff for that.\n\t */\n\tif (spanned_pages > present_pages + (present_pages >> 4) &&\n\t    IS_ENABLED(CONFIG_SPARSEMEM))\n\t\tpages = present_pages;\n\n\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat)\n{\n\tspin_lock_init(&pgdat->split_queue_lock);\n\tINIT_LIST_HEAD(&pgdat->split_queue);\n\tpgdat->split_queue_len = 0;\n}\n#else\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat) {}\n#endif\n\n#ifdef CONFIG_COMPACTION\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat)\n{\n\tinit_waitqueue_head(&pgdat->kcompactd_wait);\n}\n#else\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n#endif\n\nstatic void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n{\n\tpgdat_resize_init(pgdat);\n\n\tpgdat_init_split_queue(pgdat);\n\tpgdat_init_kcompactd(pgdat);\n\n\tinit_waitqueue_head(&pgdat->kswapd_wait);\n\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\n\n\tpgdat_page_ext_init(pgdat);\n\tspin_lock_init(&pgdat->lru_lock);\n\tlruvec_init(node_lruvec(pgdat));\n}\n\nstatic void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,\n\t\t\t\t\t\t\tunsigned long remaining_pages)\n{\n\tzone->managed_pages = remaining_pages;\n\tzone_set_nid(zone, nid);\n\tzone->name = zone_names[idx];\n\tzone->zone_pgdat = NODE_DATA(nid);\n\tspin_lock_init(&zone->lock);\n\tzone_seqlock_init(zone);\n\tzone_pcp_init(zone);\n}\n\n/*\n * Set up the zone data structures\n * - init pgdat internals\n * - init all zones belonging to this node\n *\n * NOTE: this function is only called during memory hotplug\n */\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid __ref free_area_init_core_hotplug(int nid)\n{\n\tenum zone_type z;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tpgdat_init_internals(pgdat);\n\tfor (z = 0; z < MAX_NR_ZONES; z++)\n\t\tzone_init_internals(&pgdat->node_zones[z], z, nid, 0);\n}\n#endif\n\n/*\n * Set up the zone data structures:\n *   - mark all pages reserved\n *   - mark all memory queues empty\n *   - clear the memory bitmaps\n *\n * NOTE: pgdat should get zeroed by caller.\n * NOTE: this function is only called during early init.\n */\nstatic void __init free_area_init_core(struct pglist_data *pgdat)\n{\n\tenum zone_type j;\n\tint nid = pgdat->node_id;\n\n\tpgdat_init_internals(pgdat);\n\tpgdat->per_cpu_nodestats = &boot_nodestats;\n\n\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\tunsigned long size, freesize, memmap_pages;\n\t\tunsigned long zone_start_pfn = zone->zone_start_pfn;\n\n\t\tsize = zone->spanned_pages;\n\t\tfreesize = zone->present_pages;\n\n\t\t/*\n\t\t * Adjust freesize so that it accounts for how much memory\n\t\t * is used by this zone for memmap. This affects the watermark\n\t\t * and per-cpu initialisations\n\t\t */\n\t\tmemmap_pages = calc_memmap_size(size, freesize);\n\t\tif (!is_highmem_idx(j)) {\n\t\t\tif (freesize >= memmap_pages) {\n\t\t\t\tfreesize -= memmap_pages;\n\t\t\t\tif (memmap_pages)\n\t\t\t\t\tprintk(KERN_DEBUG\n\t\t\t\t\t       \"  %s zone: %lu pages used for memmap\\n\",\n\t\t\t\t\t       zone_names[j], memmap_pages);\n\t\t\t} else\n\t\t\t\tpr_warn(\"  %s zone: %lu pages exceeds freesize %lu\\n\",\n\t\t\t\t\tzone_names[j], memmap_pages, freesize);\n\t\t}\n\n\t\t/* Account for reserved pages */\n\t\tif (j == 0 && freesize > dma_reserve) {\n\t\t\tfreesize -= dma_reserve;\n\t\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages reserved\\n\",\n\t\t\t\t\tzone_names[0], dma_reserve);\n\t\t}\n\n\t\tif (!is_highmem_idx(j))\n\t\t\tnr_kernel_pages += freesize;\n\t\t/* Charge for highmem memmap if there are enough kernel pages */\n\t\telse if (nr_kernel_pages > memmap_pages * 2)\n\t\t\tnr_kernel_pages -= memmap_pages;\n\t\tnr_all_pages += freesize;\n\n\t\t/*\n\t\t * Set an approximate value for lowmem here, it will be adjusted\n\t\t * when the bootmem allocator frees pages into the buddy system.\n\t\t * And all highmem pages will be managed by the buddy system.\n\t\t */\n\t\tzone_init_internals(zone, j, nid, freesize);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\n\t\tset_pageblock_order();\n\t\tsetup_usemap(pgdat, zone, zone_start_pfn, size);\n\t\tinit_currently_empty_zone(zone, zone_start_pfn, size);\n\t\tmemmap_init(size, nid, j, zone_start_pfn);\n\t}\n}\n\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat)\n{\n\tunsigned long __maybe_unused start = 0;\n\tunsigned long __maybe_unused offset = 0;\n\n\t/* Skip empty nodes */\n\tif (!pgdat->node_spanned_pages)\n\t\treturn;\n\n\tstart = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);\n\toffset = pgdat->node_start_pfn - start;\n\t/* ia64 gets its own node_mem_map, before this, without bootmem */\n\tif (!pgdat->node_mem_map) {\n\t\tunsigned long size, end;\n\t\tstruct page *map;\n\n\t\t/*\n\t\t * The zone's endpoints aren't required to be MAX_ORDER\n\t\t * aligned but the node_mem_map endpoints must be in order\n\t\t * for the buddy allocator to function correctly.\n\t\t */\n\t\tend = pgdat_end_pfn(pgdat);\n\t\tend = ALIGN(end, MAX_ORDER_NR_PAGES);\n\t\tsize =  (end - start) * sizeof(struct page);\n\t\tmap = memblock_alloc_node_nopanic(size, pgdat->node_id);\n\t\tpgdat->node_mem_map = map + offset;\n\t}\n\tpr_debug(\"%s: node %d, pgdat %08lx, node_mem_map %08lx\\n\",\n\t\t\t\t__func__, pgdat->node_id, (unsigned long)pgdat,\n\t\t\t\t(unsigned long)pgdat->node_mem_map);\n#ifndef CONFIG_NEED_MULTIPLE_NODES\n\t/*\n\t * With no DISCONTIG, the global mem_map is just set as node 0's\n\t */\n\tif (pgdat == NODE_DATA(0)) {\n\t\tmem_map = NODE_DATA(0)->node_mem_map;\n#if defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) || defined(CONFIG_FLATMEM)\n\t\tif (page_to_pfn(mem_map) != pgdat->node_start_pfn)\n\t\t\tmem_map -= offset;\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\t}\n#endif\n}\n#else\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }\n#endif /* CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat)\n{\n\t/*\n\t * We start only with one section of pages, more pages are added as\n\t * needed until the rest of deferred pages are initialized.\n\t */\n\tpgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,\n\t\t\t\t\t\tpgdat->node_spanned_pages);\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n}\n#else\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}\n#endif\n\nvoid __init free_area_init_node(int nid, unsigned long *zones_size,\n\t\t\t\t   unsigned long node_start_pfn,\n\t\t\t\t   unsigned long *zholes_size)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tunsigned long start_pfn = 0;\n\tunsigned long end_pfn = 0;\n\n\t/* pg_data_t should be reset to zero when it's allocated */\n\tWARN_ON(pgdat->nr_zones || pgdat->kswapd_classzone_idx);\n\n\tpgdat->node_id = nid;\n\tpgdat->node_start_pfn = node_start_pfn;\n\tpgdat->per_cpu_nodestats = NULL;\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);\n\tpr_info(\"Initmem setup node %d [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\tend_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);\n#else\n\tstart_pfn = node_start_pfn;\n#endif\n\tcalculate_node_totalpages(pgdat, start_pfn, end_pfn,\n\t\t\t\t  zones_size, zholes_size);\n\n\talloc_node_mem_map(pgdat);\n\tpgdat_set_deferred_range(pgdat);\n\n\tfree_area_init_core(pgdat);\n}\n\n#if !defined(CONFIG_FLAT_NODE_MEM_MAP)\n/*\n * Zero all valid struct pages in range [spfn, epfn), return number of struct\n * pages zeroed\n */\nstatic u64 zero_pfn_range(unsigned long spfn, unsigned long epfn)\n{\n\tunsigned long pfn;\n\tu64 pgcnt = 0;\n\n\tfor (pfn = spfn; pfn < epfn; pfn++) {\n\t\tif (!pfn_valid(ALIGN_DOWN(pfn, pageblock_nr_pages))) {\n\t\t\tpfn = ALIGN_DOWN(pfn, pageblock_nr_pages)\n\t\t\t\t+ pageblock_nr_pages - 1;\n\t\t\tcontinue;\n\t\t}\n\t\tmm_zero_struct_page(pfn_to_page(pfn));\n\t\tpgcnt++;\n\t}\n\n\treturn pgcnt;\n}\n\n/*\n * Only struct pages that are backed by physical memory are zeroed and\n * initialized by going through __init_single_page(). But, there are some\n * struct pages which are reserved in memblock allocator and their fields\n * may be accessed (for example page_to_pfn() on some configuration accesses\n * flags). We must explicitly zero those struct pages.\n *\n * This function also addresses a similar issue where struct pages are left\n * uninitialized because the physical address range is not covered by\n * memblock.memory or memblock.reserved. That could happen when memblock\n * layout is manually configured via memmap=.\n */\nvoid __init zero_resv_unavail(void)\n{\n\tphys_addr_t start, end;\n\tu64 i, pgcnt;\n\tphys_addr_t next = 0;\n\n\t/*\n\t * Loop through unavailable ranges not covered by memblock.memory.\n\t */\n\tpgcnt = 0;\n\tfor_each_mem_range(i, &memblock.memory, NULL,\n\t\t\tNUMA_NO_NODE, MEMBLOCK_NONE, &start, &end, NULL) {\n\t\tif (next < start)\n\t\t\tpgcnt += zero_pfn_range(PFN_DOWN(next), PFN_UP(start));\n\t\tnext = end;\n\t}\n\tpgcnt += zero_pfn_range(PFN_DOWN(next), max_pfn);\n\n\t/*\n\t * Struct pages that do not have backing memory. This could be because\n\t * firmware is using some of this memory, or for some other reasons.\n\t */\n\tif (pgcnt)\n\t\tpr_info(\"Zeroed struct page in unavailable ranges: %lld pages\", pgcnt);\n}\n#endif /* !CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\n#if MAX_NUMNODES > 1\n/*\n * Figure out the number of possible node ids.\n */\nvoid __init setup_nr_node_ids(void)\n{\n\tunsigned int highest;\n\n\thighest = find_last_bit(node_possible_map.bits, MAX_NUMNODES);\n\tnr_node_ids = highest + 1;\n}\n#endif\n\n/**\n * node_map_pfn_alignment - determine the maximum internode alignment\n *\n * This function should be called after node map is populated and sorted.\n * It calculates the maximum power of two alignment which can distinguish\n * all the nodes.\n *\n * For example, if all nodes are 1GiB and aligned to 1GiB, the return value\n * would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the\n * nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is\n * shifted, 1GiB is enough and this function will indicate so.\n *\n * This is used to test whether pfn -> nid mapping of the chosen memory\n * model has fine enough granularity to avoid incorrect mapping for the\n * populated node map.\n *\n * Returns the determined alignment in pfn's.  0 if there is no alignment\n * requirement (single node).\n */\nunsigned long __init node_map_pfn_alignment(void)\n{\n\tunsigned long accl_mask = 0, last_end = 0;\n\tunsigned long start, end, mask;\n\tint last_nid = -1;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid) {\n\t\tif (!start || last_nid < 0 || last_nid == nid) {\n\t\t\tlast_nid = nid;\n\t\t\tlast_end = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Start with a mask granular enough to pin-point to the\n\t\t * start pfn and tick off bits one-by-one until it becomes\n\t\t * too coarse to separate the current node from the last.\n\t\t */\n\t\tmask = ~((1 << __ffs(start)) - 1);\n\t\twhile (mask && last_end <= (start & (mask << 1)))\n\t\t\tmask <<= 1;\n\n\t\t/* accumulate all internode masks */\n\t\taccl_mask |= mask;\n\t}\n\n\t/* convert mask to number of pages */\n\treturn ~accl_mask + 1;\n}\n\n/* Find the lowest pfn for a node */\nstatic unsigned long __init find_min_pfn_for_node(int nid)\n{\n\tunsigned long min_pfn = ULONG_MAX;\n\tunsigned long start_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, NULL, NULL)\n\t\tmin_pfn = min(min_pfn, start_pfn);\n\n\tif (min_pfn == ULONG_MAX) {\n\t\tpr_warn(\"Could not find start_pfn for node %d\\n\", nid);\n\t\treturn 0;\n\t}\n\n\treturn min_pfn;\n}\n\n/**\n * find_min_pfn_with_active_regions - Find the minimum PFN registered\n *\n * It returns the minimum PFN based on information provided via\n * memblock_set_node().\n */\nunsigned long __init find_min_pfn_with_active_regions(void)\n{\n\treturn find_min_pfn_for_node(MAX_NUMNODES);\n}\n\n/*\n * early_calculate_totalpages()\n * Sum pages in active regions for movable zone.\n * Populate N_MEMORY for calculating usable_nodes.\n */\nstatic unsigned long __init early_calculate_totalpages(void)\n{\n\tunsigned long totalpages = 0;\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tunsigned long pages = end_pfn - start_pfn;\n\n\t\ttotalpages += pages;\n\t\tif (pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t}\n\treturn totalpages;\n}\n\n/*\n * Find the PFN the Movable zone begins in each node. Kernel memory\n * is spread evenly between nodes as long as the nodes have enough\n * memory. When they don't, some nodes will have more kernelcore than\n * others\n */\nstatic void __init find_zone_movable_pfns_for_nodes(void)\n{\n\tint i, nid;\n\tunsigned long usable_startpfn;\n\tunsigned long kernelcore_node, kernelcore_remaining;\n\t/* save the state before borrow the nodemask */\n\tnodemask_t saved_node_state = node_states[N_MEMORY];\n\tunsigned long totalpages = early_calculate_totalpages();\n\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);\n\tstruct memblock_region *r;\n\n\t/* Need to find movable_zone earlier when movable_node is specified. */\n\tfind_usable_zone_for_movable();\n\n\t/*\n\t * If movable_node is specified, ignore kernelcore and movablecore\n\t * options.\n\t */\n\tif (movable_node_is_enabled()) {\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (!memblock_is_hotpluggable(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = PFN_DOWN(r->base);\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=mirror is specified, ignore movablecore option\n\t */\n\tif (mirrored_kernelcore) {\n\t\tbool mem_below_4gb_not_mirrored = false;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (memblock_is_mirror(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);\n\n\t\t\tif (usable_startpfn < 0x100000) {\n\t\t\t\tmem_below_4gb_not_mirrored = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tif (mem_below_4gb_not_mirrored)\n\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\");\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=nn% or movablecore=nn% was specified, calculate the\n\t * amount of necessary memory.\n\t */\n\tif (required_kernelcore_percent)\n\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /\n\t\t\t\t       10000UL;\n\tif (required_movablecore_percent)\n\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /\n\t\t\t\t\t10000UL;\n\n\t/*\n\t * If movablecore= was specified, calculate what size of\n\t * kernelcore that corresponds so that memory usable for\n\t * any allocation type is evenly spread. If both kernelcore\n\t * and movablecore are specified, then the value of kernelcore\n\t * will be used for required_kernelcore if it's greater than\n\t * what movablecore would have allowed.\n\t */\n\tif (required_movablecore) {\n\t\tunsigned long corepages;\n\n\t\t/*\n\t\t * Round-up so that ZONE_MOVABLE is at least as large as what\n\t\t * was requested by the user\n\t\t */\n\t\trequired_movablecore =\n\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);\n\t\trequired_movablecore = min(totalpages, required_movablecore);\n\t\tcorepages = totalpages - required_movablecore;\n\n\t\trequired_kernelcore = max(required_kernelcore, corepages);\n\t}\n\n\t/*\n\t * If kernelcore was not specified or kernelcore size is larger\n\t * than totalpages, there is no ZONE_MOVABLE.\n\t */\n\tif (!required_kernelcore || required_kernelcore >= totalpages)\n\t\tgoto out;\n\n\t/* usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */\n\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];\n\nrestart:\n\t/* Spread kernelcore memory as evenly as possible throughout nodes */\n\tkernelcore_node = required_kernelcore / usable_nodes;\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\t/*\n\t\t * Recalculate kernelcore_node if the division per node\n\t\t * now exceeds what is necessary to satisfy the requested\n\t\t * amount of memory for the kernel\n\t\t */\n\t\tif (required_kernelcore < kernelcore_node)\n\t\t\tkernelcore_node = required_kernelcore / usable_nodes;\n\n\t\t/*\n\t\t * As the map is walked, we track how much memory is usable\n\t\t * by the kernel using kernelcore_remaining. When it is\n\t\t * 0, the rest of the node is usable by ZONE_MOVABLE\n\t\t */\n\t\tkernelcore_remaining = kernelcore_node;\n\n\t\t/* Go through each range of PFNs within this node */\n\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\t\tunsigned long size_pages;\n\n\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);\n\t\t\tif (start_pfn >= end_pfn)\n\t\t\t\tcontinue;\n\n\t\t\t/* Account for what is only usable for kernelcore */\n\t\t\tif (start_pfn < usable_startpfn) {\n\t\t\t\tunsigned long kernel_pages;\n\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)\n\t\t\t\t\t\t\t\t- start_pfn;\n\n\t\t\t\tkernelcore_remaining -= min(kernel_pages,\n\t\t\t\t\t\t\tkernelcore_remaining);\n\t\t\t\trequired_kernelcore -= min(kernel_pages,\n\t\t\t\t\t\t\trequired_kernelcore);\n\n\t\t\t\t/* Continue if range is now fully accounted */\n\t\t\t\tif (end_pfn <= usable_startpfn) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Push zone_movable_pfn to the end so\n\t\t\t\t\t * that if we have to rebalance\n\t\t\t\t\t * kernelcore across nodes, we will\n\t\t\t\t\t * not double account here\n\t\t\t\t\t */\n\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstart_pfn = usable_startpfn;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * The usable PFN range for ZONE_MOVABLE is from\n\t\t\t * start_pfn->end_pfn. Calculate size_pages as the\n\t\t\t * number of pages used as kernelcore\n\t\t\t */\n\t\t\tsize_pages = end_pfn - start_pfn;\n\t\t\tif (size_pages > kernelcore_remaining)\n\t\t\t\tsize_pages = kernelcore_remaining;\n\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;\n\n\t\t\t/*\n\t\t\t * Some kernelcore has been met, update counts and\n\t\t\t * break if the kernelcore for this node has been\n\t\t\t * satisfied\n\t\t\t */\n\t\t\trequired_kernelcore -= min(required_kernelcore,\n\t\t\t\t\t\t\t\tsize_pages);\n\t\t\tkernelcore_remaining -= size_pages;\n\t\t\tif (!kernelcore_remaining)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * If there is still required_kernelcore, we do another pass with one\n\t * less node in the count. This will push zone_movable_pfn[nid] further\n\t * along on the nodes that still have memory until kernelcore is\n\t * satisfied\n\t */\n\tusable_nodes--;\n\tif (usable_nodes && required_kernelcore > usable_nodes)\n\t\tgoto restart;\n\nout2:\n\t/* Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */\n\tfor (nid = 0; nid < MAX_NUMNODES; nid++)\n\t\tzone_movable_pfn[nid] =\n\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);\n\nout:\n\t/* restore the node_state */\n\tnode_states[N_MEMORY] = saved_node_state;\n}\n\n/* Any regular or high memory on that node ? */\nstatic void check_for_memory(pg_data_t *pgdat, int nid)\n{\n\tenum zone_type zone_type;\n\n\tfor (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\t\tif (populated_zone(zone)) {\n\t\t\tif (IS_ENABLED(CONFIG_HIGHMEM))\n\t\t\t\tnode_set_state(nid, N_HIGH_MEMORY);\n\t\t\tif (zone_type <= ZONE_NORMAL)\n\t\t\t\tnode_set_state(nid, N_NORMAL_MEMORY);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/**\n * free_area_init_nodes - Initialise all pg_data_t and zone data\n * @max_zone_pfn: an array of max PFNs for each zone\n *\n * This will call free_area_init_node() for each active node in the system.\n * Using the page ranges provided by memblock_set_node(), the size of each\n * zone in each node and their holes is calculated. If the maximum PFN\n * between two adjacent zones match, it is assumed that the zone is empty.\n * For example, if arch_max_dma_pfn == arch_max_dma32_pfn, it is assumed\n * that arch_max_dma32_pfn has no pages. It is also assumed that a zone\n * starts where the previous one ended. For example, ZONE_DMA32 starts\n * at arch_max_dma_pfn.\n */\nvoid __init free_area_init_nodes(unsigned long *max_zone_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\t/* Record where the zone boundaries are */\n\tmemset(arch_zone_lowest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_lowest_possible_pfn));\n\tmemset(arch_zone_highest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_highest_possible_pfn));\n\n\tstart_pfn = find_min_pfn_with_active_regions();\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tend_pfn = max(max_zone_pfn[i], start_pfn);\n\t\tarch_zone_lowest_possible_pfn[i] = start_pfn;\n\t\tarch_zone_highest_possible_pfn[i] = end_pfn;\n\n\t\tstart_pfn = end_pfn;\n\t}\n\n\t/* Find the PFNs that ZONE_MOVABLE begins at in each node */\n\tmemset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));\n\tfind_zone_movable_pfns_for_nodes();\n\n\t/* Print out the zone ranges */\n\tpr_info(\"Zone ranges:\\n\");\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\t\tpr_info(\"  %-8s \", zone_names[i]);\n\t\tif (arch_zone_lowest_possible_pfn[i] ==\n\t\t\t\tarch_zone_highest_possible_pfn[i])\n\t\t\tpr_cont(\"empty\\n\");\n\t\telse\n\t\t\tpr_cont(\"[mem %#018Lx-%#018Lx]\\n\",\n\t\t\t\t(u64)arch_zone_lowest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT,\n\t\t\t\t((u64)arch_zone_highest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT) - 1);\n\t}\n\n\t/* Print out the PFNs ZONE_MOVABLE begins at in each node */\n\tpr_info(\"Movable zone start for each node\\n\");\n\tfor (i = 0; i < MAX_NUMNODES; i++) {\n\t\tif (zone_movable_pfn[i])\n\t\t\tpr_info(\"  Node %d: %#018Lx\\n\", i,\n\t\t\t       (u64)zone_movable_pfn[i] << PAGE_SHIFT);\n\t}\n\n\t/* Print out the early node map */\n\tpr_info(\"Early memory node ranges\\n\");\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid)\n\t\tpr_info(\"  node %3d: [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\t\t((u64)end_pfn << PAGE_SHIFT) - 1);\n\n\t/* Initialise every node */\n\tmminit_verify_pageflags_layout();\n\tsetup_nr_node_ids();\n\tzero_resv_unavail();\n\tfor_each_online_node(nid) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tfree_area_init_node(nid, NULL,\n\t\t\t\tfind_min_pfn_for_node(nid), NULL);\n\n\t\t/* Any memory on that node */\n\t\tif (pgdat->node_present_pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t\tcheck_for_memory(pgdat, nid);\n\t}\n}\n\nstatic int __init cmdline_parse_core(char *p, unsigned long *core,\n\t\t\t\t     unsigned long *percent)\n{\n\tunsigned long long coremem;\n\tchar *endptr;\n\n\tif (!p)\n\t\treturn -EINVAL;\n\n\t/* Value may be a percentage of total memory, otherwise bytes */\n\tcoremem = simple_strtoull(p, &endptr, 0);\n\tif (*endptr == '%') {\n\t\t/* Paranoid check for percent values greater than 100 */\n\t\tWARN_ON(coremem > 100);\n\n\t\t*percent = coremem;\n\t} else {\n\t\tcoremem = memparse(p, &p);\n\t\t/* Paranoid check that UL is enough for the coremem value */\n\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);\n\n\t\t*core = coremem >> PAGE_SHIFT;\n\t\t*percent = 0UL;\n\t}\n\treturn 0;\n}\n\n/*\n * kernelcore=size sets the amount of memory for use for allocations that\n * cannot be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_kernelcore(char *p)\n{\n\t/* parse kernelcore=mirror */\n\tif (parse_option_str(p, \"mirror\")) {\n\t\tmirrored_kernelcore = true;\n\t\treturn 0;\n\t}\n\n\treturn cmdline_parse_core(p, &required_kernelcore,\n\t\t\t\t  &required_kernelcore_percent);\n}\n\n/*\n * movablecore=size sets the amount of memory for use for allocations that\n * can be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_movablecore(char *p)\n{\n\treturn cmdline_parse_core(p, &required_movablecore,\n\t\t\t\t  &required_movablecore_percent);\n}\n\nearly_param(\"kernelcore\", cmdline_parse_kernelcore);\nearly_param(\"movablecore\", cmdline_parse_movablecore);\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nvoid adjust_managed_page_count(struct page *page, long count)\n{\n\tspin_lock(&managed_page_count_lock);\n\tpage_zone(page)->managed_pages += count;\n\ttotalram_pages += count;\n#ifdef CONFIG_HIGHMEM\n\tif (PageHighMem(page))\n\t\ttotalhigh_pages += count;\n#endif\n\tspin_unlock(&managed_page_count_lock);\n}\nEXPORT_SYMBOL(adjust_managed_page_count);\n\nunsigned long free_reserved_area(void *start, void *end, int poison, char *s)\n{\n\tvoid *pos;\n\tunsigned long pages = 0;\n\n\tstart = (void *)PAGE_ALIGN((unsigned long)start);\n\tend = (void *)((unsigned long)end & PAGE_MASK);\n\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {\n\t\tstruct page *page = virt_to_page(pos);\n\t\tvoid *direct_map_addr;\n\n\t\t/*\n\t\t * 'direct_map_addr' might be different from 'pos'\n\t\t * because some architectures' virt_to_page()\n\t\t * work with aliases.  Getting the direct map\n\t\t * address ensures that we get a _writeable_\n\t\t * alias for the memset().\n\t\t */\n\t\tdirect_map_addr = page_address(page);\n\t\tif ((unsigned int)poison <= 0xFF)\n\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);\n\n\t\tfree_reserved_page(page);\n\t}\n\n\tif (pages && s)\n\t\tpr_info(\"Freeing %s memory: %ldK\\n\",\n\t\t\ts, pages << (PAGE_SHIFT - 10));\n\n\treturn pages;\n}\nEXPORT_SYMBOL(free_reserved_area);\n\n#ifdef\tCONFIG_HIGHMEM\nvoid free_highmem_page(struct page *page)\n{\n\t__free_reserved_page(page);\n\ttotalram_pages++;\n\tpage_zone(page)->managed_pages++;\n\ttotalhigh_pages++;\n}\n#endif\n\n\nvoid __init mem_init_print_info(const char *str)\n{\n\tunsigned long physpages, codesize, datasize, rosize, bss_size;\n\tunsigned long init_code_size, init_data_size;\n\n\tphyspages = get_num_physpages();\n\tcodesize = _etext - _stext;\n\tdatasize = _edata - _sdata;\n\trosize = __end_rodata - __start_rodata;\n\tbss_size = __bss_stop - __bss_start;\n\tinit_data_size = __init_end - __init_begin;\n\tinit_code_size = _einittext - _sinittext;\n\n\t/*\n\t * Detect special cases and adjust section sizes accordingly:\n\t * 1) .init.* may be embedded into .data sections\n\t * 2) .init.text.* may be out of [__init_begin, __init_end],\n\t *    please refer to arch/tile/kernel/vmlinux.lds.S.\n\t * 3) .rodata.* may be embedded into .text or .data sections.\n\t */\n#define adj_init_size(start, end, size, pos, adj) \\\n\tdo { \\\n\t\tif (start <= pos && pos < end && size > adj) \\\n\t\t\tsize -= adj; \\\n\t} while (0)\n\n\tadj_init_size(__init_begin, __init_end, init_data_size,\n\t\t     _sinittext, init_code_size);\n\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);\n\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);\n\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);\n\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);\n\n#undef\tadj_init_size\n\n\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"\n#ifdef\tCONFIG_HIGHMEM\n\t\t\", %luK highmem\"\n#endif\n\t\t\"%s%s)\\n\",\n\t\tnr_free_pages() << (PAGE_SHIFT - 10),\n\t\tphyspages << (PAGE_SHIFT - 10),\n\t\tcodesize >> 10, datasize >> 10, rosize >> 10,\n\t\t(init_data_size + init_code_size) >> 10, bss_size >> 10,\n\t\t(physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT - 10),\n\t\ttotalcma_pages << (PAGE_SHIFT - 10),\n#ifdef\tCONFIG_HIGHMEM\n\t\ttotalhigh_pages << (PAGE_SHIFT - 10),\n#endif\n\t\tstr ? \", \" : \"\", str ? str : \"\");\n}\n\n/**\n * set_dma_reserve - set the specified number of pages reserved in the first zone\n * @new_dma_reserve: The number of pages to mark reserved\n *\n * The per-cpu batchsize and zone watermarks are determined by managed_pages.\n * In the DMA zone, a significant percentage may be consumed by kernel image\n * and other unfreeable allocations which can skew the watermarks badly. This\n * function may optionally be used to account for unfreeable pages in the\n * first zone (e.g., ZONE_DMA). The effect will be lower watermarks and\n * smaller per-cpu batchsize.\n */\nvoid __init set_dma_reserve(unsigned long new_dma_reserve)\n{\n\tdma_reserve = new_dma_reserve;\n}\n\nvoid __init free_area_init(unsigned long *zones_size)\n{\n\tzero_resv_unavail();\n\tfree_area_init_node(0, zones_size,\n\t\t\t__pa(PAGE_OFFSET) >> PAGE_SHIFT, NULL);\n}\n\nstatic int page_alloc_cpu_dead(unsigned int cpu)\n{\n\n\tlru_add_drain_cpu(cpu);\n\tdrain_pages(cpu);\n\n\t/*\n\t * Spill the event counters of the dead processor\n\t * into the current processors event counters.\n\t * This artificially elevates the count of the current\n\t * processor.\n\t */\n\tvm_events_fold_cpu(cpu);\n\n\t/*\n\t * Zero the differential counters of the dead processor\n\t * so that the vm statistics are consistent.\n\t *\n\t * This is only okay since the processor is dead and cannot\n\t * race with what we are doing.\n\t */\n\tcpu_vm_stats_fold(cpu);\n\treturn 0;\n}\n\nvoid __init page_alloc_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC_DEAD,\n\t\t\t\t\t\"mm/page_alloc:dead\", NULL,\n\t\t\t\t\tpage_alloc_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n\n/*\n * calculate_totalreserve_pages - called when sysctl_lowmem_reserve_ratio\n *\tor min_free_kbytes changes.\n */\nstatic void calculate_totalreserve_pages(void)\n{\n\tstruct pglist_data *pgdat;\n\tunsigned long reserve_pages = 0;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\n\t\tpgdat->totalreserve_pages = 0;\n\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\t\tlong max = 0;\n\n\t\t\t/* Find valid and maximum lowmem_reserve in the zone */\n\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {\n\t\t\t\tif (zone->lowmem_reserve[j] > max)\n\t\t\t\t\tmax = zone->lowmem_reserve[j];\n\t\t\t}\n\n\t\t\t/* we treat the high watermark as reserved pages. */\n\t\t\tmax += high_wmark_pages(zone);\n\n\t\t\tif (max > zone->managed_pages)\n\t\t\t\tmax = zone->managed_pages;\n\n\t\t\tpgdat->totalreserve_pages += max;\n\n\t\t\treserve_pages += max;\n\t\t}\n\t}\n\ttotalreserve_pages = reserve_pages;\n}\n\n/*\n * setup_per_zone_lowmem_reserve - called whenever\n *\tsysctl_lowmem_reserve_ratio changes.  Ensures that each zone\n *\thas a correct pages reserved value, so an adequate number of\n *\tpages are left in the zone after a successful __alloc_pages().\n */\nstatic void setup_per_zone_lowmem_reserve(void)\n{\n\tstruct pglist_data *pgdat;\n\tenum zone_type j, idx;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\t\tunsigned long managed_pages = zone->managed_pages;\n\n\t\t\tzone->lowmem_reserve[j] = 0;\n\n\t\t\tidx = j;\n\t\t\twhile (idx) {\n\t\t\t\tstruct zone *lower_zone;\n\n\t\t\t\tidx--;\n\t\t\t\tlower_zone = pgdat->node_zones + idx;\n\n\t\t\t\tif (sysctl_lowmem_reserve_ratio[idx] < 1) {\n\t\t\t\t\tsysctl_lowmem_reserve_ratio[idx] = 0;\n\t\t\t\t\tlower_zone->lowmem_reserve[j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\tlower_zone->lowmem_reserve[j] =\n\t\t\t\t\t\tmanaged_pages / sysctl_lowmem_reserve_ratio[idx];\n\t\t\t\t}\n\t\t\t\tmanaged_pages += lower_zone->managed_pages;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\nstatic void __setup_per_zone_wmarks(void)\n{\n\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);\n\tunsigned long lowmem_pages = 0;\n\tstruct zone *zone;\n\tunsigned long flags;\n\n\t/* Calculate total number of !ZONE_HIGHMEM pages */\n\tfor_each_zone(zone) {\n\t\tif (!is_highmem(zone))\n\t\t\tlowmem_pages += zone->managed_pages;\n\t}\n\n\tfor_each_zone(zone) {\n\t\tu64 tmp;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\ttmp = (u64)pages_min * zone->managed_pages;\n\t\tdo_div(tmp, lowmem_pages);\n\t\tif (is_highmem(zone)) {\n\t\t\t/*\n\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't\n\t\t\t * need highmem pages, so cap pages_min to a small\n\t\t\t * value here.\n\t\t\t *\n\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)\n\t\t\t * deltas control asynch page reclaim, and so should\n\t\t\t * not be capped for highmem.\n\t\t\t */\n\t\t\tunsigned long min_pages;\n\n\t\t\tmin_pages = zone->managed_pages / 1024;\n\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);\n\t\t\tzone->watermark[WMARK_MIN] = min_pages;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If it's a lowmem zone, reserve a number of pages\n\t\t\t * proportionate to the zone's size.\n\t\t\t */\n\t\t\tzone->watermark[WMARK_MIN] = tmp;\n\t\t}\n\n\t\t/*\n\t\t * Set the kswapd watermarks distance according to the\n\t\t * scale factor in proportion to available memory, but\n\t\t * ensure a minimum size on small systems.\n\t\t */\n\t\ttmp = max_t(u64, tmp >> 2,\n\t\t\t    mult_frac(zone->managed_pages,\n\t\t\t\t      watermark_scale_factor, 10000));\n\n\t\tzone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n\t\tzone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;\n\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\n/**\n * setup_per_zone_wmarks - called when min_free_kbytes changes\n * or when memory is hot-{added|removed}\n *\n * Ensures that the watermark[min,low,high] values for each zone are set\n * correctly with respect to min_free_kbytes.\n */\nvoid setup_per_zone_wmarks(void)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\t__setup_per_zone_wmarks();\n\tspin_unlock(&lock);\n}\n\n/*\n * Initialise min_free_kbytes.\n *\n * For small machines we want it small (128k min).  For large machines\n * we want it large (64MB max).  But it is not linear, because network\n * bandwidth does not increase linearly with machine size.  We use\n *\n *\tmin_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:\n *\tmin_free_kbytes = sqrt(lowmem_kbytes * 16)\n *\n * which yields\n *\n * 16MB:\t512k\n * 32MB:\t724k\n * 64MB:\t1024k\n * 128MB:\t1448k\n * 256MB:\t2048k\n * 512MB:\t2896k\n * 1024MB:\t4096k\n * 2048MB:\t5792k\n * 4096MB:\t8192k\n * 8192MB:\t11584k\n * 16384MB:\t16384k\n */\nint __meminit init_per_zone_wmark_min(void)\n{\n\tunsigned long lowmem_kbytes;\n\tint new_min_free_kbytes;\n\n\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);\n\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);\n\n\tif (new_min_free_kbytes > user_min_free_kbytes) {\n\t\tmin_free_kbytes = new_min_free_kbytes;\n\t\tif (min_free_kbytes < 128)\n\t\t\tmin_free_kbytes = 128;\n\t\tif (min_free_kbytes > 65536)\n\t\t\tmin_free_kbytes = 65536;\n\t} else {\n\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",\n\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);\n\t}\n\tsetup_per_zone_wmarks();\n\trefresh_zone_stat_thresholds();\n\tsetup_per_zone_lowmem_reserve();\n\n#ifdef CONFIG_NUMA\n\tsetup_min_unmapped_ratio();\n\tsetup_min_slab_ratio();\n#endif\n\n\treturn 0;\n}\ncore_initcall(init_per_zone_wmark_min)\n\n/*\n * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so\n *\tthat we can call two helper functions whenever min_free_kbytes\n *\tchanges.\n */\nint min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write) {\n\t\tuser_min_free_kbytes = min_free_kbytes;\n\t\tsetup_per_zone_wmarks();\n\t}\n\treturn 0;\n}\n\nint watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write)\n\t\tsetup_per_zone_wmarks();\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic void setup_min_unmapped_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_unmapped_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_unmapped_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_unmapped_ratio) / 100;\n}\n\n\nint sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_unmapped_ratio();\n\n\treturn 0;\n}\n\nstatic void setup_min_slab_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_slab_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_slab_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_slab_ratio) / 100;\n}\n\nint sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_slab_ratio();\n\n\treturn 0;\n}\n#endif\n\n/*\n * lowmem_reserve_ratio_sysctl_handler - just a wrapper around\n *\tproc_dointvec() so that we can call setup_per_zone_lowmem_reserve()\n *\twhenever sysctl_lowmem_reserve_ratio changes.\n *\n * The reserve ratio obviously has absolutely no relation with the\n * minimum watermarks. The lowmem reserve ratio can only make sense\n * if in function of the boot time zone sizes.\n */\nint lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tproc_dointvec_minmax(table, write, buffer, length, ppos);\n\tsetup_per_zone_lowmem_reserve();\n\treturn 0;\n}\n\n/*\n * percpu_pagelist_fraction - changes the pcp->high for each zone on each\n * cpu.  It is the fraction of total pages in each zone that a hot per cpu\n * pagelist can have before it gets flushed back to buddy allocator.\n */\nint percpu_pagelist_fraction_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct zone *zone;\n\tint old_percpu_pagelist_fraction;\n\tint ret;\n\n\tmutex_lock(&pcp_batch_high_lock);\n\told_percpu_pagelist_fraction = percpu_pagelist_fraction;\n\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (!write || ret < 0)\n\t\tgoto out;\n\n\t/* Sanity checking to avoid pcp imbalance */\n\tif (percpu_pagelist_fraction &&\n\t    percpu_pagelist_fraction < MIN_PERCPU_PAGELIST_FRACTION) {\n\t\tpercpu_pagelist_fraction = old_percpu_pagelist_fraction;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* No change? */\n\tif (percpu_pagelist_fraction == old_percpu_pagelist_fraction)\n\t\tgoto out;\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int cpu;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tpageset_set_high_and_batch(zone,\n\t\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\t}\nout:\n\tmutex_unlock(&pcp_batch_high_lock);\n\treturn ret;\n}\n\n#ifdef CONFIG_NUMA\nint hashdist = HASHDIST_DEFAULT;\n\nstatic int __init set_hashdist(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\thashdist = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"hashdist=\", set_hashdist);\n#endif\n\n#ifndef __HAVE_ARCH_RESERVED_KERNEL_PAGES\n/*\n * Returns the number of pages that arch has reserved but\n * is not known to alloc_large_system_hash().\n */\nstatic unsigned long __init arch_reserved_kernel_pages(void)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Adaptive scale is meant to reduce sizes of hash tables on large memory\n * machines. As memory size is increased the scale is also increased but at\n * slower pace.  Starting from ADAPT_SCALE_BASE (64G), every time memory\n * quadruples the scale is increased by one, which means the size of hash table\n * only doubles, instead of quadrupling as well.\n * Because 32-bit systems cannot have large physical memory, where this scaling\n * makes sense, it is disabled on such platforms.\n */\n#if __BITS_PER_LONG > 32\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#endif\n\n/*\n * allocate a large system hash table from bootmem\n * - it is assumed that the hash table must contain an exact power-of-2\n *   quantity of entries\n * - limit is the number of hash buckets, not the total allocation size\n */\nvoid *__init alloc_large_system_hash(const char *tablename,\n\t\t\t\t     unsigned long bucketsize,\n\t\t\t\t     unsigned long numentries,\n\t\t\t\t     int scale,\n\t\t\t\t     int flags,\n\t\t\t\t     unsigned int *_hash_shift,\n\t\t\t\t     unsigned int *_hash_mask,\n\t\t\t\t     unsigned long low_limit,\n\t\t\t\t     unsigned long high_limit)\n{\n\tunsigned long long max = high_limit;\n\tunsigned long log2qty, size;\n\tvoid *table = NULL;\n\tgfp_t gfp_flags;\n\n\t/* allow the kernel cmdline to have a say */\n\tif (!numentries) {\n\t\t/* round applicable memory size up to nearest megabyte */\n\t\tnumentries = nr_kernel_pages;\n\t\tnumentries -= arch_reserved_kernel_pages();\n\n\t\t/* It isn't necessary when PAGE_SIZE >= 1MB */\n\t\tif (PAGE_SHIFT < 20)\n\t\t\tnumentries = round_up(numentries, (1<<20)/PAGE_SIZE);\n\n#if __BITS_PER_LONG > 32\n\t\tif (!high_limit) {\n\t\t\tunsigned long adapt;\n\n\t\t\tfor (adapt = ADAPT_SCALE_NPAGES; adapt < numentries;\n\t\t\t     adapt <<= ADAPT_SCALE_SHIFT)\n\t\t\t\tscale++;\n\t\t}\n#endif\n\n\t\t/* limit to 1 bucket per 2^scale bytes of low memory */\n\t\tif (scale > PAGE_SHIFT)\n\t\t\tnumentries >>= (scale - PAGE_SHIFT);\n\t\telse\n\t\t\tnumentries <<= (PAGE_SHIFT - scale);\n\n\t\t/* Make sure we've got at least a 0-order allocation.. */\n\t\tif (unlikely(flags & HASH_SMALL)) {\n\t\t\t/* Makes no sense without HASH_EARLY */\n\t\t\tWARN_ON(!(flags & HASH_EARLY));\n\t\t\tif (!(numentries >> *_hash_shift)) {\n\t\t\t\tnumentries = 1UL << *_hash_shift;\n\t\t\t\tBUG_ON(!numentries);\n\t\t\t}\n\t\t} else if (unlikely((numentries * bucketsize) < PAGE_SIZE))\n\t\t\tnumentries = PAGE_SIZE / bucketsize;\n\t}\n\tnumentries = roundup_pow_of_two(numentries);\n\n\t/* limit allocation size to 1/16 total memory by default */\n\tif (max == 0) {\n\t\tmax = ((unsigned long long)nr_all_pages << PAGE_SHIFT) >> 4;\n\t\tdo_div(max, bucketsize);\n\t}\n\tmax = min(max, 0x80000000ULL);\n\n\tif (numentries < low_limit)\n\t\tnumentries = low_limit;\n\tif (numentries > max)\n\t\tnumentries = max;\n\n\tlog2qty = ilog2(numentries);\n\n\tgfp_flags = (flags & HASH_ZERO) ? GFP_ATOMIC | __GFP_ZERO : GFP_ATOMIC;\n\tdo {\n\t\tsize = bucketsize << log2qty;\n\t\tif (flags & HASH_EARLY) {\n\t\t\tif (flags & HASH_ZERO)\n\t\t\t\ttable = memblock_alloc_nopanic(size,\n\t\t\t\t\t\t\t       SMP_CACHE_BYTES);\n\t\t\telse\n\t\t\t\ttable = memblock_alloc_raw(size,\n\t\t\t\t\t\t\t   SMP_CACHE_BYTES);\n\t\t} else if (hashdist) {\n\t\t\ttable = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\t\t} else {\n\t\t\t/*\n\t\t\t * If bucketsize is not a power-of-two, we may free\n\t\t\t * some pages at the end of hash table which\n\t\t\t * alloc_pages_exact() automatically does\n\t\t\t */\n\t\t\tif (get_order(size) < MAX_ORDER) {\n\t\t\t\ttable = alloc_pages_exact(size, gfp_flags);\n\t\t\t\tkmemleak_alloc(table, size, 1, gfp_flags);\n\t\t\t}\n\t\t}\n\t} while (!table && size > PAGE_SIZE && --log2qty);\n\n\tif (!table)\n\t\tpanic(\"Failed to allocate %s hash table\\n\", tablename);\n\n\tpr_info(\"%s hash table entries: %ld (order: %d, %lu bytes)\\n\",\n\t\ttablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size);\n\n\tif (_hash_shift)\n\t\t*_hash_shift = log2qty;\n\tif (_hash_mask)\n\t\t*_hash_mask = (1 << log2qty) - 1;\n\n\treturn table;\n}\n\n/*\n * This function checks whether pageblock includes unmovable pages or not.\n * If @count is not zero, it is okay to include less @count unmovable pages\n *\n * PageLRU check without isolation or lru_lock could race so that\n * MIGRATE_MOVABLE block might include unmovable pages. And __PageMovable\n * check without lock_page also may miss some movable non-lru pages at\n * race condition. So you can't expect this function should be exact.\n */\nbool has_unmovable_pages(struct zone *zone, struct page *page, int count,\n\t\t\t int migratetype,\n\t\t\t bool skip_hwpoisoned_pages)\n{\n\tunsigned long pfn, iter, found;\n\n\t/*\n\t * TODO we could make this much more efficient by not checking every\n\t * page in the range if we know all of them are in MOVABLE_ZONE and\n\t * that the movable zone guarantees that pages are migratable but\n\t * the later is not the case right now unfortunatelly. E.g. movablecore\n\t * can still lead to having bootmem allocations in zone_movable.\n\t */\n\n\t/*\n\t * CMA allocations (alloc_contig_range) really need to mark isolate\n\t * CMA pageblocks even when they are not movable in fact so consider\n\t * them movable here.\n\t */\n\tif (is_migrate_cma(migratetype) &&\n\t\t\tis_migrate_cma(get_pageblock_migratetype(page)))\n\t\treturn false;\n\n\tpfn = page_to_pfn(page);\n\tfor (found = 0, iter = 0; iter < pageblock_nr_pages; iter++) {\n\t\tunsigned long check = pfn + iter;\n\n\t\tif (!pfn_valid_within(check))\n\t\t\tcontinue;\n\n\t\tpage = pfn_to_page(check);\n\n\t\tif (PageReserved(page))\n\t\t\tgoto unmovable;\n\n\t\t/*\n\t\t * If the zone is movable and we have ruled out all reserved\n\t\t * pages then it should be reasonably safe to assume the rest\n\t\t * is movable.\n\t\t */\n\t\tif (zone_idx(zone) == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Hugepages are not in LRU lists, but they're movable.\n\t\t * We need not scan over tail pages bacause we don't\n\t\t * handle each tail page individually in migration.\n\t\t */\n\t\tif (PageHuge(page)) {\n\n\t\t\tif (!hugepage_migration_supported(page_hstate(page)))\n\t\t\t\tgoto unmovable;\n\n\t\t\titer = round_up(iter + 1, 1<<compound_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We can't use page_count without pin a page\n\t\t * because another CPU can free compound page.\n\t\t * This check already skips compound tails of THP\n\t\t * because their page->_refcount is zero at all time.\n\t\t */\n\t\tif (!page_ref_count(page)) {\n\t\t\tif (PageBuddy(page))\n\t\t\t\titer += (1 << page_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (skip_hwpoisoned_pages && PageHWPoison(page))\n\t\t\tcontinue;\n\n\t\tif (__PageMovable(page))\n\t\t\tcontinue;\n\n\t\tif (!PageLRU(page))\n\t\t\tfound++;\n\t\t/*\n\t\t * If there are RECLAIMABLE pages, we need to check\n\t\t * it.  But now, memory offline itself doesn't call\n\t\t * shrink_node_slabs() and it still to be fixed.\n\t\t */\n\t\t/*\n\t\t * If the page is not RAM, page_count()should be 0.\n\t\t * we don't need more check. This is an _used_ not-movable page.\n\t\t *\n\t\t * The problematic thing here is PG_reserved pages. PG_reserved\n\t\t * is set to both of a memory hole page and a _used_ kernel\n\t\t * page at boot.\n\t\t */\n\t\tif (found > count)\n\t\t\tgoto unmovable;\n\t}\n\treturn false;\nunmovable:\n\tWARN_ON_ONCE(zone_idx(zone) == ZONE_MOVABLE);\n\treturn true;\n}\n\n#if (defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || defined(CONFIG_CMA)\n\nstatic unsigned long pfn_max_align_down(unsigned long pfn)\n{\n\treturn pfn & ~(max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t     pageblock_nr_pages) - 1);\n}\n\nstatic unsigned long pfn_max_align_up(unsigned long pfn)\n{\n\treturn ALIGN(pfn, max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t\tpageblock_nr_pages));\n}\n\n/* [start, end) must belong to a single zone. */\nstatic int __alloc_contig_migrate_range(struct compact_control *cc,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\t/* This function is based on compact_zone() from compaction.c. */\n\tunsigned long nr_reclaimed;\n\tunsigned long pfn = start;\n\tunsigned int tries = 0;\n\tint ret = 0;\n\n\tmigrate_prep();\n\n\twhile (pfn < end || !list_empty(&cc->migratepages)) {\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (list_empty(&cc->migratepages)) {\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tpfn = isolate_migratepages_range(cc, pfn, end);\n\t\t\tif (!pfn) {\n\t\t\t\tret = -EINTR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttries = 0;\n\t\t} else if (++tries == 5) {\n\t\t\tret = ret < 0 ? ret : -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\tnr_reclaimed = reclaim_clean_pages_from_list(cc->zone,\n\t\t\t\t\t\t\t&cc->migratepages);\n\t\tcc->nr_migratepages -= nr_reclaimed;\n\n\t\tret = migrate_pages(&cc->migratepages, alloc_migrate_target,\n\t\t\t\t    NULL, 0, cc->mode, MR_CONTIG_RANGE);\n\t}\n\tif (ret < 0) {\n\t\tputback_movable_pages(&cc->migratepages);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n/**\n * alloc_contig_range() -- tries to allocate given range of pages\n * @start:\tstart PFN to allocate\n * @end:\tone-past-the-last PFN to allocate\n * @migratetype:\tmigratetype of the underlaying pageblocks (either\n *\t\t\t#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks\n *\t\t\tin range must have the same migratetype and it must\n *\t\t\tbe either of the two.\n * @gfp_mask:\tGFP mask to use during compaction\n *\n * The PFN range does not have to be pageblock or MAX_ORDER_NR_PAGES\n * aligned.  The PFN range must belong to a single zone.\n *\n * The first thing this routine does is attempt to MIGRATE_ISOLATE all\n * pageblocks in the range.  Once isolated, the pageblocks should not\n * be modified by others.\n *\n * Returns zero on success or negative error code.  On success all\n * pages which PFN is in [start, end) are allocated for the caller and\n * need to be freed with free_contig_range().\n */\nint alloc_contig_range(unsigned long start, unsigned long end,\n\t\t       unsigned migratetype, gfp_t gfp_mask)\n{\n\tunsigned long outer_start, outer_end;\n\tunsigned int order;\n\tint ret = 0;\n\n\tstruct compact_control cc = {\n\t\t.nr_migratepages = 0,\n\t\t.order = -1,\n\t\t.zone = page_zone(pfn_to_page(start)),\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.no_set_skip_hint = true,\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t};\n\tINIT_LIST_HEAD(&cc.migratepages);\n\n\t/*\n\t * What we do here is we mark all pageblocks in range as\n\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may\n\t * have different sizes, and due to the way page allocator\n\t * work, we align the range to biggest of the two pages so\n\t * that page allocator won't try to merge buddies from\n\t * different pageblocks and change MIGRATE_ISOLATE to some\n\t * other migration type.\n\t *\n\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we\n\t * migrate the pages from an unaligned range (ie. pages that\n\t * we are interested in).  This will put all the pages in\n\t * range back to page allocator as MIGRATE_ISOLATE.\n\t *\n\t * When this is done, we take the pages in range from page\n\t * allocator removing them from the buddy system.  This way\n\t * page allocator will never consider using them.\n\t *\n\t * This lets us mark the pageblocks back as\n\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the\n\t * aligned range but not in the unaligned, original range are\n\t * put back to page allocator so that buddy can use them.\n\t */\n\n\tret = start_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\t       pfn_max_align_up(end), migratetype,\n\t\t\t\t       false);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * In case of -EBUSY, we'd like to know which page causes problem.\n\t * So, just fall through. test_pages_isolated() has a tracepoint\n\t * which will report the busy page.\n\t *\n\t * It is possible that busy pages could become available before\n\t * the call to test_pages_isolated, and the range will actually be\n\t * allocated.  So, if we fall through be sure to clear ret so that\n\t * -EBUSY is not accidentally used or returned to caller.\n\t */\n\tret = __alloc_contig_migrate_range(&cc, start, end);\n\tif (ret && ret != -EBUSY)\n\t\tgoto done;\n\tret =0;\n\n\t/*\n\t * Pages from [start, end) are within a MAX_ORDER_NR_PAGES\n\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's\n\t * more, all pages in [start, end) are free in page allocator.\n\t * What we are going to do is to allocate all pages from\n\t * [start, end) (that is remove them from page allocator).\n\t *\n\t * The only problem is that pages at the beginning and at the\n\t * end of interesting range may be not aligned with pages that\n\t * page allocator holds, ie. they can be part of higher order\n\t * pages.  Because of this, we reserve the bigger range and\n\t * once this is done free the pages we are not interested in.\n\t *\n\t * We don't have to hold zone->lock here because the pages are\n\t * isolated thus they won't get removed from buddy.\n\t */\n\n\tlru_add_drain_all();\n\tdrain_all_pages(cc.zone);\n\n\torder = 0;\n\touter_start = start;\n\twhile (!PageBuddy(pfn_to_page(outer_start))) {\n\t\tif (++order >= MAX_ORDER) {\n\t\t\touter_start = start;\n\t\t\tbreak;\n\t\t}\n\t\touter_start &= ~0UL << order;\n\t}\n\n\tif (outer_start != start) {\n\t\torder = page_order(pfn_to_page(outer_start));\n\n\t\t/*\n\t\t * outer_start page could be small order buddy page and\n\t\t * it doesn't include start page. Adjust outer_start\n\t\t * in this case to report failed page properly\n\t\t * on tracepoint in test_pages_isolated()\n\t\t */\n\t\tif (outer_start + (1UL << order) <= start)\n\t\t\touter_start = start;\n\t}\n\n\t/* Make sure the range is really isolated. */\n\tif (test_pages_isolated(outer_start, end, false)) {\n\t\tpr_info_ratelimited(\"%s: [%lx, %lx) PFNs busy\\n\",\n\t\t\t__func__, outer_start, end);\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Grab isolated pages from freelists. */\n\touter_end = isolate_freepages_range(&cc, outer_start, end);\n\tif (!outer_end) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Free head and tail (if any) */\n\tif (start != outer_start)\n\t\tfree_contig_range(outer_start, start - outer_start);\n\tif (end != outer_end)\n\t\tfree_contig_range(end, outer_end - end);\n\ndone:\n\tundo_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\tpfn_max_align_up(end), migratetype);\n\treturn ret;\n}\n\nvoid free_contig_range(unsigned long pfn, unsigned nr_pages)\n{\n\tunsigned int count = 0;\n\n\tfor (; nr_pages--; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tcount += page_count(page) != 1;\n\t\t__free_page(page);\n\t}\n\tWARN(count != 0, \"%d pages are still in use!\\n\", count);\n}\n#endif\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n/*\n * The zone indicated has a new number of managed_pages; batch sizes and percpu\n * page high values need to be recalulated.\n */\nvoid __meminit zone_pcp_update(struct zone *zone)\n{\n\tunsigned cpu;\n\tmutex_lock(&pcp_batch_high_lock);\n\tfor_each_possible_cpu(cpu)\n\t\tpageset_set_high_and_batch(zone,\n\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n#endif\n\nvoid zone_pcp_reset(struct zone *zone)\n{\n\tunsigned long flags;\n\tint cpu;\n\tstruct per_cpu_pageset *pset;\n\n\t/* avoid races with drain_pages()  */\n\tlocal_irq_save(flags);\n\tif (zone->pageset != &boot_pageset) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpset = per_cpu_ptr(zone->pageset, cpu);\n\t\t\tdrain_zonestat(zone, pset);\n\t\t}\n\t\tfree_percpu(zone->pageset);\n\t\tzone->pageset = &boot_pageset;\n\t}\n\tlocal_irq_restore(flags);\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n/*\n * All pages in the range must be in a single zone and isolated\n * before calling this.\n */\nvoid\n__offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tstruct page *page;\n\tstruct zone *zone;\n\tunsigned int order, i;\n\tunsigned long pfn;\n\tunsigned long flags;\n\t/* find the first valid pfn */\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++)\n\t\tif (pfn_valid(pfn))\n\t\t\tbreak;\n\tif (pfn == end_pfn)\n\t\treturn;\n\toffline_mem_sections(pfn, end_pfn);\n\tzone = page_zone(pfn_to_page(pfn));\n\tspin_lock_irqsave(&zone->lock, flags);\n\tpfn = start_pfn;\n\twhile (pfn < end_pfn) {\n\t\tif (!pfn_valid(pfn)) {\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\t\tpage = pfn_to_page(pfn);\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (unlikely(!PageBuddy(page) && PageHWPoison(page))) {\n\t\t\tpfn++;\n\t\t\tSetPageReserved(page);\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(page_count(page));\n\t\tBUG_ON(!PageBuddy(page));\n\t\torder = page_order(page);\n#ifdef CONFIG_DEBUG_VM\n\t\tpr_info(\"remove from free list %lx %d %lx\\n\",\n\t\t\tpfn, 1 << order, end_pfn);\n#endif\n\t\tlist_del(&page->lru);\n\t\trmv_page_order(page);\n\t\tzone->free_area[order].nr_free--;\n\t\tfor (i = 0; i < (1 << order); i++)\n\t\t\tSetPageReserved((page+i));\n\t\tpfn += (1 << order);\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n#endif\n\nbool is_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn order < MAX_ORDER;\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n/*\n * Set PG_hwpoison flag if a given page is confirmed to be a free page.  This\n * test is performed under the zone lock to prevent a race against page\n * allocation.\n */\nbool set_hwpoison_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\tbool hwpoisoned = false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order) {\n\t\t\tif (!TestSetPageHWPoison(page))\n\t\t\t\thwpoisoned = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn hwpoisoned;\n}\n#endif"
        }
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"Allocation of vm region for %lu byte allocation from process %d failed\\n\"",
            "len",
            "current->pid"
          ],
          "line": 1385
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"Allocation of vma for %lu byte allocation from process %d failed\\n\"",
            "len",
            "current->pid"
          ],
          "line": 1379
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kmem_cache_free",
          "args": [
            "vm_region_jar",
            "region"
          ],
          "line": 1378
        },
        "resolved": true,
        "details": {
          "function_name": "kmem_cache_free",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/slab.c",
          "lines": "3749-3764",
          "snippet": "void kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}",
          "includes": [
            "#include\t\"slab.h\"",
            "#include\t\"internal.h\"",
            "#include <trace/events/kmem.h>",
            "#include\t<asm/page.h>",
            "#include\t<asm/tlbflush.h>",
            "#include\t<asm/cacheflush.h>",
            "#include\t<net/sock.h>",
            "#include\t<linux/sched/task_stack.h>",
            "#include\t<linux/prefetch.h>",
            "#include\t<linux/memory.h>",
            "#include\t<linux/debugobjects.h>",
            "#include\t<linux/reciprocal_div.h>",
            "#include\t<linux/rtmutex.h>",
            "#include\t<linux/fault-inject.h>",
            "#include\t<linux/mutex.h>",
            "#include\t<linux/mempolicy.h>",
            "#include\t<linux/kmemleak.h>",
            "#include\t<linux/nodemask.h>",
            "#include\t<linux/uaccess.h>",
            "#include\t<linux/string.h>",
            "#include\t<linux/rcupdate.h>",
            "#include\t<linux/module.h>",
            "#include\t<linux/sysctl.h>",
            "#include\t<linux/cpu.h>",
            "#include\t<linux/kallsyms.h>",
            "#include\t<linux/notifier.h>",
            "#include\t<linux/seq_file.h>",
            "#include\t<linux/proc_fs.h>",
            "#include\t<linux/cpuset.h>",
            "#include\t<linux/compiler.h>",
            "#include\t<linux/init.h>",
            "#include\t<linux/interrupt.h>",
            "#include\t<linux/cache.h>",
            "#include\t<linux/swap.h>",
            "#include\t<linux/poison.h>",
            "#include\t<linux/mm.h>",
            "#include\t<linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static noinline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include\t\"slab.h\"\n#include\t\"internal.h\"\n#include <trace/events/kmem.h>\n#include\t<asm/page.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/cacheflush.h>\n#include\t<net/sock.h>\n#include\t<linux/sched/task_stack.h>\n#include\t<linux/prefetch.h>\n#include\t<linux/memory.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/mutex.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/string.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/module.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/cpu.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/notifier.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/compiler.h>\n#include\t<linux/init.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/cache.h>\n#include\t<linux/swap.h>\n#include\t<linux/poison.h>\n#include\t<linux/mm.h>\n#include\t<linux/slab.h>\n\nstatic noinline struct;\n\nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}"
        }
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"Attempt to share mismatched mappings\\n\""
          ],
          "line": 1373
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1372
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vm_area_free",
          "args": [
            "vma"
          ],
          "line": 1368
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "fput",
          "args": [
            "vma->vm_file"
          ],
          "line": 1367
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "fput",
          "args": [
            "region->vm_file"
          ],
          "line": 1364
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1361
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1356
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "flush_icache_range",
          "args": [
            "region->vm_start",
            "region->vm_end"
          ],
          "line": 1352
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "add_vma_to_mm",
          "args": [
            "current->mm",
            "vma"
          ],
          "line": 1347
        },
        "resolved": true,
        "details": {
          "function_name": "add_vma_to_mm",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "655-714",
          "snippet": "static void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}"
        }
      },
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "(void *)region->vm_start",
            "0",
            "region->vm_end - region->vm_start"
          ],
          "line": 1338
        },
        "resolved": true,
        "details": {
          "function_name": "memset",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/kasan/kasan.c",
          "lines": "283-288",
          "snippet": "void *memset(void *addr, int c, size_t len)\n{\n\tcheck_memory_region((unsigned long)addr, len, true, _RET_IP_);\n\n\treturn __memset(addr, c, len);\n}",
          "includes": [
            "#include \"../slab.h\"",
            "#include \"kasan.h\"",
            "#include <linux/bug.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/types.h>",
            "#include <linux/string.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched.h>",
            "#include <linux/printk.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/memory.h>",
            "#include <linux/memblock.h>",
            "#include <linux/linkage.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/kernel.h>",
            "#include <linux/kasan.h>",
            "#include <linux/init.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);",
            "__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../slab.h\"\n#include \"kasan.h\"\n#include <linux/bug.h>\n#include <linux/vmalloc.h>\n#include <linux/types.h>\n#include <linux/string.h>\n#include <linux/stacktrace.h>\n#include <linux/slab.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched.h>\n#include <linux/printk.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/memory.h>\n#include <linux/memblock.h>\n#include <linux/linkage.h>\n#include <linux/kmemleak.h>\n#include <linux/kernel.h>\n#include <linux/kasan.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n\n__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);\n__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);\n\nvoid *memset(void *addr, int c, size_t len)\n{\n\tcheck_memory_region((unsigned long)addr, len, true, _RET_IP_);\n\n\treturn __memset(addr, c, len);\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_nommu_region",
          "args": [
            "region"
          ],
          "line": 1334
        },
        "resolved": true,
        "details": {
          "function_name": "add_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "558-584",
          "snippet": "static void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "struct rb_root nommu_region_tree = RB_ROOT;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}"
        }
      },
      {
        "call_info": {
          "callee": "do_mmap_private",
          "args": [
            "vma",
            "region",
            "len",
            "capabilities"
          ],
          "line": 1331
        },
        "resolved": true,
        "details": {
          "function_name": "do_mmap_private",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1073-1162",
          "snippet": "static int do_mmap_private(struct vm_area_struct *vma,\n\t\t\t   struct vm_region *region,\n\t\t\t   unsigned long len,\n\t\t\t   unsigned long capabilities)\n{\n\tunsigned long total, point;\n\tvoid *base;\n\tint ret, order;\n\n\t/* invoke the file's mapping function so that it can keep track of\n\t * shared mappings on devices or memory\n\t * - VM_MAYSHARE will be set if it may attempt to share\n\t */\n\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\tret = call_mmap(vma->vm_file, vma);\n\t\tif (ret == 0) {\n\t\t\t/* shouldn't return success if we're not sharing */\n\t\t\tBUG_ON(!(vma->vm_flags & VM_MAYSHARE));\n\t\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\t\treturn 0;\n\t\t}\n\t\tif (ret != -ENOSYS)\n\t\t\treturn ret;\n\n\t\t/* getting an ENOSYS error indicates that direct mmap isn't\n\t\t * possible (as opposed to tried but failed) so we'll try to\n\t\t * make a private copy of the data and map that instead */\n\t}\n\n\n\t/* allocate some memory to hold the mapping\n\t * - note that this may not return a page-aligned address if the object\n\t *   we're allocating is smaller than a page\n\t */\n\torder = get_order(len);\n\ttotal = 1 << order;\n\tpoint = len >> PAGE_SHIFT;\n\n\t/* we don't want to allocate a power-of-2 sized page set */\n\tif (sysctl_nr_trim_pages && total - point >= sysctl_nr_trim_pages)\n\t\ttotal = point;\n\n\tbase = alloc_pages_exact(total << PAGE_SHIFT, GFP_KERNEL);\n\tif (!base)\n\t\tgoto enomem;\n\n\tatomic_long_add(total, &mmap_pages_allocated);\n\n\tregion->vm_flags = vma->vm_flags |= VM_MAPPED_COPY;\n\tregion->vm_start = (unsigned long) base;\n\tregion->vm_end   = region->vm_start + len;\n\tregion->vm_top   = region->vm_start + (total << PAGE_SHIFT);\n\n\tvma->vm_start = region->vm_start;\n\tvma->vm_end   = region->vm_start + len;\n\n\tif (vma->vm_file) {\n\t\t/* read the contents of a file into the copy */\n\t\tloff_t fpos;\n\n\t\tfpos = vma->vm_pgoff;\n\t\tfpos <<= PAGE_SHIFT;\n\n\t\tret = kernel_read(vma->vm_file, base, len, &fpos);\n\t\tif (ret < 0)\n\t\t\tgoto error_free;\n\n\t\t/* clear the last little bit */\n\t\tif (ret < len)\n\t\t\tmemset(base + ret, 0, len - ret);\n\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\treturn 0;\n\nerror_free:\n\tfree_page_series(region->vm_start, region->vm_top);\n\tregion->vm_start = vma->vm_start = 0;\n\tregion->vm_end   = vma->vm_end = 0;\n\tregion->vm_top   = 0;\n\treturn ret;\n\nenomem:\n\tpr_err(\"Allocation of length %lu from process %d (%s) failed\\n\",\n\t       len, current->pid, current->comm);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "int sysctl_nr_trim_pages = CONFIG_NOMMU_INITIAL_TRIM_EXCESS;",
            "atomic_long_t mmap_pages_allocated;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint sysctl_nr_trim_pages = CONFIG_NOMMU_INITIAL_TRIM_EXCESS;\natomic_long_t mmap_pages_allocated;\n\nstatic int do_mmap_private(struct vm_area_struct *vma,\n\t\t\t   struct vm_region *region,\n\t\t\t   unsigned long len,\n\t\t\t   unsigned long capabilities)\n{\n\tunsigned long total, point;\n\tvoid *base;\n\tint ret, order;\n\n\t/* invoke the file's mapping function so that it can keep track of\n\t * shared mappings on devices or memory\n\t * - VM_MAYSHARE will be set if it may attempt to share\n\t */\n\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\tret = call_mmap(vma->vm_file, vma);\n\t\tif (ret == 0) {\n\t\t\t/* shouldn't return success if we're not sharing */\n\t\t\tBUG_ON(!(vma->vm_flags & VM_MAYSHARE));\n\t\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\t\treturn 0;\n\t\t}\n\t\tif (ret != -ENOSYS)\n\t\t\treturn ret;\n\n\t\t/* getting an ENOSYS error indicates that direct mmap isn't\n\t\t * possible (as opposed to tried but failed) so we'll try to\n\t\t * make a private copy of the data and map that instead */\n\t}\n\n\n\t/* allocate some memory to hold the mapping\n\t * - note that this may not return a page-aligned address if the object\n\t *   we're allocating is smaller than a page\n\t */\n\torder = get_order(len);\n\ttotal = 1 << order;\n\tpoint = len >> PAGE_SHIFT;\n\n\t/* we don't want to allocate a power-of-2 sized page set */\n\tif (sysctl_nr_trim_pages && total - point >= sysctl_nr_trim_pages)\n\t\ttotal = point;\n\n\tbase = alloc_pages_exact(total << PAGE_SHIFT, GFP_KERNEL);\n\tif (!base)\n\t\tgoto enomem;\n\n\tatomic_long_add(total, &mmap_pages_allocated);\n\n\tregion->vm_flags = vma->vm_flags |= VM_MAPPED_COPY;\n\tregion->vm_start = (unsigned long) base;\n\tregion->vm_end   = region->vm_start + len;\n\tregion->vm_top   = region->vm_start + (total << PAGE_SHIFT);\n\n\tvma->vm_start = region->vm_start;\n\tvma->vm_end   = region->vm_start + len;\n\n\tif (vma->vm_file) {\n\t\t/* read the contents of a file into the copy */\n\t\tloff_t fpos;\n\n\t\tfpos = vma->vm_pgoff;\n\t\tfpos <<= PAGE_SHIFT;\n\n\t\tret = kernel_read(vma->vm_file, base, len, &fpos);\n\t\tif (ret < 0)\n\t\t\tgoto error_free;\n\n\t\t/* clear the last little bit */\n\t\tif (ret < len)\n\t\t\tmemset(base + ret, 0, len - ret);\n\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\treturn 0;\n\nerror_free:\n\tfree_page_series(region->vm_start, region->vm_top);\n\tregion->vm_start = vma->vm_start = 0;\n\tregion->vm_end   = vma->vm_end = 0;\n\tregion->vm_top   = 0;\n\treturn ret;\n\nenomem:\n\tpr_err(\"Allocation of length %lu from process %d (%s) failed\\n\",\n\t       len, current->pid, current->comm);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n}"
        }
      },
      {
        "call_info": {
          "callee": "do_mmap_shared_file",
          "args": [
            "vma"
          ],
          "line": 1329
        },
        "resolved": true,
        "details": {
          "function_name": "do_mmap_shared_file",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1052-1068",
          "snippet": "static int do_mmap_shared_file(struct vm_area_struct *vma)\n{\n\tint ret;\n\n\tret = call_mmap(vma->vm_file, vma);\n\tif (ret == 0) {\n\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\treturn 0;\n\t}\n\tif (ret != -ENOSYS)\n\t\treturn ret;\n\n\t/* getting -ENOSYS indicates that direct mmap isn't possible (as\n\t * opposed to tried but failed) so we can only give a suitable error as\n\t * it's not possible to make a private copy if MAP_SHARED was given */\n\treturn -ENODEV;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int do_mmap_shared_file(struct vm_area_struct *vma)\n{\n\tint ret;\n\n\tret = call_mmap(vma->vm_file, vma);\n\tif (ret == 0) {\n\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\treturn 0;\n\t}\n\tif (ret != -ENOSYS)\n\t\treturn ret;\n\n\t/* getting -ENOSYS indicates that direct mmap isn't possible (as\n\t * opposed to tried but failed) so we can only give a suitable error as\n\t * it's not possible to make a private copy if MAP_SHARED was given */\n\treturn -ENODEV;\n}"
        }
      },
      {
        "call_info": {
          "callee": "IS_ERR_VALUE",
          "args": [
            "addr"
          ],
          "line": 1303
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "file->f_op->get_unmapped_area",
          "args": [
            "file",
            "addr",
            "len",
            "pgoff",
            "flags"
          ],
          "line": 1301
        },
        "resolved": true,
        "details": {
          "function_name": "get_unmapped_area",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/mmap.c",
          "lines": "2170-2210",
          "snippet": "unsigned long\nget_unmapped_area(struct file *file, unsigned long addr, unsigned long len,\n\t\tunsigned long pgoff, unsigned long flags)\n{\n\tunsigned long (*get_area)(struct file *, unsigned long,\n\t\t\t\t  unsigned long, unsigned long, unsigned long);\n\n\tunsigned long error = arch_mmap_check(addr, len, flags);\n\tif (error)\n\t\treturn error;\n\n\t/* Careful about overflows.. */\n\tif (len > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\tget_area = current->mm->get_unmapped_area;\n\tif (file) {\n\t\tif (file->f_op->get_unmapped_area)\n\t\t\tget_area = file->f_op->get_unmapped_area;\n\t} else if (flags & MAP_SHARED) {\n\t\t/*\n\t\t * mmap_region() will call shmem_zero_setup() to create a file,\n\t\t * so use shmem's get_unmapped_area in case it can be huge.\n\t\t * do_mmap_pgoff() will clear pgoff, so match alignment.\n\t\t */\n\t\tpgoff = 0;\n\t\tget_area = shmem_get_unmapped_area;\n\t}\n\n\taddr = get_area(file, addr, len, pgoff, flags);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tif (addr > TASK_SIZE - len)\n\t\treturn -ENOMEM;\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\terror = security_mmap_addr(addr);\n\treturn error ? error : addr;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlb.h>",
            "#include <asm/cacheflush.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/oom.h>",
            "#include <linux/pkeys.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/printk.h>",
            "#include <linux/memory.h>",
            "#include <linux/notifier.h>",
            "#include <linux/rbtree_augmented.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/audit.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/rmap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/mount.h>",
            "#include <linux/export.h>",
            "#include <linux/profile.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/fs.h>",
            "#include <linux/file.h>",
            "#include <linux/init.h>",
            "#include <linux/capability.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mman.h>",
            "#include <linux/shm.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/slab.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlb.h>\n#include <asm/cacheflush.h>\n#include <linux/uaccess.h>\n#include <linux/oom.h>\n#include <linux/pkeys.h>\n#include <linux/moduleparam.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/printk.h>\n#include <linux/memory.h>\n#include <linux/notifier.h>\n#include <linux/rbtree_augmented.h>\n#include <linux/uprobes.h>\n#include <linux/khugepaged.h>\n#include <linux/audit.h>\n#include <linux/perf_event.h>\n#include <linux/mmdebug.h>\n#include <linux/mmu_notifier.h>\n#include <linux/rmap.h>\n#include <linux/mempolicy.h>\n#include <linux/mount.h>\n#include <linux/export.h>\n#include <linux/profile.h>\n#include <linux/shmem_fs.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/init.h>\n#include <linux/capability.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/pagemap.h>\n#include <linux/mman.h>\n#include <linux/shm.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/backing-dev.h>\n#include <linux/slab.h>\n#include <linux/kernel.h>\n\nunsigned long\nget_unmapped_area(struct file *file, unsigned long addr, unsigned long len,\n\t\tunsigned long pgoff, unsigned long flags)\n{\n\tunsigned long (*get_area)(struct file *, unsigned long,\n\t\t\t\t  unsigned long, unsigned long, unsigned long);\n\n\tunsigned long error = arch_mmap_check(addr, len, flags);\n\tif (error)\n\t\treturn error;\n\n\t/* Careful about overflows.. */\n\tif (len > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\tget_area = current->mm->get_unmapped_area;\n\tif (file) {\n\t\tif (file->f_op->get_unmapped_area)\n\t\t\tget_area = file->f_op->get_unmapped_area;\n\t} else if (flags & MAP_SHARED) {\n\t\t/*\n\t\t * mmap_region() will call shmem_zero_setup() to create a file,\n\t\t * so use shmem's get_unmapped_area in case it can be huge.\n\t\t * do_mmap_pgoff() will clear pgoff, so match alignment.\n\t\t */\n\t\tpgoff = 0;\n\t\tget_area = shmem_get_unmapped_area;\n\t}\n\n\taddr = get_area(file, addr, len, pgoff, flags);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tif (addr > TASK_SIZE - len)\n\t\treturn -ENOMEM;\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\terror = security_mmap_addr(addr);\n\treturn error ? error : addr;\n}"
        }
      },
      {
        "call_info": {
          "callee": "fput",
          "args": [
            "region->vm_file"
          ],
          "line": 1289
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "file_inode",
          "args": [
            "file"
          ],
          "line": 1246
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "file_inode",
          "args": [
            "pregion->vm_file"
          ],
          "line": 1245
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "rb",
            "structvm_region",
            "vm_rb"
          ],
          "line": 1239
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_next",
          "args": [
            "rb"
          ],
          "line": 1238
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_first",
          "args": [
            "&nommu_region_tree"
          ],
          "line": 1238
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 1221
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_file",
          "args": [
            "file"
          ],
          "line": 1218
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_file",
          "args": [
            "file"
          ],
          "line": 1217
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vm_area_alloc",
          "args": [
            "current->mm"
          ],
          "line": 1205
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kmem_cache_zalloc",
          "args": [
            "vm_region_jar",
            "GFP_KERNEL"
          ],
          "line": 1201
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "determine_vm_flags",
          "args": [
            "file",
            "prot",
            "flags",
            "capabilities"
          ],
          "line": 1198
        },
        "resolved": true,
        "details": {
          "function_name": "determine_vm_flags",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "1014-1046",
          "snippet": "static unsigned long determine_vm_flags(struct file *file,\n\t\t\t\t\tunsigned long prot,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tunsigned long capabilities)\n{\n\tunsigned long vm_flags;\n\n\tvm_flags = calc_vm_prot_bits(prot, 0) | calc_vm_flag_bits(flags);\n\t/* vm_flags |= mm->def_flags; */\n\n\tif (!(capabilities & NOMMU_MAP_DIRECT)) {\n\t\t/* attempt to share read-only copies of mapped file chunks */\n\t\tvm_flags |= VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\t\tif (file && !(prot & PROT_WRITE))\n\t\t\tvm_flags |= VM_MAYSHARE;\n\t} else {\n\t\t/* overlay a shareable mapping on the backing device or inode\n\t\t * if possible - used for chardevs, ramfs/tmpfs/shmfs and\n\t\t * romfs/cramfs */\n\t\tvm_flags |= VM_MAYSHARE | (capabilities & NOMMU_VMFLAGS);\n\t\tif (flags & MAP_SHARED)\n\t\t\tvm_flags |= VM_SHARED;\n\t}\n\n\t/* refuse to let anyone share private mappings with this process if\n\t * it's being traced - otherwise breakpoints set in it may interfere\n\t * with another untraced process\n\t */\n\tif ((flags & MAP_PRIVATE) && current->ptrace)\n\t\tvm_flags &= ~VM_MAYSHARE;\n\n\treturn vm_flags;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic unsigned long determine_vm_flags(struct file *file,\n\t\t\t\t\tunsigned long prot,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tunsigned long capabilities)\n{\n\tunsigned long vm_flags;\n\n\tvm_flags = calc_vm_prot_bits(prot, 0) | calc_vm_flag_bits(flags);\n\t/* vm_flags |= mm->def_flags; */\n\n\tif (!(capabilities & NOMMU_MAP_DIRECT)) {\n\t\t/* attempt to share read-only copies of mapped file chunks */\n\t\tvm_flags |= VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\t\tif (file && !(prot & PROT_WRITE))\n\t\t\tvm_flags |= VM_MAYSHARE;\n\t} else {\n\t\t/* overlay a shareable mapping on the backing device or inode\n\t\t * if possible - used for chardevs, ramfs/tmpfs/shmfs and\n\t\t * romfs/cramfs */\n\t\tvm_flags |= VM_MAYSHARE | (capabilities & NOMMU_VMFLAGS);\n\t\tif (flags & MAP_SHARED)\n\t\t\tvm_flags |= VM_SHARED;\n\t}\n\n\t/* refuse to let anyone share private mappings with this process if\n\t * it's being traced - otherwise breakpoints set in it may interfere\n\t * with another untraced process\n\t */\n\tif ((flags & MAP_PRIVATE) && current->ptrace)\n\t\tvm_flags &= ~VM_MAYSHARE;\n\n\treturn vm_flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "PAGE_ALIGN",
          "args": [
            "len"
          ],
          "line": 1194
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "validate_mmap_request",
          "args": [
            "file",
            "addr",
            "len",
            "prot",
            "flags",
            "pgoff",
            "&capabilities"
          ],
          "line": 1187
        },
        "resolved": true,
        "details": {
          "function_name": "validate_mmap_request",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "853-1008",
          "snippet": "static int validate_mmap_request(struct file *file,\n\t\t\t\t unsigned long addr,\n\t\t\t\t unsigned long len,\n\t\t\t\t unsigned long prot,\n\t\t\t\t unsigned long flags,\n\t\t\t\t unsigned long pgoff,\n\t\t\t\t unsigned long *_capabilities)\n{\n\tunsigned long capabilities, rlen;\n\tint ret;\n\n\t/* do the simple checks first */\n\tif (flags & MAP_FIXED)\n\t\treturn -EINVAL;\n\n\tif ((flags & MAP_TYPE) != MAP_PRIVATE &&\n\t    (flags & MAP_TYPE) != MAP_SHARED)\n\t\treturn -EINVAL;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t/* Careful about overflows.. */\n\trlen = PAGE_ALIGN(len);\n\tif (!rlen || rlen > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\t/* offset overflow? */\n\tif ((pgoff + (rlen >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\tif (file) {\n\t\t/* files must support mmap */\n\t\tif (!file->f_op->mmap)\n\t\t\treturn -ENODEV;\n\n\t\t/* work out if what we've got could possibly be shared\n\t\t * - we support chardevs that provide their own \"memory\"\n\t\t * - we support files/blockdevs that are memory backed\n\t\t */\n\t\tif (file->f_op->mmap_capabilities) {\n\t\t\tcapabilities = file->f_op->mmap_capabilities(file);\n\t\t} else {\n\t\t\t/* no explicit capabilities set, so assume some\n\t\t\t * defaults */\n\t\t\tswitch (file_inode(file)->i_mode & S_IFMT) {\n\t\t\tcase S_IFREG:\n\t\t\tcase S_IFBLK:\n\t\t\t\tcapabilities = NOMMU_MAP_COPY;\n\t\t\t\tbreak;\n\n\t\t\tcase S_IFCHR:\n\t\t\t\tcapabilities =\n\t\t\t\t\tNOMMU_MAP_DIRECT |\n\t\t\t\t\tNOMMU_MAP_READ |\n\t\t\t\t\tNOMMU_MAP_WRITE;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* eliminate any capabilities that we can't support on this\n\t\t * device */\n\t\tif (!file->f_op->get_unmapped_area)\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\tif (!(file->f_mode & FMODE_CAN_READ))\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\n\t\t/* The file shall have been opened with read permission. */\n\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\treturn -EACCES;\n\n\t\tif (flags & MAP_SHARED) {\n\t\t\t/* do checks for writing, appending and locking */\n\t\t\tif ((prot & PROT_WRITE) &&\n\t\t\t    !(file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (IS_APPEND(file_inode(file)) &&\n\t\t\t    (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (locks_verify_locked(file))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we mustn't privatise shared mappings */\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\t\t} else {\n\t\t\t/* we're going to read the file into private memory we\n\t\t\t * allocate */\n\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we don't permit a private writable mapping to be\n\t\t\t * shared with the backing device */\n\t\t\tif (prot & PROT_WRITE)\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\tif (((prot & PROT_READ)  && !(capabilities & NOMMU_MAP_READ))  ||\n\t\t\t    ((prot & PROT_WRITE) && !(capabilities & NOMMU_MAP_WRITE)) ||\n\t\t\t    ((prot & PROT_EXEC)  && !(capabilities & NOMMU_MAP_EXEC))\n\t\t\t    ) {\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t\tif (flags & MAP_SHARED) {\n\t\t\t\t\tpr_warn(\"MAP_SHARED not completely supported on !MMU\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* handle executable mappings and implied executable\n\t\t * mappings */\n\t\tif (path_noexec(&file->f_path)) {\n\t\t\tif (prot & PROT_EXEC)\n\t\t\t\treturn -EPERM;\n\t\t} else if ((prot & PROT_READ) && !(prot & PROT_EXEC)) {\n\t\t\t/* handle implication of PROT_EXEC by PROT_READ */\n\t\t\tif (current->personality & READ_IMPLIES_EXEC) {\n\t\t\t\tif (capabilities & NOMMU_MAP_EXEC)\n\t\t\t\t\tprot |= PROT_EXEC;\n\t\t\t}\n\t\t} else if ((prot & PROT_READ) &&\n\t\t\t (prot & PROT_EXEC) &&\n\t\t\t !(capabilities & NOMMU_MAP_EXEC)\n\t\t\t ) {\n\t\t\t/* backing file is not executable, try to copy */\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\t} else {\n\t\t/* anonymous mappings are always memory backed and can be\n\t\t * privately mapped\n\t\t */\n\t\tcapabilities = NOMMU_MAP_COPY;\n\n\t\t/* handle PROT_EXEC implication by PROT_READ */\n\t\tif ((prot & PROT_READ) &&\n\t\t    (current->personality & READ_IMPLIES_EXEC))\n\t\t\tprot |= PROT_EXEC;\n\t}\n\n\t/* allow the security API to have its say */\n\tret = security_mmap_addr(addr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* looks okay */\n\t*_capabilities = capabilities;\n\treturn 0;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int validate_mmap_request(struct file *file,\n\t\t\t\t unsigned long addr,\n\t\t\t\t unsigned long len,\n\t\t\t\t unsigned long prot,\n\t\t\t\t unsigned long flags,\n\t\t\t\t unsigned long pgoff,\n\t\t\t\t unsigned long *_capabilities)\n{\n\tunsigned long capabilities, rlen;\n\tint ret;\n\n\t/* do the simple checks first */\n\tif (flags & MAP_FIXED)\n\t\treturn -EINVAL;\n\n\tif ((flags & MAP_TYPE) != MAP_PRIVATE &&\n\t    (flags & MAP_TYPE) != MAP_SHARED)\n\t\treturn -EINVAL;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t/* Careful about overflows.. */\n\trlen = PAGE_ALIGN(len);\n\tif (!rlen || rlen > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\t/* offset overflow? */\n\tif ((pgoff + (rlen >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\tif (file) {\n\t\t/* files must support mmap */\n\t\tif (!file->f_op->mmap)\n\t\t\treturn -ENODEV;\n\n\t\t/* work out if what we've got could possibly be shared\n\t\t * - we support chardevs that provide their own \"memory\"\n\t\t * - we support files/blockdevs that are memory backed\n\t\t */\n\t\tif (file->f_op->mmap_capabilities) {\n\t\t\tcapabilities = file->f_op->mmap_capabilities(file);\n\t\t} else {\n\t\t\t/* no explicit capabilities set, so assume some\n\t\t\t * defaults */\n\t\t\tswitch (file_inode(file)->i_mode & S_IFMT) {\n\t\t\tcase S_IFREG:\n\t\t\tcase S_IFBLK:\n\t\t\t\tcapabilities = NOMMU_MAP_COPY;\n\t\t\t\tbreak;\n\n\t\t\tcase S_IFCHR:\n\t\t\t\tcapabilities =\n\t\t\t\t\tNOMMU_MAP_DIRECT |\n\t\t\t\t\tNOMMU_MAP_READ |\n\t\t\t\t\tNOMMU_MAP_WRITE;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* eliminate any capabilities that we can't support on this\n\t\t * device */\n\t\tif (!file->f_op->get_unmapped_area)\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\tif (!(file->f_mode & FMODE_CAN_READ))\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\n\t\t/* The file shall have been opened with read permission. */\n\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\treturn -EACCES;\n\n\t\tif (flags & MAP_SHARED) {\n\t\t\t/* do checks for writing, appending and locking */\n\t\t\tif ((prot & PROT_WRITE) &&\n\t\t\t    !(file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (IS_APPEND(file_inode(file)) &&\n\t\t\t    (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (locks_verify_locked(file))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we mustn't privatise shared mappings */\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\t\t} else {\n\t\t\t/* we're going to read the file into private memory we\n\t\t\t * allocate */\n\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we don't permit a private writable mapping to be\n\t\t\t * shared with the backing device */\n\t\t\tif (prot & PROT_WRITE)\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\tif (((prot & PROT_READ)  && !(capabilities & NOMMU_MAP_READ))  ||\n\t\t\t    ((prot & PROT_WRITE) && !(capabilities & NOMMU_MAP_WRITE)) ||\n\t\t\t    ((prot & PROT_EXEC)  && !(capabilities & NOMMU_MAP_EXEC))\n\t\t\t    ) {\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t\tif (flags & MAP_SHARED) {\n\t\t\t\t\tpr_warn(\"MAP_SHARED not completely supported on !MMU\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* handle executable mappings and implied executable\n\t\t * mappings */\n\t\tif (path_noexec(&file->f_path)) {\n\t\t\tif (prot & PROT_EXEC)\n\t\t\t\treturn -EPERM;\n\t\t} else if ((prot & PROT_READ) && !(prot & PROT_EXEC)) {\n\t\t\t/* handle implication of PROT_EXEC by PROT_READ */\n\t\t\tif (current->personality & READ_IMPLIES_EXEC) {\n\t\t\t\tif (capabilities & NOMMU_MAP_EXEC)\n\t\t\t\t\tprot |= PROT_EXEC;\n\t\t\t}\n\t\t} else if ((prot & PROT_READ) &&\n\t\t\t (prot & PROT_EXEC) &&\n\t\t\t !(capabilities & NOMMU_MAP_EXEC)\n\t\t\t ) {\n\t\t\t/* backing file is not executable, try to copy */\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\t} else {\n\t\t/* anonymous mappings are always memory backed and can be\n\t\t * privately mapped\n\t\t */\n\t\tcapabilities = NOMMU_MAP_COPY;\n\n\t\t/* handle PROT_EXEC implication by PROT_READ */\n\t\tif ((prot & PROT_READ) &&\n\t\t    (current->personality & READ_IMPLIES_EXEC))\n\t\t\tprot |= PROT_EXEC;\n\t}\n\n\t/* allow the security API to have its say */\n\tret = security_mmap_addr(addr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* looks okay */\n\t*_capabilities = capabilities;\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct kmem_cache *vm_region_jar;\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nunsigned long do_mmap(struct file *file,\n\t\t\tunsigned long addr,\n\t\t\tunsigned long len,\n\t\t\tunsigned long prot,\n\t\t\tunsigned long flags,\n\t\t\tvm_flags_t vm_flags,\n\t\t\tunsigned long pgoff,\n\t\t\tunsigned long *populate,\n\t\t\tstruct list_head *uf)\n{\n\tstruct vm_area_struct *vma;\n\tstruct vm_region *region;\n\tstruct rb_node *rb;\n\tunsigned long capabilities, result;\n\tint ret;\n\n\t*populate = 0;\n\n\t/* decide whether we should attempt the mapping, and if so what sort of\n\t * mapping */\n\tret = validate_mmap_request(file, addr, len, prot, flags, pgoff,\n\t\t\t\t    &capabilities);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* we ignore the address hint */\n\taddr = 0;\n\tlen = PAGE_ALIGN(len);\n\n\t/* we've determined that we can make the mapping, now translate what we\n\t * now know into VMA flags */\n\tvm_flags |= determine_vm_flags(file, prot, flags, capabilities);\n\n\t/* we're going to need to record the mapping */\n\tregion = kmem_cache_zalloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\tgoto error_getting_region;\n\n\tvma = vm_area_alloc(current->mm);\n\tif (!vma)\n\t\tgoto error_getting_vma;\n\n\tregion->vm_usage = 1;\n\tregion->vm_flags = vm_flags;\n\tregion->vm_pgoff = pgoff;\n\n\tvma->vm_flags = vm_flags;\n\tvma->vm_pgoff = pgoff;\n\n\tif (file) {\n\t\tregion->vm_file = get_file(file);\n\t\tvma->vm_file = get_file(file);\n\t}\n\n\tdown_write(&nommu_region_sem);\n\n\t/* if we want to share, we need to check for regions created by other\n\t * mmap() calls that overlap with our proposed mapping\n\t * - we can only share with a superset match on most regular files\n\t * - shared mappings on character devices and memory backed files are\n\t *   permitted to overlap inexactly as far as we are concerned for in\n\t *   these cases, sharing is handled in the driver or filesystem rather\n\t *   than here\n\t */\n\tif (vm_flags & VM_MAYSHARE) {\n\t\tstruct vm_region *pregion;\n\t\tunsigned long pglen, rpglen, pgend, rpgend, start;\n\n\t\tpglen = (len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tpgend = pgoff + pglen;\n\n\t\tfor (rb = rb_first(&nommu_region_tree); rb; rb = rb_next(rb)) {\n\t\t\tpregion = rb_entry(rb, struct vm_region, vm_rb);\n\n\t\t\tif (!(pregion->vm_flags & VM_MAYSHARE))\n\t\t\t\tcontinue;\n\n\t\t\t/* search for overlapping mappings on the same file */\n\t\t\tif (file_inode(pregion->vm_file) !=\n\t\t\t    file_inode(file))\n\t\t\t\tcontinue;\n\n\t\t\tif (pregion->vm_pgoff >= pgend)\n\t\t\t\tcontinue;\n\n\t\t\trpglen = pregion->vm_end - pregion->vm_start;\n\t\t\trpglen = (rpglen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\t\trpgend = pregion->vm_pgoff + rpglen;\n\t\t\tif (pgoff >= rpgend)\n\t\t\t\tcontinue;\n\n\t\t\t/* handle inexactly overlapping matches between\n\t\t\t * mappings */\n\t\t\tif ((pregion->vm_pgoff != pgoff || rpglen != pglen) &&\n\t\t\t    !(pgoff >= pregion->vm_pgoff && pgend <= rpgend)) {\n\t\t\t\t/* new mapping is not a subset of the region */\n\t\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\t\tgoto sharing_violation;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* we've found a region we can share */\n\t\t\tpregion->vm_usage++;\n\t\t\tvma->vm_region = pregion;\n\t\t\tstart = pregion->vm_start;\n\t\t\tstart += (pgoff - pregion->vm_pgoff) << PAGE_SHIFT;\n\t\t\tvma->vm_start = start;\n\t\t\tvma->vm_end = start + len;\n\n\t\t\tif (pregion->vm_flags & VM_MAPPED_COPY)\n\t\t\t\tvma->vm_flags |= VM_MAPPED_COPY;\n\t\t\telse {\n\t\t\t\tret = do_mmap_shared_file(vma);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tvma->vm_region = NULL;\n\t\t\t\t\tvma->vm_start = 0;\n\t\t\t\t\tvma->vm_end = 0;\n\t\t\t\t\tpregion->vm_usage--;\n\t\t\t\t\tpregion = NULL;\n\t\t\t\t\tgoto error_just_free;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfput(region->vm_file);\n\t\t\tkmem_cache_free(vm_region_jar, region);\n\t\t\tregion = pregion;\n\t\t\tresult = start;\n\t\t\tgoto share;\n\t\t}\n\n\t\t/* obtain the address at which to make a shared mapping\n\t\t * - this is the hook for quasi-memory character devices to\n\t\t *   tell us the location of a shared mapping\n\t\t */\n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\taddr = file->f_op->get_unmapped_area(file, addr, len,\n\t\t\t\t\t\t\t     pgoff, flags);\n\t\t\tif (IS_ERR_VALUE(addr)) {\n\t\t\t\tret = addr;\n\t\t\t\tif (ret != -ENOSYS)\n\t\t\t\t\tgoto error_just_free;\n\n\t\t\t\t/* the driver refused to tell us where to site\n\t\t\t\t * the mapping so we'll have to attempt to copy\n\t\t\t\t * it */\n\t\t\t\tret = -ENODEV;\n\t\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\t\tgoto error_just_free;\n\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t} else {\n\t\t\t\tvma->vm_start = region->vm_start = addr;\n\t\t\t\tvma->vm_end = region->vm_end = addr + len;\n\t\t\t}\n\t\t}\n\t}\n\n\tvma->vm_region = region;\n\n\t/* set up the mapping\n\t * - the region is filled in if NOMMU_MAP_DIRECT is still set\n\t */\n\tif (file && vma->vm_flags & VM_SHARED)\n\t\tret = do_mmap_shared_file(vma);\n\telse\n\t\tret = do_mmap_private(vma, region, len, capabilities);\n\tif (ret < 0)\n\t\tgoto error_just_free;\n\tadd_nommu_region(region);\n\n\t/* clear anonymous mappings that don't ask for uninitialized data */\n\tif (!vma->vm_file && !(flags & MAP_UNINITIALIZED))\n\t\tmemset((void *)region->vm_start, 0,\n\t\t       region->vm_end - region->vm_start);\n\n\t/* okay... we have a mapping; now we have to register it */\n\tresult = vma->vm_start;\n\n\tcurrent->mm->total_vm += len >> PAGE_SHIFT;\n\nshare:\n\tadd_vma_to_mm(current->mm, vma);\n\n\t/* we flush the region from the icache only when the first executable\n\t * mapping of it is made  */\n\tif (vma->vm_flags & VM_EXEC && !region->vm_icache_flushed) {\n\t\tflush_icache_range(region->vm_start, region->vm_end);\n\t\tregion->vm_icache_flushed = true;\n\t}\n\n\tup_write(&nommu_region_sem);\n\n\treturn result;\n\nerror_just_free:\n\tup_write(&nommu_region_sem);\nerror:\n\tif (region->vm_file)\n\t\tfput(region->vm_file);\n\tkmem_cache_free(vm_region_jar, region);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tvm_area_free(vma);\n\treturn ret;\n\nsharing_violation:\n\tup_write(&nommu_region_sem);\n\tpr_warn(\"Attempt to share mismatched mappings\\n\");\n\tret = -EINVAL;\n\tgoto error;\n\nerror_getting_vma:\n\tkmem_cache_free(vm_region_jar, region);\n\tpr_warn(\"Allocation of vma for %lu byte allocation from process %d failed\\n\",\n\t\t\tlen, current->pid);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n\nerror_getting_region:\n\tpr_warn(\"Allocation of vm region for %lu byte allocation from process %d failed\\n\",\n\t\t\tlen, current->pid);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n}"
  },
  {
    "function_name": "do_mmap_private",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1073-1162",
    "snippet": "static int do_mmap_private(struct vm_area_struct *vma,\n\t\t\t   struct vm_region *region,\n\t\t\t   unsigned long len,\n\t\t\t   unsigned long capabilities)\n{\n\tunsigned long total, point;\n\tvoid *base;\n\tint ret, order;\n\n\t/* invoke the file's mapping function so that it can keep track of\n\t * shared mappings on devices or memory\n\t * - VM_MAYSHARE will be set if it may attempt to share\n\t */\n\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\tret = call_mmap(vma->vm_file, vma);\n\t\tif (ret == 0) {\n\t\t\t/* shouldn't return success if we're not sharing */\n\t\t\tBUG_ON(!(vma->vm_flags & VM_MAYSHARE));\n\t\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\t\treturn 0;\n\t\t}\n\t\tif (ret != -ENOSYS)\n\t\t\treturn ret;\n\n\t\t/* getting an ENOSYS error indicates that direct mmap isn't\n\t\t * possible (as opposed to tried but failed) so we'll try to\n\t\t * make a private copy of the data and map that instead */\n\t}\n\n\n\t/* allocate some memory to hold the mapping\n\t * - note that this may not return a page-aligned address if the object\n\t *   we're allocating is smaller than a page\n\t */\n\torder = get_order(len);\n\ttotal = 1 << order;\n\tpoint = len >> PAGE_SHIFT;\n\n\t/* we don't want to allocate a power-of-2 sized page set */\n\tif (sysctl_nr_trim_pages && total - point >= sysctl_nr_trim_pages)\n\t\ttotal = point;\n\n\tbase = alloc_pages_exact(total << PAGE_SHIFT, GFP_KERNEL);\n\tif (!base)\n\t\tgoto enomem;\n\n\tatomic_long_add(total, &mmap_pages_allocated);\n\n\tregion->vm_flags = vma->vm_flags |= VM_MAPPED_COPY;\n\tregion->vm_start = (unsigned long) base;\n\tregion->vm_end   = region->vm_start + len;\n\tregion->vm_top   = region->vm_start + (total << PAGE_SHIFT);\n\n\tvma->vm_start = region->vm_start;\n\tvma->vm_end   = region->vm_start + len;\n\n\tif (vma->vm_file) {\n\t\t/* read the contents of a file into the copy */\n\t\tloff_t fpos;\n\n\t\tfpos = vma->vm_pgoff;\n\t\tfpos <<= PAGE_SHIFT;\n\n\t\tret = kernel_read(vma->vm_file, base, len, &fpos);\n\t\tif (ret < 0)\n\t\t\tgoto error_free;\n\n\t\t/* clear the last little bit */\n\t\tif (ret < len)\n\t\t\tmemset(base + ret, 0, len - ret);\n\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\treturn 0;\n\nerror_free:\n\tfree_page_series(region->vm_start, region->vm_top);\n\tregion->vm_start = vma->vm_start = 0;\n\tregion->vm_end   = vma->vm_end = 0;\n\tregion->vm_top   = 0;\n\treturn ret;\n\nenomem:\n\tpr_err(\"Allocation of length %lu from process %d (%s) failed\\n\",\n\t       len, current->pid, current->comm);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "int sysctl_nr_trim_pages = CONFIG_NOMMU_INITIAL_TRIM_EXCESS;",
      "atomic_long_t mmap_pages_allocated;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "show_free_areas",
          "args": [
            "0",
            "NULL"
          ],
          "line": 1160
        },
        "resolved": true,
        "details": {
          "function_name": "show_free_areas",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/page_alloc.c",
          "lines": "4851-8225",
          "snippet": "void show_free_areas(unsigned int filter, nodemask_t *nodemask)\n{\n\tunsigned long free_pcp = 0;\n\tint cpu;\n\tstruct zone *zone;\n\tpg_data_t *pgdat;\n\n\tfor_each_populated_zone(zone) {\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\t}\n\n\tprintk(\"active_anon:%lu inactive_anon:%lu isolated_anon:%lu\\n\"\n\t\t\" active_file:%lu inactive_file:%lu isolated_file:%lu\\n\"\n\t\t\" unevictable:%lu dirty:%lu writeback:%lu unstable:%lu\\n\"\n\t\t\" slab_reclaimable:%lu slab_unreclaimable:%lu\\n\"\n\t\t\" mapped:%lu shmem:%lu pagetables:%lu bounce:%lu\\n\"\n\t\t\" free:%lu free_pcp:%lu free_cma:%lu\\n\",\n\t\tglobal_node_page_state(NR_ACTIVE_ANON),\n\t\tglobal_node_page_state(NR_INACTIVE_ANON),\n\t\tglobal_node_page_state(NR_ISOLATED_ANON),\n\t\tglobal_node_page_state(NR_ACTIVE_FILE),\n\t\tglobal_node_page_state(NR_INACTIVE_FILE),\n\t\tglobal_node_page_state(NR_ISOLATED_FILE),\n\t\tglobal_node_page_state(NR_UNEVICTABLE),\n\t\tglobal_node_page_state(NR_FILE_DIRTY),\n\t\tglobal_node_page_state(NR_WRITEBACK),\n\t\tglobal_node_page_state(NR_UNSTABLE_NFS),\n\t\tglobal_node_page_state(NR_SLAB_RECLAIMABLE),\n\t\tglobal_node_page_state(NR_SLAB_UNRECLAIMABLE),\n\t\tglobal_node_page_state(NR_FILE_MAPPED),\n\t\tglobal_node_page_state(NR_SHMEM),\n\t\tglobal_zone_page_state(NR_PAGETABLE),\n\t\tglobal_zone_page_state(NR_BOUNCE),\n\t\tglobal_zone_page_state(NR_FREE_PAGES),\n\t\tfree_pcp,\n\t\tglobal_zone_page_state(NR_FREE_CMA_PAGES));\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tif (show_mem_node_skip(filter, pgdat->node_id, nodemask))\n\t\t\tcontinue;\n\n\t\tprintk(\"Node %d\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" isolated(anon):%lukB\"\n\t\t\t\" isolated(file):%lukB\"\n\t\t\t\" mapped:%lukB\"\n\t\t\t\" dirty:%lukB\"\n\t\t\t\" writeback:%lukB\"\n\t\t\t\" shmem:%lukB\"\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\t\" shmem_thp: %lukB\"\n\t\t\t\" shmem_pmdmapped: %lukB\"\n\t\t\t\" anon_thp: %lukB\"\n#endif\n\t\t\t\" writeback_tmp:%lukB\"\n\t\t\t\" unstable:%lukB\"\n\t\t\t\" all_unreclaimable? %s\"\n\t\t\t\"\\n\",\n\t\t\tpgdat->node_id,\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_UNEVICTABLE)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_MAPPED)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_DIRTY)),\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK)),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM)),\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_THPS) * HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_PMDMAPPED)\n\t\t\t\t\t* HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_ANON_THPS) * HPAGE_PMD_NR),\n#endif\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK_TEMP)),\n\t\t\tK(node_page_state(pgdat, NR_UNSTABLE_NFS)),\n\t\t\tpgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ?\n\t\t\t\t\"yes\" : \"no\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tint i;\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfree_pcp = 0;\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT\n\t\t\t\"%s\"\n\t\t\t\" free:%lukB\"\n\t\t\t\" min:%lukB\"\n\t\t\t\" low:%lukB\"\n\t\t\t\" high:%lukB\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" writepending:%lukB\"\n\t\t\t\" present:%lukB\"\n\t\t\t\" managed:%lukB\"\n\t\t\t\" mlocked:%lukB\"\n\t\t\t\" kernel_stack:%lukB\"\n\t\t\t\" pagetables:%lukB\"\n\t\t\t\" bounce:%lukB\"\n\t\t\t\" free_pcp:%lukB\"\n\t\t\t\" local_pcp:%ukB\"\n\t\t\t\" free_cma:%lukB\"\n\t\t\t\"\\n\",\n\t\t\tzone->name,\n\t\t\tK(zone_page_state(zone, NR_FREE_PAGES)),\n\t\t\tK(min_wmark_pages(zone)),\n\t\t\tK(low_wmark_pages(zone)),\n\t\t\tK(high_wmark_pages(zone)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_UNEVICTABLE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_WRITE_PENDING)),\n\t\t\tK(zone->present_pages),\n\t\t\tK(zone->managed_pages),\n\t\t\tK(zone_page_state(zone, NR_MLOCK)),\n\t\t\tzone_page_state(zone, NR_KERNEL_STACK_KB),\n\t\t\tK(zone_page_state(zone, NR_PAGETABLE)),\n\t\t\tK(zone_page_state(zone, NR_BOUNCE)),\n\t\t\tK(free_pcp),\n\t\t\tK(this_cpu_read(zone->pageset->pcp.count)),\n\t\t\tK(zone_page_state(zone, NR_FREE_CMA_PAGES)));\n\t\tprintk(\"lowmem_reserve[]:\");\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++)\n\t\t\tprintk(KERN_CONT \" %ld\", zone->lowmem_reserve[i]);\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int order;\n\t\tunsigned long nr[MAX_ORDER], flags, total = 0;\n\t\tunsigned char types[MAX_ORDER];\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT \"%s: \", zone->name);\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tstruct free_area *area = &zone->free_area[order];\n\t\t\tint type;\n\n\t\t\tnr[order] = area->nr_free;\n\t\t\ttotal += nr[order] << order;\n\n\t\t\ttypes[order] = 0;\n\t\t\tfor (type = 0; type < MIGRATE_TYPES; type++) {\n\t\t\t\tif (!list_empty(&area->free_list[type]))\n\t\t\t\t\ttypes[order] |= 1 << type;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n\t\t\t       nr[order], K(1UL) << order);\n\t\t\tif (nr[order])\n\t\t\t\tshow_migration_types(types[order]);\n\t\t}\n\t\tprintk(KERN_CONT \"= %lukB\\n\", K(total));\n\t}\n\n\thugetlb_show_meminfo();\n\n\tprintk(\"%ld total pagecache pages\\n\", global_node_page_state(NR_FILE_PAGES));\n\n\tshow_swap_cache_info();\n}\n\nstatic void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)\n{\n\tzoneref->zone = zone;\n\tzoneref->zone_idx = zone_idx(zone);\n}\n\n/*\n * Builds allocation fallback zone lists.\n *\n * Add all populated zones of a node to the zonelist.\n */\nstatic int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)\n{\n\tstruct zone *zone;\n\tenum zone_type zone_type = MAX_NR_ZONES;\n\tint nr_zones = 0;\n\n\tdo {\n\t\tzone_type--;\n\t\tzone = pgdat->node_zones + zone_type;\n\t\tif (managed_zone(zone)) {\n\t\t\tzoneref_set_zone(zone, &zonerefs[nr_zones++]);\n\t\t\tcheck_highest_zone(zone_type);\n\t\t}\n\t} while (zone_type);\n\n\treturn nr_zones;\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int __parse_numa_zonelist_order(char *s)\n{\n\t/*\n\t * We used to support different zonlists modes but they turned\n\t * out to be just not useful. Let's keep the warning in place\n\t * if somebody still use the cmd line parameter so that we do\n\t * not fail it silently\n\t */\n\tif (!(*s == 'd' || *s == 'D' || *s == 'n' || *s == 'N')) {\n\t\tpr_warn(\"Ignoring unsupported numa_zonelist_order value:  %s\\n\", s);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic __init int setup_numa_zonelist_order(char *s)\n{\n\tif (!s)\n\t\treturn 0;\n\n\treturn __parse_numa_zonelist_order(s);\n}\nearly_param(\"numa_zonelist_order\", setup_numa_zonelist_order);\n\nchar numa_zonelist_order[] = \"Node\";\n\n/*\n * sysctl handler for numa_zonelist_order\n */\nint numa_zonelist_order_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *length,\n\t\tloff_t *ppos)\n{\n\tchar *str;\n\tint ret;\n\n\tif (!write)\n\t\treturn proc_dostring(table, write, buffer, length, ppos);\n\tstr = memdup_user_nul(buffer, 16);\n\tif (IS_ERR(str))\n\t\treturn PTR_ERR(str);\n\n\tret = __parse_numa_zonelist_order(str);\n\tkfree(str);\n\treturn ret;\n}\n\n\n#define MAX_NODE_LOAD (nr_online_nodes)\nstatic int node_load[MAX_NUMNODES];\n\n/**\n * find_next_best_node - find the next node that should appear in a given node's fallback list\n * @node: node whose fallback list we're appending\n * @used_node_mask: nodemask_t of already used nodes\n *\n * We use a number of factors to determine which is the next node that should\n * appear on a given node's fallback list.  The node should not have appeared\n * already in @node's fallback list, and it should be the next closest node\n * according to the distance array (which contains arbitrary distance values\n * from each node to each node in the system), and should also prefer nodes\n * with no CPUs, since presumably they'll have very little allocation pressure\n * on them otherwise.\n * It returns -1 if no node is found.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_node_mask)\n{\n\tint n, val;\n\tint min_val = INT_MAX;\n\tint best_node = NUMA_NO_NODE;\n\tconst struct cpumask *tmp = cpumask_of_node(0);\n\n\t/* Use the local node if we haven't already */\n\tif (!node_isset(node, *used_node_mask)) {\n\t\tnode_set(node, *used_node_mask);\n\t\treturn node;\n\t}\n\n\tfor_each_node_state(n, N_MEMORY) {\n\n\t\t/* Don't want a node to appear more than once */\n\t\tif (node_isset(n, *used_node_mask))\n\t\t\tcontinue;\n\n\t\t/* Use the distance array to find the distance */\n\t\tval = node_distance(node, n);\n\n\t\t/* Penalize nodes under us (\"prefer the next node\") */\n\t\tval += (n < node);\n\n\t\t/* Give preference to headless and unused nodes */\n\t\ttmp = cpumask_of_node(n);\n\t\tif (!cpumask_empty(tmp))\n\t\t\tval += PENALTY_FOR_NODE_WITH_CPUS;\n\n\t\t/* Slight preference for less loaded node */\n\t\tval *= (MAX_NODE_LOAD*MAX_NUMNODES);\n\t\tval += node_load[n];\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node >= 0)\n\t\tnode_set(best_node, *used_node_mask);\n\n\treturn best_node;\n}\n\n\n/*\n * Build zonelists ordered by node and zones within node.\n * This results in maximum locality--normal zone overflows into local\n * DMA zone, if any--but risks exhausting DMA zone.\n */\nstatic void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order,\n\t\tunsigned nr_nodes)\n{\n\tstruct zoneref *zonerefs;\n\tint i;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\n\tfor (i = 0; i < nr_nodes; i++) {\n\t\tint nr_zones;\n\n\t\tpg_data_t *node = NODE_DATA(node_order[i]);\n\n\t\tnr_zones = build_zonerefs_node(node, zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build gfp_thisnode zonelists\n */\nstatic void build_thisnode_zonelists(pg_data_t *pgdat)\n{\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build zonelists ordered by zone and nodes within zones.\n * This results in conserving DMA zone[s] until all Normal memory is\n * exhausted, but results in overflowing to remote node while memory\n * may still exist in local DMA zone.\n */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tstatic int node_order[MAX_NUMNODES];\n\tint node, load, nr_nodes = 0;\n\tnodemask_t used_mask;\n\tint local_node, prev_node;\n\n\t/* NUMA-aware ordering of nodes */\n\tlocal_node = pgdat->node_id;\n\tload = nr_online_nodes;\n\tprev_node = local_node;\n\tnodes_clear(used_mask);\n\n\tmemset(node_order, 0, sizeof(node_order));\n\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n\t\t/*\n\t\t * We don't want to pressure a particular node.\n\t\t * So adding penalty to the first node in same\n\t\t * distance group to make it round-robin.\n\t\t */\n\t\tif (node_distance(local_node, node) !=\n\t\t    node_distance(local_node, prev_node))\n\t\t\tnode_load[node] = load;\n\n\t\tnode_order[nr_nodes++] = node;\n\t\tprev_node = node;\n\t\tload--;\n\t}\n\n\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n\tbuild_thisnode_zonelists(pgdat);\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n/*\n * Return node id of node used for \"local\" allocations.\n * I.e., first node id of first zone in arg node's generic zonelist.\n * Used for initializing percpu 'numa_mem', which is used primarily\n * for kernel allocations, so use GFP_KERNEL flags to locate zonelist.\n */\nint local_memory_node(int node)\n{\n\tstruct zoneref *z;\n\n\tz = first_zones_zonelist(node_zonelist(node, GFP_KERNEL),\n\t\t\t\t   gfp_zone(GFP_KERNEL),\n\t\t\t\t   NULL);\n\treturn zone_to_nid(z->zone);\n}\n#endif\n\nstatic void setup_min_unmapped_ratio(void);\nstatic void setup_min_slab_ratio(void);\n#else\t/* CONFIG_NUMA */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tint node, local_node;\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tlocal_node = pgdat->node_id;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\n\t/*\n\t * Now we build the zonelist so that it contains the zones\n\t * of all the other nodes.\n\t * We don't want to pressure a particular node, so when\n\t * building the zones for node N, we make sure that the\n\t * zones coming right after the local ones are those from\n\t * node N+1 (modulo N)\n\t */\n\tfor (node = local_node + 1; node < MAX_NUMNODES; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tfor (node = 0; node < local_node; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n#endif\t/* CONFIG_NUMA */\n\n/*\n * Boot pageset table. One per cpu which is going to be used for all\n * zones and all nodes. The parameters will be set in such a way\n * that an item put on a list will immediately be handed over to\n * the buddy list. This is safe since pageset manipulation is done\n * with interrupts disabled.\n *\n * The boot_pagesets must be kept even after bootup is complete for\n * unused processors and/or zones. They do play a role for bootstrapping\n * hotplugged processors.\n *\n * zoneinfo_show() and maybe other functions do\n * not check if the processor is online before following the pageset pointer.\n * Other parts of the kernel may not check if the zone is available.\n */\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch);\nstatic DEFINE_PER_CPU(struct per_cpu_pageset, boot_pageset);\nstatic DEFINE_PER_CPU(struct per_cpu_nodestat, boot_nodestats);\n\nstatic void __build_all_zonelists(void *data)\n{\n\tint nid;\n\tint __maybe_unused cpu;\n\tpg_data_t *self = data;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\n#ifdef CONFIG_NUMA\n\tmemset(node_load, 0, sizeof(node_load));\n#endif\n\n\t/*\n\t * This node is hotadded and no memory is yet present.   So just\n\t * building zonelists is fine - no need to touch other nodes.\n\t */\n\tif (self && !node_online(self->node_id)) {\n\t\tbuild_zonelists(self);\n\t} else {\n\t\tfor_each_online_node(nid) {\n\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\t\t\tbuild_zonelists(pgdat);\n\t\t}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n\t\t/*\n\t\t * We now know the \"local memory node\" for each node--\n\t\t * i.e., the node of the first zone in the generic zonelist.\n\t\t * Set up numa_mem percpu variable for on-line cpus.  During\n\t\t * boot, only the boot cpu should be on-line;  we'll init the\n\t\t * secondary cpus' numa_mem as they come on-line.  During\n\t\t * node/memory hotplug, we'll fixup all on-line cpus.\n\t\t */\n\t\tfor_each_online_cpu(cpu)\n\t\t\tset_cpu_numa_mem(cpu, local_memory_node(cpu_to_node(cpu)));\n#endif\n\t}\n\n\tspin_unlock(&lock);\n}\n\nstatic noinline void __init\nbuild_all_zonelists_init(void)\n{\n\tint cpu;\n\n\t__build_all_zonelists(NULL);\n\n\t/*\n\t * Initialize the boot_pagesets that are going to be used\n\t * for bootstrapping processors. The real pagesets for\n\t * each zone will be allocated later when the per cpu\n\t * allocator is available.\n\t *\n\t * boot_pagesets are used also for bootstrapping offline\n\t * cpus if the system is already booted because the pagesets\n\t * are needed to initialize allocators on a specific cpu too.\n\t * F.e. the percpu allocator needs the page allocator which\n\t * needs the percpu allocator in order to allocate its pagesets\n\t * (a chicken-egg dilemma).\n\t */\n\tfor_each_possible_cpu(cpu)\n\t\tsetup_pageset(&per_cpu(boot_pageset, cpu), 0);\n\n\tmminit_verify_zonelist();\n\tcpuset_init_current_mems_allowed();\n}\n\n/*\n * unless system_state == SYSTEM_BOOTING.\n *\n * __ref due to call of __init annotated helper build_all_zonelists_init\n * [protected by SYSTEM_BOOTING].\n */\nvoid __ref build_all_zonelists(pg_data_t *pgdat)\n{\n\tif (system_state == SYSTEM_BOOTING) {\n\t\tbuild_all_zonelists_init();\n\t} else {\n\t\t__build_all_zonelists(pgdat);\n\t\t/* cpuset refresh routine should be here */\n\t}\n\tvm_total_pages = nr_free_pagecache_pages();\n\t/*\n\t * Disable grouping by mobility if the number of pages in the\n\t * system is too low to allow the mechanism to work. It would be\n\t * more accurate, but expensive to check per-zone. This check is\n\t * made on memory-hotadd so a system can start with mobility\n\t * disabled and enable it later\n\t */\n\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))\n\t\tpage_group_by_mobility_disabled = 1;\n\telse\n\t\tpage_group_by_mobility_disabled = 0;\n\n\tpr_info(\"Built %i zonelists, mobility grouping %s.  Total pages: %ld\\n\",\n\t\tnr_online_nodes,\n\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",\n\t\tvm_total_pages);\n#ifdef CONFIG_NUMA\n\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);\n#endif\n}\n\n/* If zone is ZONE_MOVABLE but memory is mirrored, it is an overlapped init */\nstatic bool __meminit\noverlap_memmap_init(unsigned long zone, unsigned long *pfn)\n{\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tstatic struct memblock_region *r;\n\n\tif (mirrored_kernelcore && zone == ZONE_MOVABLE) {\n\t\tif (!r || *pfn >= memblock_region_memory_end_pfn(r)) {\n\t\t\tfor_each_memblock(memory, r) {\n\t\t\t\tif (*pfn < memblock_region_memory_end_pfn(r))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (*pfn >= memblock_region_memory_base_pfn(r) &&\n\t\t    memblock_is_mirror(r)) {\n\t\t\t*pfn = memblock_region_memory_end_pfn(r);\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\treturn false;\n}\n\n/*\n * Initially all pages are reserved - free ones are freed\n * up by memblock_free_all() once the early boot process is\n * done. Non-atomic initialization, single-pass.\n */\nvoid __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,\n\t\tunsigned long start_pfn, enum memmap_context context,\n\t\tstruct vmem_altmap *altmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct page *page;\n\n\tif (highest_memmap_pfn < end_pfn - 1)\n\t\thighest_memmap_pfn = end_pfn - 1;\n\n#ifdef CONFIG_ZONE_DEVICE\n\t/*\n\t * Honor reservation requested by the driver for this ZONE_DEVICE\n\t * memory. We limit the total number of pages to initialize to just\n\t * those that might contain the memory mapping. We will defer the\n\t * ZONE_DEVICE page initialization until after we have released\n\t * the hotplug lock.\n\t */\n\tif (zone == ZONE_DEVICE) {\n\t\tif (!altmap)\n\t\t\treturn;\n\n\t\tif (start_pfn == altmap->base_pfn)\n\t\t\tstart_pfn += altmap->reserve;\n\t\tend_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t}\n#endif\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\t/*\n\t\t * There can be holes in boot-time mem_map[]s handed to this\n\t\t * function.  They do not exist on hotplugged memory.\n\t\t */\n\t\tif (context == MEMMAP_EARLY) {\n\t\t\tif (!early_pfn_valid(pfn))\n\t\t\t\tcontinue;\n\t\t\tif (!early_pfn_in_nid(pfn, nid))\n\t\t\t\tcontinue;\n\t\t\tif (overlap_memmap_init(zone, &pfn))\n\t\t\t\tcontinue;\n\t\t\tif (defer_init(nid, pfn, end_pfn))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tpage = pfn_to_page(pfn);\n\t\t__init_single_page(page, pfn, zone, nid);\n\t\tif (context == MEMMAP_HOTPLUG)\n\t\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nvoid __ref memmap_init_zone_device(struct zone *zone,\n\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t   unsigned long size,\n\t\t\t\t   struct dev_pagemap *pgmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tunsigned long zone_idx = zone_idx(zone);\n\tunsigned long start = jiffies;\n\tint nid = pgdat->node_id;\n\n\tif (WARN_ON_ONCE(!pgmap || !is_dev_zone(zone)))\n\t\treturn;\n\n\t/*\n\t * The call to memmap_init_zone should have already taken care\n\t * of the pages reserved for the memmap, so we can just jump to\n\t * the end of that region and start processing the device pages.\n\t */\n\tif (pgmap->altmap_valid) {\n\t\tstruct vmem_altmap *altmap = &pgmap->altmap;\n\n\t\tstart_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t\tsize = end_pfn - start_pfn;\n\t}\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t__init_single_page(page, pfn, zone_idx, nid);\n\n\t\t/*\n\t\t * Mark page reserved as it will need to wait for onlining\n\t\t * phase for it to be fully associated with a zone.\n\t\t *\n\t\t * We can use the non-atomic __set_bit operation for setting\n\t\t * the flag as we are still initializing the pages.\n\t\t */\n\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * ZONE_DEVICE pages union ->lru with a ->pgmap back\n\t\t * pointer and hmm_data.  It is a bug if a ZONE_DEVICE\n\t\t * page is ever freed or placed on a driver-private list.\n\t\t */\n\t\tpage->pgmap = pgmap;\n\t\tpage->hmm_data = 0;\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t *\n\t\t * Please note that MEMMAP_HOTPLUG path doesn't clear memmap\n\t\t * because this is done early in sparse_add_one_section\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tpr_info(\"%s initialised, %lu pages in %ums\\n\", dev_name(pgmap->dev),\n\t\tsize, jiffies_to_msecs(jiffies - start));\n}\n\n#endif\nstatic void __meminit zone_init_free_lists(struct zone *zone)\n{\n\tunsigned int order, t;\n\tfor_each_migratetype_order(order, t) {\n\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);\n\t\tzone->free_area[order].nr_free = 0;\n\t}\n}\n\nvoid __meminit __weak memmap_init(unsigned long size, int nid,\n\t\t\t\t  unsigned long zone, unsigned long start_pfn)\n{\n\tmemmap_init_zone(size, nid, zone, start_pfn, MEMMAP_EARLY, NULL);\n}\n\nstatic int zone_batchsize(struct zone *zone)\n{\n#ifdef CONFIG_MMU\n\tint batch;\n\n\t/*\n\t * The per-cpu-pages pools are set to around 1000th of the\n\t * size of the zone.\n\t */\n\tbatch = zone->managed_pages / 1024;\n\t/* But no more than a meg. */\n\tif (batch * PAGE_SIZE > 1024 * 1024)\n\t\tbatch = (1024 * 1024) / PAGE_SIZE;\n\tbatch /= 4;\t\t/* We effectively *= 4 below */\n\tif (batch < 1)\n\t\tbatch = 1;\n\n\t/*\n\t * Clamp the batch to a 2^n - 1 value. Having a power\n\t * of 2 value was found to be more likely to have\n\t * suboptimal cache aliasing properties in some cases.\n\t *\n\t * For example if 2 tasks are alternately allocating\n\t * batches of pages, one task can end up with a lot\n\t * of pages of one half of the possible page colors\n\t * and the other with pages of the other colors.\n\t */\n\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;\n\n\treturn batch;\n\n#else\n\t/* The deferral and batching of frees should be suppressed under NOMMU\n\t * conditions.\n\t *\n\t * The problem is that NOMMU needs to be able to allocate large chunks\n\t * of contiguous memory as there's no hardware page translation to\n\t * assemble apparent contiguous memory from discontiguous pages.\n\t *\n\t * Queueing large contiguous runs of pages for batching, however,\n\t * causes the pages to actually be freed in smaller chunks.  As there\n\t * can be a significant delay between the individual batches being\n\t * recycled, this leads to the once large chunks of space being\n\t * fragmented and becoming unavailable for high-order allocations.\n\t */\n\treturn 0;\n#endif\n}\n\n/*\n * pcp->high and pcp->batch values are related and dependent on one another:\n * ->batch must never be higher then ->high.\n * The following function updates them in a safe manner without read side\n * locking.\n *\n * Any new users of pcp->batch and pcp->high should ensure they can cope with\n * those fields changing asynchronously (acording the the above rule).\n *\n * mutex_is_locked(&pcp_batch_high_lock) required when calling this function\n * outside of boot time (or some other assurance that no concurrent updaters\n * exist).\n */\nstatic void pageset_update(struct per_cpu_pages *pcp, unsigned long high,\n\t\tunsigned long batch)\n{\n       /* start with a fail safe value for batch */\n\tpcp->batch = 1;\n\tsmp_wmb();\n\n       /* Update high, then batch, in order */\n\tpcp->high = high;\n\tsmp_wmb();\n\n\tpcp->batch = batch;\n}\n\n/* a companion to pageset_set_high() */\nstatic void pageset_set_batch(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_update(&p->pcp, 6 * batch, max(1UL, 1 * batch));\n}\n\nstatic void pageset_init(struct per_cpu_pageset *p)\n{\n\tstruct per_cpu_pages *pcp;\n\tint migratetype;\n\n\tmemset(p, 0, sizeof(*p));\n\n\tpcp = &p->pcp;\n\tpcp->count = 0;\n\tfor (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)\n\t\tINIT_LIST_HEAD(&pcp->lists[migratetype]);\n}\n\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_init(p);\n\tpageset_set_batch(p, batch);\n}\n\n/*\n * pageset_set_high() sets the high water mark for hot per_cpu_pagelist\n * to the value high for the pageset p.\n */\nstatic void pageset_set_high(struct per_cpu_pageset *p,\n\t\t\t\tunsigned long high)\n{\n\tunsigned long batch = max(1UL, high / 4);\n\tif ((high / 4) > (PAGE_SHIFT * 8))\n\t\tbatch = PAGE_SHIFT * 8;\n\n\tpageset_update(&p->pcp, high, batch);\n}\n\nstatic void pageset_set_high_and_batch(struct zone *zone,\n\t\t\t\t       struct per_cpu_pageset *pcp)\n{\n\tif (percpu_pagelist_fraction)\n\t\tpageset_set_high(pcp,\n\t\t\t(zone->managed_pages /\n\t\t\t\tpercpu_pagelist_fraction));\n\telse\n\t\tpageset_set_batch(pcp, zone_batchsize(zone));\n}\n\nstatic void __meminit zone_pageset_init(struct zone *zone, int cpu)\n{\n\tstruct per_cpu_pageset *pcp = per_cpu_ptr(zone->pageset, cpu);\n\n\tpageset_init(pcp);\n\tpageset_set_high_and_batch(zone, pcp);\n}\n\nvoid __meminit setup_zone_pageset(struct zone *zone)\n{\n\tint cpu;\n\tzone->pageset = alloc_percpu(struct per_cpu_pageset);\n\tfor_each_possible_cpu(cpu)\n\t\tzone_pageset_init(zone, cpu);\n}\n\n/*\n * Allocate per cpu pagesets and initialize them.\n * Before this call only boot pagesets were available.\n */\nvoid __init setup_per_cpu_pageset(void)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone)\n\t\tsetup_zone_pageset(zone);\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->per_cpu_nodestats =\n\t\t\talloc_percpu(struct per_cpu_nodestat);\n}\n\nstatic __meminit void zone_pcp_init(struct zone *zone)\n{\n\t/*\n\t * per cpu subsystem is not up at this point. The following code\n\t * relies on the ability of the linker to provide the\n\t * offset of a (static) per cpu variable into the per cpu area.\n\t */\n\tzone->pageset = &boot_pageset;\n\n\tif (populated_zone(zone))\n\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages, LIFO batch:%u\\n\",\n\t\t\tzone->name, zone->present_pages,\n\t\t\t\t\t zone_batchsize(zone));\n}\n\nvoid __meminit init_currently_empty_zone(struct zone *zone,\n\t\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\t\tunsigned long size)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tint zone_idx = zone_idx(zone) + 1;\n\n\tif (zone_idx > pgdat->nr_zones)\n\t\tpgdat->nr_zones = zone_idx;\n\n\tzone->zone_start_pfn = zone_start_pfn;\n\n\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",\n\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",\n\t\t\tpgdat->node_id,\n\t\t\t(unsigned long)zone_idx(zone),\n\t\t\tzone_start_pfn, (zone_start_pfn + size));\n\n\tzone_init_free_lists(zone);\n\tzone->initialized = 1;\n}\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n#ifndef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID\n\n/*\n * Required by SPARSEMEM. Given a PFN, return what node the PFN is on.\n */\nint __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint nid;\n\n\tif (state->last_start <= pfn && pfn < state->last_end)\n\t\treturn state->last_nid;\n\n\tnid = memblock_search_pfn_nid(pfn, &start_pfn, &end_pfn);\n\tif (nid != -1) {\n\t\tstate->last_start = start_pfn;\n\t\tstate->last_end = end_pfn;\n\t\tstate->last_nid = nid;\n\t}\n\n\treturn nid;\n}\n#endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */\n\n/**\n * free_bootmem_with_active_regions - Call memblock_free_early_nid for each active range\n * @nid: The node to free memory on. If MAX_NUMNODES, all nodes are freed.\n * @max_low_pfn: The highest PFN that will be passed to memblock_free_early_nid\n *\n * If an architecture guarantees that all ranges registered contain no holes\n * and may be freed, this this function may be used instead of calling\n * memblock_free_early_nid() manually.\n */\nvoid __init free_bootmem_with_active_regions(int nid, unsigned long max_low_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid) {\n\t\tstart_pfn = min(start_pfn, max_low_pfn);\n\t\tend_pfn = min(end_pfn, max_low_pfn);\n\n\t\tif (start_pfn < end_pfn)\n\t\t\tmemblock_free_early_nid(PFN_PHYS(start_pfn),\n\t\t\t\t\t(end_pfn - start_pfn) << PAGE_SHIFT,\n\t\t\t\t\tthis_nid);\n\t}\n}\n\n/**\n * sparse_memory_present_with_active_regions - Call memory_present for each active range\n * @nid: The node to call memory_present for. If MAX_NUMNODES, all nodes will be used.\n *\n * If an architecture guarantees that all ranges registered contain no holes and may\n * be freed, this function may be used instead of calling memory_present() manually.\n */\nvoid __init sparse_memory_present_with_active_regions(int nid)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid)\n\t\tmemory_present(this_nid, start_pfn, end_pfn);\n}\n\n/**\n * get_pfn_range_for_nid - Return the start and end page frames for a node\n * @nid: The nid to return the range for. If MAX_NUMNODES, the min and max PFN are returned.\n * @start_pfn: Passed by reference. On return, it will have the node start_pfn.\n * @end_pfn: Passed by reference. On return, it will have the node end_pfn.\n *\n * It returns the start and end page frame of a node based on information\n * provided by memblock_set_node(). If called for a node\n * with no available memory, a warning is printed and the start and end\n * PFNs will be 0.\n */\nvoid __meminit get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn)\n{\n\tunsigned long this_start_pfn, this_end_pfn;\n\tint i;\n\n\t*start_pfn = -1UL;\n\t*end_pfn = 0;\n\n\tfor_each_mem_pfn_range(i, nid, &this_start_pfn, &this_end_pfn, NULL) {\n\t\t*start_pfn = min(*start_pfn, this_start_pfn);\n\t\t*end_pfn = max(*end_pfn, this_end_pfn);\n\t}\n\n\tif (*start_pfn == -1UL)\n\t\t*start_pfn = 0;\n}\n\n/*\n * This finds a zone that can be used for ZONE_MOVABLE pages. The\n * assumption is made that zones within a node are ordered in monotonic\n * increasing memory addresses so that the \"highest\" populated zone is used\n */\nstatic void __init find_usable_zone_for_movable(void)\n{\n\tint zone_index;\n\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {\n\t\tif (zone_index == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tif (arch_zone_highest_possible_pfn[zone_index] >\n\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])\n\t\t\tbreak;\n\t}\n\n\tVM_BUG_ON(zone_index == -1);\n\tmovable_zone = zone_index;\n}\n\n/*\n * The zone ranges provided by the architecture do not include ZONE_MOVABLE\n * because it is sized independent of architecture. Unlike the other zones,\n * the starting point for ZONE_MOVABLE is not fixed. It may be different\n * in each node depending on the size of each node and how evenly kernelcore\n * is distributed. This helper function adjusts the zone ranges\n * provided by the architecture for a given node by using the end of the\n * highest usable zone for ZONE_MOVABLE. This preserves the assumption that\n * zones within a node are in order of monotonic increases memory addresses\n */\nstatic void __meminit adjust_zone_range_for_zone_movable(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\t/* Only adjust if ZONE_MOVABLE is on this node */\n\tif (zone_movable_pfn[nid]) {\n\t\t/* Size ZONE_MOVABLE */\n\t\tif (zone_type == ZONE_MOVABLE) {\n\t\t\t*zone_start_pfn = zone_movable_pfn[nid];\n\t\t\t*zone_end_pfn = min(node_end_pfn,\n\t\t\t\tarch_zone_highest_possible_pfn[movable_zone]);\n\n\t\t/* Adjust for ZONE_MOVABLE starting within this range */\n\t\t} else if (!mirrored_kernelcore &&\n\t\t\t*zone_start_pfn < zone_movable_pfn[nid] &&\n\t\t\t*zone_end_pfn > zone_movable_pfn[nid]) {\n\t\t\t*zone_end_pfn = zone_movable_pfn[nid];\n\n\t\t/* Check if this whole range is within ZONE_MOVABLE */\n\t\t} else if (*zone_start_pfn >= zone_movable_pfn[nid])\n\t\t\t*zone_start_pfn = *zone_end_pfn;\n\t}\n}\n\n/*\n * Return the number of pages a zone spans in a node, including holes\n * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()\n */\nstatic unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\t/* Get the start and end of the zone */\n\t*zone_start_pfn = arch_zone_lowest_possible_pfn[zone_type];\n\t*zone_end_pfn = arch_zone_highest_possible_pfn[zone_type];\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t/* Check that this node has pages within the zone's required range */\n\tif (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)\n\t\treturn 0;\n\n\t/* Move the zone boundaries inside the node if necessary */\n\t*zone_end_pfn = min(*zone_end_pfn, node_end_pfn);\n\t*zone_start_pfn = max(*zone_start_pfn, node_start_pfn);\n\n\t/* Return the spanned pages */\n\treturn *zone_end_pfn - *zone_start_pfn;\n}\n\n/*\n * Return the number of holes in a range on a node. If nid is MAX_NUMNODES,\n * then all holes in the requested range will be accounted for.\n */\nunsigned long __meminit __absent_pages_in_range(int nid,\n\t\t\t\tunsigned long range_start_pfn,\n\t\t\t\tunsigned long range_end_pfn)\n{\n\tunsigned long nr_absent = range_end_pfn - range_start_pfn;\n\tunsigned long start_pfn, end_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);\n\t\tend_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);\n\t\tnr_absent -= end_pfn - start_pfn;\n\t}\n\treturn nr_absent;\n}\n\n/**\n * absent_pages_in_range - Return number of page frames in holes within a range\n * @start_pfn: The start PFN to start searching for holes\n * @end_pfn: The end PFN to stop searching for holes\n *\n * It returns the number of pages frames in memory holes within a range.\n */\nunsigned long __init absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\treturn __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);\n}\n\n/* Return the number of page frames in holes in a zone on a node */\nstatic unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\tunsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type];\n\tunsigned long zone_high = arch_zone_highest_possible_pfn[zone_type];\n\tunsigned long zone_start_pfn, zone_end_pfn;\n\tunsigned long nr_absent;\n\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\tzone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);\n\tzone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);\n\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t&zone_start_pfn, &zone_end_pfn);\n\tnr_absent = __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn);\n\n\t/*\n\t * ZONE_MOVABLE handling.\n\t * Treat pages to be ZONE_MOVABLE in ZONE_NORMAL as absent pages\n\t * and vice versa.\n\t */\n\tif (mirrored_kernelcore && zone_movable_pfn[nid]) {\n\t\tunsigned long start_pfn, end_pfn;\n\t\tstruct memblock_region *r;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tstart_pfn = clamp(memblock_region_memory_base_pfn(r),\n\t\t\t\t\t  zone_start_pfn, zone_end_pfn);\n\t\t\tend_pfn = clamp(memblock_region_memory_end_pfn(r),\n\t\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t\t\tif (zone_type == ZONE_MOVABLE &&\n\t\t\t    memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\n\t\t\tif (zone_type == ZONE_NORMAL &&\n\t\t\t    !memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\t\t}\n\t}\n\n\treturn nr_absent;\n}\n\n#else /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\nstatic inline unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *zones_size)\n{\n\tunsigned int zone;\n\n\t*zone_start_pfn = node_start_pfn;\n\tfor (zone = 0; zone < zone_type; zone++)\n\t\t*zone_start_pfn += zones_size[zone];\n\n\t*zone_end_pfn = *zone_start_pfn + zones_size[zone_type];\n\n\treturn zones_size[zone_type];\n}\n\nstatic inline unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tif (!zholes_size)\n\t\treturn 0;\n\n\treturn zholes_size[zone_type];\n}\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nstatic void __meminit calculate_node_totalpages(struct pglist_data *pgdat,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zones_size,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tunsigned long realtotalpages = 0, totalpages = 0;\n\tenum zone_type i;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\tunsigned long zone_start_pfn, zone_end_pfn;\n\t\tunsigned long size, real_size;\n\n\t\tsize = zone_spanned_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn,\n\t\t\t\t\t\t  node_end_pfn,\n\t\t\t\t\t\t  &zone_start_pfn,\n\t\t\t\t\t\t  &zone_end_pfn,\n\t\t\t\t\t\t  zones_size);\n\t\treal_size = size - zone_absent_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn, node_end_pfn,\n\t\t\t\t\t\t  zholes_size);\n\t\tif (size)\n\t\t\tzone->zone_start_pfn = zone_start_pfn;\n\t\telse\n\t\t\tzone->zone_start_pfn = 0;\n\t\tzone->spanned_pages = size;\n\t\tzone->present_pages = real_size;\n\n\t\ttotalpages += size;\n\t\trealtotalpages += real_size;\n\t}\n\n\tpgdat->node_spanned_pages = totalpages;\n\tpgdat->node_present_pages = realtotalpages;\n\tprintk(KERN_DEBUG \"On node %d totalpages: %lu\\n\", pgdat->node_id,\n\t\t\t\t\t\t\trealtotalpages);\n}\n\n#ifndef CONFIG_SPARSEMEM\n/*\n * Calculate the size of the zone->blockflags rounded to an unsigned long\n * Start by making sure zonesize is a multiple of pageblock_order by rounding\n * up. Then use 1 NR_PAGEBLOCK_BITS worth of bits per pageblock, finally\n * round what is now in bits to nearest long in bits, then return it in\n * bytes.\n */\nstatic unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)\n{\n\tunsigned long usemapsize;\n\n\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);\n\tusemapsize = roundup(zonesize, pageblock_nr_pages);\n\tusemapsize = usemapsize >> pageblock_order;\n\tusemapsize *= NR_PAGEBLOCK_BITS;\n\tusemapsize = roundup(usemapsize, 8 * sizeof(unsigned long));\n\n\treturn usemapsize / 8;\n}\n\nstatic void __ref setup_usemap(struct pglist_data *pgdat,\n\t\t\t\tstruct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\tunsigned long zonesize)\n{\n\tunsigned long usemapsize = usemap_size(zone_start_pfn, zonesize);\n\tzone->pageblock_flags = NULL;\n\tif (usemapsize)\n\t\tzone->pageblock_flags =\n\t\t\tmemblock_alloc_node_nopanic(usemapsize,\n\t\t\t\t\t\t\t pgdat->node_id);\n}\n#else\nstatic inline void setup_usemap(struct pglist_data *pgdat, struct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn, unsigned long zonesize) {}\n#endif /* CONFIG_SPARSEMEM */\n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\n\n/* Initialise the number of pages represented by NR_PAGEBLOCK_BITS */\nvoid __init set_pageblock_order(void)\n{\n\tunsigned int order;\n\n\t/* Check that pageblock_nr_pages has not already been setup */\n\tif (pageblock_order)\n\t\treturn;\n\n\tif (HPAGE_SHIFT > PAGE_SHIFT)\n\t\torder = HUGETLB_PAGE_ORDER;\n\telse\n\t\torder = MAX_ORDER - 1;\n\n\t/*\n\t * Assume the largest contiguous order of interest is a huge page.\n\t * This value may be variable depending on boot parameters on IA64 and\n\t * powerpc.\n\t */\n\tpageblock_order = order;\n}\n#else /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\n/*\n * When CONFIG_HUGETLB_PAGE_SIZE_VARIABLE is not set, set_pageblock_order()\n * is unused as pageblock_order is set at compile-time. See\n * include/linux/pageblock-flags.h for the values of pageblock_order based on\n * the kernel config\n */\nvoid __init set_pageblock_order(void)\n{\n}\n\n#endif /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\nstatic unsigned long __init calc_memmap_size(unsigned long spanned_pages,\n\t\t\t\t\t\tunsigned long present_pages)\n{\n\tunsigned long pages = spanned_pages;\n\n\t/*\n\t * Provide a more accurate estimation if there are holes within\n\t * the zone and SPARSEMEM is in use. If there are holes within the\n\t * zone, each populated memory region may cost us one or two extra\n\t * memmap pages due to alignment because memmap pages for each\n\t * populated regions may not be naturally aligned on page boundary.\n\t * So the (present_pages >> 4) heuristic is a tradeoff for that.\n\t */\n\tif (spanned_pages > present_pages + (present_pages >> 4) &&\n\t    IS_ENABLED(CONFIG_SPARSEMEM))\n\t\tpages = present_pages;\n\n\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat)\n{\n\tspin_lock_init(&pgdat->split_queue_lock);\n\tINIT_LIST_HEAD(&pgdat->split_queue);\n\tpgdat->split_queue_len = 0;\n}\n#else\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat) {}\n#endif\n\n#ifdef CONFIG_COMPACTION\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat)\n{\n\tinit_waitqueue_head(&pgdat->kcompactd_wait);\n}\n#else\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n#endif\n\nstatic void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n{\n\tpgdat_resize_init(pgdat);\n\n\tpgdat_init_split_queue(pgdat);\n\tpgdat_init_kcompactd(pgdat);\n\n\tinit_waitqueue_head(&pgdat->kswapd_wait);\n\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\n\n\tpgdat_page_ext_init(pgdat);\n\tspin_lock_init(&pgdat->lru_lock);\n\tlruvec_init(node_lruvec(pgdat));\n}\n\nstatic void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,\n\t\t\t\t\t\t\tunsigned long remaining_pages)\n{\n\tzone->managed_pages = remaining_pages;\n\tzone_set_nid(zone, nid);\n\tzone->name = zone_names[idx];\n\tzone->zone_pgdat = NODE_DATA(nid);\n\tspin_lock_init(&zone->lock);\n\tzone_seqlock_init(zone);\n\tzone_pcp_init(zone);\n}\n\n/*\n * Set up the zone data structures\n * - init pgdat internals\n * - init all zones belonging to this node\n *\n * NOTE: this function is only called during memory hotplug\n */\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid __ref free_area_init_core_hotplug(int nid)\n{\n\tenum zone_type z;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tpgdat_init_internals(pgdat);\n\tfor (z = 0; z < MAX_NR_ZONES; z++)\n\t\tzone_init_internals(&pgdat->node_zones[z], z, nid, 0);\n}\n#endif\n\n/*\n * Set up the zone data structures:\n *   - mark all pages reserved\n *   - mark all memory queues empty\n *   - clear the memory bitmaps\n *\n * NOTE: pgdat should get zeroed by caller.\n * NOTE: this function is only called during early init.\n */\nstatic void __init free_area_init_core(struct pglist_data *pgdat)\n{\n\tenum zone_type j;\n\tint nid = pgdat->node_id;\n\n\tpgdat_init_internals(pgdat);\n\tpgdat->per_cpu_nodestats = &boot_nodestats;\n\n\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\tunsigned long size, freesize, memmap_pages;\n\t\tunsigned long zone_start_pfn = zone->zone_start_pfn;\n\n\t\tsize = zone->spanned_pages;\n\t\tfreesize = zone->present_pages;\n\n\t\t/*\n\t\t * Adjust freesize so that it accounts for how much memory\n\t\t * is used by this zone for memmap. This affects the watermark\n\t\t * and per-cpu initialisations\n\t\t */\n\t\tmemmap_pages = calc_memmap_size(size, freesize);\n\t\tif (!is_highmem_idx(j)) {\n\t\t\tif (freesize >= memmap_pages) {\n\t\t\t\tfreesize -= memmap_pages;\n\t\t\t\tif (memmap_pages)\n\t\t\t\t\tprintk(KERN_DEBUG\n\t\t\t\t\t       \"  %s zone: %lu pages used for memmap\\n\",\n\t\t\t\t\t       zone_names[j], memmap_pages);\n\t\t\t} else\n\t\t\t\tpr_warn(\"  %s zone: %lu pages exceeds freesize %lu\\n\",\n\t\t\t\t\tzone_names[j], memmap_pages, freesize);\n\t\t}\n\n\t\t/* Account for reserved pages */\n\t\tif (j == 0 && freesize > dma_reserve) {\n\t\t\tfreesize -= dma_reserve;\n\t\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages reserved\\n\",\n\t\t\t\t\tzone_names[0], dma_reserve);\n\t\t}\n\n\t\tif (!is_highmem_idx(j))\n\t\t\tnr_kernel_pages += freesize;\n\t\t/* Charge for highmem memmap if there are enough kernel pages */\n\t\telse if (nr_kernel_pages > memmap_pages * 2)\n\t\t\tnr_kernel_pages -= memmap_pages;\n\t\tnr_all_pages += freesize;\n\n\t\t/*\n\t\t * Set an approximate value for lowmem here, it will be adjusted\n\t\t * when the bootmem allocator frees pages into the buddy system.\n\t\t * And all highmem pages will be managed by the buddy system.\n\t\t */\n\t\tzone_init_internals(zone, j, nid, freesize);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\n\t\tset_pageblock_order();\n\t\tsetup_usemap(pgdat, zone, zone_start_pfn, size);\n\t\tinit_currently_empty_zone(zone, zone_start_pfn, size);\n\t\tmemmap_init(size, nid, j, zone_start_pfn);\n\t}\n}\n\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat)\n{\n\tunsigned long __maybe_unused start = 0;\n\tunsigned long __maybe_unused offset = 0;\n\n\t/* Skip empty nodes */\n\tif (!pgdat->node_spanned_pages)\n\t\treturn;\n\n\tstart = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);\n\toffset = pgdat->node_start_pfn - start;\n\t/* ia64 gets its own node_mem_map, before this, without bootmem */\n\tif (!pgdat->node_mem_map) {\n\t\tunsigned long size, end;\n\t\tstruct page *map;\n\n\t\t/*\n\t\t * The zone's endpoints aren't required to be MAX_ORDER\n\t\t * aligned but the node_mem_map endpoints must be in order\n\t\t * for the buddy allocator to function correctly.\n\t\t */\n\t\tend = pgdat_end_pfn(pgdat);\n\t\tend = ALIGN(end, MAX_ORDER_NR_PAGES);\n\t\tsize =  (end - start) * sizeof(struct page);\n\t\tmap = memblock_alloc_node_nopanic(size, pgdat->node_id);\n\t\tpgdat->node_mem_map = map + offset;\n\t}\n\tpr_debug(\"%s: node %d, pgdat %08lx, node_mem_map %08lx\\n\",\n\t\t\t\t__func__, pgdat->node_id, (unsigned long)pgdat,\n\t\t\t\t(unsigned long)pgdat->node_mem_map);\n#ifndef CONFIG_NEED_MULTIPLE_NODES\n\t/*\n\t * With no DISCONTIG, the global mem_map is just set as node 0's\n\t */\n\tif (pgdat == NODE_DATA(0)) {\n\t\tmem_map = NODE_DATA(0)->node_mem_map;\n#if defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) || defined(CONFIG_FLATMEM)\n\t\tif (page_to_pfn(mem_map) != pgdat->node_start_pfn)\n\t\t\tmem_map -= offset;\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\t}\n#endif\n}\n#else\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }\n#endif /* CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat)\n{\n\t/*\n\t * We start only with one section of pages, more pages are added as\n\t * needed until the rest of deferred pages are initialized.\n\t */\n\tpgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,\n\t\t\t\t\t\tpgdat->node_spanned_pages);\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n}\n#else\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}\n#endif\n\nvoid __init free_area_init_node(int nid, unsigned long *zones_size,\n\t\t\t\t   unsigned long node_start_pfn,\n\t\t\t\t   unsigned long *zholes_size)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tunsigned long start_pfn = 0;\n\tunsigned long end_pfn = 0;\n\n\t/* pg_data_t should be reset to zero when it's allocated */\n\tWARN_ON(pgdat->nr_zones || pgdat->kswapd_classzone_idx);\n\n\tpgdat->node_id = nid;\n\tpgdat->node_start_pfn = node_start_pfn;\n\tpgdat->per_cpu_nodestats = NULL;\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);\n\tpr_info(\"Initmem setup node %d [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\tend_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);\n#else\n\tstart_pfn = node_start_pfn;\n#endif\n\tcalculate_node_totalpages(pgdat, start_pfn, end_pfn,\n\t\t\t\t  zones_size, zholes_size);\n\n\talloc_node_mem_map(pgdat);\n\tpgdat_set_deferred_range(pgdat);\n\n\tfree_area_init_core(pgdat);\n}\n\n#if !defined(CONFIG_FLAT_NODE_MEM_MAP)\n/*\n * Zero all valid struct pages in range [spfn, epfn), return number of struct\n * pages zeroed\n */\nstatic u64 zero_pfn_range(unsigned long spfn, unsigned long epfn)\n{\n\tunsigned long pfn;\n\tu64 pgcnt = 0;\n\n\tfor (pfn = spfn; pfn < epfn; pfn++) {\n\t\tif (!pfn_valid(ALIGN_DOWN(pfn, pageblock_nr_pages))) {\n\t\t\tpfn = ALIGN_DOWN(pfn, pageblock_nr_pages)\n\t\t\t\t+ pageblock_nr_pages - 1;\n\t\t\tcontinue;\n\t\t}\n\t\tmm_zero_struct_page(pfn_to_page(pfn));\n\t\tpgcnt++;\n\t}\n\n\treturn pgcnt;\n}\n\n/*\n * Only struct pages that are backed by physical memory are zeroed and\n * initialized by going through __init_single_page(). But, there are some\n * struct pages which are reserved in memblock allocator and their fields\n * may be accessed (for example page_to_pfn() on some configuration accesses\n * flags). We must explicitly zero those struct pages.\n *\n * This function also addresses a similar issue where struct pages are left\n * uninitialized because the physical address range is not covered by\n * memblock.memory or memblock.reserved. That could happen when memblock\n * layout is manually configured via memmap=.\n */\nvoid __init zero_resv_unavail(void)\n{\n\tphys_addr_t start, end;\n\tu64 i, pgcnt;\n\tphys_addr_t next = 0;\n\n\t/*\n\t * Loop through unavailable ranges not covered by memblock.memory.\n\t */\n\tpgcnt = 0;\n\tfor_each_mem_range(i, &memblock.memory, NULL,\n\t\t\tNUMA_NO_NODE, MEMBLOCK_NONE, &start, &end, NULL) {\n\t\tif (next < start)\n\t\t\tpgcnt += zero_pfn_range(PFN_DOWN(next), PFN_UP(start));\n\t\tnext = end;\n\t}\n\tpgcnt += zero_pfn_range(PFN_DOWN(next), max_pfn);\n\n\t/*\n\t * Struct pages that do not have backing memory. This could be because\n\t * firmware is using some of this memory, or for some other reasons.\n\t */\n\tif (pgcnt)\n\t\tpr_info(\"Zeroed struct page in unavailable ranges: %lld pages\", pgcnt);\n}\n#endif /* !CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\n#if MAX_NUMNODES > 1\n/*\n * Figure out the number of possible node ids.\n */\nvoid __init setup_nr_node_ids(void)\n{\n\tunsigned int highest;\n\n\thighest = find_last_bit(node_possible_map.bits, MAX_NUMNODES);\n\tnr_node_ids = highest + 1;\n}\n#endif\n\n/**\n * node_map_pfn_alignment - determine the maximum internode alignment\n *\n * This function should be called after node map is populated and sorted.\n * It calculates the maximum power of two alignment which can distinguish\n * all the nodes.\n *\n * For example, if all nodes are 1GiB and aligned to 1GiB, the return value\n * would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the\n * nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is\n * shifted, 1GiB is enough and this function will indicate so.\n *\n * This is used to test whether pfn -> nid mapping of the chosen memory\n * model has fine enough granularity to avoid incorrect mapping for the\n * populated node map.\n *\n * Returns the determined alignment in pfn's.  0 if there is no alignment\n * requirement (single node).\n */\nunsigned long __init node_map_pfn_alignment(void)\n{\n\tunsigned long accl_mask = 0, last_end = 0;\n\tunsigned long start, end, mask;\n\tint last_nid = -1;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid) {\n\t\tif (!start || last_nid < 0 || last_nid == nid) {\n\t\t\tlast_nid = nid;\n\t\t\tlast_end = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Start with a mask granular enough to pin-point to the\n\t\t * start pfn and tick off bits one-by-one until it becomes\n\t\t * too coarse to separate the current node from the last.\n\t\t */\n\t\tmask = ~((1 << __ffs(start)) - 1);\n\t\twhile (mask && last_end <= (start & (mask << 1)))\n\t\t\tmask <<= 1;\n\n\t\t/* accumulate all internode masks */\n\t\taccl_mask |= mask;\n\t}\n\n\t/* convert mask to number of pages */\n\treturn ~accl_mask + 1;\n}\n\n/* Find the lowest pfn for a node */\nstatic unsigned long __init find_min_pfn_for_node(int nid)\n{\n\tunsigned long min_pfn = ULONG_MAX;\n\tunsigned long start_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, NULL, NULL)\n\t\tmin_pfn = min(min_pfn, start_pfn);\n\n\tif (min_pfn == ULONG_MAX) {\n\t\tpr_warn(\"Could not find start_pfn for node %d\\n\", nid);\n\t\treturn 0;\n\t}\n\n\treturn min_pfn;\n}\n\n/**\n * find_min_pfn_with_active_regions - Find the minimum PFN registered\n *\n * It returns the minimum PFN based on information provided via\n * memblock_set_node().\n */\nunsigned long __init find_min_pfn_with_active_regions(void)\n{\n\treturn find_min_pfn_for_node(MAX_NUMNODES);\n}\n\n/*\n * early_calculate_totalpages()\n * Sum pages in active regions for movable zone.\n * Populate N_MEMORY for calculating usable_nodes.\n */\nstatic unsigned long __init early_calculate_totalpages(void)\n{\n\tunsigned long totalpages = 0;\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tunsigned long pages = end_pfn - start_pfn;\n\n\t\ttotalpages += pages;\n\t\tif (pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t}\n\treturn totalpages;\n}\n\n/*\n * Find the PFN the Movable zone begins in each node. Kernel memory\n * is spread evenly between nodes as long as the nodes have enough\n * memory. When they don't, some nodes will have more kernelcore than\n * others\n */\nstatic void __init find_zone_movable_pfns_for_nodes(void)\n{\n\tint i, nid;\n\tunsigned long usable_startpfn;\n\tunsigned long kernelcore_node, kernelcore_remaining;\n\t/* save the state before borrow the nodemask */\n\tnodemask_t saved_node_state = node_states[N_MEMORY];\n\tunsigned long totalpages = early_calculate_totalpages();\n\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);\n\tstruct memblock_region *r;\n\n\t/* Need to find movable_zone earlier when movable_node is specified. */\n\tfind_usable_zone_for_movable();\n\n\t/*\n\t * If movable_node is specified, ignore kernelcore and movablecore\n\t * options.\n\t */\n\tif (movable_node_is_enabled()) {\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (!memblock_is_hotpluggable(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = PFN_DOWN(r->base);\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=mirror is specified, ignore movablecore option\n\t */\n\tif (mirrored_kernelcore) {\n\t\tbool mem_below_4gb_not_mirrored = false;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (memblock_is_mirror(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);\n\n\t\t\tif (usable_startpfn < 0x100000) {\n\t\t\t\tmem_below_4gb_not_mirrored = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tif (mem_below_4gb_not_mirrored)\n\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\");\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=nn% or movablecore=nn% was specified, calculate the\n\t * amount of necessary memory.\n\t */\n\tif (required_kernelcore_percent)\n\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /\n\t\t\t\t       10000UL;\n\tif (required_movablecore_percent)\n\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /\n\t\t\t\t\t10000UL;\n\n\t/*\n\t * If movablecore= was specified, calculate what size of\n\t * kernelcore that corresponds so that memory usable for\n\t * any allocation type is evenly spread. If both kernelcore\n\t * and movablecore are specified, then the value of kernelcore\n\t * will be used for required_kernelcore if it's greater than\n\t * what movablecore would have allowed.\n\t */\n\tif (required_movablecore) {\n\t\tunsigned long corepages;\n\n\t\t/*\n\t\t * Round-up so that ZONE_MOVABLE is at least as large as what\n\t\t * was requested by the user\n\t\t */\n\t\trequired_movablecore =\n\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);\n\t\trequired_movablecore = min(totalpages, required_movablecore);\n\t\tcorepages = totalpages - required_movablecore;\n\n\t\trequired_kernelcore = max(required_kernelcore, corepages);\n\t}\n\n\t/*\n\t * If kernelcore was not specified or kernelcore size is larger\n\t * than totalpages, there is no ZONE_MOVABLE.\n\t */\n\tif (!required_kernelcore || required_kernelcore >= totalpages)\n\t\tgoto out;\n\n\t/* usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */\n\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];\n\nrestart:\n\t/* Spread kernelcore memory as evenly as possible throughout nodes */\n\tkernelcore_node = required_kernelcore / usable_nodes;\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\t/*\n\t\t * Recalculate kernelcore_node if the division per node\n\t\t * now exceeds what is necessary to satisfy the requested\n\t\t * amount of memory for the kernel\n\t\t */\n\t\tif (required_kernelcore < kernelcore_node)\n\t\t\tkernelcore_node = required_kernelcore / usable_nodes;\n\n\t\t/*\n\t\t * As the map is walked, we track how much memory is usable\n\t\t * by the kernel using kernelcore_remaining. When it is\n\t\t * 0, the rest of the node is usable by ZONE_MOVABLE\n\t\t */\n\t\tkernelcore_remaining = kernelcore_node;\n\n\t\t/* Go through each range of PFNs within this node */\n\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\t\tunsigned long size_pages;\n\n\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);\n\t\t\tif (start_pfn >= end_pfn)\n\t\t\t\tcontinue;\n\n\t\t\t/* Account for what is only usable for kernelcore */\n\t\t\tif (start_pfn < usable_startpfn) {\n\t\t\t\tunsigned long kernel_pages;\n\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)\n\t\t\t\t\t\t\t\t- start_pfn;\n\n\t\t\t\tkernelcore_remaining -= min(kernel_pages,\n\t\t\t\t\t\t\tkernelcore_remaining);\n\t\t\t\trequired_kernelcore -= min(kernel_pages,\n\t\t\t\t\t\t\trequired_kernelcore);\n\n\t\t\t\t/* Continue if range is now fully accounted */\n\t\t\t\tif (end_pfn <= usable_startpfn) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Push zone_movable_pfn to the end so\n\t\t\t\t\t * that if we have to rebalance\n\t\t\t\t\t * kernelcore across nodes, we will\n\t\t\t\t\t * not double account here\n\t\t\t\t\t */\n\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstart_pfn = usable_startpfn;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * The usable PFN range for ZONE_MOVABLE is from\n\t\t\t * start_pfn->end_pfn. Calculate size_pages as the\n\t\t\t * number of pages used as kernelcore\n\t\t\t */\n\t\t\tsize_pages = end_pfn - start_pfn;\n\t\t\tif (size_pages > kernelcore_remaining)\n\t\t\t\tsize_pages = kernelcore_remaining;\n\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;\n\n\t\t\t/*\n\t\t\t * Some kernelcore has been met, update counts and\n\t\t\t * break if the kernelcore for this node has been\n\t\t\t * satisfied\n\t\t\t */\n\t\t\trequired_kernelcore -= min(required_kernelcore,\n\t\t\t\t\t\t\t\tsize_pages);\n\t\t\tkernelcore_remaining -= size_pages;\n\t\t\tif (!kernelcore_remaining)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * If there is still required_kernelcore, we do another pass with one\n\t * less node in the count. This will push zone_movable_pfn[nid] further\n\t * along on the nodes that still have memory until kernelcore is\n\t * satisfied\n\t */\n\tusable_nodes--;\n\tif (usable_nodes && required_kernelcore > usable_nodes)\n\t\tgoto restart;\n\nout2:\n\t/* Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */\n\tfor (nid = 0; nid < MAX_NUMNODES; nid++)\n\t\tzone_movable_pfn[nid] =\n\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);\n\nout:\n\t/* restore the node_state */\n\tnode_states[N_MEMORY] = saved_node_state;\n}\n\n/* Any regular or high memory on that node ? */\nstatic void check_for_memory(pg_data_t *pgdat, int nid)\n{\n\tenum zone_type zone_type;\n\n\tfor (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\t\tif (populated_zone(zone)) {\n\t\t\tif (IS_ENABLED(CONFIG_HIGHMEM))\n\t\t\t\tnode_set_state(nid, N_HIGH_MEMORY);\n\t\t\tif (zone_type <= ZONE_NORMAL)\n\t\t\t\tnode_set_state(nid, N_NORMAL_MEMORY);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/**\n * free_area_init_nodes - Initialise all pg_data_t and zone data\n * @max_zone_pfn: an array of max PFNs for each zone\n *\n * This will call free_area_init_node() for each active node in the system.\n * Using the page ranges provided by memblock_set_node(), the size of each\n * zone in each node and their holes is calculated. If the maximum PFN\n * between two adjacent zones match, it is assumed that the zone is empty.\n * For example, if arch_max_dma_pfn == arch_max_dma32_pfn, it is assumed\n * that arch_max_dma32_pfn has no pages. It is also assumed that a zone\n * starts where the previous one ended. For example, ZONE_DMA32 starts\n * at arch_max_dma_pfn.\n */\nvoid __init free_area_init_nodes(unsigned long *max_zone_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\t/* Record where the zone boundaries are */\n\tmemset(arch_zone_lowest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_lowest_possible_pfn));\n\tmemset(arch_zone_highest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_highest_possible_pfn));\n\n\tstart_pfn = find_min_pfn_with_active_regions();\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tend_pfn = max(max_zone_pfn[i], start_pfn);\n\t\tarch_zone_lowest_possible_pfn[i] = start_pfn;\n\t\tarch_zone_highest_possible_pfn[i] = end_pfn;\n\n\t\tstart_pfn = end_pfn;\n\t}\n\n\t/* Find the PFNs that ZONE_MOVABLE begins at in each node */\n\tmemset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));\n\tfind_zone_movable_pfns_for_nodes();\n\n\t/* Print out the zone ranges */\n\tpr_info(\"Zone ranges:\\n\");\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\t\tpr_info(\"  %-8s \", zone_names[i]);\n\t\tif (arch_zone_lowest_possible_pfn[i] ==\n\t\t\t\tarch_zone_highest_possible_pfn[i])\n\t\t\tpr_cont(\"empty\\n\");\n\t\telse\n\t\t\tpr_cont(\"[mem %#018Lx-%#018Lx]\\n\",\n\t\t\t\t(u64)arch_zone_lowest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT,\n\t\t\t\t((u64)arch_zone_highest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT) - 1);\n\t}\n\n\t/* Print out the PFNs ZONE_MOVABLE begins at in each node */\n\tpr_info(\"Movable zone start for each node\\n\");\n\tfor (i = 0; i < MAX_NUMNODES; i++) {\n\t\tif (zone_movable_pfn[i])\n\t\t\tpr_info(\"  Node %d: %#018Lx\\n\", i,\n\t\t\t       (u64)zone_movable_pfn[i] << PAGE_SHIFT);\n\t}\n\n\t/* Print out the early node map */\n\tpr_info(\"Early memory node ranges\\n\");\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid)\n\t\tpr_info(\"  node %3d: [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\t\t((u64)end_pfn << PAGE_SHIFT) - 1);\n\n\t/* Initialise every node */\n\tmminit_verify_pageflags_layout();\n\tsetup_nr_node_ids();\n\tzero_resv_unavail();\n\tfor_each_online_node(nid) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tfree_area_init_node(nid, NULL,\n\t\t\t\tfind_min_pfn_for_node(nid), NULL);\n\n\t\t/* Any memory on that node */\n\t\tif (pgdat->node_present_pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t\tcheck_for_memory(pgdat, nid);\n\t}\n}\n\nstatic int __init cmdline_parse_core(char *p, unsigned long *core,\n\t\t\t\t     unsigned long *percent)\n{\n\tunsigned long long coremem;\n\tchar *endptr;\n\n\tif (!p)\n\t\treturn -EINVAL;\n\n\t/* Value may be a percentage of total memory, otherwise bytes */\n\tcoremem = simple_strtoull(p, &endptr, 0);\n\tif (*endptr == '%') {\n\t\t/* Paranoid check for percent values greater than 100 */\n\t\tWARN_ON(coremem > 100);\n\n\t\t*percent = coremem;\n\t} else {\n\t\tcoremem = memparse(p, &p);\n\t\t/* Paranoid check that UL is enough for the coremem value */\n\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);\n\n\t\t*core = coremem >> PAGE_SHIFT;\n\t\t*percent = 0UL;\n\t}\n\treturn 0;\n}\n\n/*\n * kernelcore=size sets the amount of memory for use for allocations that\n * cannot be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_kernelcore(char *p)\n{\n\t/* parse kernelcore=mirror */\n\tif (parse_option_str(p, \"mirror\")) {\n\t\tmirrored_kernelcore = true;\n\t\treturn 0;\n\t}\n\n\treturn cmdline_parse_core(p, &required_kernelcore,\n\t\t\t\t  &required_kernelcore_percent);\n}\n\n/*\n * movablecore=size sets the amount of memory for use for allocations that\n * can be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_movablecore(char *p)\n{\n\treturn cmdline_parse_core(p, &required_movablecore,\n\t\t\t\t  &required_movablecore_percent);\n}\n\nearly_param(\"kernelcore\", cmdline_parse_kernelcore);\nearly_param(\"movablecore\", cmdline_parse_movablecore);\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nvoid adjust_managed_page_count(struct page *page, long count)\n{\n\tspin_lock(&managed_page_count_lock);\n\tpage_zone(page)->managed_pages += count;\n\ttotalram_pages += count;\n#ifdef CONFIG_HIGHMEM\n\tif (PageHighMem(page))\n\t\ttotalhigh_pages += count;\n#endif\n\tspin_unlock(&managed_page_count_lock);\n}\nEXPORT_SYMBOL(adjust_managed_page_count);\n\nunsigned long free_reserved_area(void *start, void *end, int poison, char *s)\n{\n\tvoid *pos;\n\tunsigned long pages = 0;\n\n\tstart = (void *)PAGE_ALIGN((unsigned long)start);\n\tend = (void *)((unsigned long)end & PAGE_MASK);\n\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {\n\t\tstruct page *page = virt_to_page(pos);\n\t\tvoid *direct_map_addr;\n\n\t\t/*\n\t\t * 'direct_map_addr' might be different from 'pos'\n\t\t * because some architectures' virt_to_page()\n\t\t * work with aliases.  Getting the direct map\n\t\t * address ensures that we get a _writeable_\n\t\t * alias for the memset().\n\t\t */\n\t\tdirect_map_addr = page_address(page);\n\t\tif ((unsigned int)poison <= 0xFF)\n\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);\n\n\t\tfree_reserved_page(page);\n\t}\n\n\tif (pages && s)\n\t\tpr_info(\"Freeing %s memory: %ldK\\n\",\n\t\t\ts, pages << (PAGE_SHIFT - 10));\n\n\treturn pages;\n}\nEXPORT_SYMBOL(free_reserved_area);\n\n#ifdef\tCONFIG_HIGHMEM\nvoid free_highmem_page(struct page *page)\n{\n\t__free_reserved_page(page);\n\ttotalram_pages++;\n\tpage_zone(page)->managed_pages++;\n\ttotalhigh_pages++;\n}\n#endif\n\n\nvoid __init mem_init_print_info(const char *str)\n{\n\tunsigned long physpages, codesize, datasize, rosize, bss_size;\n\tunsigned long init_code_size, init_data_size;\n\n\tphyspages = get_num_physpages();\n\tcodesize = _etext - _stext;\n\tdatasize = _edata - _sdata;\n\trosize = __end_rodata - __start_rodata;\n\tbss_size = __bss_stop - __bss_start;\n\tinit_data_size = __init_end - __init_begin;\n\tinit_code_size = _einittext - _sinittext;\n\n\t/*\n\t * Detect special cases and adjust section sizes accordingly:\n\t * 1) .init.* may be embedded into .data sections\n\t * 2) .init.text.* may be out of [__init_begin, __init_end],\n\t *    please refer to arch/tile/kernel/vmlinux.lds.S.\n\t * 3) .rodata.* may be embedded into .text or .data sections.\n\t */\n#define adj_init_size(start, end, size, pos, adj) \\\n\tdo { \\\n\t\tif (start <= pos && pos < end && size > adj) \\\n\t\t\tsize -= adj; \\\n\t} while (0)\n\n\tadj_init_size(__init_begin, __init_end, init_data_size,\n\t\t     _sinittext, init_code_size);\n\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);\n\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);\n\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);\n\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);\n\n#undef\tadj_init_size\n\n\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"\n#ifdef\tCONFIG_HIGHMEM\n\t\t\", %luK highmem\"\n#endif\n\t\t\"%s%s)\\n\",\n\t\tnr_free_pages() << (PAGE_SHIFT - 10),\n\t\tphyspages << (PAGE_SHIFT - 10),\n\t\tcodesize >> 10, datasize >> 10, rosize >> 10,\n\t\t(init_data_size + init_code_size) >> 10, bss_size >> 10,\n\t\t(physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT - 10),\n\t\ttotalcma_pages << (PAGE_SHIFT - 10),\n#ifdef\tCONFIG_HIGHMEM\n\t\ttotalhigh_pages << (PAGE_SHIFT - 10),\n#endif\n\t\tstr ? \", \" : \"\", str ? str : \"\");\n}\n\n/**\n * set_dma_reserve - set the specified number of pages reserved in the first zone\n * @new_dma_reserve: The number of pages to mark reserved\n *\n * The per-cpu batchsize and zone watermarks are determined by managed_pages.\n * In the DMA zone, a significant percentage may be consumed by kernel image\n * and other unfreeable allocations which can skew the watermarks badly. This\n * function may optionally be used to account for unfreeable pages in the\n * first zone (e.g., ZONE_DMA). The effect will be lower watermarks and\n * smaller per-cpu batchsize.\n */\nvoid __init set_dma_reserve(unsigned long new_dma_reserve)\n{\n\tdma_reserve = new_dma_reserve;\n}\n\nvoid __init free_area_init(unsigned long *zones_size)\n{\n\tzero_resv_unavail();\n\tfree_area_init_node(0, zones_size,\n\t\t\t__pa(PAGE_OFFSET) >> PAGE_SHIFT, NULL);\n}\n\nstatic int page_alloc_cpu_dead(unsigned int cpu)\n{\n\n\tlru_add_drain_cpu(cpu);\n\tdrain_pages(cpu);\n\n\t/*\n\t * Spill the event counters of the dead processor\n\t * into the current processors event counters.\n\t * This artificially elevates the count of the current\n\t * processor.\n\t */\n\tvm_events_fold_cpu(cpu);\n\n\t/*\n\t * Zero the differential counters of the dead processor\n\t * so that the vm statistics are consistent.\n\t *\n\t * This is only okay since the processor is dead and cannot\n\t * race with what we are doing.\n\t */\n\tcpu_vm_stats_fold(cpu);\n\treturn 0;\n}\n\nvoid __init page_alloc_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC_DEAD,\n\t\t\t\t\t\"mm/page_alloc:dead\", NULL,\n\t\t\t\t\tpage_alloc_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n\n/*\n * calculate_totalreserve_pages - called when sysctl_lowmem_reserve_ratio\n *\tor min_free_kbytes changes.\n */\nstatic void calculate_totalreserve_pages(void)\n{\n\tstruct pglist_data *pgdat;\n\tunsigned long reserve_pages = 0;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\n\t\tpgdat->totalreserve_pages = 0;\n\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\t\tlong max = 0;\n\n\t\t\t/* Find valid and maximum lowmem_reserve in the zone */\n\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {\n\t\t\t\tif (zone->lowmem_reserve[j] > max)\n\t\t\t\t\tmax = zone->lowmem_reserve[j];\n\t\t\t}\n\n\t\t\t/* we treat the high watermark as reserved pages. */\n\t\t\tmax += high_wmark_pages(zone);\n\n\t\t\tif (max > zone->managed_pages)\n\t\t\t\tmax = zone->managed_pages;\n\n\t\t\tpgdat->totalreserve_pages += max;\n\n\t\t\treserve_pages += max;\n\t\t}\n\t}\n\ttotalreserve_pages = reserve_pages;\n}\n\n/*\n * setup_per_zone_lowmem_reserve - called whenever\n *\tsysctl_lowmem_reserve_ratio changes.  Ensures that each zone\n *\thas a correct pages reserved value, so an adequate number of\n *\tpages are left in the zone after a successful __alloc_pages().\n */\nstatic void setup_per_zone_lowmem_reserve(void)\n{\n\tstruct pglist_data *pgdat;\n\tenum zone_type j, idx;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\t\tunsigned long managed_pages = zone->managed_pages;\n\n\t\t\tzone->lowmem_reserve[j] = 0;\n\n\t\t\tidx = j;\n\t\t\twhile (idx) {\n\t\t\t\tstruct zone *lower_zone;\n\n\t\t\t\tidx--;\n\t\t\t\tlower_zone = pgdat->node_zones + idx;\n\n\t\t\t\tif (sysctl_lowmem_reserve_ratio[idx] < 1) {\n\t\t\t\t\tsysctl_lowmem_reserve_ratio[idx] = 0;\n\t\t\t\t\tlower_zone->lowmem_reserve[j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\tlower_zone->lowmem_reserve[j] =\n\t\t\t\t\t\tmanaged_pages / sysctl_lowmem_reserve_ratio[idx];\n\t\t\t\t}\n\t\t\t\tmanaged_pages += lower_zone->managed_pages;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\nstatic void __setup_per_zone_wmarks(void)\n{\n\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);\n\tunsigned long lowmem_pages = 0;\n\tstruct zone *zone;\n\tunsigned long flags;\n\n\t/* Calculate total number of !ZONE_HIGHMEM pages */\n\tfor_each_zone(zone) {\n\t\tif (!is_highmem(zone))\n\t\t\tlowmem_pages += zone->managed_pages;\n\t}\n\n\tfor_each_zone(zone) {\n\t\tu64 tmp;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\ttmp = (u64)pages_min * zone->managed_pages;\n\t\tdo_div(tmp, lowmem_pages);\n\t\tif (is_highmem(zone)) {\n\t\t\t/*\n\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't\n\t\t\t * need highmem pages, so cap pages_min to a small\n\t\t\t * value here.\n\t\t\t *\n\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)\n\t\t\t * deltas control asynch page reclaim, and so should\n\t\t\t * not be capped for highmem.\n\t\t\t */\n\t\t\tunsigned long min_pages;\n\n\t\t\tmin_pages = zone->managed_pages / 1024;\n\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);\n\t\t\tzone->watermark[WMARK_MIN] = min_pages;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If it's a lowmem zone, reserve a number of pages\n\t\t\t * proportionate to the zone's size.\n\t\t\t */\n\t\t\tzone->watermark[WMARK_MIN] = tmp;\n\t\t}\n\n\t\t/*\n\t\t * Set the kswapd watermarks distance according to the\n\t\t * scale factor in proportion to available memory, but\n\t\t * ensure a minimum size on small systems.\n\t\t */\n\t\ttmp = max_t(u64, tmp >> 2,\n\t\t\t    mult_frac(zone->managed_pages,\n\t\t\t\t      watermark_scale_factor, 10000));\n\n\t\tzone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n\t\tzone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;\n\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\n/**\n * setup_per_zone_wmarks - called when min_free_kbytes changes\n * or when memory is hot-{added|removed}\n *\n * Ensures that the watermark[min,low,high] values for each zone are set\n * correctly with respect to min_free_kbytes.\n */\nvoid setup_per_zone_wmarks(void)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\t__setup_per_zone_wmarks();\n\tspin_unlock(&lock);\n}\n\n/*\n * Initialise min_free_kbytes.\n *\n * For small machines we want it small (128k min).  For large machines\n * we want it large (64MB max).  But it is not linear, because network\n * bandwidth does not increase linearly with machine size.  We use\n *\n *\tmin_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:\n *\tmin_free_kbytes = sqrt(lowmem_kbytes * 16)\n *\n * which yields\n *\n * 16MB:\t512k\n * 32MB:\t724k\n * 64MB:\t1024k\n * 128MB:\t1448k\n * 256MB:\t2048k\n * 512MB:\t2896k\n * 1024MB:\t4096k\n * 2048MB:\t5792k\n * 4096MB:\t8192k\n * 8192MB:\t11584k\n * 16384MB:\t16384k\n */\nint __meminit init_per_zone_wmark_min(void)\n{\n\tunsigned long lowmem_kbytes;\n\tint new_min_free_kbytes;\n\n\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);\n\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);\n\n\tif (new_min_free_kbytes > user_min_free_kbytes) {\n\t\tmin_free_kbytes = new_min_free_kbytes;\n\t\tif (min_free_kbytes < 128)\n\t\t\tmin_free_kbytes = 128;\n\t\tif (min_free_kbytes > 65536)\n\t\t\tmin_free_kbytes = 65536;\n\t} else {\n\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",\n\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);\n\t}\n\tsetup_per_zone_wmarks();\n\trefresh_zone_stat_thresholds();\n\tsetup_per_zone_lowmem_reserve();\n\n#ifdef CONFIG_NUMA\n\tsetup_min_unmapped_ratio();\n\tsetup_min_slab_ratio();\n#endif\n\n\treturn 0;\n}\ncore_initcall(init_per_zone_wmark_min)\n\n/*\n * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so\n *\tthat we can call two helper functions whenever min_free_kbytes\n *\tchanges.\n */\nint min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write) {\n\t\tuser_min_free_kbytes = min_free_kbytes;\n\t\tsetup_per_zone_wmarks();\n\t}\n\treturn 0;\n}\n\nint watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write)\n\t\tsetup_per_zone_wmarks();\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic void setup_min_unmapped_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_unmapped_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_unmapped_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_unmapped_ratio) / 100;\n}\n\n\nint sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_unmapped_ratio();\n\n\treturn 0;\n}\n\nstatic void setup_min_slab_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_slab_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_slab_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_slab_ratio) / 100;\n}\n\nint sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_slab_ratio();\n\n\treturn 0;\n}\n#endif\n\n/*\n * lowmem_reserve_ratio_sysctl_handler - just a wrapper around\n *\tproc_dointvec() so that we can call setup_per_zone_lowmem_reserve()\n *\twhenever sysctl_lowmem_reserve_ratio changes.\n *\n * The reserve ratio obviously has absolutely no relation with the\n * minimum watermarks. The lowmem reserve ratio can only make sense\n * if in function of the boot time zone sizes.\n */\nint lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tproc_dointvec_minmax(table, write, buffer, length, ppos);\n\tsetup_per_zone_lowmem_reserve();\n\treturn 0;\n}\n\n/*\n * percpu_pagelist_fraction - changes the pcp->high for each zone on each\n * cpu.  It is the fraction of total pages in each zone that a hot per cpu\n * pagelist can have before it gets flushed back to buddy allocator.\n */\nint percpu_pagelist_fraction_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct zone *zone;\n\tint old_percpu_pagelist_fraction;\n\tint ret;\n\n\tmutex_lock(&pcp_batch_high_lock);\n\told_percpu_pagelist_fraction = percpu_pagelist_fraction;\n\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (!write || ret < 0)\n\t\tgoto out;\n\n\t/* Sanity checking to avoid pcp imbalance */\n\tif (percpu_pagelist_fraction &&\n\t    percpu_pagelist_fraction < MIN_PERCPU_PAGELIST_FRACTION) {\n\t\tpercpu_pagelist_fraction = old_percpu_pagelist_fraction;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* No change? */\n\tif (percpu_pagelist_fraction == old_percpu_pagelist_fraction)\n\t\tgoto out;\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int cpu;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tpageset_set_high_and_batch(zone,\n\t\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\t}\nout:\n\tmutex_unlock(&pcp_batch_high_lock);\n\treturn ret;\n}\n\n#ifdef CONFIG_NUMA\nint hashdist = HASHDIST_DEFAULT;\n\nstatic int __init set_hashdist(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\thashdist = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"hashdist=\", set_hashdist);\n#endif\n\n#ifndef __HAVE_ARCH_RESERVED_KERNEL_PAGES\n/*\n * Returns the number of pages that arch has reserved but\n * is not known to alloc_large_system_hash().\n */\nstatic unsigned long __init arch_reserved_kernel_pages(void)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Adaptive scale is meant to reduce sizes of hash tables on large memory\n * machines. As memory size is increased the scale is also increased but at\n * slower pace.  Starting from ADAPT_SCALE_BASE (64G), every time memory\n * quadruples the scale is increased by one, which means the size of hash table\n * only doubles, instead of quadrupling as well.\n * Because 32-bit systems cannot have large physical memory, where this scaling\n * makes sense, it is disabled on such platforms.\n */\n#if __BITS_PER_LONG > 32\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#endif\n\n/*\n * allocate a large system hash table from bootmem\n * - it is assumed that the hash table must contain an exact power-of-2\n *   quantity of entries\n * - limit is the number of hash buckets, not the total allocation size\n */\nvoid *__init alloc_large_system_hash(const char *tablename,\n\t\t\t\t     unsigned long bucketsize,\n\t\t\t\t     unsigned long numentries,\n\t\t\t\t     int scale,\n\t\t\t\t     int flags,\n\t\t\t\t     unsigned int *_hash_shift,\n\t\t\t\t     unsigned int *_hash_mask,\n\t\t\t\t     unsigned long low_limit,\n\t\t\t\t     unsigned long high_limit)\n{\n\tunsigned long long max = high_limit;\n\tunsigned long log2qty, size;\n\tvoid *table = NULL;\n\tgfp_t gfp_flags;\n\n\t/* allow the kernel cmdline to have a say */\n\tif (!numentries) {\n\t\t/* round applicable memory size up to nearest megabyte */\n\t\tnumentries = nr_kernel_pages;\n\t\tnumentries -= arch_reserved_kernel_pages();\n\n\t\t/* It isn't necessary when PAGE_SIZE >= 1MB */\n\t\tif (PAGE_SHIFT < 20)\n\t\t\tnumentries = round_up(numentries, (1<<20)/PAGE_SIZE);\n\n#if __BITS_PER_LONG > 32\n\t\tif (!high_limit) {\n\t\t\tunsigned long adapt;\n\n\t\t\tfor (adapt = ADAPT_SCALE_NPAGES; adapt < numentries;\n\t\t\t     adapt <<= ADAPT_SCALE_SHIFT)\n\t\t\t\tscale++;\n\t\t}\n#endif\n\n\t\t/* limit to 1 bucket per 2^scale bytes of low memory */\n\t\tif (scale > PAGE_SHIFT)\n\t\t\tnumentries >>= (scale - PAGE_SHIFT);\n\t\telse\n\t\t\tnumentries <<= (PAGE_SHIFT - scale);\n\n\t\t/* Make sure we've got at least a 0-order allocation.. */\n\t\tif (unlikely(flags & HASH_SMALL)) {\n\t\t\t/* Makes no sense without HASH_EARLY */\n\t\t\tWARN_ON(!(flags & HASH_EARLY));\n\t\t\tif (!(numentries >> *_hash_shift)) {\n\t\t\t\tnumentries = 1UL << *_hash_shift;\n\t\t\t\tBUG_ON(!numentries);\n\t\t\t}\n\t\t} else if (unlikely((numentries * bucketsize) < PAGE_SIZE))\n\t\t\tnumentries = PAGE_SIZE / bucketsize;\n\t}\n\tnumentries = roundup_pow_of_two(numentries);\n\n\t/* limit allocation size to 1/16 total memory by default */\n\tif (max == 0) {\n\t\tmax = ((unsigned long long)nr_all_pages << PAGE_SHIFT) >> 4;\n\t\tdo_div(max, bucketsize);\n\t}\n\tmax = min(max, 0x80000000ULL);\n\n\tif (numentries < low_limit)\n\t\tnumentries = low_limit;\n\tif (numentries > max)\n\t\tnumentries = max;\n\n\tlog2qty = ilog2(numentries);\n\n\tgfp_flags = (flags & HASH_ZERO) ? GFP_ATOMIC | __GFP_ZERO : GFP_ATOMIC;\n\tdo {\n\t\tsize = bucketsize << log2qty;\n\t\tif (flags & HASH_EARLY) {\n\t\t\tif (flags & HASH_ZERO)\n\t\t\t\ttable = memblock_alloc_nopanic(size,\n\t\t\t\t\t\t\t       SMP_CACHE_BYTES);\n\t\t\telse\n\t\t\t\ttable = memblock_alloc_raw(size,\n\t\t\t\t\t\t\t   SMP_CACHE_BYTES);\n\t\t} else if (hashdist) {\n\t\t\ttable = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\t\t} else {\n\t\t\t/*\n\t\t\t * If bucketsize is not a power-of-two, we may free\n\t\t\t * some pages at the end of hash table which\n\t\t\t * alloc_pages_exact() automatically does\n\t\t\t */\n\t\t\tif (get_order(size) < MAX_ORDER) {\n\t\t\t\ttable = alloc_pages_exact(size, gfp_flags);\n\t\t\t\tkmemleak_alloc(table, size, 1, gfp_flags);\n\t\t\t}\n\t\t}\n\t} while (!table && size > PAGE_SIZE && --log2qty);\n\n\tif (!table)\n\t\tpanic(\"Failed to allocate %s hash table\\n\", tablename);\n\n\tpr_info(\"%s hash table entries: %ld (order: %d, %lu bytes)\\n\",\n\t\ttablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size);\n\n\tif (_hash_shift)\n\t\t*_hash_shift = log2qty;\n\tif (_hash_mask)\n\t\t*_hash_mask = (1 << log2qty) - 1;\n\n\treturn table;\n}\n\n/*\n * This function checks whether pageblock includes unmovable pages or not.\n * If @count is not zero, it is okay to include less @count unmovable pages\n *\n * PageLRU check without isolation or lru_lock could race so that\n * MIGRATE_MOVABLE block might include unmovable pages. And __PageMovable\n * check without lock_page also may miss some movable non-lru pages at\n * race condition. So you can't expect this function should be exact.\n */\nbool has_unmovable_pages(struct zone *zone, struct page *page, int count,\n\t\t\t int migratetype,\n\t\t\t bool skip_hwpoisoned_pages)\n{\n\tunsigned long pfn, iter, found;\n\n\t/*\n\t * TODO we could make this much more efficient by not checking every\n\t * page in the range if we know all of them are in MOVABLE_ZONE and\n\t * that the movable zone guarantees that pages are migratable but\n\t * the later is not the case right now unfortunatelly. E.g. movablecore\n\t * can still lead to having bootmem allocations in zone_movable.\n\t */\n\n\t/*\n\t * CMA allocations (alloc_contig_range) really need to mark isolate\n\t * CMA pageblocks even when they are not movable in fact so consider\n\t * them movable here.\n\t */\n\tif (is_migrate_cma(migratetype) &&\n\t\t\tis_migrate_cma(get_pageblock_migratetype(page)))\n\t\treturn false;\n\n\tpfn = page_to_pfn(page);\n\tfor (found = 0, iter = 0; iter < pageblock_nr_pages; iter++) {\n\t\tunsigned long check = pfn + iter;\n\n\t\tif (!pfn_valid_within(check))\n\t\t\tcontinue;\n\n\t\tpage = pfn_to_page(check);\n\n\t\tif (PageReserved(page))\n\t\t\tgoto unmovable;\n\n\t\t/*\n\t\t * If the zone is movable and we have ruled out all reserved\n\t\t * pages then it should be reasonably safe to assume the rest\n\t\t * is movable.\n\t\t */\n\t\tif (zone_idx(zone) == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Hugepages are not in LRU lists, but they're movable.\n\t\t * We need not scan over tail pages bacause we don't\n\t\t * handle each tail page individually in migration.\n\t\t */\n\t\tif (PageHuge(page)) {\n\n\t\t\tif (!hugepage_migration_supported(page_hstate(page)))\n\t\t\t\tgoto unmovable;\n\n\t\t\titer = round_up(iter + 1, 1<<compound_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We can't use page_count without pin a page\n\t\t * because another CPU can free compound page.\n\t\t * This check already skips compound tails of THP\n\t\t * because their page->_refcount is zero at all time.\n\t\t */\n\t\tif (!page_ref_count(page)) {\n\t\t\tif (PageBuddy(page))\n\t\t\t\titer += (1 << page_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (skip_hwpoisoned_pages && PageHWPoison(page))\n\t\t\tcontinue;\n\n\t\tif (__PageMovable(page))\n\t\t\tcontinue;\n\n\t\tif (!PageLRU(page))\n\t\t\tfound++;\n\t\t/*\n\t\t * If there are RECLAIMABLE pages, we need to check\n\t\t * it.  But now, memory offline itself doesn't call\n\t\t * shrink_node_slabs() and it still to be fixed.\n\t\t */\n\t\t/*\n\t\t * If the page is not RAM, page_count()should be 0.\n\t\t * we don't need more check. This is an _used_ not-movable page.\n\t\t *\n\t\t * The problematic thing here is PG_reserved pages. PG_reserved\n\t\t * is set to both of a memory hole page and a _used_ kernel\n\t\t * page at boot.\n\t\t */\n\t\tif (found > count)\n\t\t\tgoto unmovable;\n\t}\n\treturn false;\nunmovable:\n\tWARN_ON_ONCE(zone_idx(zone) == ZONE_MOVABLE);\n\treturn true;\n}\n\n#if (defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || defined(CONFIG_CMA)\n\nstatic unsigned long pfn_max_align_down(unsigned long pfn)\n{\n\treturn pfn & ~(max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t     pageblock_nr_pages) - 1);\n}\n\nstatic unsigned long pfn_max_align_up(unsigned long pfn)\n{\n\treturn ALIGN(pfn, max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t\tpageblock_nr_pages));\n}\n\n/* [start, end) must belong to a single zone. */\nstatic int __alloc_contig_migrate_range(struct compact_control *cc,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\t/* This function is based on compact_zone() from compaction.c. */\n\tunsigned long nr_reclaimed;\n\tunsigned long pfn = start;\n\tunsigned int tries = 0;\n\tint ret = 0;\n\n\tmigrate_prep();\n\n\twhile (pfn < end || !list_empty(&cc->migratepages)) {\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (list_empty(&cc->migratepages)) {\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tpfn = isolate_migratepages_range(cc, pfn, end);\n\t\t\tif (!pfn) {\n\t\t\t\tret = -EINTR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttries = 0;\n\t\t} else if (++tries == 5) {\n\t\t\tret = ret < 0 ? ret : -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\tnr_reclaimed = reclaim_clean_pages_from_list(cc->zone,\n\t\t\t\t\t\t\t&cc->migratepages);\n\t\tcc->nr_migratepages -= nr_reclaimed;\n\n\t\tret = migrate_pages(&cc->migratepages, alloc_migrate_target,\n\t\t\t\t    NULL, 0, cc->mode, MR_CONTIG_RANGE);\n\t}\n\tif (ret < 0) {\n\t\tputback_movable_pages(&cc->migratepages);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n/**\n * alloc_contig_range() -- tries to allocate given range of pages\n * @start:\tstart PFN to allocate\n * @end:\tone-past-the-last PFN to allocate\n * @migratetype:\tmigratetype of the underlaying pageblocks (either\n *\t\t\t#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks\n *\t\t\tin range must have the same migratetype and it must\n *\t\t\tbe either of the two.\n * @gfp_mask:\tGFP mask to use during compaction\n *\n * The PFN range does not have to be pageblock or MAX_ORDER_NR_PAGES\n * aligned.  The PFN range must belong to a single zone.\n *\n * The first thing this routine does is attempt to MIGRATE_ISOLATE all\n * pageblocks in the range.  Once isolated, the pageblocks should not\n * be modified by others.\n *\n * Returns zero on success or negative error code.  On success all\n * pages which PFN is in [start, end) are allocated for the caller and\n * need to be freed with free_contig_range().\n */\nint alloc_contig_range(unsigned long start, unsigned long end,\n\t\t       unsigned migratetype, gfp_t gfp_mask)\n{\n\tunsigned long outer_start, outer_end;\n\tunsigned int order;\n\tint ret = 0;\n\n\tstruct compact_control cc = {\n\t\t.nr_migratepages = 0,\n\t\t.order = -1,\n\t\t.zone = page_zone(pfn_to_page(start)),\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.no_set_skip_hint = true,\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t};\n\tINIT_LIST_HEAD(&cc.migratepages);\n\n\t/*\n\t * What we do here is we mark all pageblocks in range as\n\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may\n\t * have different sizes, and due to the way page allocator\n\t * work, we align the range to biggest of the two pages so\n\t * that page allocator won't try to merge buddies from\n\t * different pageblocks and change MIGRATE_ISOLATE to some\n\t * other migration type.\n\t *\n\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we\n\t * migrate the pages from an unaligned range (ie. pages that\n\t * we are interested in).  This will put all the pages in\n\t * range back to page allocator as MIGRATE_ISOLATE.\n\t *\n\t * When this is done, we take the pages in range from page\n\t * allocator removing them from the buddy system.  This way\n\t * page allocator will never consider using them.\n\t *\n\t * This lets us mark the pageblocks back as\n\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the\n\t * aligned range but not in the unaligned, original range are\n\t * put back to page allocator so that buddy can use them.\n\t */\n\n\tret = start_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\t       pfn_max_align_up(end), migratetype,\n\t\t\t\t       false);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * In case of -EBUSY, we'd like to know which page causes problem.\n\t * So, just fall through. test_pages_isolated() has a tracepoint\n\t * which will report the busy page.\n\t *\n\t * It is possible that busy pages could become available before\n\t * the call to test_pages_isolated, and the range will actually be\n\t * allocated.  So, if we fall through be sure to clear ret so that\n\t * -EBUSY is not accidentally used or returned to caller.\n\t */\n\tret = __alloc_contig_migrate_range(&cc, start, end);\n\tif (ret && ret != -EBUSY)\n\t\tgoto done;\n\tret =0;\n\n\t/*\n\t * Pages from [start, end) are within a MAX_ORDER_NR_PAGES\n\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's\n\t * more, all pages in [start, end) are free in page allocator.\n\t * What we are going to do is to allocate all pages from\n\t * [start, end) (that is remove them from page allocator).\n\t *\n\t * The only problem is that pages at the beginning and at the\n\t * end of interesting range may be not aligned with pages that\n\t * page allocator holds, ie. they can be part of higher order\n\t * pages.  Because of this, we reserve the bigger range and\n\t * once this is done free the pages we are not interested in.\n\t *\n\t * We don't have to hold zone->lock here because the pages are\n\t * isolated thus they won't get removed from buddy.\n\t */\n\n\tlru_add_drain_all();\n\tdrain_all_pages(cc.zone);\n\n\torder = 0;\n\touter_start = start;\n\twhile (!PageBuddy(pfn_to_page(outer_start))) {\n\t\tif (++order >= MAX_ORDER) {\n\t\t\touter_start = start;\n\t\t\tbreak;\n\t\t}\n\t\touter_start &= ~0UL << order;\n\t}\n\n\tif (outer_start != start) {\n\t\torder = page_order(pfn_to_page(outer_start));\n\n\t\t/*\n\t\t * outer_start page could be small order buddy page and\n\t\t * it doesn't include start page. Adjust outer_start\n\t\t * in this case to report failed page properly\n\t\t * on tracepoint in test_pages_isolated()\n\t\t */\n\t\tif (outer_start + (1UL << order) <= start)\n\t\t\touter_start = start;\n\t}\n\n\t/* Make sure the range is really isolated. */\n\tif (test_pages_isolated(outer_start, end, false)) {\n\t\tpr_info_ratelimited(\"%s: [%lx, %lx) PFNs busy\\n\",\n\t\t\t__func__, outer_start, end);\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Grab isolated pages from freelists. */\n\touter_end = isolate_freepages_range(&cc, outer_start, end);\n\tif (!outer_end) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Free head and tail (if any) */\n\tif (start != outer_start)\n\t\tfree_contig_range(outer_start, start - outer_start);\n\tif (end != outer_end)\n\t\tfree_contig_range(end, outer_end - end);\n\ndone:\n\tundo_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\tpfn_max_align_up(end), migratetype);\n\treturn ret;\n}\n\nvoid free_contig_range(unsigned long pfn, unsigned nr_pages)\n{\n\tunsigned int count = 0;\n\n\tfor (; nr_pages--; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tcount += page_count(page) != 1;\n\t\t__free_page(page);\n\t}\n\tWARN(count != 0, \"%d pages are still in use!\\n\", count);\n}\n#endif\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n/*\n * The zone indicated has a new number of managed_pages; batch sizes and percpu\n * page high values need to be recalulated.\n */\nvoid __meminit zone_pcp_update(struct zone *zone)\n{\n\tunsigned cpu;\n\tmutex_lock(&pcp_batch_high_lock);\n\tfor_each_possible_cpu(cpu)\n\t\tpageset_set_high_and_batch(zone,\n\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n#endif\n\nvoid zone_pcp_reset(struct zone *zone)\n{\n\tunsigned long flags;\n\tint cpu;\n\tstruct per_cpu_pageset *pset;\n\n\t/* avoid races with drain_pages()  */\n\tlocal_irq_save(flags);\n\tif (zone->pageset != &boot_pageset) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpset = per_cpu_ptr(zone->pageset, cpu);\n\t\t\tdrain_zonestat(zone, pset);\n\t\t}\n\t\tfree_percpu(zone->pageset);\n\t\tzone->pageset = &boot_pageset;\n\t}\n\tlocal_irq_restore(flags);\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n/*\n * All pages in the range must be in a single zone and isolated\n * before calling this.\n */\nvoid\n__offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tstruct page *page;\n\tstruct zone *zone;\n\tunsigned int order, i;\n\tunsigned long pfn;\n\tunsigned long flags;\n\t/* find the first valid pfn */\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++)\n\t\tif (pfn_valid(pfn))\n\t\t\tbreak;\n\tif (pfn == end_pfn)\n\t\treturn;\n\toffline_mem_sections(pfn, end_pfn);\n\tzone = page_zone(pfn_to_page(pfn));\n\tspin_lock_irqsave(&zone->lock, flags);\n\tpfn = start_pfn;\n\twhile (pfn < end_pfn) {\n\t\tif (!pfn_valid(pfn)) {\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\t\tpage = pfn_to_page(pfn);\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (unlikely(!PageBuddy(page) && PageHWPoison(page))) {\n\t\t\tpfn++;\n\t\t\tSetPageReserved(page);\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(page_count(page));\n\t\tBUG_ON(!PageBuddy(page));\n\t\torder = page_order(page);\n#ifdef CONFIG_DEBUG_VM\n\t\tpr_info(\"remove from free list %lx %d %lx\\n\",\n\t\t\tpfn, 1 << order, end_pfn);\n#endif\n\t\tlist_del(&page->lru);\n\t\trmv_page_order(page);\n\t\tzone->free_area[order].nr_free--;\n\t\tfor (i = 0; i < (1 << order); i++)\n\t\t\tSetPageReserved((page+i));\n\t\tpfn += (1 << order);\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n#endif\n\nbool is_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn order < MAX_ORDER;\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n/*\n * Set PG_hwpoison flag if a given page is confirmed to be a free page.  This\n * test is performed under the zone lock to prevent a race against page\n * allocation.\n */\nbool set_hwpoison_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\tbool hwpoisoned = false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order) {\n\t\t\tif (!TestSetPageHWPoison(page))\n\t\t\t\thwpoisoned = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn hwpoisoned;\n}\n#endif",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/div64.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/sections.h>",
            "#include <linux/psi.h>",
            "#include <linux/nmi.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/kthread.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/migrate.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/prefetch.h>",
            "#include <trace/events/oom.h>",
            "#include <trace/events/kmem.h>",
            "#include <linux/compaction.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/pfn.h>",
            "#include <linux/sort.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/memremap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/topology.h>",
            "#include <linux/oom.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/slab.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/suspend.h>",
            "#include <linux/module.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kernel.h>",
            "#include <linux/compiler.h>",
            "#include <linux/memblock.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/stddef.h>"
          ],
          "macros_used": [
            "#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)",
            "#define ADAPT_SCALE_SHIFT\t2",
            "#define ADAPT_SCALE_BASE\t(64ul << 30)",
            "#define MAX_NODE_LOAD (nr_online_nodes)",
            "#define MIN_PERCPU_PAGELIST_FRACTION\t(8)"
          ],
          "globals_used": [
            "static DEFINE_MUTEX(pcp_batch_high_lock);",
            "nodemask_t node_states[NR_NODE_STATES] __read_mostly = {\n\t[N_POSSIBLE] = NODE_MASK_ALL,\n\t[N_ONLINE] = { { [0] = 1UL } },\n#ifndef CONFIG_NUMA\n\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },\n#ifdef CONFIG_HIGHMEM\n\t[N_HIGH_MEMORY] = { { [0] = 1UL } },\n#endif\n\t[N_MEMORY] = { { [0] = 1UL } },\n\t[N_CPU] = { { [0] = 1UL } },\n#endif\t/* NUMA */\n};",
            "static DEFINE_SPINLOCK(managed_page_count_lock);",
            "int percpu_pagelist_fraction;",
            "static void __free_pages_ok(struct page *page, unsigned int order);",
            "int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t[ZONE_DMA] = 256,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t[ZONE_DMA32] = 256,\n#endif\n\t[ZONE_NORMAL] = 32,\n#ifdef CONFIG_HIGHMEM\n\t[ZONE_HIGHMEM] = 0,\n#endif\n\t[ZONE_MOVABLE] = 0,\n};",
            "static char * const zone_names[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t \"DMA\",\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t \"DMA32\",\n#endif\n\t \"Normal\",\n#ifdef CONFIG_HIGHMEM\n\t \"HighMem\",\n#endif\n\t \"Movable\",\n#ifdef CONFIG_ZONE_DEVICE\n\t \"Device\",\n#endif\n};",
            "int min_free_kbytes = 1024;",
            "int user_min_free_kbytes = -1;",
            "int watermark_scale_factor = 10;",
            "int page_group_by_mobility_disabled",
            "static __always_inline\nstruct",
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/div64.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <linux/psi.h>\n#include <linux/nmi.h>\n#include <linux/lockdep.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/kthread.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/migrate.h>\n#include <linux/mm_inline.h>\n#include <linux/prefetch.h>\n#include <trace/events/oom.h>\n#include <trace/events/kmem.h>\n#include <linux/compaction.h>\n#include <linux/kmemleak.h>\n#include <linux/debugobjects.h>\n#include <linux/page_ext.h>\n#include <linux/page-isolation.h>\n#include <linux/fault-inject.h>\n#include <linux/backing-dev.h>\n#include <linux/pfn.h>\n#include <linux/sort.h>\n#include <linux/stop_machine.h>\n#include <linux/memremap.h>\n#include <linux/mempolicy.h>\n#include <linux/vmstat.h>\n#include <linux/vmalloc.h>\n#include <linux/nodemask.h>\n#include <linux/memory_hotplug.h>\n#include <linux/cpuset.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/topology.h>\n#include <linux/oom.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/suspend.h>\n#include <linux/module.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/memblock.h>\n#include <linux/jiffies.h>\n#include <linux/pagemap.h>\n#include <linux/interrupt.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define MAX_NODE_LOAD (nr_online_nodes)\n#define MIN_PERCPU_PAGELIST_FRACTION\t(8)\n\nstatic DEFINE_MUTEX(pcp_batch_high_lock);\nnodemask_t node_states[NR_NODE_STATES] __read_mostly = {\n\t[N_POSSIBLE] = NODE_MASK_ALL,\n\t[N_ONLINE] = { { [0] = 1UL } },\n#ifndef CONFIG_NUMA\n\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },\n#ifdef CONFIG_HIGHMEM\n\t[N_HIGH_MEMORY] = { { [0] = 1UL } },\n#endif\n\t[N_MEMORY] = { { [0] = 1UL } },\n\t[N_CPU] = { { [0] = 1UL } },\n#endif\t/* NUMA */\n};\nstatic DEFINE_SPINLOCK(managed_page_count_lock);\nint percpu_pagelist_fraction;\nstatic void __free_pages_ok(struct page *page, unsigned int order);\nint sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t[ZONE_DMA] = 256,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t[ZONE_DMA32] = 256,\n#endif\n\t[ZONE_NORMAL] = 32,\n#ifdef CONFIG_HIGHMEM\n\t[ZONE_HIGHMEM] = 0,\n#endif\n\t[ZONE_MOVABLE] = 0,\n};\nstatic char * const zone_names[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t \"DMA\",\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t \"DMA32\",\n#endif\n\t \"Normal\",\n#ifdef CONFIG_HIGHMEM\n\t \"HighMem\",\n#endif\n\t \"Movable\",\n#ifdef CONFIG_ZONE_DEVICE\n\t \"Device\",\n#endif\n};\nint min_free_kbytes = 1024;\nint user_min_free_kbytes = -1;\nint watermark_scale_factor = 10;\nint page_group_by_mobility_disabled;\nstatic __always_inline\nstruct;\nstatic __always_inline struct;\n\nvoid show_free_areas(unsigned int filter, nodemask_t *nodemask)\n{\n\tunsigned long free_pcp = 0;\n\tint cpu;\n\tstruct zone *zone;\n\tpg_data_t *pgdat;\n\n\tfor_each_populated_zone(zone) {\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\t}\n\n\tprintk(\"active_anon:%lu inactive_anon:%lu isolated_anon:%lu\\n\"\n\t\t\" active_file:%lu inactive_file:%lu isolated_file:%lu\\n\"\n\t\t\" unevictable:%lu dirty:%lu writeback:%lu unstable:%lu\\n\"\n\t\t\" slab_reclaimable:%lu slab_unreclaimable:%lu\\n\"\n\t\t\" mapped:%lu shmem:%lu pagetables:%lu bounce:%lu\\n\"\n\t\t\" free:%lu free_pcp:%lu free_cma:%lu\\n\",\n\t\tglobal_node_page_state(NR_ACTIVE_ANON),\n\t\tglobal_node_page_state(NR_INACTIVE_ANON),\n\t\tglobal_node_page_state(NR_ISOLATED_ANON),\n\t\tglobal_node_page_state(NR_ACTIVE_FILE),\n\t\tglobal_node_page_state(NR_INACTIVE_FILE),\n\t\tglobal_node_page_state(NR_ISOLATED_FILE),\n\t\tglobal_node_page_state(NR_UNEVICTABLE),\n\t\tglobal_node_page_state(NR_FILE_DIRTY),\n\t\tglobal_node_page_state(NR_WRITEBACK),\n\t\tglobal_node_page_state(NR_UNSTABLE_NFS),\n\t\tglobal_node_page_state(NR_SLAB_RECLAIMABLE),\n\t\tglobal_node_page_state(NR_SLAB_UNRECLAIMABLE),\n\t\tglobal_node_page_state(NR_FILE_MAPPED),\n\t\tglobal_node_page_state(NR_SHMEM),\n\t\tglobal_zone_page_state(NR_PAGETABLE),\n\t\tglobal_zone_page_state(NR_BOUNCE),\n\t\tglobal_zone_page_state(NR_FREE_PAGES),\n\t\tfree_pcp,\n\t\tglobal_zone_page_state(NR_FREE_CMA_PAGES));\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tif (show_mem_node_skip(filter, pgdat->node_id, nodemask))\n\t\t\tcontinue;\n\n\t\tprintk(\"Node %d\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" isolated(anon):%lukB\"\n\t\t\t\" isolated(file):%lukB\"\n\t\t\t\" mapped:%lukB\"\n\t\t\t\" dirty:%lukB\"\n\t\t\t\" writeback:%lukB\"\n\t\t\t\" shmem:%lukB\"\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\t\" shmem_thp: %lukB\"\n\t\t\t\" shmem_pmdmapped: %lukB\"\n\t\t\t\" anon_thp: %lukB\"\n#endif\n\t\t\t\" writeback_tmp:%lukB\"\n\t\t\t\" unstable:%lukB\"\n\t\t\t\" all_unreclaimable? %s\"\n\t\t\t\"\\n\",\n\t\t\tpgdat->node_id,\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_UNEVICTABLE)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_MAPPED)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_DIRTY)),\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK)),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM)),\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_THPS) * HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_PMDMAPPED)\n\t\t\t\t\t* HPAGE_PMD_NR),\n\t\t\tK(node_page_state(pgdat, NR_ANON_THPS) * HPAGE_PMD_NR),\n#endif\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK_TEMP)),\n\t\t\tK(node_page_state(pgdat, NR_UNSTABLE_NFS)),\n\t\t\tpgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ?\n\t\t\t\t\"yes\" : \"no\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tint i;\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfree_pcp = 0;\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT\n\t\t\t\"%s\"\n\t\t\t\" free:%lukB\"\n\t\t\t\" min:%lukB\"\n\t\t\t\" low:%lukB\"\n\t\t\t\" high:%lukB\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" writepending:%lukB\"\n\t\t\t\" present:%lukB\"\n\t\t\t\" managed:%lukB\"\n\t\t\t\" mlocked:%lukB\"\n\t\t\t\" kernel_stack:%lukB\"\n\t\t\t\" pagetables:%lukB\"\n\t\t\t\" bounce:%lukB\"\n\t\t\t\" free_pcp:%lukB\"\n\t\t\t\" local_pcp:%ukB\"\n\t\t\t\" free_cma:%lukB\"\n\t\t\t\"\\n\",\n\t\t\tzone->name,\n\t\t\tK(zone_page_state(zone, NR_FREE_PAGES)),\n\t\t\tK(min_wmark_pages(zone)),\n\t\t\tK(low_wmark_pages(zone)),\n\t\t\tK(high_wmark_pages(zone)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_UNEVICTABLE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_WRITE_PENDING)),\n\t\t\tK(zone->present_pages),\n\t\t\tK(zone->managed_pages),\n\t\t\tK(zone_page_state(zone, NR_MLOCK)),\n\t\t\tzone_page_state(zone, NR_KERNEL_STACK_KB),\n\t\t\tK(zone_page_state(zone, NR_PAGETABLE)),\n\t\t\tK(zone_page_state(zone, NR_BOUNCE)),\n\t\t\tK(free_pcp),\n\t\t\tK(this_cpu_read(zone->pageset->pcp.count)),\n\t\t\tK(zone_page_state(zone, NR_FREE_CMA_PAGES)));\n\t\tprintk(\"lowmem_reserve[]:\");\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++)\n\t\t\tprintk(KERN_CONT \" %ld\", zone->lowmem_reserve[i]);\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int order;\n\t\tunsigned long nr[MAX_ORDER], flags, total = 0;\n\t\tunsigned char types[MAX_ORDER];\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT \"%s: \", zone->name);\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tstruct free_area *area = &zone->free_area[order];\n\t\t\tint type;\n\n\t\t\tnr[order] = area->nr_free;\n\t\t\ttotal += nr[order] << order;\n\n\t\t\ttypes[order] = 0;\n\t\t\tfor (type = 0; type < MIGRATE_TYPES; type++) {\n\t\t\t\tif (!list_empty(&area->free_list[type]))\n\t\t\t\t\ttypes[order] |= 1 << type;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n\t\t\t       nr[order], K(1UL) << order);\n\t\t\tif (nr[order])\n\t\t\t\tshow_migration_types(types[order]);\n\t\t}\n\t\tprintk(KERN_CONT \"= %lukB\\n\", K(total));\n\t}\n\n\thugetlb_show_meminfo();\n\n\tprintk(\"%ld total pagecache pages\\n\", global_node_page_state(NR_FILE_PAGES));\n\n\tshow_swap_cache_info();\n}\n\nstatic void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)\n{\n\tzoneref->zone = zone;\n\tzoneref->zone_idx = zone_idx(zone);\n}\n\n/*\n * Builds allocation fallback zone lists.\n *\n * Add all populated zones of a node to the zonelist.\n */\nstatic int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)\n{\n\tstruct zone *zone;\n\tenum zone_type zone_type = MAX_NR_ZONES;\n\tint nr_zones = 0;\n\n\tdo {\n\t\tzone_type--;\n\t\tzone = pgdat->node_zones + zone_type;\n\t\tif (managed_zone(zone)) {\n\t\t\tzoneref_set_zone(zone, &zonerefs[nr_zones++]);\n\t\t\tcheck_highest_zone(zone_type);\n\t\t}\n\t} while (zone_type);\n\n\treturn nr_zones;\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int __parse_numa_zonelist_order(char *s)\n{\n\t/*\n\t * We used to support different zonlists modes but they turned\n\t * out to be just not useful. Let's keep the warning in place\n\t * if somebody still use the cmd line parameter so that we do\n\t * not fail it silently\n\t */\n\tif (!(*s == 'd' || *s == 'D' || *s == 'n' || *s == 'N')) {\n\t\tpr_warn(\"Ignoring unsupported numa_zonelist_order value:  %s\\n\", s);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic __init int setup_numa_zonelist_order(char *s)\n{\n\tif (!s)\n\t\treturn 0;\n\n\treturn __parse_numa_zonelist_order(s);\n}\nearly_param(\"numa_zonelist_order\", setup_numa_zonelist_order);\n\nchar numa_zonelist_order[] = \"Node\";\n\n/*\n * sysctl handler for numa_zonelist_order\n */\nint numa_zonelist_order_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *length,\n\t\tloff_t *ppos)\n{\n\tchar *str;\n\tint ret;\n\n\tif (!write)\n\t\treturn proc_dostring(table, write, buffer, length, ppos);\n\tstr = memdup_user_nul(buffer, 16);\n\tif (IS_ERR(str))\n\t\treturn PTR_ERR(str);\n\n\tret = __parse_numa_zonelist_order(str);\n\tkfree(str);\n\treturn ret;\n}\n\n\n#define MAX_NODE_LOAD (nr_online_nodes)\nstatic int node_load[MAX_NUMNODES];\n\n/**\n * find_next_best_node - find the next node that should appear in a given node's fallback list\n * @node: node whose fallback list we're appending\n * @used_node_mask: nodemask_t of already used nodes\n *\n * We use a number of factors to determine which is the next node that should\n * appear on a given node's fallback list.  The node should not have appeared\n * already in @node's fallback list, and it should be the next closest node\n * according to the distance array (which contains arbitrary distance values\n * from each node to each node in the system), and should also prefer nodes\n * with no CPUs, since presumably they'll have very little allocation pressure\n * on them otherwise.\n * It returns -1 if no node is found.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_node_mask)\n{\n\tint n, val;\n\tint min_val = INT_MAX;\n\tint best_node = NUMA_NO_NODE;\n\tconst struct cpumask *tmp = cpumask_of_node(0);\n\n\t/* Use the local node if we haven't already */\n\tif (!node_isset(node, *used_node_mask)) {\n\t\tnode_set(node, *used_node_mask);\n\t\treturn node;\n\t}\n\n\tfor_each_node_state(n, N_MEMORY) {\n\n\t\t/* Don't want a node to appear more than once */\n\t\tif (node_isset(n, *used_node_mask))\n\t\t\tcontinue;\n\n\t\t/* Use the distance array to find the distance */\n\t\tval = node_distance(node, n);\n\n\t\t/* Penalize nodes under us (\"prefer the next node\") */\n\t\tval += (n < node);\n\n\t\t/* Give preference to headless and unused nodes */\n\t\ttmp = cpumask_of_node(n);\n\t\tif (!cpumask_empty(tmp))\n\t\t\tval += PENALTY_FOR_NODE_WITH_CPUS;\n\n\t\t/* Slight preference for less loaded node */\n\t\tval *= (MAX_NODE_LOAD*MAX_NUMNODES);\n\t\tval += node_load[n];\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node >= 0)\n\t\tnode_set(best_node, *used_node_mask);\n\n\treturn best_node;\n}\n\n\n/*\n * Build zonelists ordered by node and zones within node.\n * This results in maximum locality--normal zone overflows into local\n * DMA zone, if any--but risks exhausting DMA zone.\n */\nstatic void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order,\n\t\tunsigned nr_nodes)\n{\n\tstruct zoneref *zonerefs;\n\tint i;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\n\tfor (i = 0; i < nr_nodes; i++) {\n\t\tint nr_zones;\n\n\t\tpg_data_t *node = NODE_DATA(node_order[i]);\n\n\t\tnr_zones = build_zonerefs_node(node, zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build gfp_thisnode zonelists\n */\nstatic void build_thisnode_zonelists(pg_data_t *pgdat)\n{\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build zonelists ordered by zone and nodes within zones.\n * This results in conserving DMA zone[s] until all Normal memory is\n * exhausted, but results in overflowing to remote node while memory\n * may still exist in local DMA zone.\n */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tstatic int node_order[MAX_NUMNODES];\n\tint node, load, nr_nodes = 0;\n\tnodemask_t used_mask;\n\tint local_node, prev_node;\n\n\t/* NUMA-aware ordering of nodes */\n\tlocal_node = pgdat->node_id;\n\tload = nr_online_nodes;\n\tprev_node = local_node;\n\tnodes_clear(used_mask);\n\n\tmemset(node_order, 0, sizeof(node_order));\n\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n\t\t/*\n\t\t * We don't want to pressure a particular node.\n\t\t * So adding penalty to the first node in same\n\t\t * distance group to make it round-robin.\n\t\t */\n\t\tif (node_distance(local_node, node) !=\n\t\t    node_distance(local_node, prev_node))\n\t\t\tnode_load[node] = load;\n\n\t\tnode_order[nr_nodes++] = node;\n\t\tprev_node = node;\n\t\tload--;\n\t}\n\n\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n\tbuild_thisnode_zonelists(pgdat);\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n/*\n * Return node id of node used for \"local\" allocations.\n * I.e., first node id of first zone in arg node's generic zonelist.\n * Used for initializing percpu 'numa_mem', which is used primarily\n * for kernel allocations, so use GFP_KERNEL flags to locate zonelist.\n */\nint local_memory_node(int node)\n{\n\tstruct zoneref *z;\n\n\tz = first_zones_zonelist(node_zonelist(node, GFP_KERNEL),\n\t\t\t\t   gfp_zone(GFP_KERNEL),\n\t\t\t\t   NULL);\n\treturn zone_to_nid(z->zone);\n}\n#endif\n\nstatic void setup_min_unmapped_ratio(void);\nstatic void setup_min_slab_ratio(void);\n#else\t/* CONFIG_NUMA */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tint node, local_node;\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tlocal_node = pgdat->node_id;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\n\t/*\n\t * Now we build the zonelist so that it contains the zones\n\t * of all the other nodes.\n\t * We don't want to pressure a particular node, so when\n\t * building the zones for node N, we make sure that the\n\t * zones coming right after the local ones are those from\n\t * node N+1 (modulo N)\n\t */\n\tfor (node = local_node + 1; node < MAX_NUMNODES; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tfor (node = 0; node < local_node; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n#endif\t/* CONFIG_NUMA */\n\n/*\n * Boot pageset table. One per cpu which is going to be used for all\n * zones and all nodes. The parameters will be set in such a way\n * that an item put on a list will immediately be handed over to\n * the buddy list. This is safe since pageset manipulation is done\n * with interrupts disabled.\n *\n * The boot_pagesets must be kept even after bootup is complete for\n * unused processors and/or zones. They do play a role for bootstrapping\n * hotplugged processors.\n *\n * zoneinfo_show() and maybe other functions do\n * not check if the processor is online before following the pageset pointer.\n * Other parts of the kernel may not check if the zone is available.\n */\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch);\nstatic DEFINE_PER_CPU(struct per_cpu_pageset, boot_pageset);\nstatic DEFINE_PER_CPU(struct per_cpu_nodestat, boot_nodestats);\n\nstatic void __build_all_zonelists(void *data)\n{\n\tint nid;\n\tint __maybe_unused cpu;\n\tpg_data_t *self = data;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\n#ifdef CONFIG_NUMA\n\tmemset(node_load, 0, sizeof(node_load));\n#endif\n\n\t/*\n\t * This node is hotadded and no memory is yet present.   So just\n\t * building zonelists is fine - no need to touch other nodes.\n\t */\n\tif (self && !node_online(self->node_id)) {\n\t\tbuild_zonelists(self);\n\t} else {\n\t\tfor_each_online_node(nid) {\n\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\t\t\tbuild_zonelists(pgdat);\n\t\t}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n\t\t/*\n\t\t * We now know the \"local memory node\" for each node--\n\t\t * i.e., the node of the first zone in the generic zonelist.\n\t\t * Set up numa_mem percpu variable for on-line cpus.  During\n\t\t * boot, only the boot cpu should be on-line;  we'll init the\n\t\t * secondary cpus' numa_mem as they come on-line.  During\n\t\t * node/memory hotplug, we'll fixup all on-line cpus.\n\t\t */\n\t\tfor_each_online_cpu(cpu)\n\t\t\tset_cpu_numa_mem(cpu, local_memory_node(cpu_to_node(cpu)));\n#endif\n\t}\n\n\tspin_unlock(&lock);\n}\n\nstatic noinline void __init\nbuild_all_zonelists_init(void)\n{\n\tint cpu;\n\n\t__build_all_zonelists(NULL);\n\n\t/*\n\t * Initialize the boot_pagesets that are going to be used\n\t * for bootstrapping processors. The real pagesets for\n\t * each zone will be allocated later when the per cpu\n\t * allocator is available.\n\t *\n\t * boot_pagesets are used also for bootstrapping offline\n\t * cpus if the system is already booted because the pagesets\n\t * are needed to initialize allocators on a specific cpu too.\n\t * F.e. the percpu allocator needs the page allocator which\n\t * needs the percpu allocator in order to allocate its pagesets\n\t * (a chicken-egg dilemma).\n\t */\n\tfor_each_possible_cpu(cpu)\n\t\tsetup_pageset(&per_cpu(boot_pageset, cpu), 0);\n\n\tmminit_verify_zonelist();\n\tcpuset_init_current_mems_allowed();\n}\n\n/*\n * unless system_state == SYSTEM_BOOTING.\n *\n * __ref due to call of __init annotated helper build_all_zonelists_init\n * [protected by SYSTEM_BOOTING].\n */\nvoid __ref build_all_zonelists(pg_data_t *pgdat)\n{\n\tif (system_state == SYSTEM_BOOTING) {\n\t\tbuild_all_zonelists_init();\n\t} else {\n\t\t__build_all_zonelists(pgdat);\n\t\t/* cpuset refresh routine should be here */\n\t}\n\tvm_total_pages = nr_free_pagecache_pages();\n\t/*\n\t * Disable grouping by mobility if the number of pages in the\n\t * system is too low to allow the mechanism to work. It would be\n\t * more accurate, but expensive to check per-zone. This check is\n\t * made on memory-hotadd so a system can start with mobility\n\t * disabled and enable it later\n\t */\n\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))\n\t\tpage_group_by_mobility_disabled = 1;\n\telse\n\t\tpage_group_by_mobility_disabled = 0;\n\n\tpr_info(\"Built %i zonelists, mobility grouping %s.  Total pages: %ld\\n\",\n\t\tnr_online_nodes,\n\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",\n\t\tvm_total_pages);\n#ifdef CONFIG_NUMA\n\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);\n#endif\n}\n\n/* If zone is ZONE_MOVABLE but memory is mirrored, it is an overlapped init */\nstatic bool __meminit\noverlap_memmap_init(unsigned long zone, unsigned long *pfn)\n{\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tstatic struct memblock_region *r;\n\n\tif (mirrored_kernelcore && zone == ZONE_MOVABLE) {\n\t\tif (!r || *pfn >= memblock_region_memory_end_pfn(r)) {\n\t\t\tfor_each_memblock(memory, r) {\n\t\t\t\tif (*pfn < memblock_region_memory_end_pfn(r))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (*pfn >= memblock_region_memory_base_pfn(r) &&\n\t\t    memblock_is_mirror(r)) {\n\t\t\t*pfn = memblock_region_memory_end_pfn(r);\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\treturn false;\n}\n\n/*\n * Initially all pages are reserved - free ones are freed\n * up by memblock_free_all() once the early boot process is\n * done. Non-atomic initialization, single-pass.\n */\nvoid __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,\n\t\tunsigned long start_pfn, enum memmap_context context,\n\t\tstruct vmem_altmap *altmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct page *page;\n\n\tif (highest_memmap_pfn < end_pfn - 1)\n\t\thighest_memmap_pfn = end_pfn - 1;\n\n#ifdef CONFIG_ZONE_DEVICE\n\t/*\n\t * Honor reservation requested by the driver for this ZONE_DEVICE\n\t * memory. We limit the total number of pages to initialize to just\n\t * those that might contain the memory mapping. We will defer the\n\t * ZONE_DEVICE page initialization until after we have released\n\t * the hotplug lock.\n\t */\n\tif (zone == ZONE_DEVICE) {\n\t\tif (!altmap)\n\t\t\treturn;\n\n\t\tif (start_pfn == altmap->base_pfn)\n\t\t\tstart_pfn += altmap->reserve;\n\t\tend_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t}\n#endif\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\t/*\n\t\t * There can be holes in boot-time mem_map[]s handed to this\n\t\t * function.  They do not exist on hotplugged memory.\n\t\t */\n\t\tif (context == MEMMAP_EARLY) {\n\t\t\tif (!early_pfn_valid(pfn))\n\t\t\t\tcontinue;\n\t\t\tif (!early_pfn_in_nid(pfn, nid))\n\t\t\t\tcontinue;\n\t\t\tif (overlap_memmap_init(zone, &pfn))\n\t\t\t\tcontinue;\n\t\t\tif (defer_init(nid, pfn, end_pfn))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tpage = pfn_to_page(pfn);\n\t\t__init_single_page(page, pfn, zone, nid);\n\t\tif (context == MEMMAP_HOTPLUG)\n\t\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nvoid __ref memmap_init_zone_device(struct zone *zone,\n\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t   unsigned long size,\n\t\t\t\t   struct dev_pagemap *pgmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tunsigned long zone_idx = zone_idx(zone);\n\tunsigned long start = jiffies;\n\tint nid = pgdat->node_id;\n\n\tif (WARN_ON_ONCE(!pgmap || !is_dev_zone(zone)))\n\t\treturn;\n\n\t/*\n\t * The call to memmap_init_zone should have already taken care\n\t * of the pages reserved for the memmap, so we can just jump to\n\t * the end of that region and start processing the device pages.\n\t */\n\tif (pgmap->altmap_valid) {\n\t\tstruct vmem_altmap *altmap = &pgmap->altmap;\n\n\t\tstart_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t\tsize = end_pfn - start_pfn;\n\t}\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t__init_single_page(page, pfn, zone_idx, nid);\n\n\t\t/*\n\t\t * Mark page reserved as it will need to wait for onlining\n\t\t * phase for it to be fully associated with a zone.\n\t\t *\n\t\t * We can use the non-atomic __set_bit operation for setting\n\t\t * the flag as we are still initializing the pages.\n\t\t */\n\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * ZONE_DEVICE pages union ->lru with a ->pgmap back\n\t\t * pointer and hmm_data.  It is a bug if a ZONE_DEVICE\n\t\t * page is ever freed or placed on a driver-private list.\n\t\t */\n\t\tpage->pgmap = pgmap;\n\t\tpage->hmm_data = 0;\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * bitmap is created for zone's valid pfn range. but memmap\n\t\t * can be created for invalid pages (for alignment)\n\t\t * check here not to call set_pageblock_migratetype() against\n\t\t * pfn out of zone.\n\t\t *\n\t\t * Please note that MEMMAP_HOTPLUG path doesn't clear memmap\n\t\t * because this is done early in sparse_add_one_section\n\t\t */\n\t\tif (!(pfn & (pageblock_nr_pages - 1))) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tpr_info(\"%s initialised, %lu pages in %ums\\n\", dev_name(pgmap->dev),\n\t\tsize, jiffies_to_msecs(jiffies - start));\n}\n\n#endif\nstatic void __meminit zone_init_free_lists(struct zone *zone)\n{\n\tunsigned int order, t;\n\tfor_each_migratetype_order(order, t) {\n\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);\n\t\tzone->free_area[order].nr_free = 0;\n\t}\n}\n\nvoid __meminit __weak memmap_init(unsigned long size, int nid,\n\t\t\t\t  unsigned long zone, unsigned long start_pfn)\n{\n\tmemmap_init_zone(size, nid, zone, start_pfn, MEMMAP_EARLY, NULL);\n}\n\nstatic int zone_batchsize(struct zone *zone)\n{\n#ifdef CONFIG_MMU\n\tint batch;\n\n\t/*\n\t * The per-cpu-pages pools are set to around 1000th of the\n\t * size of the zone.\n\t */\n\tbatch = zone->managed_pages / 1024;\n\t/* But no more than a meg. */\n\tif (batch * PAGE_SIZE > 1024 * 1024)\n\t\tbatch = (1024 * 1024) / PAGE_SIZE;\n\tbatch /= 4;\t\t/* We effectively *= 4 below */\n\tif (batch < 1)\n\t\tbatch = 1;\n\n\t/*\n\t * Clamp the batch to a 2^n - 1 value. Having a power\n\t * of 2 value was found to be more likely to have\n\t * suboptimal cache aliasing properties in some cases.\n\t *\n\t * For example if 2 tasks are alternately allocating\n\t * batches of pages, one task can end up with a lot\n\t * of pages of one half of the possible page colors\n\t * and the other with pages of the other colors.\n\t */\n\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;\n\n\treturn batch;\n\n#else\n\t/* The deferral and batching of frees should be suppressed under NOMMU\n\t * conditions.\n\t *\n\t * The problem is that NOMMU needs to be able to allocate large chunks\n\t * of contiguous memory as there's no hardware page translation to\n\t * assemble apparent contiguous memory from discontiguous pages.\n\t *\n\t * Queueing large contiguous runs of pages for batching, however,\n\t * causes the pages to actually be freed in smaller chunks.  As there\n\t * can be a significant delay between the individual batches being\n\t * recycled, this leads to the once large chunks of space being\n\t * fragmented and becoming unavailable for high-order allocations.\n\t */\n\treturn 0;\n#endif\n}\n\n/*\n * pcp->high and pcp->batch values are related and dependent on one another:\n * ->batch must never be higher then ->high.\n * The following function updates them in a safe manner without read side\n * locking.\n *\n * Any new users of pcp->batch and pcp->high should ensure they can cope with\n * those fields changing asynchronously (acording the the above rule).\n *\n * mutex_is_locked(&pcp_batch_high_lock) required when calling this function\n * outside of boot time (or some other assurance that no concurrent updaters\n * exist).\n */\nstatic void pageset_update(struct per_cpu_pages *pcp, unsigned long high,\n\t\tunsigned long batch)\n{\n       /* start with a fail safe value for batch */\n\tpcp->batch = 1;\n\tsmp_wmb();\n\n       /* Update high, then batch, in order */\n\tpcp->high = high;\n\tsmp_wmb();\n\n\tpcp->batch = batch;\n}\n\n/* a companion to pageset_set_high() */\nstatic void pageset_set_batch(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_update(&p->pcp, 6 * batch, max(1UL, 1 * batch));\n}\n\nstatic void pageset_init(struct per_cpu_pageset *p)\n{\n\tstruct per_cpu_pages *pcp;\n\tint migratetype;\n\n\tmemset(p, 0, sizeof(*p));\n\n\tpcp = &p->pcp;\n\tpcp->count = 0;\n\tfor (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)\n\t\tINIT_LIST_HEAD(&pcp->lists[migratetype]);\n}\n\nstatic void setup_pageset(struct per_cpu_pageset *p, unsigned long batch)\n{\n\tpageset_init(p);\n\tpageset_set_batch(p, batch);\n}\n\n/*\n * pageset_set_high() sets the high water mark for hot per_cpu_pagelist\n * to the value high for the pageset p.\n */\nstatic void pageset_set_high(struct per_cpu_pageset *p,\n\t\t\t\tunsigned long high)\n{\n\tunsigned long batch = max(1UL, high / 4);\n\tif ((high / 4) > (PAGE_SHIFT * 8))\n\t\tbatch = PAGE_SHIFT * 8;\n\n\tpageset_update(&p->pcp, high, batch);\n}\n\nstatic void pageset_set_high_and_batch(struct zone *zone,\n\t\t\t\t       struct per_cpu_pageset *pcp)\n{\n\tif (percpu_pagelist_fraction)\n\t\tpageset_set_high(pcp,\n\t\t\t(zone->managed_pages /\n\t\t\t\tpercpu_pagelist_fraction));\n\telse\n\t\tpageset_set_batch(pcp, zone_batchsize(zone));\n}\n\nstatic void __meminit zone_pageset_init(struct zone *zone, int cpu)\n{\n\tstruct per_cpu_pageset *pcp = per_cpu_ptr(zone->pageset, cpu);\n\n\tpageset_init(pcp);\n\tpageset_set_high_and_batch(zone, pcp);\n}\n\nvoid __meminit setup_zone_pageset(struct zone *zone)\n{\n\tint cpu;\n\tzone->pageset = alloc_percpu(struct per_cpu_pageset);\n\tfor_each_possible_cpu(cpu)\n\t\tzone_pageset_init(zone, cpu);\n}\n\n/*\n * Allocate per cpu pagesets and initialize them.\n * Before this call only boot pagesets were available.\n */\nvoid __init setup_per_cpu_pageset(void)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone)\n\t\tsetup_zone_pageset(zone);\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->per_cpu_nodestats =\n\t\t\talloc_percpu(struct per_cpu_nodestat);\n}\n\nstatic __meminit void zone_pcp_init(struct zone *zone)\n{\n\t/*\n\t * per cpu subsystem is not up at this point. The following code\n\t * relies on the ability of the linker to provide the\n\t * offset of a (static) per cpu variable into the per cpu area.\n\t */\n\tzone->pageset = &boot_pageset;\n\n\tif (populated_zone(zone))\n\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages, LIFO batch:%u\\n\",\n\t\t\tzone->name, zone->present_pages,\n\t\t\t\t\t zone_batchsize(zone));\n}\n\nvoid __meminit init_currently_empty_zone(struct zone *zone,\n\t\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\t\tunsigned long size)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tint zone_idx = zone_idx(zone) + 1;\n\n\tif (zone_idx > pgdat->nr_zones)\n\t\tpgdat->nr_zones = zone_idx;\n\n\tzone->zone_start_pfn = zone_start_pfn;\n\n\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",\n\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",\n\t\t\tpgdat->node_id,\n\t\t\t(unsigned long)zone_idx(zone),\n\t\t\tzone_start_pfn, (zone_start_pfn + size));\n\n\tzone_init_free_lists(zone);\n\tzone->initialized = 1;\n}\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n#ifndef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID\n\n/*\n * Required by SPARSEMEM. Given a PFN, return what node the PFN is on.\n */\nint __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint nid;\n\n\tif (state->last_start <= pfn && pfn < state->last_end)\n\t\treturn state->last_nid;\n\n\tnid = memblock_search_pfn_nid(pfn, &start_pfn, &end_pfn);\n\tif (nid != -1) {\n\t\tstate->last_start = start_pfn;\n\t\tstate->last_end = end_pfn;\n\t\tstate->last_nid = nid;\n\t}\n\n\treturn nid;\n}\n#endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */\n\n/**\n * free_bootmem_with_active_regions - Call memblock_free_early_nid for each active range\n * @nid: The node to free memory on. If MAX_NUMNODES, all nodes are freed.\n * @max_low_pfn: The highest PFN that will be passed to memblock_free_early_nid\n *\n * If an architecture guarantees that all ranges registered contain no holes\n * and may be freed, this this function may be used instead of calling\n * memblock_free_early_nid() manually.\n */\nvoid __init free_bootmem_with_active_regions(int nid, unsigned long max_low_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid) {\n\t\tstart_pfn = min(start_pfn, max_low_pfn);\n\t\tend_pfn = min(end_pfn, max_low_pfn);\n\n\t\tif (start_pfn < end_pfn)\n\t\t\tmemblock_free_early_nid(PFN_PHYS(start_pfn),\n\t\t\t\t\t(end_pfn - start_pfn) << PAGE_SHIFT,\n\t\t\t\t\tthis_nid);\n\t}\n}\n\n/**\n * sparse_memory_present_with_active_regions - Call memory_present for each active range\n * @nid: The node to call memory_present for. If MAX_NUMNODES, all nodes will be used.\n *\n * If an architecture guarantees that all ranges registered contain no holes and may\n * be freed, this function may be used instead of calling memory_present() manually.\n */\nvoid __init sparse_memory_present_with_active_regions(int nid)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, this_nid;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid)\n\t\tmemory_present(this_nid, start_pfn, end_pfn);\n}\n\n/**\n * get_pfn_range_for_nid - Return the start and end page frames for a node\n * @nid: The nid to return the range for. If MAX_NUMNODES, the min and max PFN are returned.\n * @start_pfn: Passed by reference. On return, it will have the node start_pfn.\n * @end_pfn: Passed by reference. On return, it will have the node end_pfn.\n *\n * It returns the start and end page frame of a node based on information\n * provided by memblock_set_node(). If called for a node\n * with no available memory, a warning is printed and the start and end\n * PFNs will be 0.\n */\nvoid __meminit get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn)\n{\n\tunsigned long this_start_pfn, this_end_pfn;\n\tint i;\n\n\t*start_pfn = -1UL;\n\t*end_pfn = 0;\n\n\tfor_each_mem_pfn_range(i, nid, &this_start_pfn, &this_end_pfn, NULL) {\n\t\t*start_pfn = min(*start_pfn, this_start_pfn);\n\t\t*end_pfn = max(*end_pfn, this_end_pfn);\n\t}\n\n\tif (*start_pfn == -1UL)\n\t\t*start_pfn = 0;\n}\n\n/*\n * This finds a zone that can be used for ZONE_MOVABLE pages. The\n * assumption is made that zones within a node are ordered in monotonic\n * increasing memory addresses so that the \"highest\" populated zone is used\n */\nstatic void __init find_usable_zone_for_movable(void)\n{\n\tint zone_index;\n\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {\n\t\tif (zone_index == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tif (arch_zone_highest_possible_pfn[zone_index] >\n\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])\n\t\t\tbreak;\n\t}\n\n\tVM_BUG_ON(zone_index == -1);\n\tmovable_zone = zone_index;\n}\n\n/*\n * The zone ranges provided by the architecture do not include ZONE_MOVABLE\n * because it is sized independent of architecture. Unlike the other zones,\n * the starting point for ZONE_MOVABLE is not fixed. It may be different\n * in each node depending on the size of each node and how evenly kernelcore\n * is distributed. This helper function adjusts the zone ranges\n * provided by the architecture for a given node by using the end of the\n * highest usable zone for ZONE_MOVABLE. This preserves the assumption that\n * zones within a node are in order of monotonic increases memory addresses\n */\nstatic void __meminit adjust_zone_range_for_zone_movable(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\t/* Only adjust if ZONE_MOVABLE is on this node */\n\tif (zone_movable_pfn[nid]) {\n\t\t/* Size ZONE_MOVABLE */\n\t\tif (zone_type == ZONE_MOVABLE) {\n\t\t\t*zone_start_pfn = zone_movable_pfn[nid];\n\t\t\t*zone_end_pfn = min(node_end_pfn,\n\t\t\t\tarch_zone_highest_possible_pfn[movable_zone]);\n\n\t\t/* Adjust for ZONE_MOVABLE starting within this range */\n\t\t} else if (!mirrored_kernelcore &&\n\t\t\t*zone_start_pfn < zone_movable_pfn[nid] &&\n\t\t\t*zone_end_pfn > zone_movable_pfn[nid]) {\n\t\t\t*zone_end_pfn = zone_movable_pfn[nid];\n\n\t\t/* Check if this whole range is within ZONE_MOVABLE */\n\t\t} else if (*zone_start_pfn >= zone_movable_pfn[nid])\n\t\t\t*zone_start_pfn = *zone_end_pfn;\n\t}\n}\n\n/*\n * Return the number of pages a zone spans in a node, including holes\n * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()\n */\nstatic unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\t/* Get the start and end of the zone */\n\t*zone_start_pfn = arch_zone_lowest_possible_pfn[zone_type];\n\t*zone_end_pfn = arch_zone_highest_possible_pfn[zone_type];\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t/* Check that this node has pages within the zone's required range */\n\tif (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)\n\t\treturn 0;\n\n\t/* Move the zone boundaries inside the node if necessary */\n\t*zone_end_pfn = min(*zone_end_pfn, node_end_pfn);\n\t*zone_start_pfn = max(*zone_start_pfn, node_start_pfn);\n\n\t/* Return the spanned pages */\n\treturn *zone_end_pfn - *zone_start_pfn;\n}\n\n/*\n * Return the number of holes in a range on a node. If nid is MAX_NUMNODES,\n * then all holes in the requested range will be accounted for.\n */\nunsigned long __meminit __absent_pages_in_range(int nid,\n\t\t\t\tunsigned long range_start_pfn,\n\t\t\t\tunsigned long range_end_pfn)\n{\n\tunsigned long nr_absent = range_end_pfn - range_start_pfn;\n\tunsigned long start_pfn, end_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);\n\t\tend_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);\n\t\tnr_absent -= end_pfn - start_pfn;\n\t}\n\treturn nr_absent;\n}\n\n/**\n * absent_pages_in_range - Return number of page frames in holes within a range\n * @start_pfn: The start PFN to start searching for holes\n * @end_pfn: The end PFN to stop searching for holes\n *\n * It returns the number of pages frames in memory holes within a range.\n */\nunsigned long __init absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\treturn __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);\n}\n\n/* Return the number of page frames in holes in a zone on a node */\nstatic unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *ignored)\n{\n\tunsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type];\n\tunsigned long zone_high = arch_zone_highest_possible_pfn[zone_type];\n\tunsigned long zone_start_pfn, zone_end_pfn;\n\tunsigned long nr_absent;\n\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\tzone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);\n\tzone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);\n\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t&zone_start_pfn, &zone_end_pfn);\n\tnr_absent = __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn);\n\n\t/*\n\t * ZONE_MOVABLE handling.\n\t * Treat pages to be ZONE_MOVABLE in ZONE_NORMAL as absent pages\n\t * and vice versa.\n\t */\n\tif (mirrored_kernelcore && zone_movable_pfn[nid]) {\n\t\tunsigned long start_pfn, end_pfn;\n\t\tstruct memblock_region *r;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tstart_pfn = clamp(memblock_region_memory_base_pfn(r),\n\t\t\t\t\t  zone_start_pfn, zone_end_pfn);\n\t\t\tend_pfn = clamp(memblock_region_memory_end_pfn(r),\n\t\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t\t\tif (zone_type == ZONE_MOVABLE &&\n\t\t\t    memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\n\t\t\tif (zone_type == ZONE_NORMAL &&\n\t\t\t    !memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\t\t}\n\t}\n\n\treturn nr_absent;\n}\n\n#else /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\nstatic inline unsigned long __meminit zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn,\n\t\t\t\t\tunsigned long *zones_size)\n{\n\tunsigned int zone;\n\n\t*zone_start_pfn = node_start_pfn;\n\tfor (zone = 0; zone < zone_type; zone++)\n\t\t*zone_start_pfn += zones_size[zone];\n\n\t*zone_end_pfn = *zone_start_pfn + zones_size[zone_type];\n\n\treturn zones_size[zone_type];\n}\n\nstatic inline unsigned long __meminit zone_absent_pages_in_node(int nid,\n\t\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tif (!zholes_size)\n\t\treturn 0;\n\n\treturn zholes_size[zone_type];\n}\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nstatic void __meminit calculate_node_totalpages(struct pglist_data *pgdat,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\t\tunsigned long *zones_size,\n\t\t\t\t\t\tunsigned long *zholes_size)\n{\n\tunsigned long realtotalpages = 0, totalpages = 0;\n\tenum zone_type i;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\tunsigned long zone_start_pfn, zone_end_pfn;\n\t\tunsigned long size, real_size;\n\n\t\tsize = zone_spanned_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn,\n\t\t\t\t\t\t  node_end_pfn,\n\t\t\t\t\t\t  &zone_start_pfn,\n\t\t\t\t\t\t  &zone_end_pfn,\n\t\t\t\t\t\t  zones_size);\n\t\treal_size = size - zone_absent_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t  node_start_pfn, node_end_pfn,\n\t\t\t\t\t\t  zholes_size);\n\t\tif (size)\n\t\t\tzone->zone_start_pfn = zone_start_pfn;\n\t\telse\n\t\t\tzone->zone_start_pfn = 0;\n\t\tzone->spanned_pages = size;\n\t\tzone->present_pages = real_size;\n\n\t\ttotalpages += size;\n\t\trealtotalpages += real_size;\n\t}\n\n\tpgdat->node_spanned_pages = totalpages;\n\tpgdat->node_present_pages = realtotalpages;\n\tprintk(KERN_DEBUG \"On node %d totalpages: %lu\\n\", pgdat->node_id,\n\t\t\t\t\t\t\trealtotalpages);\n}\n\n#ifndef CONFIG_SPARSEMEM\n/*\n * Calculate the size of the zone->blockflags rounded to an unsigned long\n * Start by making sure zonesize is a multiple of pageblock_order by rounding\n * up. Then use 1 NR_PAGEBLOCK_BITS worth of bits per pageblock, finally\n * round what is now in bits to nearest long in bits, then return it in\n * bytes.\n */\nstatic unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)\n{\n\tunsigned long usemapsize;\n\n\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);\n\tusemapsize = roundup(zonesize, pageblock_nr_pages);\n\tusemapsize = usemapsize >> pageblock_order;\n\tusemapsize *= NR_PAGEBLOCK_BITS;\n\tusemapsize = roundup(usemapsize, 8 * sizeof(unsigned long));\n\n\treturn usemapsize / 8;\n}\n\nstatic void __ref setup_usemap(struct pglist_data *pgdat,\n\t\t\t\tstruct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\tunsigned long zonesize)\n{\n\tunsigned long usemapsize = usemap_size(zone_start_pfn, zonesize);\n\tzone->pageblock_flags = NULL;\n\tif (usemapsize)\n\t\tzone->pageblock_flags =\n\t\t\tmemblock_alloc_node_nopanic(usemapsize,\n\t\t\t\t\t\t\t pgdat->node_id);\n}\n#else\nstatic inline void setup_usemap(struct pglist_data *pgdat, struct zone *zone,\n\t\t\t\tunsigned long zone_start_pfn, unsigned long zonesize) {}\n#endif /* CONFIG_SPARSEMEM */\n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\n\n/* Initialise the number of pages represented by NR_PAGEBLOCK_BITS */\nvoid __init set_pageblock_order(void)\n{\n\tunsigned int order;\n\n\t/* Check that pageblock_nr_pages has not already been setup */\n\tif (pageblock_order)\n\t\treturn;\n\n\tif (HPAGE_SHIFT > PAGE_SHIFT)\n\t\torder = HUGETLB_PAGE_ORDER;\n\telse\n\t\torder = MAX_ORDER - 1;\n\n\t/*\n\t * Assume the largest contiguous order of interest is a huge page.\n\t * This value may be variable depending on boot parameters on IA64 and\n\t * powerpc.\n\t */\n\tpageblock_order = order;\n}\n#else /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\n/*\n * When CONFIG_HUGETLB_PAGE_SIZE_VARIABLE is not set, set_pageblock_order()\n * is unused as pageblock_order is set at compile-time. See\n * include/linux/pageblock-flags.h for the values of pageblock_order based on\n * the kernel config\n */\nvoid __init set_pageblock_order(void)\n{\n}\n\n#endif /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\nstatic unsigned long __init calc_memmap_size(unsigned long spanned_pages,\n\t\t\t\t\t\tunsigned long present_pages)\n{\n\tunsigned long pages = spanned_pages;\n\n\t/*\n\t * Provide a more accurate estimation if there are holes within\n\t * the zone and SPARSEMEM is in use. If there are holes within the\n\t * zone, each populated memory region may cost us one or two extra\n\t * memmap pages due to alignment because memmap pages for each\n\t * populated regions may not be naturally aligned on page boundary.\n\t * So the (present_pages >> 4) heuristic is a tradeoff for that.\n\t */\n\tif (spanned_pages > present_pages + (present_pages >> 4) &&\n\t    IS_ENABLED(CONFIG_SPARSEMEM))\n\t\tpages = present_pages;\n\n\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat)\n{\n\tspin_lock_init(&pgdat->split_queue_lock);\n\tINIT_LIST_HEAD(&pgdat->split_queue);\n\tpgdat->split_queue_len = 0;\n}\n#else\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat) {}\n#endif\n\n#ifdef CONFIG_COMPACTION\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat)\n{\n\tinit_waitqueue_head(&pgdat->kcompactd_wait);\n}\n#else\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n#endif\n\nstatic void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n{\n\tpgdat_resize_init(pgdat);\n\n\tpgdat_init_split_queue(pgdat);\n\tpgdat_init_kcompactd(pgdat);\n\n\tinit_waitqueue_head(&pgdat->kswapd_wait);\n\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\n\n\tpgdat_page_ext_init(pgdat);\n\tspin_lock_init(&pgdat->lru_lock);\n\tlruvec_init(node_lruvec(pgdat));\n}\n\nstatic void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,\n\t\t\t\t\t\t\tunsigned long remaining_pages)\n{\n\tzone->managed_pages = remaining_pages;\n\tzone_set_nid(zone, nid);\n\tzone->name = zone_names[idx];\n\tzone->zone_pgdat = NODE_DATA(nid);\n\tspin_lock_init(&zone->lock);\n\tzone_seqlock_init(zone);\n\tzone_pcp_init(zone);\n}\n\n/*\n * Set up the zone data structures\n * - init pgdat internals\n * - init all zones belonging to this node\n *\n * NOTE: this function is only called during memory hotplug\n */\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid __ref free_area_init_core_hotplug(int nid)\n{\n\tenum zone_type z;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tpgdat_init_internals(pgdat);\n\tfor (z = 0; z < MAX_NR_ZONES; z++)\n\t\tzone_init_internals(&pgdat->node_zones[z], z, nid, 0);\n}\n#endif\n\n/*\n * Set up the zone data structures:\n *   - mark all pages reserved\n *   - mark all memory queues empty\n *   - clear the memory bitmaps\n *\n * NOTE: pgdat should get zeroed by caller.\n * NOTE: this function is only called during early init.\n */\nstatic void __init free_area_init_core(struct pglist_data *pgdat)\n{\n\tenum zone_type j;\n\tint nid = pgdat->node_id;\n\n\tpgdat_init_internals(pgdat);\n\tpgdat->per_cpu_nodestats = &boot_nodestats;\n\n\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\tunsigned long size, freesize, memmap_pages;\n\t\tunsigned long zone_start_pfn = zone->zone_start_pfn;\n\n\t\tsize = zone->spanned_pages;\n\t\tfreesize = zone->present_pages;\n\n\t\t/*\n\t\t * Adjust freesize so that it accounts for how much memory\n\t\t * is used by this zone for memmap. This affects the watermark\n\t\t * and per-cpu initialisations\n\t\t */\n\t\tmemmap_pages = calc_memmap_size(size, freesize);\n\t\tif (!is_highmem_idx(j)) {\n\t\t\tif (freesize >= memmap_pages) {\n\t\t\t\tfreesize -= memmap_pages;\n\t\t\t\tif (memmap_pages)\n\t\t\t\t\tprintk(KERN_DEBUG\n\t\t\t\t\t       \"  %s zone: %lu pages used for memmap\\n\",\n\t\t\t\t\t       zone_names[j], memmap_pages);\n\t\t\t} else\n\t\t\t\tpr_warn(\"  %s zone: %lu pages exceeds freesize %lu\\n\",\n\t\t\t\t\tzone_names[j], memmap_pages, freesize);\n\t\t}\n\n\t\t/* Account for reserved pages */\n\t\tif (j == 0 && freesize > dma_reserve) {\n\t\t\tfreesize -= dma_reserve;\n\t\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages reserved\\n\",\n\t\t\t\t\tzone_names[0], dma_reserve);\n\t\t}\n\n\t\tif (!is_highmem_idx(j))\n\t\t\tnr_kernel_pages += freesize;\n\t\t/* Charge for highmem memmap if there are enough kernel pages */\n\t\telse if (nr_kernel_pages > memmap_pages * 2)\n\t\t\tnr_kernel_pages -= memmap_pages;\n\t\tnr_all_pages += freesize;\n\n\t\t/*\n\t\t * Set an approximate value for lowmem here, it will be adjusted\n\t\t * when the bootmem allocator frees pages into the buddy system.\n\t\t * And all highmem pages will be managed by the buddy system.\n\t\t */\n\t\tzone_init_internals(zone, j, nid, freesize);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\n\t\tset_pageblock_order();\n\t\tsetup_usemap(pgdat, zone, zone_start_pfn, size);\n\t\tinit_currently_empty_zone(zone, zone_start_pfn, size);\n\t\tmemmap_init(size, nid, j, zone_start_pfn);\n\t}\n}\n\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat)\n{\n\tunsigned long __maybe_unused start = 0;\n\tunsigned long __maybe_unused offset = 0;\n\n\t/* Skip empty nodes */\n\tif (!pgdat->node_spanned_pages)\n\t\treturn;\n\n\tstart = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);\n\toffset = pgdat->node_start_pfn - start;\n\t/* ia64 gets its own node_mem_map, before this, without bootmem */\n\tif (!pgdat->node_mem_map) {\n\t\tunsigned long size, end;\n\t\tstruct page *map;\n\n\t\t/*\n\t\t * The zone's endpoints aren't required to be MAX_ORDER\n\t\t * aligned but the node_mem_map endpoints must be in order\n\t\t * for the buddy allocator to function correctly.\n\t\t */\n\t\tend = pgdat_end_pfn(pgdat);\n\t\tend = ALIGN(end, MAX_ORDER_NR_PAGES);\n\t\tsize =  (end - start) * sizeof(struct page);\n\t\tmap = memblock_alloc_node_nopanic(size, pgdat->node_id);\n\t\tpgdat->node_mem_map = map + offset;\n\t}\n\tpr_debug(\"%s: node %d, pgdat %08lx, node_mem_map %08lx\\n\",\n\t\t\t\t__func__, pgdat->node_id, (unsigned long)pgdat,\n\t\t\t\t(unsigned long)pgdat->node_mem_map);\n#ifndef CONFIG_NEED_MULTIPLE_NODES\n\t/*\n\t * With no DISCONTIG, the global mem_map is just set as node 0's\n\t */\n\tif (pgdat == NODE_DATA(0)) {\n\t\tmem_map = NODE_DATA(0)->node_mem_map;\n#if defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) || defined(CONFIG_FLATMEM)\n\t\tif (page_to_pfn(mem_map) != pgdat->node_start_pfn)\n\t\t\tmem_map -= offset;\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\t}\n#endif\n}\n#else\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }\n#endif /* CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat)\n{\n\t/*\n\t * We start only with one section of pages, more pages are added as\n\t * needed until the rest of deferred pages are initialized.\n\t */\n\tpgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,\n\t\t\t\t\t\tpgdat->node_spanned_pages);\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n}\n#else\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}\n#endif\n\nvoid __init free_area_init_node(int nid, unsigned long *zones_size,\n\t\t\t\t   unsigned long node_start_pfn,\n\t\t\t\t   unsigned long *zholes_size)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tunsigned long start_pfn = 0;\n\tunsigned long end_pfn = 0;\n\n\t/* pg_data_t should be reset to zero when it's allocated */\n\tWARN_ON(pgdat->nr_zones || pgdat->kswapd_classzone_idx);\n\n\tpgdat->node_id = nid;\n\tpgdat->node_start_pfn = node_start_pfn;\n\tpgdat->per_cpu_nodestats = NULL;\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);\n\tpr_info(\"Initmem setup node %d [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\tend_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);\n#else\n\tstart_pfn = node_start_pfn;\n#endif\n\tcalculate_node_totalpages(pgdat, start_pfn, end_pfn,\n\t\t\t\t  zones_size, zholes_size);\n\n\talloc_node_mem_map(pgdat);\n\tpgdat_set_deferred_range(pgdat);\n\n\tfree_area_init_core(pgdat);\n}\n\n#if !defined(CONFIG_FLAT_NODE_MEM_MAP)\n/*\n * Zero all valid struct pages in range [spfn, epfn), return number of struct\n * pages zeroed\n */\nstatic u64 zero_pfn_range(unsigned long spfn, unsigned long epfn)\n{\n\tunsigned long pfn;\n\tu64 pgcnt = 0;\n\n\tfor (pfn = spfn; pfn < epfn; pfn++) {\n\t\tif (!pfn_valid(ALIGN_DOWN(pfn, pageblock_nr_pages))) {\n\t\t\tpfn = ALIGN_DOWN(pfn, pageblock_nr_pages)\n\t\t\t\t+ pageblock_nr_pages - 1;\n\t\t\tcontinue;\n\t\t}\n\t\tmm_zero_struct_page(pfn_to_page(pfn));\n\t\tpgcnt++;\n\t}\n\n\treturn pgcnt;\n}\n\n/*\n * Only struct pages that are backed by physical memory are zeroed and\n * initialized by going through __init_single_page(). But, there are some\n * struct pages which are reserved in memblock allocator and their fields\n * may be accessed (for example page_to_pfn() on some configuration accesses\n * flags). We must explicitly zero those struct pages.\n *\n * This function also addresses a similar issue where struct pages are left\n * uninitialized because the physical address range is not covered by\n * memblock.memory or memblock.reserved. That could happen when memblock\n * layout is manually configured via memmap=.\n */\nvoid __init zero_resv_unavail(void)\n{\n\tphys_addr_t start, end;\n\tu64 i, pgcnt;\n\tphys_addr_t next = 0;\n\n\t/*\n\t * Loop through unavailable ranges not covered by memblock.memory.\n\t */\n\tpgcnt = 0;\n\tfor_each_mem_range(i, &memblock.memory, NULL,\n\t\t\tNUMA_NO_NODE, MEMBLOCK_NONE, &start, &end, NULL) {\n\t\tif (next < start)\n\t\t\tpgcnt += zero_pfn_range(PFN_DOWN(next), PFN_UP(start));\n\t\tnext = end;\n\t}\n\tpgcnt += zero_pfn_range(PFN_DOWN(next), max_pfn);\n\n\t/*\n\t * Struct pages that do not have backing memory. This could be because\n\t * firmware is using some of this memory, or for some other reasons.\n\t */\n\tif (pgcnt)\n\t\tpr_info(\"Zeroed struct page in unavailable ranges: %lld pages\", pgcnt);\n}\n#endif /* !CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP\n\n#if MAX_NUMNODES > 1\n/*\n * Figure out the number of possible node ids.\n */\nvoid __init setup_nr_node_ids(void)\n{\n\tunsigned int highest;\n\n\thighest = find_last_bit(node_possible_map.bits, MAX_NUMNODES);\n\tnr_node_ids = highest + 1;\n}\n#endif\n\n/**\n * node_map_pfn_alignment - determine the maximum internode alignment\n *\n * This function should be called after node map is populated and sorted.\n * It calculates the maximum power of two alignment which can distinguish\n * all the nodes.\n *\n * For example, if all nodes are 1GiB and aligned to 1GiB, the return value\n * would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the\n * nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is\n * shifted, 1GiB is enough and this function will indicate so.\n *\n * This is used to test whether pfn -> nid mapping of the chosen memory\n * model has fine enough granularity to avoid incorrect mapping for the\n * populated node map.\n *\n * Returns the determined alignment in pfn's.  0 if there is no alignment\n * requirement (single node).\n */\nunsigned long __init node_map_pfn_alignment(void)\n{\n\tunsigned long accl_mask = 0, last_end = 0;\n\tunsigned long start, end, mask;\n\tint last_nid = -1;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid) {\n\t\tif (!start || last_nid < 0 || last_nid == nid) {\n\t\t\tlast_nid = nid;\n\t\t\tlast_end = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Start with a mask granular enough to pin-point to the\n\t\t * start pfn and tick off bits one-by-one until it becomes\n\t\t * too coarse to separate the current node from the last.\n\t\t */\n\t\tmask = ~((1 << __ffs(start)) - 1);\n\t\twhile (mask && last_end <= (start & (mask << 1)))\n\t\t\tmask <<= 1;\n\n\t\t/* accumulate all internode masks */\n\t\taccl_mask |= mask;\n\t}\n\n\t/* convert mask to number of pages */\n\treturn ~accl_mask + 1;\n}\n\n/* Find the lowest pfn for a node */\nstatic unsigned long __init find_min_pfn_for_node(int nid)\n{\n\tunsigned long min_pfn = ULONG_MAX;\n\tunsigned long start_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, NULL, NULL)\n\t\tmin_pfn = min(min_pfn, start_pfn);\n\n\tif (min_pfn == ULONG_MAX) {\n\t\tpr_warn(\"Could not find start_pfn for node %d\\n\", nid);\n\t\treturn 0;\n\t}\n\n\treturn min_pfn;\n}\n\n/**\n * find_min_pfn_with_active_regions - Find the minimum PFN registered\n *\n * It returns the minimum PFN based on information provided via\n * memblock_set_node().\n */\nunsigned long __init find_min_pfn_with_active_regions(void)\n{\n\treturn find_min_pfn_for_node(MAX_NUMNODES);\n}\n\n/*\n * early_calculate_totalpages()\n * Sum pages in active regions for movable zone.\n * Populate N_MEMORY for calculating usable_nodes.\n */\nstatic unsigned long __init early_calculate_totalpages(void)\n{\n\tunsigned long totalpages = 0;\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tunsigned long pages = end_pfn - start_pfn;\n\n\t\ttotalpages += pages;\n\t\tif (pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t}\n\treturn totalpages;\n}\n\n/*\n * Find the PFN the Movable zone begins in each node. Kernel memory\n * is spread evenly between nodes as long as the nodes have enough\n * memory. When they don't, some nodes will have more kernelcore than\n * others\n */\nstatic void __init find_zone_movable_pfns_for_nodes(void)\n{\n\tint i, nid;\n\tunsigned long usable_startpfn;\n\tunsigned long kernelcore_node, kernelcore_remaining;\n\t/* save the state before borrow the nodemask */\n\tnodemask_t saved_node_state = node_states[N_MEMORY];\n\tunsigned long totalpages = early_calculate_totalpages();\n\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);\n\tstruct memblock_region *r;\n\n\t/* Need to find movable_zone earlier when movable_node is specified. */\n\tfind_usable_zone_for_movable();\n\n\t/*\n\t * If movable_node is specified, ignore kernelcore and movablecore\n\t * options.\n\t */\n\tif (movable_node_is_enabled()) {\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (!memblock_is_hotpluggable(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = PFN_DOWN(r->base);\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=mirror is specified, ignore movablecore option\n\t */\n\tif (mirrored_kernelcore) {\n\t\tbool mem_below_4gb_not_mirrored = false;\n\n\t\tfor_each_memblock(memory, r) {\n\t\t\tif (memblock_is_mirror(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = r->nid;\n\n\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);\n\n\t\t\tif (usable_startpfn < 0x100000) {\n\t\t\t\tmem_below_4gb_not_mirrored = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tif (mem_below_4gb_not_mirrored)\n\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\");\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=nn% or movablecore=nn% was specified, calculate the\n\t * amount of necessary memory.\n\t */\n\tif (required_kernelcore_percent)\n\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /\n\t\t\t\t       10000UL;\n\tif (required_movablecore_percent)\n\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /\n\t\t\t\t\t10000UL;\n\n\t/*\n\t * If movablecore= was specified, calculate what size of\n\t * kernelcore that corresponds so that memory usable for\n\t * any allocation type is evenly spread. If both kernelcore\n\t * and movablecore are specified, then the value of kernelcore\n\t * will be used for required_kernelcore if it's greater than\n\t * what movablecore would have allowed.\n\t */\n\tif (required_movablecore) {\n\t\tunsigned long corepages;\n\n\t\t/*\n\t\t * Round-up so that ZONE_MOVABLE is at least as large as what\n\t\t * was requested by the user\n\t\t */\n\t\trequired_movablecore =\n\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);\n\t\trequired_movablecore = min(totalpages, required_movablecore);\n\t\tcorepages = totalpages - required_movablecore;\n\n\t\trequired_kernelcore = max(required_kernelcore, corepages);\n\t}\n\n\t/*\n\t * If kernelcore was not specified or kernelcore size is larger\n\t * than totalpages, there is no ZONE_MOVABLE.\n\t */\n\tif (!required_kernelcore || required_kernelcore >= totalpages)\n\t\tgoto out;\n\n\t/* usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */\n\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];\n\nrestart:\n\t/* Spread kernelcore memory as evenly as possible throughout nodes */\n\tkernelcore_node = required_kernelcore / usable_nodes;\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\t/*\n\t\t * Recalculate kernelcore_node if the division per node\n\t\t * now exceeds what is necessary to satisfy the requested\n\t\t * amount of memory for the kernel\n\t\t */\n\t\tif (required_kernelcore < kernelcore_node)\n\t\t\tkernelcore_node = required_kernelcore / usable_nodes;\n\n\t\t/*\n\t\t * As the map is walked, we track how much memory is usable\n\t\t * by the kernel using kernelcore_remaining. When it is\n\t\t * 0, the rest of the node is usable by ZONE_MOVABLE\n\t\t */\n\t\tkernelcore_remaining = kernelcore_node;\n\n\t\t/* Go through each range of PFNs within this node */\n\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\t\tunsigned long size_pages;\n\n\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);\n\t\t\tif (start_pfn >= end_pfn)\n\t\t\t\tcontinue;\n\n\t\t\t/* Account for what is only usable for kernelcore */\n\t\t\tif (start_pfn < usable_startpfn) {\n\t\t\t\tunsigned long kernel_pages;\n\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)\n\t\t\t\t\t\t\t\t- start_pfn;\n\n\t\t\t\tkernelcore_remaining -= min(kernel_pages,\n\t\t\t\t\t\t\tkernelcore_remaining);\n\t\t\t\trequired_kernelcore -= min(kernel_pages,\n\t\t\t\t\t\t\trequired_kernelcore);\n\n\t\t\t\t/* Continue if range is now fully accounted */\n\t\t\t\tif (end_pfn <= usable_startpfn) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Push zone_movable_pfn to the end so\n\t\t\t\t\t * that if we have to rebalance\n\t\t\t\t\t * kernelcore across nodes, we will\n\t\t\t\t\t * not double account here\n\t\t\t\t\t */\n\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstart_pfn = usable_startpfn;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * The usable PFN range for ZONE_MOVABLE is from\n\t\t\t * start_pfn->end_pfn. Calculate size_pages as the\n\t\t\t * number of pages used as kernelcore\n\t\t\t */\n\t\t\tsize_pages = end_pfn - start_pfn;\n\t\t\tif (size_pages > kernelcore_remaining)\n\t\t\t\tsize_pages = kernelcore_remaining;\n\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;\n\n\t\t\t/*\n\t\t\t * Some kernelcore has been met, update counts and\n\t\t\t * break if the kernelcore for this node has been\n\t\t\t * satisfied\n\t\t\t */\n\t\t\trequired_kernelcore -= min(required_kernelcore,\n\t\t\t\t\t\t\t\tsize_pages);\n\t\t\tkernelcore_remaining -= size_pages;\n\t\t\tif (!kernelcore_remaining)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * If there is still required_kernelcore, we do another pass with one\n\t * less node in the count. This will push zone_movable_pfn[nid] further\n\t * along on the nodes that still have memory until kernelcore is\n\t * satisfied\n\t */\n\tusable_nodes--;\n\tif (usable_nodes && required_kernelcore > usable_nodes)\n\t\tgoto restart;\n\nout2:\n\t/* Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */\n\tfor (nid = 0; nid < MAX_NUMNODES; nid++)\n\t\tzone_movable_pfn[nid] =\n\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);\n\nout:\n\t/* restore the node_state */\n\tnode_states[N_MEMORY] = saved_node_state;\n}\n\n/* Any regular or high memory on that node ? */\nstatic void check_for_memory(pg_data_t *pgdat, int nid)\n{\n\tenum zone_type zone_type;\n\n\tfor (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\t\tif (populated_zone(zone)) {\n\t\t\tif (IS_ENABLED(CONFIG_HIGHMEM))\n\t\t\t\tnode_set_state(nid, N_HIGH_MEMORY);\n\t\t\tif (zone_type <= ZONE_NORMAL)\n\t\t\t\tnode_set_state(nid, N_NORMAL_MEMORY);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/**\n * free_area_init_nodes - Initialise all pg_data_t and zone data\n * @max_zone_pfn: an array of max PFNs for each zone\n *\n * This will call free_area_init_node() for each active node in the system.\n * Using the page ranges provided by memblock_set_node(), the size of each\n * zone in each node and their holes is calculated. If the maximum PFN\n * between two adjacent zones match, it is assumed that the zone is empty.\n * For example, if arch_max_dma_pfn == arch_max_dma32_pfn, it is assumed\n * that arch_max_dma32_pfn has no pages. It is also assumed that a zone\n * starts where the previous one ended. For example, ZONE_DMA32 starts\n * at arch_max_dma_pfn.\n */\nvoid __init free_area_init_nodes(unsigned long *max_zone_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\t/* Record where the zone boundaries are */\n\tmemset(arch_zone_lowest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_lowest_possible_pfn));\n\tmemset(arch_zone_highest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_highest_possible_pfn));\n\n\tstart_pfn = find_min_pfn_with_active_regions();\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tend_pfn = max(max_zone_pfn[i], start_pfn);\n\t\tarch_zone_lowest_possible_pfn[i] = start_pfn;\n\t\tarch_zone_highest_possible_pfn[i] = end_pfn;\n\n\t\tstart_pfn = end_pfn;\n\t}\n\n\t/* Find the PFNs that ZONE_MOVABLE begins at in each node */\n\tmemset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));\n\tfind_zone_movable_pfns_for_nodes();\n\n\t/* Print out the zone ranges */\n\tpr_info(\"Zone ranges:\\n\");\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\t\tpr_info(\"  %-8s \", zone_names[i]);\n\t\tif (arch_zone_lowest_possible_pfn[i] ==\n\t\t\t\tarch_zone_highest_possible_pfn[i])\n\t\t\tpr_cont(\"empty\\n\");\n\t\telse\n\t\t\tpr_cont(\"[mem %#018Lx-%#018Lx]\\n\",\n\t\t\t\t(u64)arch_zone_lowest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT,\n\t\t\t\t((u64)arch_zone_highest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT) - 1);\n\t}\n\n\t/* Print out the PFNs ZONE_MOVABLE begins at in each node */\n\tpr_info(\"Movable zone start for each node\\n\");\n\tfor (i = 0; i < MAX_NUMNODES; i++) {\n\t\tif (zone_movable_pfn[i])\n\t\t\tpr_info(\"  Node %d: %#018Lx\\n\", i,\n\t\t\t       (u64)zone_movable_pfn[i] << PAGE_SHIFT);\n\t}\n\n\t/* Print out the early node map */\n\tpr_info(\"Early memory node ranges\\n\");\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid)\n\t\tpr_info(\"  node %3d: [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\t\t((u64)end_pfn << PAGE_SHIFT) - 1);\n\n\t/* Initialise every node */\n\tmminit_verify_pageflags_layout();\n\tsetup_nr_node_ids();\n\tzero_resv_unavail();\n\tfor_each_online_node(nid) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tfree_area_init_node(nid, NULL,\n\t\t\t\tfind_min_pfn_for_node(nid), NULL);\n\n\t\t/* Any memory on that node */\n\t\tif (pgdat->node_present_pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t\tcheck_for_memory(pgdat, nid);\n\t}\n}\n\nstatic int __init cmdline_parse_core(char *p, unsigned long *core,\n\t\t\t\t     unsigned long *percent)\n{\n\tunsigned long long coremem;\n\tchar *endptr;\n\n\tif (!p)\n\t\treturn -EINVAL;\n\n\t/* Value may be a percentage of total memory, otherwise bytes */\n\tcoremem = simple_strtoull(p, &endptr, 0);\n\tif (*endptr == '%') {\n\t\t/* Paranoid check for percent values greater than 100 */\n\t\tWARN_ON(coremem > 100);\n\n\t\t*percent = coremem;\n\t} else {\n\t\tcoremem = memparse(p, &p);\n\t\t/* Paranoid check that UL is enough for the coremem value */\n\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);\n\n\t\t*core = coremem >> PAGE_SHIFT;\n\t\t*percent = 0UL;\n\t}\n\treturn 0;\n}\n\n/*\n * kernelcore=size sets the amount of memory for use for allocations that\n * cannot be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_kernelcore(char *p)\n{\n\t/* parse kernelcore=mirror */\n\tif (parse_option_str(p, \"mirror\")) {\n\t\tmirrored_kernelcore = true;\n\t\treturn 0;\n\t}\n\n\treturn cmdline_parse_core(p, &required_kernelcore,\n\t\t\t\t  &required_kernelcore_percent);\n}\n\n/*\n * movablecore=size sets the amount of memory for use for allocations that\n * can be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_movablecore(char *p)\n{\n\treturn cmdline_parse_core(p, &required_movablecore,\n\t\t\t\t  &required_movablecore_percent);\n}\n\nearly_param(\"kernelcore\", cmdline_parse_kernelcore);\nearly_param(\"movablecore\", cmdline_parse_movablecore);\n\n#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */\n\nvoid adjust_managed_page_count(struct page *page, long count)\n{\n\tspin_lock(&managed_page_count_lock);\n\tpage_zone(page)->managed_pages += count;\n\ttotalram_pages += count;\n#ifdef CONFIG_HIGHMEM\n\tif (PageHighMem(page))\n\t\ttotalhigh_pages += count;\n#endif\n\tspin_unlock(&managed_page_count_lock);\n}\nEXPORT_SYMBOL(adjust_managed_page_count);\n\nunsigned long free_reserved_area(void *start, void *end, int poison, char *s)\n{\n\tvoid *pos;\n\tunsigned long pages = 0;\n\n\tstart = (void *)PAGE_ALIGN((unsigned long)start);\n\tend = (void *)((unsigned long)end & PAGE_MASK);\n\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {\n\t\tstruct page *page = virt_to_page(pos);\n\t\tvoid *direct_map_addr;\n\n\t\t/*\n\t\t * 'direct_map_addr' might be different from 'pos'\n\t\t * because some architectures' virt_to_page()\n\t\t * work with aliases.  Getting the direct map\n\t\t * address ensures that we get a _writeable_\n\t\t * alias for the memset().\n\t\t */\n\t\tdirect_map_addr = page_address(page);\n\t\tif ((unsigned int)poison <= 0xFF)\n\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);\n\n\t\tfree_reserved_page(page);\n\t}\n\n\tif (pages && s)\n\t\tpr_info(\"Freeing %s memory: %ldK\\n\",\n\t\t\ts, pages << (PAGE_SHIFT - 10));\n\n\treturn pages;\n}\nEXPORT_SYMBOL(free_reserved_area);\n\n#ifdef\tCONFIG_HIGHMEM\nvoid free_highmem_page(struct page *page)\n{\n\t__free_reserved_page(page);\n\ttotalram_pages++;\n\tpage_zone(page)->managed_pages++;\n\ttotalhigh_pages++;\n}\n#endif\n\n\nvoid __init mem_init_print_info(const char *str)\n{\n\tunsigned long physpages, codesize, datasize, rosize, bss_size;\n\tunsigned long init_code_size, init_data_size;\n\n\tphyspages = get_num_physpages();\n\tcodesize = _etext - _stext;\n\tdatasize = _edata - _sdata;\n\trosize = __end_rodata - __start_rodata;\n\tbss_size = __bss_stop - __bss_start;\n\tinit_data_size = __init_end - __init_begin;\n\tinit_code_size = _einittext - _sinittext;\n\n\t/*\n\t * Detect special cases and adjust section sizes accordingly:\n\t * 1) .init.* may be embedded into .data sections\n\t * 2) .init.text.* may be out of [__init_begin, __init_end],\n\t *    please refer to arch/tile/kernel/vmlinux.lds.S.\n\t * 3) .rodata.* may be embedded into .text or .data sections.\n\t */\n#define adj_init_size(start, end, size, pos, adj) \\\n\tdo { \\\n\t\tif (start <= pos && pos < end && size > adj) \\\n\t\t\tsize -= adj; \\\n\t} while (0)\n\n\tadj_init_size(__init_begin, __init_end, init_data_size,\n\t\t     _sinittext, init_code_size);\n\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);\n\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);\n\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);\n\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);\n\n#undef\tadj_init_size\n\n\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"\n#ifdef\tCONFIG_HIGHMEM\n\t\t\", %luK highmem\"\n#endif\n\t\t\"%s%s)\\n\",\n\t\tnr_free_pages() << (PAGE_SHIFT - 10),\n\t\tphyspages << (PAGE_SHIFT - 10),\n\t\tcodesize >> 10, datasize >> 10, rosize >> 10,\n\t\t(init_data_size + init_code_size) >> 10, bss_size >> 10,\n\t\t(physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT - 10),\n\t\ttotalcma_pages << (PAGE_SHIFT - 10),\n#ifdef\tCONFIG_HIGHMEM\n\t\ttotalhigh_pages << (PAGE_SHIFT - 10),\n#endif\n\t\tstr ? \", \" : \"\", str ? str : \"\");\n}\n\n/**\n * set_dma_reserve - set the specified number of pages reserved in the first zone\n * @new_dma_reserve: The number of pages to mark reserved\n *\n * The per-cpu batchsize and zone watermarks are determined by managed_pages.\n * In the DMA zone, a significant percentage may be consumed by kernel image\n * and other unfreeable allocations which can skew the watermarks badly. This\n * function may optionally be used to account for unfreeable pages in the\n * first zone (e.g., ZONE_DMA). The effect will be lower watermarks and\n * smaller per-cpu batchsize.\n */\nvoid __init set_dma_reserve(unsigned long new_dma_reserve)\n{\n\tdma_reserve = new_dma_reserve;\n}\n\nvoid __init free_area_init(unsigned long *zones_size)\n{\n\tzero_resv_unavail();\n\tfree_area_init_node(0, zones_size,\n\t\t\t__pa(PAGE_OFFSET) >> PAGE_SHIFT, NULL);\n}\n\nstatic int page_alloc_cpu_dead(unsigned int cpu)\n{\n\n\tlru_add_drain_cpu(cpu);\n\tdrain_pages(cpu);\n\n\t/*\n\t * Spill the event counters of the dead processor\n\t * into the current processors event counters.\n\t * This artificially elevates the count of the current\n\t * processor.\n\t */\n\tvm_events_fold_cpu(cpu);\n\n\t/*\n\t * Zero the differential counters of the dead processor\n\t * so that the vm statistics are consistent.\n\t *\n\t * This is only okay since the processor is dead and cannot\n\t * race with what we are doing.\n\t */\n\tcpu_vm_stats_fold(cpu);\n\treturn 0;\n}\n\nvoid __init page_alloc_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC_DEAD,\n\t\t\t\t\t\"mm/page_alloc:dead\", NULL,\n\t\t\t\t\tpage_alloc_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n\n/*\n * calculate_totalreserve_pages - called when sysctl_lowmem_reserve_ratio\n *\tor min_free_kbytes changes.\n */\nstatic void calculate_totalreserve_pages(void)\n{\n\tstruct pglist_data *pgdat;\n\tunsigned long reserve_pages = 0;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\n\t\tpgdat->totalreserve_pages = 0;\n\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\t\tlong max = 0;\n\n\t\t\t/* Find valid and maximum lowmem_reserve in the zone */\n\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {\n\t\t\t\tif (zone->lowmem_reserve[j] > max)\n\t\t\t\t\tmax = zone->lowmem_reserve[j];\n\t\t\t}\n\n\t\t\t/* we treat the high watermark as reserved pages. */\n\t\t\tmax += high_wmark_pages(zone);\n\n\t\t\tif (max > zone->managed_pages)\n\t\t\t\tmax = zone->managed_pages;\n\n\t\t\tpgdat->totalreserve_pages += max;\n\n\t\t\treserve_pages += max;\n\t\t}\n\t}\n\ttotalreserve_pages = reserve_pages;\n}\n\n/*\n * setup_per_zone_lowmem_reserve - called whenever\n *\tsysctl_lowmem_reserve_ratio changes.  Ensures that each zone\n *\thas a correct pages reserved value, so an adequate number of\n *\tpages are left in the zone after a successful __alloc_pages().\n */\nstatic void setup_per_zone_lowmem_reserve(void)\n{\n\tstruct pglist_data *pgdat;\n\tenum zone_type j, idx;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\t\tunsigned long managed_pages = zone->managed_pages;\n\n\t\t\tzone->lowmem_reserve[j] = 0;\n\n\t\t\tidx = j;\n\t\t\twhile (idx) {\n\t\t\t\tstruct zone *lower_zone;\n\n\t\t\t\tidx--;\n\t\t\t\tlower_zone = pgdat->node_zones + idx;\n\n\t\t\t\tif (sysctl_lowmem_reserve_ratio[idx] < 1) {\n\t\t\t\t\tsysctl_lowmem_reserve_ratio[idx] = 0;\n\t\t\t\t\tlower_zone->lowmem_reserve[j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\tlower_zone->lowmem_reserve[j] =\n\t\t\t\t\t\tmanaged_pages / sysctl_lowmem_reserve_ratio[idx];\n\t\t\t\t}\n\t\t\t\tmanaged_pages += lower_zone->managed_pages;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\nstatic void __setup_per_zone_wmarks(void)\n{\n\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);\n\tunsigned long lowmem_pages = 0;\n\tstruct zone *zone;\n\tunsigned long flags;\n\n\t/* Calculate total number of !ZONE_HIGHMEM pages */\n\tfor_each_zone(zone) {\n\t\tif (!is_highmem(zone))\n\t\t\tlowmem_pages += zone->managed_pages;\n\t}\n\n\tfor_each_zone(zone) {\n\t\tu64 tmp;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\ttmp = (u64)pages_min * zone->managed_pages;\n\t\tdo_div(tmp, lowmem_pages);\n\t\tif (is_highmem(zone)) {\n\t\t\t/*\n\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't\n\t\t\t * need highmem pages, so cap pages_min to a small\n\t\t\t * value here.\n\t\t\t *\n\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)\n\t\t\t * deltas control asynch page reclaim, and so should\n\t\t\t * not be capped for highmem.\n\t\t\t */\n\t\t\tunsigned long min_pages;\n\n\t\t\tmin_pages = zone->managed_pages / 1024;\n\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);\n\t\t\tzone->watermark[WMARK_MIN] = min_pages;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If it's a lowmem zone, reserve a number of pages\n\t\t\t * proportionate to the zone's size.\n\t\t\t */\n\t\t\tzone->watermark[WMARK_MIN] = tmp;\n\t\t}\n\n\t\t/*\n\t\t * Set the kswapd watermarks distance according to the\n\t\t * scale factor in proportion to available memory, but\n\t\t * ensure a minimum size on small systems.\n\t\t */\n\t\ttmp = max_t(u64, tmp >> 2,\n\t\t\t    mult_frac(zone->managed_pages,\n\t\t\t\t      watermark_scale_factor, 10000));\n\n\t\tzone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n\t\tzone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;\n\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\n/**\n * setup_per_zone_wmarks - called when min_free_kbytes changes\n * or when memory is hot-{added|removed}\n *\n * Ensures that the watermark[min,low,high] values for each zone are set\n * correctly with respect to min_free_kbytes.\n */\nvoid setup_per_zone_wmarks(void)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\t__setup_per_zone_wmarks();\n\tspin_unlock(&lock);\n}\n\n/*\n * Initialise min_free_kbytes.\n *\n * For small machines we want it small (128k min).  For large machines\n * we want it large (64MB max).  But it is not linear, because network\n * bandwidth does not increase linearly with machine size.  We use\n *\n *\tmin_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:\n *\tmin_free_kbytes = sqrt(lowmem_kbytes * 16)\n *\n * which yields\n *\n * 16MB:\t512k\n * 32MB:\t724k\n * 64MB:\t1024k\n * 128MB:\t1448k\n * 256MB:\t2048k\n * 512MB:\t2896k\n * 1024MB:\t4096k\n * 2048MB:\t5792k\n * 4096MB:\t8192k\n * 8192MB:\t11584k\n * 16384MB:\t16384k\n */\nint __meminit init_per_zone_wmark_min(void)\n{\n\tunsigned long lowmem_kbytes;\n\tint new_min_free_kbytes;\n\n\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);\n\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);\n\n\tif (new_min_free_kbytes > user_min_free_kbytes) {\n\t\tmin_free_kbytes = new_min_free_kbytes;\n\t\tif (min_free_kbytes < 128)\n\t\t\tmin_free_kbytes = 128;\n\t\tif (min_free_kbytes > 65536)\n\t\t\tmin_free_kbytes = 65536;\n\t} else {\n\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",\n\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);\n\t}\n\tsetup_per_zone_wmarks();\n\trefresh_zone_stat_thresholds();\n\tsetup_per_zone_lowmem_reserve();\n\n#ifdef CONFIG_NUMA\n\tsetup_min_unmapped_ratio();\n\tsetup_min_slab_ratio();\n#endif\n\n\treturn 0;\n}\ncore_initcall(init_per_zone_wmark_min)\n\n/*\n * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so\n *\tthat we can call two helper functions whenever min_free_kbytes\n *\tchanges.\n */\nint min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write) {\n\t\tuser_min_free_kbytes = min_free_kbytes;\n\t\tsetup_per_zone_wmarks();\n\t}\n\treturn 0;\n}\n\nint watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write)\n\t\tsetup_per_zone_wmarks();\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic void setup_min_unmapped_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_unmapped_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_unmapped_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_unmapped_ratio) / 100;\n}\n\n\nint sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_unmapped_ratio();\n\n\treturn 0;\n}\n\nstatic void setup_min_slab_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_slab_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_slab_pages += (zone->managed_pages *\n\t\t\t\tsysctl_min_slab_ratio) / 100;\n}\n\nint sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_slab_ratio();\n\n\treturn 0;\n}\n#endif\n\n/*\n * lowmem_reserve_ratio_sysctl_handler - just a wrapper around\n *\tproc_dointvec() so that we can call setup_per_zone_lowmem_reserve()\n *\twhenever sysctl_lowmem_reserve_ratio changes.\n *\n * The reserve ratio obviously has absolutely no relation with the\n * minimum watermarks. The lowmem reserve ratio can only make sense\n * if in function of the boot time zone sizes.\n */\nint lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tproc_dointvec_minmax(table, write, buffer, length, ppos);\n\tsetup_per_zone_lowmem_reserve();\n\treturn 0;\n}\n\n/*\n * percpu_pagelist_fraction - changes the pcp->high for each zone on each\n * cpu.  It is the fraction of total pages in each zone that a hot per cpu\n * pagelist can have before it gets flushed back to buddy allocator.\n */\nint percpu_pagelist_fraction_sysctl_handler(struct ctl_table *table, int write,\n\tvoid __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct zone *zone;\n\tint old_percpu_pagelist_fraction;\n\tint ret;\n\n\tmutex_lock(&pcp_batch_high_lock);\n\told_percpu_pagelist_fraction = percpu_pagelist_fraction;\n\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (!write || ret < 0)\n\t\tgoto out;\n\n\t/* Sanity checking to avoid pcp imbalance */\n\tif (percpu_pagelist_fraction &&\n\t    percpu_pagelist_fraction < MIN_PERCPU_PAGELIST_FRACTION) {\n\t\tpercpu_pagelist_fraction = old_percpu_pagelist_fraction;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* No change? */\n\tif (percpu_pagelist_fraction == old_percpu_pagelist_fraction)\n\t\tgoto out;\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int cpu;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tpageset_set_high_and_batch(zone,\n\t\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\t}\nout:\n\tmutex_unlock(&pcp_batch_high_lock);\n\treturn ret;\n}\n\n#ifdef CONFIG_NUMA\nint hashdist = HASHDIST_DEFAULT;\n\nstatic int __init set_hashdist(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\thashdist = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"hashdist=\", set_hashdist);\n#endif\n\n#ifndef __HAVE_ARCH_RESERVED_KERNEL_PAGES\n/*\n * Returns the number of pages that arch has reserved but\n * is not known to alloc_large_system_hash().\n */\nstatic unsigned long __init arch_reserved_kernel_pages(void)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Adaptive scale is meant to reduce sizes of hash tables on large memory\n * machines. As memory size is increased the scale is also increased but at\n * slower pace.  Starting from ADAPT_SCALE_BASE (64G), every time memory\n * quadruples the scale is increased by one, which means the size of hash table\n * only doubles, instead of quadrupling as well.\n * Because 32-bit systems cannot have large physical memory, where this scaling\n * makes sense, it is disabled on such platforms.\n */\n#if __BITS_PER_LONG > 32\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#endif\n\n/*\n * allocate a large system hash table from bootmem\n * - it is assumed that the hash table must contain an exact power-of-2\n *   quantity of entries\n * - limit is the number of hash buckets, not the total allocation size\n */\nvoid *__init alloc_large_system_hash(const char *tablename,\n\t\t\t\t     unsigned long bucketsize,\n\t\t\t\t     unsigned long numentries,\n\t\t\t\t     int scale,\n\t\t\t\t     int flags,\n\t\t\t\t     unsigned int *_hash_shift,\n\t\t\t\t     unsigned int *_hash_mask,\n\t\t\t\t     unsigned long low_limit,\n\t\t\t\t     unsigned long high_limit)\n{\n\tunsigned long long max = high_limit;\n\tunsigned long log2qty, size;\n\tvoid *table = NULL;\n\tgfp_t gfp_flags;\n\n\t/* allow the kernel cmdline to have a say */\n\tif (!numentries) {\n\t\t/* round applicable memory size up to nearest megabyte */\n\t\tnumentries = nr_kernel_pages;\n\t\tnumentries -= arch_reserved_kernel_pages();\n\n\t\t/* It isn't necessary when PAGE_SIZE >= 1MB */\n\t\tif (PAGE_SHIFT < 20)\n\t\t\tnumentries = round_up(numentries, (1<<20)/PAGE_SIZE);\n\n#if __BITS_PER_LONG > 32\n\t\tif (!high_limit) {\n\t\t\tunsigned long adapt;\n\n\t\t\tfor (adapt = ADAPT_SCALE_NPAGES; adapt < numentries;\n\t\t\t     adapt <<= ADAPT_SCALE_SHIFT)\n\t\t\t\tscale++;\n\t\t}\n#endif\n\n\t\t/* limit to 1 bucket per 2^scale bytes of low memory */\n\t\tif (scale > PAGE_SHIFT)\n\t\t\tnumentries >>= (scale - PAGE_SHIFT);\n\t\telse\n\t\t\tnumentries <<= (PAGE_SHIFT - scale);\n\n\t\t/* Make sure we've got at least a 0-order allocation.. */\n\t\tif (unlikely(flags & HASH_SMALL)) {\n\t\t\t/* Makes no sense without HASH_EARLY */\n\t\t\tWARN_ON(!(flags & HASH_EARLY));\n\t\t\tif (!(numentries >> *_hash_shift)) {\n\t\t\t\tnumentries = 1UL << *_hash_shift;\n\t\t\t\tBUG_ON(!numentries);\n\t\t\t}\n\t\t} else if (unlikely((numentries * bucketsize) < PAGE_SIZE))\n\t\t\tnumentries = PAGE_SIZE / bucketsize;\n\t}\n\tnumentries = roundup_pow_of_two(numentries);\n\n\t/* limit allocation size to 1/16 total memory by default */\n\tif (max == 0) {\n\t\tmax = ((unsigned long long)nr_all_pages << PAGE_SHIFT) >> 4;\n\t\tdo_div(max, bucketsize);\n\t}\n\tmax = min(max, 0x80000000ULL);\n\n\tif (numentries < low_limit)\n\t\tnumentries = low_limit;\n\tif (numentries > max)\n\t\tnumentries = max;\n\n\tlog2qty = ilog2(numentries);\n\n\tgfp_flags = (flags & HASH_ZERO) ? GFP_ATOMIC | __GFP_ZERO : GFP_ATOMIC;\n\tdo {\n\t\tsize = bucketsize << log2qty;\n\t\tif (flags & HASH_EARLY) {\n\t\t\tif (flags & HASH_ZERO)\n\t\t\t\ttable = memblock_alloc_nopanic(size,\n\t\t\t\t\t\t\t       SMP_CACHE_BYTES);\n\t\t\telse\n\t\t\t\ttable = memblock_alloc_raw(size,\n\t\t\t\t\t\t\t   SMP_CACHE_BYTES);\n\t\t} else if (hashdist) {\n\t\t\ttable = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\t\t} else {\n\t\t\t/*\n\t\t\t * If bucketsize is not a power-of-two, we may free\n\t\t\t * some pages at the end of hash table which\n\t\t\t * alloc_pages_exact() automatically does\n\t\t\t */\n\t\t\tif (get_order(size) < MAX_ORDER) {\n\t\t\t\ttable = alloc_pages_exact(size, gfp_flags);\n\t\t\t\tkmemleak_alloc(table, size, 1, gfp_flags);\n\t\t\t}\n\t\t}\n\t} while (!table && size > PAGE_SIZE && --log2qty);\n\n\tif (!table)\n\t\tpanic(\"Failed to allocate %s hash table\\n\", tablename);\n\n\tpr_info(\"%s hash table entries: %ld (order: %d, %lu bytes)\\n\",\n\t\ttablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size);\n\n\tif (_hash_shift)\n\t\t*_hash_shift = log2qty;\n\tif (_hash_mask)\n\t\t*_hash_mask = (1 << log2qty) - 1;\n\n\treturn table;\n}\n\n/*\n * This function checks whether pageblock includes unmovable pages or not.\n * If @count is not zero, it is okay to include less @count unmovable pages\n *\n * PageLRU check without isolation or lru_lock could race so that\n * MIGRATE_MOVABLE block might include unmovable pages. And __PageMovable\n * check without lock_page also may miss some movable non-lru pages at\n * race condition. So you can't expect this function should be exact.\n */\nbool has_unmovable_pages(struct zone *zone, struct page *page, int count,\n\t\t\t int migratetype,\n\t\t\t bool skip_hwpoisoned_pages)\n{\n\tunsigned long pfn, iter, found;\n\n\t/*\n\t * TODO we could make this much more efficient by not checking every\n\t * page in the range if we know all of them are in MOVABLE_ZONE and\n\t * that the movable zone guarantees that pages are migratable but\n\t * the later is not the case right now unfortunatelly. E.g. movablecore\n\t * can still lead to having bootmem allocations in zone_movable.\n\t */\n\n\t/*\n\t * CMA allocations (alloc_contig_range) really need to mark isolate\n\t * CMA pageblocks even when they are not movable in fact so consider\n\t * them movable here.\n\t */\n\tif (is_migrate_cma(migratetype) &&\n\t\t\tis_migrate_cma(get_pageblock_migratetype(page)))\n\t\treturn false;\n\n\tpfn = page_to_pfn(page);\n\tfor (found = 0, iter = 0; iter < pageblock_nr_pages; iter++) {\n\t\tunsigned long check = pfn + iter;\n\n\t\tif (!pfn_valid_within(check))\n\t\t\tcontinue;\n\n\t\tpage = pfn_to_page(check);\n\n\t\tif (PageReserved(page))\n\t\t\tgoto unmovable;\n\n\t\t/*\n\t\t * If the zone is movable and we have ruled out all reserved\n\t\t * pages then it should be reasonably safe to assume the rest\n\t\t * is movable.\n\t\t */\n\t\tif (zone_idx(zone) == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Hugepages are not in LRU lists, but they're movable.\n\t\t * We need not scan over tail pages bacause we don't\n\t\t * handle each tail page individually in migration.\n\t\t */\n\t\tif (PageHuge(page)) {\n\n\t\t\tif (!hugepage_migration_supported(page_hstate(page)))\n\t\t\t\tgoto unmovable;\n\n\t\t\titer = round_up(iter + 1, 1<<compound_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We can't use page_count without pin a page\n\t\t * because another CPU can free compound page.\n\t\t * This check already skips compound tails of THP\n\t\t * because their page->_refcount is zero at all time.\n\t\t */\n\t\tif (!page_ref_count(page)) {\n\t\t\tif (PageBuddy(page))\n\t\t\t\titer += (1 << page_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (skip_hwpoisoned_pages && PageHWPoison(page))\n\t\t\tcontinue;\n\n\t\tif (__PageMovable(page))\n\t\t\tcontinue;\n\n\t\tif (!PageLRU(page))\n\t\t\tfound++;\n\t\t/*\n\t\t * If there are RECLAIMABLE pages, we need to check\n\t\t * it.  But now, memory offline itself doesn't call\n\t\t * shrink_node_slabs() and it still to be fixed.\n\t\t */\n\t\t/*\n\t\t * If the page is not RAM, page_count()should be 0.\n\t\t * we don't need more check. This is an _used_ not-movable page.\n\t\t *\n\t\t * The problematic thing here is PG_reserved pages. PG_reserved\n\t\t * is set to both of a memory hole page and a _used_ kernel\n\t\t * page at boot.\n\t\t */\n\t\tif (found > count)\n\t\t\tgoto unmovable;\n\t}\n\treturn false;\nunmovable:\n\tWARN_ON_ONCE(zone_idx(zone) == ZONE_MOVABLE);\n\treturn true;\n}\n\n#if (defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || defined(CONFIG_CMA)\n\nstatic unsigned long pfn_max_align_down(unsigned long pfn)\n{\n\treturn pfn & ~(max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t     pageblock_nr_pages) - 1);\n}\n\nstatic unsigned long pfn_max_align_up(unsigned long pfn)\n{\n\treturn ALIGN(pfn, max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t\tpageblock_nr_pages));\n}\n\n/* [start, end) must belong to a single zone. */\nstatic int __alloc_contig_migrate_range(struct compact_control *cc,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\t/* This function is based on compact_zone() from compaction.c. */\n\tunsigned long nr_reclaimed;\n\tunsigned long pfn = start;\n\tunsigned int tries = 0;\n\tint ret = 0;\n\n\tmigrate_prep();\n\n\twhile (pfn < end || !list_empty(&cc->migratepages)) {\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (list_empty(&cc->migratepages)) {\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tpfn = isolate_migratepages_range(cc, pfn, end);\n\t\t\tif (!pfn) {\n\t\t\t\tret = -EINTR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttries = 0;\n\t\t} else if (++tries == 5) {\n\t\t\tret = ret < 0 ? ret : -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\tnr_reclaimed = reclaim_clean_pages_from_list(cc->zone,\n\t\t\t\t\t\t\t&cc->migratepages);\n\t\tcc->nr_migratepages -= nr_reclaimed;\n\n\t\tret = migrate_pages(&cc->migratepages, alloc_migrate_target,\n\t\t\t\t    NULL, 0, cc->mode, MR_CONTIG_RANGE);\n\t}\n\tif (ret < 0) {\n\t\tputback_movable_pages(&cc->migratepages);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n/**\n * alloc_contig_range() -- tries to allocate given range of pages\n * @start:\tstart PFN to allocate\n * @end:\tone-past-the-last PFN to allocate\n * @migratetype:\tmigratetype of the underlaying pageblocks (either\n *\t\t\t#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks\n *\t\t\tin range must have the same migratetype and it must\n *\t\t\tbe either of the two.\n * @gfp_mask:\tGFP mask to use during compaction\n *\n * The PFN range does not have to be pageblock or MAX_ORDER_NR_PAGES\n * aligned.  The PFN range must belong to a single zone.\n *\n * The first thing this routine does is attempt to MIGRATE_ISOLATE all\n * pageblocks in the range.  Once isolated, the pageblocks should not\n * be modified by others.\n *\n * Returns zero on success or negative error code.  On success all\n * pages which PFN is in [start, end) are allocated for the caller and\n * need to be freed with free_contig_range().\n */\nint alloc_contig_range(unsigned long start, unsigned long end,\n\t\t       unsigned migratetype, gfp_t gfp_mask)\n{\n\tunsigned long outer_start, outer_end;\n\tunsigned int order;\n\tint ret = 0;\n\n\tstruct compact_control cc = {\n\t\t.nr_migratepages = 0,\n\t\t.order = -1,\n\t\t.zone = page_zone(pfn_to_page(start)),\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.no_set_skip_hint = true,\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t};\n\tINIT_LIST_HEAD(&cc.migratepages);\n\n\t/*\n\t * What we do here is we mark all pageblocks in range as\n\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may\n\t * have different sizes, and due to the way page allocator\n\t * work, we align the range to biggest of the two pages so\n\t * that page allocator won't try to merge buddies from\n\t * different pageblocks and change MIGRATE_ISOLATE to some\n\t * other migration type.\n\t *\n\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we\n\t * migrate the pages from an unaligned range (ie. pages that\n\t * we are interested in).  This will put all the pages in\n\t * range back to page allocator as MIGRATE_ISOLATE.\n\t *\n\t * When this is done, we take the pages in range from page\n\t * allocator removing them from the buddy system.  This way\n\t * page allocator will never consider using them.\n\t *\n\t * This lets us mark the pageblocks back as\n\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the\n\t * aligned range but not in the unaligned, original range are\n\t * put back to page allocator so that buddy can use them.\n\t */\n\n\tret = start_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\t       pfn_max_align_up(end), migratetype,\n\t\t\t\t       false);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * In case of -EBUSY, we'd like to know which page causes problem.\n\t * So, just fall through. test_pages_isolated() has a tracepoint\n\t * which will report the busy page.\n\t *\n\t * It is possible that busy pages could become available before\n\t * the call to test_pages_isolated, and the range will actually be\n\t * allocated.  So, if we fall through be sure to clear ret so that\n\t * -EBUSY is not accidentally used or returned to caller.\n\t */\n\tret = __alloc_contig_migrate_range(&cc, start, end);\n\tif (ret && ret != -EBUSY)\n\t\tgoto done;\n\tret =0;\n\n\t/*\n\t * Pages from [start, end) are within a MAX_ORDER_NR_PAGES\n\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's\n\t * more, all pages in [start, end) are free in page allocator.\n\t * What we are going to do is to allocate all pages from\n\t * [start, end) (that is remove them from page allocator).\n\t *\n\t * The only problem is that pages at the beginning and at the\n\t * end of interesting range may be not aligned with pages that\n\t * page allocator holds, ie. they can be part of higher order\n\t * pages.  Because of this, we reserve the bigger range and\n\t * once this is done free the pages we are not interested in.\n\t *\n\t * We don't have to hold zone->lock here because the pages are\n\t * isolated thus they won't get removed from buddy.\n\t */\n\n\tlru_add_drain_all();\n\tdrain_all_pages(cc.zone);\n\n\torder = 0;\n\touter_start = start;\n\twhile (!PageBuddy(pfn_to_page(outer_start))) {\n\t\tif (++order >= MAX_ORDER) {\n\t\t\touter_start = start;\n\t\t\tbreak;\n\t\t}\n\t\touter_start &= ~0UL << order;\n\t}\n\n\tif (outer_start != start) {\n\t\torder = page_order(pfn_to_page(outer_start));\n\n\t\t/*\n\t\t * outer_start page could be small order buddy page and\n\t\t * it doesn't include start page. Adjust outer_start\n\t\t * in this case to report failed page properly\n\t\t * on tracepoint in test_pages_isolated()\n\t\t */\n\t\tif (outer_start + (1UL << order) <= start)\n\t\t\touter_start = start;\n\t}\n\n\t/* Make sure the range is really isolated. */\n\tif (test_pages_isolated(outer_start, end, false)) {\n\t\tpr_info_ratelimited(\"%s: [%lx, %lx) PFNs busy\\n\",\n\t\t\t__func__, outer_start, end);\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Grab isolated pages from freelists. */\n\touter_end = isolate_freepages_range(&cc, outer_start, end);\n\tif (!outer_end) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Free head and tail (if any) */\n\tif (start != outer_start)\n\t\tfree_contig_range(outer_start, start - outer_start);\n\tif (end != outer_end)\n\t\tfree_contig_range(end, outer_end - end);\n\ndone:\n\tundo_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\tpfn_max_align_up(end), migratetype);\n\treturn ret;\n}\n\nvoid free_contig_range(unsigned long pfn, unsigned nr_pages)\n{\n\tunsigned int count = 0;\n\n\tfor (; nr_pages--; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tcount += page_count(page) != 1;\n\t\t__free_page(page);\n\t}\n\tWARN(count != 0, \"%d pages are still in use!\\n\", count);\n}\n#endif\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n/*\n * The zone indicated has a new number of managed_pages; batch sizes and percpu\n * page high values need to be recalulated.\n */\nvoid __meminit zone_pcp_update(struct zone *zone)\n{\n\tunsigned cpu;\n\tmutex_lock(&pcp_batch_high_lock);\n\tfor_each_possible_cpu(cpu)\n\t\tpageset_set_high_and_batch(zone,\n\t\t\t\tper_cpu_ptr(zone->pageset, cpu));\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n#endif\n\nvoid zone_pcp_reset(struct zone *zone)\n{\n\tunsigned long flags;\n\tint cpu;\n\tstruct per_cpu_pageset *pset;\n\n\t/* avoid races with drain_pages()  */\n\tlocal_irq_save(flags);\n\tif (zone->pageset != &boot_pageset) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpset = per_cpu_ptr(zone->pageset, cpu);\n\t\t\tdrain_zonestat(zone, pset);\n\t\t}\n\t\tfree_percpu(zone->pageset);\n\t\tzone->pageset = &boot_pageset;\n\t}\n\tlocal_irq_restore(flags);\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n/*\n * All pages in the range must be in a single zone and isolated\n * before calling this.\n */\nvoid\n__offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tstruct page *page;\n\tstruct zone *zone;\n\tunsigned int order, i;\n\tunsigned long pfn;\n\tunsigned long flags;\n\t/* find the first valid pfn */\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++)\n\t\tif (pfn_valid(pfn))\n\t\t\tbreak;\n\tif (pfn == end_pfn)\n\t\treturn;\n\toffline_mem_sections(pfn, end_pfn);\n\tzone = page_zone(pfn_to_page(pfn));\n\tspin_lock_irqsave(&zone->lock, flags);\n\tpfn = start_pfn;\n\twhile (pfn < end_pfn) {\n\t\tif (!pfn_valid(pfn)) {\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\t\tpage = pfn_to_page(pfn);\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (unlikely(!PageBuddy(page) && PageHWPoison(page))) {\n\t\t\tpfn++;\n\t\t\tSetPageReserved(page);\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(page_count(page));\n\t\tBUG_ON(!PageBuddy(page));\n\t\torder = page_order(page);\n#ifdef CONFIG_DEBUG_VM\n\t\tpr_info(\"remove from free list %lx %d %lx\\n\",\n\t\t\tpfn, 1 << order, end_pfn);\n#endif\n\t\tlist_del(&page->lru);\n\t\trmv_page_order(page);\n\t\tzone->free_area[order].nr_free--;\n\t\tfor (i = 0; i < (1 << order); i++)\n\t\t\tSetPageReserved((page+i));\n\t\tpfn += (1 << order);\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n#endif\n\nbool is_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn order < MAX_ORDER;\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n/*\n * Set PG_hwpoison flag if a given page is confirmed to be a free page.  This\n * test is performed under the zone lock to prevent a race against page\n * allocation.\n */\nbool set_hwpoison_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\tbool hwpoisoned = false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && page_order(page_head) >= order) {\n\t\t\tif (!TestSetPageHWPoison(page))\n\t\t\t\thwpoisoned = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn hwpoisoned;\n}\n#endif"
        }
      },
      {
        "call_info": {
          "callee": "pr_err",
          "args": [
            "\"Allocation of length %lu from process %d (%s) failed\\n\"",
            "len",
            "current->pid",
            "current->comm"
          ],
          "line": 1158
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "free_page_series",
          "args": [
            "region->vm_start",
            "region->vm_top"
          ],
          "line": 1151
        },
        "resolved": true,
        "details": {
          "function_name": "free_page_series",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "601-609",
          "snippet": "static void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "atomic_long_t mmap_pages_allocated;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\natomic_long_t mmap_pages_allocated;\n\nstatic void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "vma_set_anonymous",
          "args": [
            "vma"
          ],
          "line": 1145
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "base + ret",
            "0",
            "len - ret"
          ],
          "line": 1142
        },
        "resolved": true,
        "details": {
          "function_name": "memset",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/kasan/kasan.c",
          "lines": "283-288",
          "snippet": "void *memset(void *addr, int c, size_t len)\n{\n\tcheck_memory_region((unsigned long)addr, len, true, _RET_IP_);\n\n\treturn __memset(addr, c, len);\n}",
          "includes": [
            "#include \"../slab.h\"",
            "#include \"kasan.h\"",
            "#include <linux/bug.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/types.h>",
            "#include <linux/string.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched.h>",
            "#include <linux/printk.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/memory.h>",
            "#include <linux/memblock.h>",
            "#include <linux/linkage.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/kernel.h>",
            "#include <linux/kasan.h>",
            "#include <linux/init.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);",
            "__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../slab.h\"\n#include \"kasan.h\"\n#include <linux/bug.h>\n#include <linux/vmalloc.h>\n#include <linux/types.h>\n#include <linux/string.h>\n#include <linux/stacktrace.h>\n#include <linux/slab.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched.h>\n#include <linux/printk.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/memory.h>\n#include <linux/memblock.h>\n#include <linux/linkage.h>\n#include <linux/kmemleak.h>\n#include <linux/kernel.h>\n#include <linux/kasan.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n\n__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);\n__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);\n\nvoid *memset(void *addr, int c, size_t len)\n{\n\tcheck_memory_region((unsigned long)addr, len, true, _RET_IP_);\n\n\treturn __memset(addr, c, len);\n}"
        }
      },
      {
        "call_info": {
          "callee": "kernel_read",
          "args": [
            "vma->vm_file",
            "base",
            "len",
            "&fpos"
          ],
          "line": 1136
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_add",
          "args": [
            "total",
            "&mmap_pages_allocated"
          ],
          "line": 1119
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "alloc_pages_exact",
          "args": [
            "total << PAGE_SHIFT",
            "GFP_KERNEL"
          ],
          "line": 1115
        },
        "resolved": true,
        "details": {
          "function_name": "alloc_pages_exact",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/page_alloc.c",
          "lines": "4594-4601",
          "snippet": "void *alloc_pages_exact(size_t size, gfp_t gfp_mask)\n{\n\tunsigned int order = get_order(size);\n\tunsigned long addr;\n\n\taddr = __get_free_pages(gfp_mask, order);\n\treturn make_alloc_exact(addr, order, size);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/div64.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/sections.h>",
            "#include <linux/psi.h>",
            "#include <linux/nmi.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/kthread.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/migrate.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/prefetch.h>",
            "#include <trace/events/oom.h>",
            "#include <trace/events/kmem.h>",
            "#include <linux/compaction.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/pfn.h>",
            "#include <linux/sort.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/memremap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/topology.h>",
            "#include <linux/oom.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/slab.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/suspend.h>",
            "#include <linux/module.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kernel.h>",
            "#include <linux/compiler.h>",
            "#include <linux/memblock.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/stddef.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void __free_pages_ok(struct page *page, unsigned int order);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/div64.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <linux/psi.h>\n#include <linux/nmi.h>\n#include <linux/lockdep.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/kthread.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/migrate.h>\n#include <linux/mm_inline.h>\n#include <linux/prefetch.h>\n#include <trace/events/oom.h>\n#include <trace/events/kmem.h>\n#include <linux/compaction.h>\n#include <linux/kmemleak.h>\n#include <linux/debugobjects.h>\n#include <linux/page_ext.h>\n#include <linux/page-isolation.h>\n#include <linux/fault-inject.h>\n#include <linux/backing-dev.h>\n#include <linux/pfn.h>\n#include <linux/sort.h>\n#include <linux/stop_machine.h>\n#include <linux/memremap.h>\n#include <linux/mempolicy.h>\n#include <linux/vmstat.h>\n#include <linux/vmalloc.h>\n#include <linux/nodemask.h>\n#include <linux/memory_hotplug.h>\n#include <linux/cpuset.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/topology.h>\n#include <linux/oom.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/suspend.h>\n#include <linux/module.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/memblock.h>\n#include <linux/jiffies.h>\n#include <linux/pagemap.h>\n#include <linux/interrupt.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n\nstatic void __free_pages_ok(struct page *page, unsigned int order);\n\nvoid *alloc_pages_exact(size_t size, gfp_t gfp_mask)\n{\n\tunsigned int order = get_order(size);\n\tunsigned long addr;\n\n\taddr = __get_free_pages(gfp_mask, order);\n\treturn make_alloc_exact(addr, order, size);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_order",
          "args": [
            "len"
          ],
          "line": 1107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "!(vma->vm_flags & VM_MAYSHARE)"
          ],
          "line": 1090
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "call_mmap",
          "args": [
            "vma->vm_file",
            "vma"
          ],
          "line": 1087
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint sysctl_nr_trim_pages = CONFIG_NOMMU_INITIAL_TRIM_EXCESS;\natomic_long_t mmap_pages_allocated;\n\nstatic int do_mmap_private(struct vm_area_struct *vma,\n\t\t\t   struct vm_region *region,\n\t\t\t   unsigned long len,\n\t\t\t   unsigned long capabilities)\n{\n\tunsigned long total, point;\n\tvoid *base;\n\tint ret, order;\n\n\t/* invoke the file's mapping function so that it can keep track of\n\t * shared mappings on devices or memory\n\t * - VM_MAYSHARE will be set if it may attempt to share\n\t */\n\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\tret = call_mmap(vma->vm_file, vma);\n\t\tif (ret == 0) {\n\t\t\t/* shouldn't return success if we're not sharing */\n\t\t\tBUG_ON(!(vma->vm_flags & VM_MAYSHARE));\n\t\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\t\treturn 0;\n\t\t}\n\t\tif (ret != -ENOSYS)\n\t\t\treturn ret;\n\n\t\t/* getting an ENOSYS error indicates that direct mmap isn't\n\t\t * possible (as opposed to tried but failed) so we'll try to\n\t\t * make a private copy of the data and map that instead */\n\t}\n\n\n\t/* allocate some memory to hold the mapping\n\t * - note that this may not return a page-aligned address if the object\n\t *   we're allocating is smaller than a page\n\t */\n\torder = get_order(len);\n\ttotal = 1 << order;\n\tpoint = len >> PAGE_SHIFT;\n\n\t/* we don't want to allocate a power-of-2 sized page set */\n\tif (sysctl_nr_trim_pages && total - point >= sysctl_nr_trim_pages)\n\t\ttotal = point;\n\n\tbase = alloc_pages_exact(total << PAGE_SHIFT, GFP_KERNEL);\n\tif (!base)\n\t\tgoto enomem;\n\n\tatomic_long_add(total, &mmap_pages_allocated);\n\n\tregion->vm_flags = vma->vm_flags |= VM_MAPPED_COPY;\n\tregion->vm_start = (unsigned long) base;\n\tregion->vm_end   = region->vm_start + len;\n\tregion->vm_top   = region->vm_start + (total << PAGE_SHIFT);\n\n\tvma->vm_start = region->vm_start;\n\tvma->vm_end   = region->vm_start + len;\n\n\tif (vma->vm_file) {\n\t\t/* read the contents of a file into the copy */\n\t\tloff_t fpos;\n\n\t\tfpos = vma->vm_pgoff;\n\t\tfpos <<= PAGE_SHIFT;\n\n\t\tret = kernel_read(vma->vm_file, base, len, &fpos);\n\t\tif (ret < 0)\n\t\t\tgoto error_free;\n\n\t\t/* clear the last little bit */\n\t\tif (ret < len)\n\t\t\tmemset(base + ret, 0, len - ret);\n\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\treturn 0;\n\nerror_free:\n\tfree_page_series(region->vm_start, region->vm_top);\n\tregion->vm_start = vma->vm_start = 0;\n\tregion->vm_end   = vma->vm_end = 0;\n\tregion->vm_top   = 0;\n\treturn ret;\n\nenomem:\n\tpr_err(\"Allocation of length %lu from process %d (%s) failed\\n\",\n\t       len, current->pid, current->comm);\n\tshow_free_areas(0, NULL);\n\treturn -ENOMEM;\n}"
  },
  {
    "function_name": "do_mmap_shared_file",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1052-1068",
    "snippet": "static int do_mmap_shared_file(struct vm_area_struct *vma)\n{\n\tint ret;\n\n\tret = call_mmap(vma->vm_file, vma);\n\tif (ret == 0) {\n\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\treturn 0;\n\t}\n\tif (ret != -ENOSYS)\n\t\treturn ret;\n\n\t/* getting -ENOSYS indicates that direct mmap isn't possible (as\n\t * opposed to tried but failed) so we can only give a suitable error as\n\t * it's not possible to make a private copy if MAP_SHARED was given */\n\treturn -ENODEV;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "call_mmap",
          "args": [
            "vma->vm_file",
            "vma"
          ],
          "line": 1056
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int do_mmap_shared_file(struct vm_area_struct *vma)\n{\n\tint ret;\n\n\tret = call_mmap(vma->vm_file, vma);\n\tif (ret == 0) {\n\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\treturn 0;\n\t}\n\tif (ret != -ENOSYS)\n\t\treturn ret;\n\n\t/* getting -ENOSYS indicates that direct mmap isn't possible (as\n\t * opposed to tried but failed) so we can only give a suitable error as\n\t * it's not possible to make a private copy if MAP_SHARED was given */\n\treturn -ENODEV;\n}"
  },
  {
    "function_name": "determine_vm_flags",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1014-1046",
    "snippet": "static unsigned long determine_vm_flags(struct file *file,\n\t\t\t\t\tunsigned long prot,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tunsigned long capabilities)\n{\n\tunsigned long vm_flags;\n\n\tvm_flags = calc_vm_prot_bits(prot, 0) | calc_vm_flag_bits(flags);\n\t/* vm_flags |= mm->def_flags; */\n\n\tif (!(capabilities & NOMMU_MAP_DIRECT)) {\n\t\t/* attempt to share read-only copies of mapped file chunks */\n\t\tvm_flags |= VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\t\tif (file && !(prot & PROT_WRITE))\n\t\t\tvm_flags |= VM_MAYSHARE;\n\t} else {\n\t\t/* overlay a shareable mapping on the backing device or inode\n\t\t * if possible - used for chardevs, ramfs/tmpfs/shmfs and\n\t\t * romfs/cramfs */\n\t\tvm_flags |= VM_MAYSHARE | (capabilities & NOMMU_VMFLAGS);\n\t\tif (flags & MAP_SHARED)\n\t\t\tvm_flags |= VM_SHARED;\n\t}\n\n\t/* refuse to let anyone share private mappings with this process if\n\t * it's being traced - otherwise breakpoints set in it may interfere\n\t * with another untraced process\n\t */\n\tif ((flags & MAP_PRIVATE) && current->ptrace)\n\t\tvm_flags &= ~VM_MAYSHARE;\n\n\treturn vm_flags;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "calc_vm_flag_bits",
          "args": [
            "flags"
          ],
          "line": 1021
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "calc_vm_prot_bits",
          "args": [
            "prot",
            "0"
          ],
          "line": 1021
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic unsigned long determine_vm_flags(struct file *file,\n\t\t\t\t\tunsigned long prot,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tunsigned long capabilities)\n{\n\tunsigned long vm_flags;\n\n\tvm_flags = calc_vm_prot_bits(prot, 0) | calc_vm_flag_bits(flags);\n\t/* vm_flags |= mm->def_flags; */\n\n\tif (!(capabilities & NOMMU_MAP_DIRECT)) {\n\t\t/* attempt to share read-only copies of mapped file chunks */\n\t\tvm_flags |= VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\t\tif (file && !(prot & PROT_WRITE))\n\t\t\tvm_flags |= VM_MAYSHARE;\n\t} else {\n\t\t/* overlay a shareable mapping on the backing device or inode\n\t\t * if possible - used for chardevs, ramfs/tmpfs/shmfs and\n\t\t * romfs/cramfs */\n\t\tvm_flags |= VM_MAYSHARE | (capabilities & NOMMU_VMFLAGS);\n\t\tif (flags & MAP_SHARED)\n\t\t\tvm_flags |= VM_SHARED;\n\t}\n\n\t/* refuse to let anyone share private mappings with this process if\n\t * it's being traced - otherwise breakpoints set in it may interfere\n\t * with another untraced process\n\t */\n\tif ((flags & MAP_PRIVATE) && current->ptrace)\n\t\tvm_flags &= ~VM_MAYSHARE;\n\n\treturn vm_flags;\n}"
  },
  {
    "function_name": "validate_mmap_request",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "853-1008",
    "snippet": "static int validate_mmap_request(struct file *file,\n\t\t\t\t unsigned long addr,\n\t\t\t\t unsigned long len,\n\t\t\t\t unsigned long prot,\n\t\t\t\t unsigned long flags,\n\t\t\t\t unsigned long pgoff,\n\t\t\t\t unsigned long *_capabilities)\n{\n\tunsigned long capabilities, rlen;\n\tint ret;\n\n\t/* do the simple checks first */\n\tif (flags & MAP_FIXED)\n\t\treturn -EINVAL;\n\n\tif ((flags & MAP_TYPE) != MAP_PRIVATE &&\n\t    (flags & MAP_TYPE) != MAP_SHARED)\n\t\treturn -EINVAL;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t/* Careful about overflows.. */\n\trlen = PAGE_ALIGN(len);\n\tif (!rlen || rlen > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\t/* offset overflow? */\n\tif ((pgoff + (rlen >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\tif (file) {\n\t\t/* files must support mmap */\n\t\tif (!file->f_op->mmap)\n\t\t\treturn -ENODEV;\n\n\t\t/* work out if what we've got could possibly be shared\n\t\t * - we support chardevs that provide their own \"memory\"\n\t\t * - we support files/blockdevs that are memory backed\n\t\t */\n\t\tif (file->f_op->mmap_capabilities) {\n\t\t\tcapabilities = file->f_op->mmap_capabilities(file);\n\t\t} else {\n\t\t\t/* no explicit capabilities set, so assume some\n\t\t\t * defaults */\n\t\t\tswitch (file_inode(file)->i_mode & S_IFMT) {\n\t\t\tcase S_IFREG:\n\t\t\tcase S_IFBLK:\n\t\t\t\tcapabilities = NOMMU_MAP_COPY;\n\t\t\t\tbreak;\n\n\t\t\tcase S_IFCHR:\n\t\t\t\tcapabilities =\n\t\t\t\t\tNOMMU_MAP_DIRECT |\n\t\t\t\t\tNOMMU_MAP_READ |\n\t\t\t\t\tNOMMU_MAP_WRITE;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* eliminate any capabilities that we can't support on this\n\t\t * device */\n\t\tif (!file->f_op->get_unmapped_area)\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\tif (!(file->f_mode & FMODE_CAN_READ))\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\n\t\t/* The file shall have been opened with read permission. */\n\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\treturn -EACCES;\n\n\t\tif (flags & MAP_SHARED) {\n\t\t\t/* do checks for writing, appending and locking */\n\t\t\tif ((prot & PROT_WRITE) &&\n\t\t\t    !(file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (IS_APPEND(file_inode(file)) &&\n\t\t\t    (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (locks_verify_locked(file))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we mustn't privatise shared mappings */\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\t\t} else {\n\t\t\t/* we're going to read the file into private memory we\n\t\t\t * allocate */\n\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we don't permit a private writable mapping to be\n\t\t\t * shared with the backing device */\n\t\t\tif (prot & PROT_WRITE)\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\tif (((prot & PROT_READ)  && !(capabilities & NOMMU_MAP_READ))  ||\n\t\t\t    ((prot & PROT_WRITE) && !(capabilities & NOMMU_MAP_WRITE)) ||\n\t\t\t    ((prot & PROT_EXEC)  && !(capabilities & NOMMU_MAP_EXEC))\n\t\t\t    ) {\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t\tif (flags & MAP_SHARED) {\n\t\t\t\t\tpr_warn(\"MAP_SHARED not completely supported on !MMU\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* handle executable mappings and implied executable\n\t\t * mappings */\n\t\tif (path_noexec(&file->f_path)) {\n\t\t\tif (prot & PROT_EXEC)\n\t\t\t\treturn -EPERM;\n\t\t} else if ((prot & PROT_READ) && !(prot & PROT_EXEC)) {\n\t\t\t/* handle implication of PROT_EXEC by PROT_READ */\n\t\t\tif (current->personality & READ_IMPLIES_EXEC) {\n\t\t\t\tif (capabilities & NOMMU_MAP_EXEC)\n\t\t\t\t\tprot |= PROT_EXEC;\n\t\t\t}\n\t\t} else if ((prot & PROT_READ) &&\n\t\t\t (prot & PROT_EXEC) &&\n\t\t\t !(capabilities & NOMMU_MAP_EXEC)\n\t\t\t ) {\n\t\t\t/* backing file is not executable, try to copy */\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\t} else {\n\t\t/* anonymous mappings are always memory backed and can be\n\t\t * privately mapped\n\t\t */\n\t\tcapabilities = NOMMU_MAP_COPY;\n\n\t\t/* handle PROT_EXEC implication by PROT_READ */\n\t\tif ((prot & PROT_READ) &&\n\t\t    (current->personality & READ_IMPLIES_EXEC))\n\t\t\tprot |= PROT_EXEC;\n\t}\n\n\t/* allow the security API to have its say */\n\tret = security_mmap_addr(addr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* looks okay */\n\t*_capabilities = capabilities;\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "security_mmap_addr",
          "args": [
            "addr"
          ],
          "line": 1001
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "path_noexec",
          "args": [
            "&file->f_path"
          ],
          "line": 972
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"MAP_SHARED not completely supported on !MMU\\n\""
          ],
          "line": 964
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "locks_verify_locked",
          "args": [
            "file"
          ],
          "line": 937
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_APPEND",
          "args": [
            "file_inode(file)"
          ],
          "line": 933
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "file_inode",
          "args": [
            "file"
          ],
          "line": 933
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "file_inode",
          "args": [
            "file"
          ],
          "line": 898
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "file->f_op->mmap_capabilities",
          "args": [
            "file"
          ],
          "line": 894
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "PAGE_ALIGN",
          "args": [
            "len"
          ],
          "line": 876
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic int validate_mmap_request(struct file *file,\n\t\t\t\t unsigned long addr,\n\t\t\t\t unsigned long len,\n\t\t\t\t unsigned long prot,\n\t\t\t\t unsigned long flags,\n\t\t\t\t unsigned long pgoff,\n\t\t\t\t unsigned long *_capabilities)\n{\n\tunsigned long capabilities, rlen;\n\tint ret;\n\n\t/* do the simple checks first */\n\tif (flags & MAP_FIXED)\n\t\treturn -EINVAL;\n\n\tif ((flags & MAP_TYPE) != MAP_PRIVATE &&\n\t    (flags & MAP_TYPE) != MAP_SHARED)\n\t\treturn -EINVAL;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t/* Careful about overflows.. */\n\trlen = PAGE_ALIGN(len);\n\tif (!rlen || rlen > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\t/* offset overflow? */\n\tif ((pgoff + (rlen >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\tif (file) {\n\t\t/* files must support mmap */\n\t\tif (!file->f_op->mmap)\n\t\t\treturn -ENODEV;\n\n\t\t/* work out if what we've got could possibly be shared\n\t\t * - we support chardevs that provide their own \"memory\"\n\t\t * - we support files/blockdevs that are memory backed\n\t\t */\n\t\tif (file->f_op->mmap_capabilities) {\n\t\t\tcapabilities = file->f_op->mmap_capabilities(file);\n\t\t} else {\n\t\t\t/* no explicit capabilities set, so assume some\n\t\t\t * defaults */\n\t\t\tswitch (file_inode(file)->i_mode & S_IFMT) {\n\t\t\tcase S_IFREG:\n\t\t\tcase S_IFBLK:\n\t\t\t\tcapabilities = NOMMU_MAP_COPY;\n\t\t\t\tbreak;\n\n\t\t\tcase S_IFCHR:\n\t\t\t\tcapabilities =\n\t\t\t\t\tNOMMU_MAP_DIRECT |\n\t\t\t\t\tNOMMU_MAP_READ |\n\t\t\t\t\tNOMMU_MAP_WRITE;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* eliminate any capabilities that we can't support on this\n\t\t * device */\n\t\tif (!file->f_op->get_unmapped_area)\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\tif (!(file->f_mode & FMODE_CAN_READ))\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\n\t\t/* The file shall have been opened with read permission. */\n\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\treturn -EACCES;\n\n\t\tif (flags & MAP_SHARED) {\n\t\t\t/* do checks for writing, appending and locking */\n\t\t\tif ((prot & PROT_WRITE) &&\n\t\t\t    !(file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (IS_APPEND(file_inode(file)) &&\n\t\t\t    (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (locks_verify_locked(file))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we mustn't privatise shared mappings */\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\t\t} else {\n\t\t\t/* we're going to read the file into private memory we\n\t\t\t * allocate */\n\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t/* we don't permit a private writable mapping to be\n\t\t\t * shared with the backing device */\n\t\t\tif (prot & PROT_WRITE)\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\tif (((prot & PROT_READ)  && !(capabilities & NOMMU_MAP_READ))  ||\n\t\t\t    ((prot & PROT_WRITE) && !(capabilities & NOMMU_MAP_WRITE)) ||\n\t\t\t    ((prot & PROT_EXEC)  && !(capabilities & NOMMU_MAP_EXEC))\n\t\t\t    ) {\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t\tif (flags & MAP_SHARED) {\n\t\t\t\t\tpr_warn(\"MAP_SHARED not completely supported on !MMU\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* handle executable mappings and implied executable\n\t\t * mappings */\n\t\tif (path_noexec(&file->f_path)) {\n\t\t\tif (prot & PROT_EXEC)\n\t\t\t\treturn -EPERM;\n\t\t} else if ((prot & PROT_READ) && !(prot & PROT_EXEC)) {\n\t\t\t/* handle implication of PROT_EXEC by PROT_READ */\n\t\t\tif (current->personality & READ_IMPLIES_EXEC) {\n\t\t\t\tif (capabilities & NOMMU_MAP_EXEC)\n\t\t\t\t\tprot |= PROT_EXEC;\n\t\t\t}\n\t\t} else if ((prot & PROT_READ) &&\n\t\t\t (prot & PROT_EXEC) &&\n\t\t\t !(capabilities & NOMMU_MAP_EXEC)\n\t\t\t ) {\n\t\t\t/* backing file is not executable, try to copy */\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\t} else {\n\t\t/* anonymous mappings are always memory backed and can be\n\t\t * privately mapped\n\t\t */\n\t\tcapabilities = NOMMU_MAP_COPY;\n\n\t\t/* handle PROT_EXEC implication by PROT_READ */\n\t\tif ((prot & PROT_READ) &&\n\t\t    (current->personality & READ_IMPLIES_EXEC))\n\t\t\tprot |= PROT_EXEC;\n\t}\n\n\t/* allow the security API to have its say */\n\tret = security_mmap_addr(addr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* looks okay */\n\t*_capabilities = capabilities;\n\treturn 0;\n}"
  },
  {
    "function_name": "find_vma_exact",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "821-847",
    "snippet": "static struct vm_area_struct *find_vma_exact(struct mm_struct *mm,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     unsigned long len)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end = addr + len;\n\n\t/* check the cache first */\n\tvma = vmacache_find_exact(mm, addr, end);\n\tif (vma)\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start < addr)\n\t\t\tcontinue;\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end == end) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "vmacache_update",
          "args": [
            "addr",
            "vma"
          ],
          "line": 841
        },
        "resolved": true,
        "details": {
          "function_name": "vmacache_update",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/vmacache.c",
          "lines": "36-40",
          "snippet": "void vmacache_update(unsigned long addr, struct vm_area_struct *newvma)\n{\n\tif (vmacache_valid_mm(newvma->vm_mm))\n\t\tcurrent->vmacache.vmas[VMACACHE_HASH(addr)] = newvma;\n}",
          "includes": [
            "#include <asm/pgtable.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/pgtable.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n\nvoid vmacache_update(unsigned long addr, struct vm_area_struct *newvma)\n{\n\tif (vmacache_valid_mm(newvma->vm_mm))\n\t\tcurrent->vmacache.vmas[VMACACHE_HASH(addr)] = newvma;\n}"
        }
      },
      {
        "call_info": {
          "callee": "vmacache_find_exact",
          "args": [
            "mm",
            "addr",
            "end"
          ],
          "line": 829
        },
        "resolved": true,
        "details": {
          "function_name": "vmacache_find_exact",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/vmacache.c",
          "lines": "93-117",
          "snippet": "struct vm_area_struct *vmacache_find_exact(struct mm_struct *mm,\n\t\t\t\t\t   unsigned long start,\n\t\t\t\t\t   unsigned long end)\n{\n\tint idx = VMACACHE_HASH(start);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma && vma->vm_start == start && vma->vm_end == end) {\n\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\treturn vma;\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include <asm/pgtable.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/pgtable.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n\nstruct vm_area_struct *vmacache_find_exact(struct mm_struct *mm,\n\t\t\t\t\t   unsigned long start,\n\t\t\t\t\t   unsigned long end)\n{\n\tint idx = VMACACHE_HASH(start);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma && vma->vm_start == start && vma->vm_end == end) {\n\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\treturn vma;\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct vm_area_struct *find_vma_exact(struct mm_struct *mm,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     unsigned long len)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end = addr + len;\n\n\t/* check the cache first */\n\tvma = vmacache_find_exact(mm, addr, end);\n\tif (vma)\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start < addr)\n\t\t\tcontinue;\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end == end) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
  },
  {
    "function_name": "expand_stack",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "812-815",
    "snippet": "int expand_stack(struct vm_area_struct *vma, unsigned long address)\n{\n\treturn -ENOMEM;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint expand_stack(struct vm_area_struct *vma, unsigned long address)\n{\n\treturn -ENOMEM;\n}"
  },
  {
    "function_name": "find_extend_vma",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "803-806",
    "snippet": "struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\treturn find_vma(mm, addr);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "find_vma",
          "args": [
            "mm",
            "addr"
          ],
          "line": 805
        },
        "resolved": true,
        "details": {
          "function_name": "find_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "775-796",
          "snippet": "struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\treturn find_vma(mm, addr);\n}"
  },
  {
    "function_name": "find_vma",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "775-796",
    "snippet": "struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "vmacache_update",
          "args": [
            "addr",
            "vma"
          ],
          "line": 790
        },
        "resolved": true,
        "details": {
          "function_name": "vmacache_update",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/vmacache.c",
          "lines": "36-40",
          "snippet": "void vmacache_update(unsigned long addr, struct vm_area_struct *newvma)\n{\n\tif (vmacache_valid_mm(newvma->vm_mm))\n\t\tcurrent->vmacache.vmas[VMACACHE_HASH(addr)] = newvma;\n}",
          "includes": [
            "#include <asm/pgtable.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/pgtable.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n\nvoid vmacache_update(unsigned long addr, struct vm_area_struct *newvma)\n{\n\tif (vmacache_valid_mm(newvma->vm_mm))\n\t\tcurrent->vmacache.vmas[VMACACHE_HASH(addr)] = newvma;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "vma"
          ],
          "line": 781
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vmacache_find",
          "args": [
            "mm",
            "addr"
          ],
          "line": 780
        },
        "resolved": true,
        "details": {
          "function_name": "vmacache_find",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/vmacache.c",
          "lines": "62-90",
          "snippet": "struct vm_area_struct *vmacache_find(struct mm_struct *mm, unsigned long addr)\n{\n\tint idx = VMACACHE_HASH(addr);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma) {\n#ifdef CONFIG_DEBUG_VM_VMACACHE\n\t\t\tif (WARN_ON_ONCE(vma->vm_mm != mm))\n\t\t\t\tbreak;\n#endif\n\t\t\tif (vma->vm_start <= addr && vma->vm_end > addr) {\n\t\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\t\treturn vma;\n\t\t\t}\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include <asm/pgtable.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/pgtable.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n\nstruct vm_area_struct *vmacache_find(struct mm_struct *mm, unsigned long addr)\n{\n\tint idx = VMACACHE_HASH(addr);\n\tint i;\n\n\tcount_vm_vmacache_event(VMACACHE_FIND_CALLS);\n\n\tif (!vmacache_valid(mm))\n\t\treturn NULL;\n\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\tstruct vm_area_struct *vma = current->vmacache.vmas[idx];\n\n\t\tif (vma) {\n#ifdef CONFIG_DEBUG_VM_VMACACHE\n\t\t\tif (WARN_ON_ONCE(vma->vm_mm != mm))\n\t\t\t\tbreak;\n#endif\n\t\t\tif (vma->vm_start <= addr && vma->vm_end > addr) {\n\t\t\t\tcount_vm_vmacache_event(VMACACHE_FIND_HITS);\n\t\t\t\treturn vma;\n\t\t\t}\n\t\t}\n\t\tif (++idx == VMACACHE_SIZE)\n\t\t\tidx = 0;\n\t}\n\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
  },
  {
    "function_name": "delete_vma",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "761-769",
    "snippet": "static void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tput_nommu_region(vma->vm_region);\n\tvm_area_free(vma);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "vm_area_free",
          "args": [
            "vma"
          ],
          "line": 768
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "put_nommu_region",
          "args": [
            "vma->vm_region"
          ],
          "line": 767
        },
        "resolved": true,
        "details": {
          "function_name": "put_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "643-647",
          "snippet": "static void put_nommu_region(struct vm_region *region)\n{\n\tdown_write(&nommu_region_sem);\n\t__put_nommu_region(region);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void put_nommu_region(struct vm_region *region)\n{\n\tdown_write(&nommu_region_sem);\n\t__put_nommu_region(region);\n}"
        }
      },
      {
        "call_info": {
          "callee": "fput",
          "args": [
            "vma->vm_file"
          ],
          "line": 766
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vma->vm_ops->close",
          "args": [
            "vma"
          ],
          "line": 764
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tput_nommu_region(vma->vm_region);\n\tvm_area_free(vma);\n}"
  },
  {
    "function_name": "delete_vma_from_mm",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "719-756",
    "snippet": "static void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_erase",
          "args": [
            "&vma->vm_rb",
            "&mm->mm_rb"
          ],
          "line": 747
        },
        "resolved": true,
        "details": {
          "function_name": "vma_rb_erase",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/mmap.c",
          "lines": "479-489",
          "snippet": "static __always_inline void vma_rb_erase(struct vm_area_struct *vma,\n\t\t\t\t\t struct rb_root *root)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the vma being erased.\n\t */\n\tvalidate_mm_rb(root, vma);\n\n\t__vma_rb_erase(vma, root);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlb.h>",
            "#include <asm/cacheflush.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/oom.h>",
            "#include <linux/pkeys.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/printk.h>",
            "#include <linux/memory.h>",
            "#include <linux/notifier.h>",
            "#include <linux/rbtree_augmented.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/audit.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/rmap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/mount.h>",
            "#include <linux/export.h>",
            "#include <linux/profile.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/fs.h>",
            "#include <linux/file.h>",
            "#include <linux/init.h>",
            "#include <linux/capability.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mman.h>",
            "#include <linux/shm.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/slab.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlb.h>\n#include <asm/cacheflush.h>\n#include <linux/uaccess.h>\n#include <linux/oom.h>\n#include <linux/pkeys.h>\n#include <linux/moduleparam.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/printk.h>\n#include <linux/memory.h>\n#include <linux/notifier.h>\n#include <linux/rbtree_augmented.h>\n#include <linux/uprobes.h>\n#include <linux/khugepaged.h>\n#include <linux/audit.h>\n#include <linux/perf_event.h>\n#include <linux/mmdebug.h>\n#include <linux/mmu_notifier.h>\n#include <linux/rmap.h>\n#include <linux/mempolicy.h>\n#include <linux/mount.h>\n#include <linux/export.h>\n#include <linux/profile.h>\n#include <linux/shmem_fs.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/init.h>\n#include <linux/capability.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/pagemap.h>\n#include <linux/mman.h>\n#include <linux/shm.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/backing-dev.h>\n#include <linux/slab.h>\n#include <linux/kernel.h>\n\nstatic __always_inline void vma_rb_erase(struct vm_area_struct *vma,\n\t\t\t\t\t struct rb_root *root)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the vma being erased.\n\t */\n\tvalidate_mm_rb(root, vma);\n\n\t__vma_rb_erase(vma, root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "i_mmap_unlock_write",
          "args": [
            "mapping"
          ],
          "line": 743
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "flush_dcache_mmap_unlock",
          "args": [
            "mapping"
          ],
          "line": 742
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vma_interval_tree_remove",
          "args": [
            "vma",
            "&mapping->i_mmap"
          ],
          "line": 741
        },
        "resolved": true,
        "details": {
          "function_name": "anon_vma_interval_tree_remove",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/interval_tree.c",
          "lines": "86-90",
          "snippet": "void anon_vma_interval_tree_remove(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root)\n{\n\t__anon_vma_interval_tree_remove(node, root);\n}",
          "includes": [
            "#include <linux/interval_tree_generic.h>",
            "#include <linux/rmap.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/interval_tree_generic.h>\n#include <linux/rmap.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n\nvoid anon_vma_interval_tree_remove(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root)\n{\n\t__anon_vma_interval_tree_remove(node, root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "flush_dcache_mmap_lock",
          "args": [
            "mapping"
          ],
          "line": 740
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "i_mmap_lock_write",
          "args": [
            "mapping"
          ],
          "line": 739
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vmacache_invalidate",
          "args": [
            "mm"
          ],
          "line": 730
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tint i;\n\tstruct address_space *mapping;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct task_struct *curr = current;\n\n\tmm->map_count--;\n\tfor (i = 0; i < VMACACHE_SIZE; i++) {\n\t\t/* if the vma is cached, invalidate the entire cache */\n\t\tif (curr->vmacache.vmas[i] == vma) {\n\t\t\tvmacache_invalidate(mm);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* remove the VMA from the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* remove from the MM's tree and list */\n\trb_erase(&vma->vm_rb, &mm->mm_rb);\n\n\tif (vma->vm_prev)\n\t\tvma->vm_prev->vm_next = vma->vm_next;\n\telse\n\t\tmm->mmap = vma->vm_next;\n\n\tif (vma->vm_next)\n\t\tvma->vm_next->vm_prev = vma->vm_prev;\n}"
  },
  {
    "function_name": "add_vma_to_mm",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "655-714",
    "snippet": "static void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__vma_link_list",
          "args": [
            "mm",
            "vma",
            "prev",
            "parent"
          ],
          "line": 713
        },
        "resolved": true,
        "details": {
          "function_name": "__vma_link_list",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/util.c",
          "lines": "252-272",
          "snippet": "void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tstruct vm_area_struct *prev, struct rb_node *rb_parent)\n{\n\tstruct vm_area_struct *next;\n\n\tvma->vm_prev = prev;\n\tif (prev) {\n\t\tnext = prev->vm_next;\n\t\tprev->vm_next = vma;\n\t} else {\n\t\tmm->mmap = vma;\n\t\tif (rb_parent)\n\t\t\tnext = rb_entry(rb_parent,\n\t\t\t\t\tstruct vm_area_struct, vm_rb);\n\t\telse\n\t\t\tnext = NULL;\n\t}\n\tvma->vm_next = next;\n\tif (next)\n\t\tnext->vm_prev = vma;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <linux/uaccess.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/mman.h>",
            "#include <linux/swapops.h>",
            "#include <linux/swap.h>",
            "#include <linux/security.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched.h>",
            "#include <linux/err.h>",
            "#include <linux/export.h>",
            "#include <linux/compiler.h>",
            "#include <linux/string.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <linux/uaccess.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/vmalloc.h>\n#include <linux/hugetlb.h>\n#include <linux/mman.h>\n#include <linux/swapops.h>\n#include <linux/swap.h>\n#include <linux/security.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/mm.h>\n#include <linux/sched.h>\n#include <linux/err.h>\n#include <linux/export.h>\n#include <linux/compiler.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nvoid __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tstruct vm_area_struct *prev, struct rb_node *rb_parent)\n{\n\tstruct vm_area_struct *next;\n\n\tvma->vm_prev = prev;\n\tif (prev) {\n\t\tnext = prev->vm_next;\n\t\tprev->vm_next = vma;\n\t} else {\n\t\tmm->mmap = vma;\n\t\tif (rb_parent)\n\t\t\tnext = rb_entry(rb_parent,\n\t\t\t\t\tstruct vm_area_struct, vm_rb);\n\t\telse\n\t\t\tnext = NULL;\n\t}\n\tvma->vm_next = next;\n\tif (next)\n\t\tnext->vm_prev = vma;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "rb_prev",
            "structvm_area_struct",
            "vm_rb"
          ],
          "line": 711
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_insert_color",
          "args": [
            "&vma->vm_rb",
            "&mm->mm_rb"
          ],
          "line": 706
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_link_node",
          "args": [
            "&vma->vm_rb",
            "parent",
            "p"
          ],
          "line": 705
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 702
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "parent",
            "structvm_area_struct",
            "vm_rb"
          ],
          "line": 682
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "i_mmap_unlock_write",
          "args": [
            "mapping"
          ],
          "line": 674
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "flush_dcache_mmap_unlock",
          "args": [
            "mapping"
          ],
          "line": 673
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vma_interval_tree_insert",
          "args": [
            "vma",
            "&mapping->i_mmap"
          ],
          "line": 672
        },
        "resolved": true,
        "details": {
          "function_name": "anon_vma_interval_tree_insert",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/interval_tree.c",
          "lines": "72-84",
          "snippet": "INTERVAL_TREE_DEFINE(struct anon_vma_chain, rb, unsigned long, rb_subtree_last,\n\t\t     avc_start_pgoff, avc_last_pgoff,\n\t\t     static inline, __anon_vma_interval_tree)\n\nvoid anon_vma_interval_tree_insert(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root)\n{\n#ifdef CONFIG_DEBUG_VM_RB\n\tnode->cached_vma_start = avc_start_pgoff(node);\n\tnode->cached_vma_last = avc_last_pgoff(node);\n#endif\n\t__anon_vma_interval_tree_insert(node, root);\n}",
          "includes": [
            "#include <linux/interval_tree_generic.h>",
            "#include <linux/rmap.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/interval_tree_generic.h>\n#include <linux/rmap.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n\nINTERVAL_TREE_DEFINE(struct anon_vma_chain, rb, unsigned long, rb_subtree_last,\n\t\t     avc_start_pgoff, avc_last_pgoff,\n\t\t     static inline, __anon_vma_interval_tree)\n\nvoid anon_vma_interval_tree_insert(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root)\n{\n#ifdef CONFIG_DEBUG_VM_RB\n\tnode->cached_vma_start = avc_start_pgoff(node);\n\tnode->cached_vma_last = avc_last_pgoff(node);\n#endif\n\t__anon_vma_interval_tree_insert(node, root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "flush_dcache_mmap_lock",
          "args": [
            "mapping"
          ],
          "line": 671
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "i_mmap_lock_write",
          "args": [
            "mapping"
          ],
          "line": 670
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "!vma->vm_region"
          ],
          "line": 661
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void add_vma_to_mm(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tstruct vm_area_struct *pvma, *prev;\n\tstruct address_space *mapping;\n\tstruct rb_node **p, *parent, *rb_prev;\n\n\tBUG_ON(!vma->vm_region);\n\n\tmm->map_count++;\n\tvma->vm_mm = mm;\n\n\t/* add the VMA to the mapping */\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\t/* add the VMA to the tree */\n\tparent = rb_prev = NULL;\n\tp = &mm->mm_rb.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpvma = rb_entry(parent, struct vm_area_struct, vm_rb);\n\n\t\t/* sort by: start addr, end addr, VMA struct addr in that order\n\t\t * (the latter is necessary as we may get identical VMAs) */\n\t\tif (vma->vm_start < pvma->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_start > pvma->vm_start) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma->vm_end < pvma->vm_end)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma->vm_end > pvma->vm_end) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else if (vma < pvma)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (vma > pvma) {\n\t\t\trb_prev = parent;\n\t\t\tp = &(*p)->rb_right;\n\t\t} else\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->vm_rb, parent, p);\n\trb_insert_color(&vma->vm_rb, &mm->mm_rb);\n\n\t/* add VMA to the VMA list also */\n\tprev = NULL;\n\tif (rb_prev)\n\t\tprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);\n\n\t__vma_link_list(mm, vma, prev, parent);\n}"
  },
  {
    "function_name": "put_nommu_region",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "643-647",
    "snippet": "static void put_nommu_region(struct vm_region *region)\n{\n\tdown_write(&nommu_region_sem);\n\t__put_nommu_region(region);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__put_nommu_region",
          "args": [
            "region"
          ],
          "line": 646
        },
        "resolved": true,
        "details": {
          "function_name": "__put_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "617-638",
          "snippet": "static void __put_nommu_region(struct vm_region *region)\n\t__releases(nommu_region_sem)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tif (--region->vm_usage == 0) {\n\t\tif (region->vm_top > region->vm_start)\n\t\t\tdelete_nommu_region(region);\n\t\tup_write(&nommu_region_sem);\n\n\t\tif (region->vm_file)\n\t\t\tfput(region->vm_file);\n\n\t\t/* IO memory and memory shared directly out of the pagecache\n\t\t * from ramfs/tmpfs mustn't be released here */\n\t\tif (region->vm_flags & VM_MAPPED_COPY)\n\t\t\tfree_page_series(region->vm_start, region->vm_top);\n\t\tkmem_cache_free(vm_region_jar, region);\n\t} else {\n\t\tup_write(&nommu_region_sem);\n\t}\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static struct kmem_cache *vm_region_jar;",
            "struct rb_root nommu_region_tree = RB_ROOT;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct kmem_cache *vm_region_jar;\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void __put_nommu_region(struct vm_region *region)\n\t__releases(nommu_region_sem)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tif (--region->vm_usage == 0) {\n\t\tif (region->vm_top > region->vm_start)\n\t\t\tdelete_nommu_region(region);\n\t\tup_write(&nommu_region_sem);\n\n\t\tif (region->vm_file)\n\t\t\tfput(region->vm_file);\n\n\t\t/* IO memory and memory shared directly out of the pagecache\n\t\t * from ramfs/tmpfs mustn't be released here */\n\t\tif (region->vm_flags & VM_MAPPED_COPY)\n\t\t\tfree_page_series(region->vm_start, region->vm_top);\n\t\tkmem_cache_free(vm_region_jar, region);\n\t} else {\n\t\tup_write(&nommu_region_sem);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 645
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void put_nommu_region(struct vm_region *region)\n{\n\tdown_write(&nommu_region_sem);\n\t__put_nommu_region(region);\n}"
  },
  {
    "function_name": "__put_nommu_region",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "617-638",
    "snippet": "static void __put_nommu_region(struct vm_region *region)\n\t__releases(nommu_region_sem)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tif (--region->vm_usage == 0) {\n\t\tif (region->vm_top > region->vm_start)\n\t\t\tdelete_nommu_region(region);\n\t\tup_write(&nommu_region_sem);\n\n\t\tif (region->vm_file)\n\t\t\tfput(region->vm_file);\n\n\t\t/* IO memory and memory shared directly out of the pagecache\n\t\t * from ramfs/tmpfs mustn't be released here */\n\t\tif (region->vm_flags & VM_MAPPED_COPY)\n\t\t\tfree_page_series(region->vm_start, region->vm_top);\n\t\tkmem_cache_free(vm_region_jar, region);\n\t} else {\n\t\tup_write(&nommu_region_sem);\n\t}\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static struct kmem_cache *vm_region_jar;",
      "struct rb_root nommu_region_tree = RB_ROOT;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 636
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kmem_cache_free",
          "args": [
            "vm_region_jar",
            "region"
          ],
          "line": 634
        },
        "resolved": true,
        "details": {
          "function_name": "kmem_cache_free",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/slab.c",
          "lines": "3749-3764",
          "snippet": "void kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}",
          "includes": [
            "#include\t\"slab.h\"",
            "#include\t\"internal.h\"",
            "#include <trace/events/kmem.h>",
            "#include\t<asm/page.h>",
            "#include\t<asm/tlbflush.h>",
            "#include\t<asm/cacheflush.h>",
            "#include\t<net/sock.h>",
            "#include\t<linux/sched/task_stack.h>",
            "#include\t<linux/prefetch.h>",
            "#include\t<linux/memory.h>",
            "#include\t<linux/debugobjects.h>",
            "#include\t<linux/reciprocal_div.h>",
            "#include\t<linux/rtmutex.h>",
            "#include\t<linux/fault-inject.h>",
            "#include\t<linux/mutex.h>",
            "#include\t<linux/mempolicy.h>",
            "#include\t<linux/kmemleak.h>",
            "#include\t<linux/nodemask.h>",
            "#include\t<linux/uaccess.h>",
            "#include\t<linux/string.h>",
            "#include\t<linux/rcupdate.h>",
            "#include\t<linux/module.h>",
            "#include\t<linux/sysctl.h>",
            "#include\t<linux/cpu.h>",
            "#include\t<linux/kallsyms.h>",
            "#include\t<linux/notifier.h>",
            "#include\t<linux/seq_file.h>",
            "#include\t<linux/proc_fs.h>",
            "#include\t<linux/cpuset.h>",
            "#include\t<linux/compiler.h>",
            "#include\t<linux/init.h>",
            "#include\t<linux/interrupt.h>",
            "#include\t<linux/cache.h>",
            "#include\t<linux/swap.h>",
            "#include\t<linux/poison.h>",
            "#include\t<linux/mm.h>",
            "#include\t<linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static noinline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include\t\"slab.h\"\n#include\t\"internal.h\"\n#include <trace/events/kmem.h>\n#include\t<asm/page.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/cacheflush.h>\n#include\t<net/sock.h>\n#include\t<linux/sched/task_stack.h>\n#include\t<linux/prefetch.h>\n#include\t<linux/memory.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/mutex.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/string.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/module.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/cpu.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/notifier.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/compiler.h>\n#include\t<linux/init.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/cache.h>\n#include\t<linux/swap.h>\n#include\t<linux/poison.h>\n#include\t<linux/mm.h>\n#include\t<linux/slab.h>\n\nstatic noinline struct;\n\nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}"
        }
      },
      {
        "call_info": {
          "callee": "free_page_series",
          "args": [
            "region->vm_start",
            "region->vm_top"
          ],
          "line": 633
        },
        "resolved": true,
        "details": {
          "function_name": "free_page_series",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "601-609",
          "snippet": "static void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "atomic_long_t mmap_pages_allocated;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\natomic_long_t mmap_pages_allocated;\n\nstatic void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "fput",
          "args": [
            "region->vm_file"
          ],
          "line": 628
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&nommu_region_sem"
          ],
          "line": 625
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "delete_nommu_region",
          "args": [
            "region"
          ],
          "line": 624
        },
        "resolved": true,
        "details": {
          "function_name": "delete_nommu_region",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "589-596",
          "snippet": "static void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "struct rb_root nommu_region_tree = RB_ROOT;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}"
        }
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "!nommu_region_tree.rb_node"
          ],
          "line": 620
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__releases",
          "args": [
            "nommu_region_sem"
          ],
          "line": 618
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct kmem_cache *vm_region_jar;\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void __put_nommu_region(struct vm_region *region)\n\t__releases(nommu_region_sem)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tif (--region->vm_usage == 0) {\n\t\tif (region->vm_top > region->vm_start)\n\t\t\tdelete_nommu_region(region);\n\t\tup_write(&nommu_region_sem);\n\n\t\tif (region->vm_file)\n\t\t\tfput(region->vm_file);\n\n\t\t/* IO memory and memory shared directly out of the pagecache\n\t\t * from ramfs/tmpfs mustn't be released here */\n\t\tif (region->vm_flags & VM_MAPPED_COPY)\n\t\t\tfree_page_series(region->vm_start, region->vm_top);\n\t\tkmem_cache_free(vm_region_jar, region);\n\t} else {\n\t\tup_write(&nommu_region_sem);\n\t}\n}"
  },
  {
    "function_name": "free_page_series",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "601-609",
    "snippet": "static void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "atomic_long_t mmap_pages_allocated;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "put_page",
          "args": [
            "page"
          ],
          "line": 607
        },
        "resolved": true,
        "details": {
          "function_name": "put_page_bootmem",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/memory_hotplug.c",
          "lines": "143-158",
          "snippet": "void put_page_bootmem(struct page *page)\n{\n\tunsigned long type;\n\n\ttype = (unsigned long) page->freelist;\n\tBUG_ON(type < MEMORY_HOTPLUG_MIN_BOOTMEM_TYPE ||\n\t       type > MEMORY_HOTPLUG_MAX_BOOTMEM_TYPE);\n\n\tif (page_ref_dec_return(page) == 1) {\n\t\tpage->freelist = NULL;\n\t\tClearPagePrivate(page);\n\t\tset_page_private(page, 0);\n\t\tINIT_LIST_HEAD(&page->lru);\n\t\tfree_reserved_page(page);\n\t}\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/tlbflush.h>",
            "#include <linux/compaction.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/firmware-map.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/suspend.h>",
            "#include <linux/pfn.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/migrate.h>",
            "#include <linux/delay.h>",
            "#include <linux/ioport.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/highmem.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/memremap.h>",
            "#include <linux/memory.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/slab.h>",
            "#include <linux/writeback.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/export.h>",
            "#include <linux/compiler.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/swap.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/stddef.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void generic_online_page(struct page *page);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/tlbflush.h>\n#include <linux/compaction.h>\n#include <linux/memblock.h>\n#include <linux/hugetlb.h>\n#include <linux/stop_machine.h>\n#include <linux/firmware-map.h>\n#include <linux/mm_inline.h>\n#include <linux/suspend.h>\n#include <linux/pfn.h>\n#include <linux/page-isolation.h>\n#include <linux/migrate.h>\n#include <linux/delay.h>\n#include <linux/ioport.h>\n#include <linux/vmalloc.h>\n#include <linux/highmem.h>\n#include <linux/memory_hotplug.h>\n#include <linux/memremap.h>\n#include <linux/memory.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/slab.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/export.h>\n#include <linux/compiler.h>\n#include <linux/pagemap.h>\n#include <linux/interrupt.h>\n#include <linux/swap.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n\nstatic void generic_online_page(struct page *page);\n\nvoid put_page_bootmem(struct page *page)\n{\n\tunsigned long type;\n\n\ttype = (unsigned long) page->freelist;\n\tBUG_ON(type < MEMORY_HOTPLUG_MIN_BOOTMEM_TYPE ||\n\t       type > MEMORY_HOTPLUG_MAX_BOOTMEM_TYPE);\n\n\tif (page_ref_dec_return(page) == 1) {\n\t\tpage->freelist = NULL;\n\t\tClearPagePrivate(page);\n\t\tset_page_private(page, 0);\n\t\tINIT_LIST_HEAD(&page->lru);\n\t\tfree_reserved_page(page);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_dec",
          "args": [
            "&mmap_pages_allocated"
          ],
          "line": 606
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "virt_to_page",
          "args": [
            "from"
          ],
          "line": 604
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\natomic_long_t mmap_pages_allocated;\n\nstatic void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page(from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}"
  },
  {
    "function_name": "delete_nommu_region",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "589-596",
    "snippet": "static void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "struct rb_root nommu_region_tree = RB_ROOT;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "validate_nommu_regions",
          "args": [],
          "line": 595
        },
        "resolved": true,
        "details": {
          "function_name": "validate_nommu_regions",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "550-552",
          "snippet": "static void validate_nommu_regions(void)\n{\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void validate_nommu_regions(void)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "rb_erase",
          "args": [
            "&region->vm_rb",
            "&nommu_region_tree"
          ],
          "line": 594
        },
        "resolved": true,
        "details": {
          "function_name": "vma_rb_erase",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/mmap.c",
          "lines": "479-489",
          "snippet": "static __always_inline void vma_rb_erase(struct vm_area_struct *vma,\n\t\t\t\t\t struct rb_root *root)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the vma being erased.\n\t */\n\tvalidate_mm_rb(root, vma);\n\n\t__vma_rb_erase(vma, root);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlb.h>",
            "#include <asm/cacheflush.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/oom.h>",
            "#include <linux/pkeys.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/printk.h>",
            "#include <linux/memory.h>",
            "#include <linux/notifier.h>",
            "#include <linux/rbtree_augmented.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/audit.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/rmap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/mount.h>",
            "#include <linux/export.h>",
            "#include <linux/profile.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/fs.h>",
            "#include <linux/file.h>",
            "#include <linux/init.h>",
            "#include <linux/capability.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mman.h>",
            "#include <linux/shm.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/slab.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlb.h>\n#include <asm/cacheflush.h>\n#include <linux/uaccess.h>\n#include <linux/oom.h>\n#include <linux/pkeys.h>\n#include <linux/moduleparam.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/printk.h>\n#include <linux/memory.h>\n#include <linux/notifier.h>\n#include <linux/rbtree_augmented.h>\n#include <linux/uprobes.h>\n#include <linux/khugepaged.h>\n#include <linux/audit.h>\n#include <linux/perf_event.h>\n#include <linux/mmdebug.h>\n#include <linux/mmu_notifier.h>\n#include <linux/rmap.h>\n#include <linux/mempolicy.h>\n#include <linux/mount.h>\n#include <linux/export.h>\n#include <linux/profile.h>\n#include <linux/shmem_fs.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/init.h>\n#include <linux/capability.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/pagemap.h>\n#include <linux/mman.h>\n#include <linux/shm.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/backing-dev.h>\n#include <linux/slab.h>\n#include <linux/kernel.h>\n\nstatic __always_inline void vma_rb_erase(struct vm_area_struct *vma,\n\t\t\t\t\t struct rb_root *root)\n{\n\t/*\n\t * All rb_subtree_gap values must be consistent prior to erase,\n\t * with the possible exception of the vma being erased.\n\t */\n\tvalidate_mm_rb(root, vma);\n\n\t__vma_rb_erase(vma, root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "!nommu_region_tree.rb_node"
          ],
          "line": 591
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}"
  },
  {
    "function_name": "add_nommu_region",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "558-584",
    "snippet": "static void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "struct rb_root nommu_region_tree = RB_ROOT;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "validate_nommu_regions",
          "args": [],
          "line": 583
        },
        "resolved": true,
        "details": {
          "function_name": "validate_nommu_regions",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "550-552",
          "snippet": "static void validate_nommu_regions(void)\n{\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void validate_nommu_regions(void)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "rb_insert_color",
          "args": [
            "&region->vm_rb",
            "&nommu_region_tree"
          ],
          "line": 581
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_link_node",
          "args": [
            "&region->vm_rb",
            "parent",
            "p"
          ],
          "line": 580
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 577
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "parent",
            "structvm_region",
            "vm_rb"
          ],
          "line": 569
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}"
  },
  {
    "function_name": "validate_nommu_regions",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "550-552",
    "snippet": "static void validate_nommu_regions(void)\n{\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic void validate_nommu_regions(void)\n{\n}"
  },
  {
    "function_name": "validate_nommu_regions",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "525-548",
    "snippet": "static noinline void validate_nommu_regions(void)\n{\n\tstruct vm_region *region, *last;\n\tstruct rb_node *p, *lastp;\n\n\tlastp = rb_first(&nommu_region_tree);\n\tif (!lastp)\n\t\treturn;\n\n\tlast = rb_entry(lastp, struct vm_region, vm_rb);\n\tBUG_ON(last->vm_end <= last->vm_start);\n\tBUG_ON(last->vm_top < last->vm_end);\n\n\twhile ((p = rb_next(lastp))) {\n\t\tregion = rb_entry(p, struct vm_region, vm_rb);\n\t\tlast = rb_entry(lastp, struct vm_region, vm_rb);\n\n\t\tBUG_ON(region->vm_end <= region->vm_start);\n\t\tBUG_ON(region->vm_top < region->vm_end);\n\t\tBUG_ON(region->vm_start < last->vm_top);\n\n\t\tlastp = p;\n\t}\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "struct rb_root nommu_region_tree = RB_ROOT;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "region->vm_start < last->vm_top"
          ],
          "line": 544
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "region->vm_top < region->vm_end"
          ],
          "line": 543
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "region->vm_end <= region->vm_start"
          ],
          "line": 542
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "lastp",
            "structvm_region",
            "vm_rb"
          ],
          "line": 540
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "p",
            "structvm_region",
            "vm_rb"
          ],
          "line": 539
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_next",
          "args": [
            "lastp"
          ],
          "line": 538
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "last->vm_top < last->vm_end"
          ],
          "line": 536
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "last->vm_end <= last->vm_start"
          ],
          "line": 535
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "lastp",
            "structvm_region",
            "vm_rb"
          ],
          "line": 534
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_first",
          "args": [
            "&nommu_region_tree"
          ],
          "line": 530
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct rb_root nommu_region_tree = RB_ROOT;\n\nstatic noinline void validate_nommu_regions(void)\n{\n\tstruct vm_region *region, *last;\n\tstruct rb_node *p, *lastp;\n\n\tlastp = rb_first(&nommu_region_tree);\n\tif (!lastp)\n\t\treturn;\n\n\tlast = rb_entry(lastp, struct vm_region, vm_rb);\n\tBUG_ON(last->vm_end <= last->vm_start);\n\tBUG_ON(last->vm_top < last->vm_end);\n\n\twhile ((p = rb_next(lastp))) {\n\t\tregion = rb_entry(p, struct vm_region, vm_rb);\n\t\tlast = rb_entry(lastp, struct vm_region, vm_rb);\n\n\t\tBUG_ON(region->vm_end <= region->vm_start);\n\t\tBUG_ON(region->vm_top < region->vm_end);\n\t\tBUG_ON(region->vm_start < last->vm_top);\n\n\t\tlastp = p;\n\t}\n}"
  },
  {
    "function_name": "mmap_init",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "511-518",
    "snippet": "void __init mmap_init(void)\n{\n\tint ret;\n\n\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);\n\tVM_BUG_ON(ret);\n\tvm_region_jar = KMEM_CACHE(vm_region, SLAB_PANIC|SLAB_ACCOUNT);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "static struct kmem_cache *vm_region_jar;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "KMEM_CACHE",
          "args": [
            "vm_region",
            "SLAB_PANIC|SLAB_ACCOUNT"
          ],
          "line": 517
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "VM_BUG_ON",
          "args": [
            "ret"
          ],
          "line": 516
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "percpu_counter_init",
          "args": [
            "&vm_committed_as",
            "0",
            "GFP_KERNEL"
          ],
          "line": 515
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic struct kmem_cache *vm_region_jar;\n\nvoid __init mmap_init(void)\n{\n\tint ret;\n\n\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);\n\tVM_BUG_ON(ret);\n\tvm_region_jar = KMEM_CACHE(vm_region, SLAB_PANIC|SLAB_ACCOUNT);\n}"
  },
  {
    "function_name": "vm_insert_page",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "469-473",
    "snippet": "int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t   struct page *page)\n{\n\treturn -EINVAL;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint vm_insert_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t   struct page *page)\n{\n\treturn -EINVAL;\n}"
  },
  {
    "function_name": "free_vm_area",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "463-466",
    "snippet": "void free_vm_area(struct vm_struct *area)\n{\n\tBUG();\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 465
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid free_vm_area(struct vm_struct *area)\n{\n\tBUG();\n}"
  },
  {
    "function_name": "alloc_vm_area",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "456-460",
    "snippet": "struct vm_struct *alloc_vm_area(size_t size, pte_t **ptes)\n{\n\tBUG();\n\treturn NULL;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 458
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_struct *alloc_vm_area(size_t size, pte_t **ptes)\n{\n\tBUG();\n\treturn NULL;\n}"
  },
  {
    "function_name": "vmalloc_sync_all",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "452-454",
    "snippet": "void __weak vmalloc_sync_all(void)\n{\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid __weak vmalloc_sync_all(void)\n{\n}"
  },
  {
    "function_name": "vm_unmap_aliases",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "443-445",
    "snippet": "void vm_unmap_aliases(void)\n{\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid vm_unmap_aliases(void)\n{\n}"
  },
  {
    "function_name": "vm_unmap_ram",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "437-440",
    "snippet": "void vm_unmap_ram(const void *mem, unsigned int count)\n{\n\tBUG();\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 439
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid vm_unmap_ram(const void *mem, unsigned int count)\n{\n\tBUG();\n}"
  },
  {
    "function_name": "vm_map_ram",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "430-434",
    "snippet": "void *vm_map_ram(struct page **pages, unsigned int count, int node, pgprot_t prot)\n{\n\tBUG();\n\treturn NULL;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 432
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vm_map_ram(struct page **pages, unsigned int count, int node, pgprot_t prot)\n{\n\tBUG();\n\treturn NULL;\n}"
  },
  {
    "function_name": "vunmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "424-427",
    "snippet": "void vunmap(const void *addr)\n{\n\tBUG();\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 426
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid vunmap(const void *addr)\n{\n\tBUG();\n}"
  },
  {
    "function_name": "vmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "417-421",
    "snippet": "void *vmap(struct page **pages, unsigned int count, unsigned long flags, pgprot_t prot)\n{\n\tBUG();\n\treturn NULL;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG",
          "args": [],
          "line": 419
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmap(struct page **pages, unsigned int count, unsigned long flags, pgprot_t prot)\n{\n\tBUG();\n\treturn NULL;\n}"
  },
  {
    "function_name": "vmalloc_32_user",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "407-414",
    "snippet": "void *vmalloc_32_user(unsigned long size)\n{\n\t/*\n\t * We'll have to sort out the ZONE_DMA bits for 64-bit,\n\t * but for now this can simply use vmalloc_user() directly.\n\t */\n\treturn vmalloc_user(size);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "vmalloc_user",
          "args": [
            "size"
          ],
          "line": 413
        },
        "resolved": true,
        "details": {
          "function_name": "vmalloc_user",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "245-261",
          "snippet": "void *vmalloc_user(unsigned long size)\n{\n\tvoid *ret;\n\n\tret = __vmalloc(size, GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL);\n\tif (ret) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tdown_write(&current->mm->mmap_sem);\n\t\tvma = find_vma(current->mm, (unsigned long)ret);\n\t\tif (vma)\n\t\t\tvma->vm_flags |= VM_USERMAP;\n\t\tup_write(&current->mm->mmap_sem);\n\t}\n\n\treturn ret;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc_user(unsigned long size)\n{\n\tvoid *ret;\n\n\tret = __vmalloc(size, GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL);\n\tif (ret) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tdown_write(&current->mm->mmap_sem);\n\t\tvma = find_vma(current->mm, (unsigned long)ret);\n\t\tif (vma)\n\t\t\tvma->vm_flags |= VM_USERMAP;\n\t\tup_write(&current->mm->mmap_sem);\n\t}\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc_32_user(unsigned long size)\n{\n\t/*\n\t * We'll have to sort out the ZONE_DMA bits for 64-bit,\n\t * but for now this can simply use vmalloc_user() directly.\n\t */\n\treturn vmalloc_user(size);\n}"
  },
  {
    "function_name": "vmalloc_32",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "391-394",
    "snippet": "void *vmalloc_32(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL, PAGE_KERNEL);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__vmalloc",
          "args": [
            "size",
            "GFP_KERNEL",
            "PAGE_KERNEL"
          ],
          "line": 393
        },
        "resolved": true,
        "details": {
          "function_name": "__vmalloc_node_flags",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "240-243",
          "snippet": "void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc_32(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL, PAGE_KERNEL);\n}"
  },
  {
    "function_name": "vmalloc_exec",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "379-382",
    "snippet": "void *vmalloc_exec(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM, PAGE_KERNEL_EXEC);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__vmalloc",
          "args": [
            "size",
            "GFP_KERNEL | __GFP_HIGHMEM",
            "PAGE_KERNEL_EXEC"
          ],
          "line": 381
        },
        "resolved": true,
        "details": {
          "function_name": "__vmalloc_node_flags",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "240-243",
          "snippet": "void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc_exec(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM, PAGE_KERNEL_EXEC);\n}"
  },
  {
    "function_name": "vzalloc_node",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "361-364",
    "snippet": "void *vzalloc_node(unsigned long size, int node)\n{\n\treturn vzalloc(size);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "vzalloc",
          "args": [
            "size"
          ],
          "line": 363
        },
        "resolved": true,
        "details": {
          "function_name": "vzalloc",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "325-329",
          "snippet": "void *vzalloc(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO,\n\t\t\tPAGE_KERNEL);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vzalloc(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO,\n\t\t\tPAGE_KERNEL);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vzalloc_node(unsigned long size, int node)\n{\n\treturn vzalloc(size);\n}"
  },
  {
    "function_name": "vmalloc_node",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "343-346",
    "snippet": "void *vmalloc_node(unsigned long size, int node)\n{\n\treturn vmalloc(size);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "vmalloc",
          "args": [
            "size"
          ],
          "line": 345
        },
        "resolved": true,
        "details": {
          "function_name": "vmalloc_32_user",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "407-414",
          "snippet": "void *vmalloc_32_user(unsigned long size)\n{\n\t/*\n\t * We'll have to sort out the ZONE_DMA bits for 64-bit,\n\t * but for now this can simply use vmalloc_user() directly.\n\t */\n\treturn vmalloc_user(size);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc_32_user(unsigned long size)\n{\n\t/*\n\t * We'll have to sort out the ZONE_DMA bits for 64-bit,\n\t * but for now this can simply use vmalloc_user() directly.\n\t */\n\treturn vmalloc_user(size);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc_node(unsigned long size, int node)\n{\n\treturn vmalloc(size);\n}"
  },
  {
    "function_name": "vzalloc",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "325-329",
    "snippet": "void *vzalloc(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO,\n\t\t\tPAGE_KERNEL);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__vmalloc",
          "args": [
            "size",
            "GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO",
            "PAGE_KERNEL"
          ],
          "line": 327
        },
        "resolved": true,
        "details": {
          "function_name": "__vmalloc_node_flags",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "240-243",
          "snippet": "void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vzalloc(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO,\n\t\t\tPAGE_KERNEL);\n}"
  },
  {
    "function_name": "vmalloc",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "307-310",
    "snippet": "void *vmalloc(unsigned long size)\n{\n       return __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM, PAGE_KERNEL);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__vmalloc",
          "args": [
            "size",
            "GFP_KERNEL | __GFP_HIGHMEM",
            "PAGE_KERNEL"
          ],
          "line": 309
        },
        "resolved": true,
        "details": {
          "function_name": "__vmalloc_node_flags",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "240-243",
          "snippet": "void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc(unsigned long size)\n{\n       return __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM, PAGE_KERNEL);\n}"
  },
  {
    "function_name": "vwrite",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "286-294",
    "snippet": "long vwrite(char *buf, char *addr, unsigned long count)\n{\n\t/* Don't allow overflow */\n\tif ((unsigned long) addr + count < count)\n\t\tcount = -(unsigned long) addr;\n\n\tmemcpy(addr, buf, count);\n\treturn count;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "memcpy",
          "args": [
            "addr",
            "buf",
            "count"
          ],
          "line": 292
        },
        "resolved": true,
        "details": {
          "function_name": "memcpy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/kasan/kasan.c",
          "lines": "300-306",
          "snippet": "void *memcpy(void *dest, const void *src, size_t len)\n{\n\tcheck_memory_region((unsigned long)src, len, false, _RET_IP_);\n\tcheck_memory_region((unsigned long)dest, len, true, _RET_IP_);\n\n\treturn __memcpy(dest, src, len);\n}",
          "includes": [
            "#include \"../slab.h\"",
            "#include \"kasan.h\"",
            "#include <linux/bug.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/types.h>",
            "#include <linux/string.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched.h>",
            "#include <linux/printk.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/memory.h>",
            "#include <linux/memblock.h>",
            "#include <linux/linkage.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/kernel.h>",
            "#include <linux/kasan.h>",
            "#include <linux/init.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);",
            "__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../slab.h\"\n#include \"kasan.h\"\n#include <linux/bug.h>\n#include <linux/vmalloc.h>\n#include <linux/types.h>\n#include <linux/string.h>\n#include <linux/stacktrace.h>\n#include <linux/slab.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched.h>\n#include <linux/printk.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/memory.h>\n#include <linux/memblock.h>\n#include <linux/linkage.h>\n#include <linux/kmemleak.h>\n#include <linux/kernel.h>\n#include <linux/kasan.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n\n__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);\n__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);\n\nvoid *memcpy(void *dest, const void *src, size_t len)\n{\n\tcheck_memory_region((unsigned long)src, len, false, _RET_IP_);\n\tcheck_memory_region((unsigned long)dest, len, true, _RET_IP_);\n\n\treturn __memcpy(dest, src, len);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nlong vwrite(char *buf, char *addr, unsigned long count)\n{\n\t/* Don't allow overflow */\n\tif ((unsigned long) addr + count < count)\n\t\tcount = -(unsigned long) addr;\n\n\tmemcpy(addr, buf, count);\n\treturn count;\n}"
  },
  {
    "function_name": "vread",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "276-284",
    "snippet": "long vread(char *buf, char *addr, unsigned long count)\n{\n\t/* Don't allow overflow */\n\tif ((unsigned long) buf + count < count)\n\t\tcount = -(unsigned long) buf;\n\n\tmemcpy(buf, addr, count);\n\treturn count;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "memcpy",
          "args": [
            "buf",
            "addr",
            "count"
          ],
          "line": 282
        },
        "resolved": true,
        "details": {
          "function_name": "memcpy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/kasan/kasan.c",
          "lines": "300-306",
          "snippet": "void *memcpy(void *dest, const void *src, size_t len)\n{\n\tcheck_memory_region((unsigned long)src, len, false, _RET_IP_);\n\tcheck_memory_region((unsigned long)dest, len, true, _RET_IP_);\n\n\treturn __memcpy(dest, src, len);\n}",
          "includes": [
            "#include \"../slab.h\"",
            "#include \"kasan.h\"",
            "#include <linux/bug.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/types.h>",
            "#include <linux/string.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched.h>",
            "#include <linux/printk.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/memory.h>",
            "#include <linux/memblock.h>",
            "#include <linux/linkage.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/kernel.h>",
            "#include <linux/kasan.h>",
            "#include <linux/init.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);",
            "__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"../slab.h\"\n#include \"kasan.h\"\n#include <linux/bug.h>\n#include <linux/vmalloc.h>\n#include <linux/types.h>\n#include <linux/string.h>\n#include <linux/stacktrace.h>\n#include <linux/slab.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched.h>\n#include <linux/printk.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/memory.h>\n#include <linux/memblock.h>\n#include <linux/linkage.h>\n#include <linux/kmemleak.h>\n#include <linux/kernel.h>\n#include <linux/kasan.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n\n__alias(__asan_loadN)\nvoid __asan_loadN_noabort(unsigned long, size_t);\n__alias(__asan_storeN)\nvoid __asan_storeN_noabort(unsigned long, size_t);\n\nvoid *memcpy(void *dest, const void *src, size_t len)\n{\n\tcheck_memory_region((unsigned long)src, len, false, _RET_IP_);\n\tcheck_memory_region((unsigned long)dest, len, true, _RET_IP_);\n\n\treturn __memcpy(dest, src, len);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nlong vread(char *buf, char *addr, unsigned long count)\n{\n\t/* Don't allow overflow */\n\tif ((unsigned long) buf + count < count)\n\t\tcount = -(unsigned long) buf;\n\n\tmemcpy(buf, addr, count);\n\treturn count;\n}"
  },
  {
    "function_name": "vmalloc_to_pfn",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "270-273",
    "snippet": "unsigned long vmalloc_to_pfn(const void *addr)\n{\n\treturn page_to_pfn(virt_to_page(addr));\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "page_to_pfn",
          "args": [
            "virt_to_page(addr)"
          ],
          "line": 272
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "virt_to_page",
          "args": [
            "addr"
          ],
          "line": 272
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nunsigned long vmalloc_to_pfn(const void *addr)\n{\n\treturn page_to_pfn(virt_to_page(addr));\n}"
  },
  {
    "function_name": "vmalloc_to_page",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "264-267",
    "snippet": "struct page *vmalloc_to_page(const void *addr)\n{\n\treturn virt_to_page(addr);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "virt_to_page",
          "args": [
            "addr"
          ],
          "line": 266
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct page *vmalloc_to_page(const void *addr)\n{\n\treturn virt_to_page(addr);\n}"
  },
  {
    "function_name": "vmalloc_user",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "245-261",
    "snippet": "void *vmalloc_user(unsigned long size)\n{\n\tvoid *ret;\n\n\tret = __vmalloc(size, GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL);\n\tif (ret) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tdown_write(&current->mm->mmap_sem);\n\t\tvma = find_vma(current->mm, (unsigned long)ret);\n\t\tif (vma)\n\t\t\tvma->vm_flags |= VM_USERMAP;\n\t\tup_write(&current->mm->mmap_sem);\n\t}\n\n\treturn ret;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&current->mm->mmap_sem"
          ],
          "line": 257
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_vma",
          "args": [
            "current->mm",
            "(unsigned long)ret"
          ],
          "line": 254
        },
        "resolved": true,
        "details": {
          "function_name": "find_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "775-796",
          "snippet": "struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&current->mm->mmap_sem"
          ],
          "line": 253
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__vmalloc",
          "args": [
            "size",
            "GFP_KERNEL | __GFP_ZERO",
            "PAGE_KERNEL"
          ],
          "line": 249
        },
        "resolved": true,
        "details": {
          "function_name": "__vmalloc_node_flags",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "240-243",
          "snippet": "void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *vmalloc_user(unsigned long size)\n{\n\tvoid *ret;\n\n\tret = __vmalloc(size, GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL);\n\tif (ret) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tdown_write(&current->mm->mmap_sem);\n\t\tvma = find_vma(current->mm, (unsigned long)ret);\n\t\tif (vma)\n\t\t\tvma->vm_flags |= VM_USERMAP;\n\t\tup_write(&current->mm->mmap_sem);\n\t}\n\n\treturn ret;\n}"
  },
  {
    "function_name": "__vmalloc_node_flags",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "240-243",
    "snippet": "void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__vmalloc",
          "args": [
            "size",
            "flags",
            "PAGE_KERNEL"
          ],
          "line": 242
        },
        "resolved": true,
        "details": {
          "function_name": "__vmalloc_node_flags",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "240-243",
          "snippet": "void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}",
          "note": "cyclic_reference_detected"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags)\n{\n\treturn __vmalloc(size, flags, PAGE_KERNEL);\n}"
  },
  {
    "function_name": "__vmalloc",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "230-237",
    "snippet": "void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot)\n{\n\t/*\n\t *  You can't specify __GFP_HIGHMEM with kmalloc() since kmalloc()\n\t * returns only a logical address.\n\t */\n\treturn kmalloc(size, (gfp_mask | __GFP_COMP) & ~__GFP_HIGHMEM);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kmalloc",
          "args": [
            "size",
            "(gfp_mask | __GFP_COMP) & ~__GFP_HIGHMEM"
          ],
          "line": 236
        },
        "resolved": true,
        "details": {
          "function_name": "__kmalloc",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/slab.c",
          "lines": "3729-3732",
          "snippet": "void *__kmalloc(size_t size, gfp_t flags)\n{\n\treturn __do_kmalloc(size, flags, _RET_IP_);\n}",
          "includes": [
            "#include\t\"slab.h\"",
            "#include\t\"internal.h\"",
            "#include <trace/events/kmem.h>",
            "#include\t<asm/page.h>",
            "#include\t<asm/tlbflush.h>",
            "#include\t<asm/cacheflush.h>",
            "#include\t<net/sock.h>",
            "#include\t<linux/sched/task_stack.h>",
            "#include\t<linux/prefetch.h>",
            "#include\t<linux/memory.h>",
            "#include\t<linux/debugobjects.h>",
            "#include\t<linux/reciprocal_div.h>",
            "#include\t<linux/rtmutex.h>",
            "#include\t<linux/fault-inject.h>",
            "#include\t<linux/mutex.h>",
            "#include\t<linux/mempolicy.h>",
            "#include\t<linux/kmemleak.h>",
            "#include\t<linux/nodemask.h>",
            "#include\t<linux/uaccess.h>",
            "#include\t<linux/string.h>",
            "#include\t<linux/rcupdate.h>",
            "#include\t<linux/module.h>",
            "#include\t<linux/sysctl.h>",
            "#include\t<linux/cpu.h>",
            "#include\t<linux/kallsyms.h>",
            "#include\t<linux/notifier.h>",
            "#include\t<linux/seq_file.h>",
            "#include\t<linux/proc_fs.h>",
            "#include\t<linux/cpuset.h>",
            "#include\t<linux/compiler.h>",
            "#include\t<linux/init.h>",
            "#include\t<linux/interrupt.h>",
            "#include\t<linux/cache.h>",
            "#include\t<linux/swap.h>",
            "#include\t<linux/poison.h>",
            "#include\t<linux/mm.h>",
            "#include\t<linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include\t\"slab.h\"\n#include\t\"internal.h\"\n#include <trace/events/kmem.h>\n#include\t<asm/page.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/cacheflush.h>\n#include\t<net/sock.h>\n#include\t<linux/sched/task_stack.h>\n#include\t<linux/prefetch.h>\n#include\t<linux/memory.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/mutex.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/string.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/module.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/cpu.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/notifier.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/compiler.h>\n#include\t<linux/init.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/cache.h>\n#include\t<linux/swap.h>\n#include\t<linux/poison.h>\n#include\t<linux/mm.h>\n#include\t<linux/slab.h>\n\nvoid *__kmalloc(size_t size, gfp_t flags)\n{\n\treturn __do_kmalloc(size, flags, _RET_IP_);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot)\n{\n\t/*\n\t *  You can't specify __GFP_HIGHMEM with kmalloc() since kmalloc()\n\t * returns only a logical address.\n\t */\n\treturn kmalloc(size, (gfp_mask | __GFP_COMP) & ~__GFP_HIGHMEM);\n}"
  },
  {
    "function_name": "vfree",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "224-227",
    "snippet": "void vfree(const void *addr)\n{\n\tkfree(addr);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "addr"
          ],
          "line": 226
        },
        "resolved": true,
        "details": {
          "function_name": "bulkfree_pcp_prepare",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/page_alloc.c",
          "lines": "1061-1064",
          "snippet": "static bool bulkfree_pcp_prepare(struct page *page)\n{\n\treturn free_pages_check(page);\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/div64.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/sections.h>",
            "#include <linux/psi.h>",
            "#include <linux/nmi.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/kthread.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/migrate.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/prefetch.h>",
            "#include <trace/events/oom.h>",
            "#include <trace/events/kmem.h>",
            "#include <linux/compaction.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/fault-inject.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/pfn.h>",
            "#include <linux/sort.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/memremap.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/topology.h>",
            "#include <linux/oom.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/slab.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/pagevec.h>",
            "#include <linux/suspend.h>",
            "#include <linux/module.h>",
            "#include <linux/kasan.h>",
            "#include <linux/kernel.h>",
            "#include <linux/compiler.h>",
            "#include <linux/memblock.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/stddef.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline\nstruct",
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/div64.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <linux/psi.h>\n#include <linux/nmi.h>\n#include <linux/lockdep.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/kthread.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/rt.h>\n#include <linux/hugetlb.h>\n#include <linux/migrate.h>\n#include <linux/mm_inline.h>\n#include <linux/prefetch.h>\n#include <trace/events/oom.h>\n#include <trace/events/kmem.h>\n#include <linux/compaction.h>\n#include <linux/kmemleak.h>\n#include <linux/debugobjects.h>\n#include <linux/page_ext.h>\n#include <linux/page-isolation.h>\n#include <linux/fault-inject.h>\n#include <linux/backing-dev.h>\n#include <linux/pfn.h>\n#include <linux/sort.h>\n#include <linux/stop_machine.h>\n#include <linux/memremap.h>\n#include <linux/mempolicy.h>\n#include <linux/vmstat.h>\n#include <linux/vmalloc.h>\n#include <linux/nodemask.h>\n#include <linux/memory_hotplug.h>\n#include <linux/cpuset.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/topology.h>\n#include <linux/oom.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/pagevec.h>\n#include <linux/suspend.h>\n#include <linux/module.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/memblock.h>\n#include <linux/jiffies.h>\n#include <linux/pagemap.h>\n#include <linux/interrupt.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/stddef.h>\n\nstatic __always_inline\nstruct;\nstatic __always_inline struct;\n\nstatic bool bulkfree_pcp_prepare(struct page *page)\n{\n\treturn free_pages_check(page);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nvoid vfree(const void *addr)\n{\n\tkfree(addr);\n}"
  },
  {
    "function_name": "follow_pfn",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "211-219",
    "snippet": "int follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn)\n{\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\treturn -EINVAL;\n\n\t*pfn = address >> PAGE_SHIFT;\n\treturn 0;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn)\n{\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\treturn -EINVAL;\n\n\t*pfn = address >> PAGE_SHIFT;\n\treturn 0;\n}"
  },
  {
    "function_name": "get_user_pages_unlocked",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "193-198",
    "snippet": "long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t\t     struct page **pages, unsigned int gup_flags)\n{\n\treturn __get_user_pages_unlocked(current, current->mm, start, nr_pages,\n\t\t\t\t\t pages, gup_flags);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__get_user_pages_unlocked",
          "args": [
            "current",
            "current->mm",
            "start",
            "nr_pages",
            "pages",
            "gup_flags"
          ],
          "line": 196
        },
        "resolved": true,
        "details": {
          "function_name": "__get_user_pages_unlocked",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "180-191",
          "snippet": "static long __get_user_pages_unlocked(struct task_struct *tsk,\n\t\t\tstruct mm_struct *mm, unsigned long start,\n\t\t\tunsigned long nr_pages, struct page **pages,\n\t\t\tunsigned int gup_flags)\n{\n\tlong ret;\n\tdown_read(&mm->mmap_sem);\n\tret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,\n\t\t\t\tNULL, NULL);\n\tup_read(&mm->mmap_sem);\n\treturn ret;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic long __get_user_pages_unlocked(struct task_struct *tsk,\n\t\t\tstruct mm_struct *mm, unsigned long start,\n\t\t\tunsigned long nr_pages, struct page **pages,\n\t\t\tunsigned int gup_flags)\n{\n\tlong ret;\n\tdown_read(&mm->mmap_sem);\n\tret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,\n\t\t\t\tNULL, NULL);\n\tup_read(&mm->mmap_sem);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t\t     struct page **pages, unsigned int gup_flags)\n{\n\treturn __get_user_pages_unlocked(current, current->mm, start, nr_pages,\n\t\t\t\t\t pages, gup_flags);\n}"
  },
  {
    "function_name": "__get_user_pages_unlocked",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "180-191",
    "snippet": "static long __get_user_pages_unlocked(struct task_struct *tsk,\n\t\t\tstruct mm_struct *mm, unsigned long start,\n\t\t\tunsigned long nr_pages, struct page **pages,\n\t\t\tunsigned int gup_flags)\n{\n\tlong ret;\n\tdown_read(&mm->mmap_sem);\n\tret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,\n\t\t\t\tNULL, NULL);\n\tup_read(&mm->mmap_sem);\n\treturn ret;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "up_read",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 189
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__get_user_pages",
          "args": [
            "tsk",
            "mm",
            "start",
            "nr_pages",
            "gup_flags",
            "pages",
            "NULL",
            "NULL"
          ],
          "line": 187
        },
        "resolved": true,
        "details": {
          "function_name": "__get_user_pages",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "113-154",
          "snippet": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long vm_flags;\n\tint i;\n\n\t/* calculate required read or write permissions.\n\t * If FOLL_FORCE is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = (foll_flags & FOLL_WRITE) ?\n\t\t\t(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= (foll_flags & FOLL_FORCE) ?\n\t\t\t(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tvma = find_vma(mm, start);\n\t\tif (!vma)\n\t\t\tgoto finish_or_fault;\n\n\t\t/* protect what we can, including chardevs */\n\t\tif ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||\n\t\t    !(vm_flags & vma->vm_flags))\n\t\t\tgoto finish_or_fault;\n\n\t\tif (pages) {\n\t\t\tpages[i] = virt_to_page(start);\n\t\t\tif (pages[i])\n\t\t\t\tget_page(pages[i]);\n\t\t}\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t}\n\n\treturn i;\n\nfinish_or_fault:\n\treturn i ? : -EFAULT;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long vm_flags;\n\tint i;\n\n\t/* calculate required read or write permissions.\n\t * If FOLL_FORCE is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = (foll_flags & FOLL_WRITE) ?\n\t\t\t(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= (foll_flags & FOLL_FORCE) ?\n\t\t\t(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tvma = find_vma(mm, start);\n\t\tif (!vma)\n\t\t\tgoto finish_or_fault;\n\n\t\t/* protect what we can, including chardevs */\n\t\tif ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||\n\t\t    !(vm_flags & vma->vm_flags))\n\t\t\tgoto finish_or_fault;\n\n\t\tif (pages) {\n\t\t\tpages[i] = virt_to_page(start);\n\t\t\tif (pages[i])\n\t\t\t\tget_page(pages[i]);\n\t\t}\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t}\n\n\treturn i;\n\nfinish_or_fault:\n\treturn i ? : -EFAULT;\n}"
        }
      },
      {
        "call_info": {
          "callee": "down_read",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 186
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic long __get_user_pages_unlocked(struct task_struct *tsk,\n\t\t\tstruct mm_struct *mm, unsigned long start,\n\t\t\tunsigned long nr_pages, struct page **pages,\n\t\t\tunsigned int gup_flags)\n{\n\tlong ret;\n\tdown_read(&mm->mmap_sem);\n\tret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,\n\t\t\t\tNULL, NULL);\n\tup_read(&mm->mmap_sem);\n\treturn ret;\n}"
  },
  {
    "function_name": "get_user_pages_locked",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "172-177",
    "snippet": "long get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t\t    unsigned int gup_flags, struct page **pages,\n\t\t\t    int *locked)\n{\n\treturn get_user_pages(start, nr_pages, gup_flags, pages, NULL);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "get_user_pages",
          "args": [
            "start",
            "nr_pages",
            "gup_flags",
            "pages",
            "NULL"
          ],
          "line": 176
        },
        "resolved": true,
        "details": {
          "function_name": "get_user_pages_locked",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "172-177",
          "snippet": "long get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t\t    unsigned int gup_flags, struct page **pages,\n\t\t\t    int *locked)\n{\n\treturn get_user_pages(start, nr_pages, gup_flags, pages, NULL);\n}",
          "note": "cyclic_reference_detected"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nlong get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t\t    unsigned int gup_flags, struct page **pages,\n\t\t\t    int *locked)\n{\n\treturn get_user_pages(start, nr_pages, gup_flags, pages, NULL);\n}"
  },
  {
    "function_name": "get_user_pages",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "163-169",
    "snippet": "long get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages,\n\t\t    struct vm_area_struct **vmas)\n{\n\treturn __get_user_pages(current, current->mm, start, nr_pages,\n\t\t\t\tgup_flags, pages, vmas, NULL);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__get_user_pages",
          "args": [
            "current",
            "current->mm",
            "start",
            "nr_pages",
            "gup_flags",
            "pages",
            "vmas",
            "NULL"
          ],
          "line": 167
        },
        "resolved": true,
        "details": {
          "function_name": "__get_user_pages",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "113-154",
          "snippet": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long vm_flags;\n\tint i;\n\n\t/* calculate required read or write permissions.\n\t * If FOLL_FORCE is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = (foll_flags & FOLL_WRITE) ?\n\t\t\t(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= (foll_flags & FOLL_FORCE) ?\n\t\t\t(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tvma = find_vma(mm, start);\n\t\tif (!vma)\n\t\t\tgoto finish_or_fault;\n\n\t\t/* protect what we can, including chardevs */\n\t\tif ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||\n\t\t    !(vm_flags & vma->vm_flags))\n\t\t\tgoto finish_or_fault;\n\n\t\tif (pages) {\n\t\t\tpages[i] = virt_to_page(start);\n\t\t\tif (pages[i])\n\t\t\t\tget_page(pages[i]);\n\t\t}\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t}\n\n\treturn i;\n\nfinish_or_fault:\n\treturn i ? : -EFAULT;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long vm_flags;\n\tint i;\n\n\t/* calculate required read or write permissions.\n\t * If FOLL_FORCE is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = (foll_flags & FOLL_WRITE) ?\n\t\t\t(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= (foll_flags & FOLL_FORCE) ?\n\t\t\t(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tvma = find_vma(mm, start);\n\t\tif (!vma)\n\t\t\tgoto finish_or_fault;\n\n\t\t/* protect what we can, including chardevs */\n\t\tif ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||\n\t\t    !(vm_flags & vma->vm_flags))\n\t\t\tgoto finish_or_fault;\n\n\t\tif (pages) {\n\t\t\tpages[i] = virt_to_page(start);\n\t\t\tif (pages[i])\n\t\t\t\tget_page(pages[i]);\n\t\t}\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t}\n\n\treturn i;\n\nfinish_or_fault:\n\treturn i ? : -EFAULT;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages,\n\t\t    struct vm_area_struct **vmas)\n{\n\treturn __get_user_pages(current, current->mm, start, nr_pages,\n\t\t\t\tgup_flags, pages, vmas, NULL);\n}"
  },
  {
    "function_name": "__get_user_pages",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "113-154",
    "snippet": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long vm_flags;\n\tint i;\n\n\t/* calculate required read or write permissions.\n\t * If FOLL_FORCE is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = (foll_flags & FOLL_WRITE) ?\n\t\t\t(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= (foll_flags & FOLL_FORCE) ?\n\t\t\t(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tvma = find_vma(mm, start);\n\t\tif (!vma)\n\t\t\tgoto finish_or_fault;\n\n\t\t/* protect what we can, including chardevs */\n\t\tif ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||\n\t\t    !(vm_flags & vma->vm_flags))\n\t\t\tgoto finish_or_fault;\n\n\t\tif (pages) {\n\t\t\tpages[i] = virt_to_page(start);\n\t\t\tif (pages[i])\n\t\t\t\tget_page(pages[i]);\n\t\t}\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t}\n\n\treturn i;\n\nfinish_or_fault:\n\treturn i ? : -EFAULT;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "get_page",
          "args": [
            "pages[i]"
          ],
          "line": 143
        },
        "resolved": true,
        "details": {
          "function_name": "pcpu_get_page_chunk",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/percpu.c",
          "lines": "247-250",
          "snippet": "static struct pcpu_chunk *pcpu_get_page_chunk(struct page *page)\n{\n\treturn (struct pcpu_chunk *)page->index;\n}",
          "includes": [
            "#include \"percpu-vm.c\"",
            "#include \"percpu-km.c\"",
            "#include \"percpu-internal.h\"",
            "#include <trace/events/percpu.h>",
            "#include <asm/io.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/sections.h>",
            "#include <asm/cacheflush.h>",
            "#include <linux/sched.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/pfn.h>",
            "#include <linux/percpu.h>",
            "#include <linux/mutex.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/log2.h>",
            "#include <linux/list.h>",
            "#include <linux/lcm.h>",
            "#include <linux/err.h>",
            "#include <linux/memblock.h>",
            "#include <linux/bitmap.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"percpu-vm.c\"\n#include \"percpu-km.c\"\n#include \"percpu-internal.h\"\n#include <trace/events/percpu.h>\n#include <asm/io.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <asm/cacheflush.h>\n#include <linux/sched.h>\n#include <linux/kmemleak.h>\n#include <linux/workqueue.h>\n#include <linux/vmalloc.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/pfn.h>\n#include <linux/percpu.h>\n#include <linux/mutex.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/log2.h>\n#include <linux/list.h>\n#include <linux/lcm.h>\n#include <linux/err.h>\n#include <linux/memblock.h>\n#include <linux/bitmap.h>\n\nstatic struct pcpu_chunk *pcpu_get_page_chunk(struct page *page)\n{\n\treturn (struct pcpu_chunk *)page->index;\n}"
        }
      },
      {
        "call_info": {
          "callee": "virt_to_page",
          "args": [
            "start"
          ],
          "line": 141
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_vma",
          "args": [
            "mm",
            "start"
          ],
          "line": 131
        },
        "resolved": true,
        "details": {
          "function_name": "find_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "775-796",
          "snippet": "struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstatic long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t      unsigned long start, unsigned long nr_pages,\n\t\t      unsigned int foll_flags, struct page **pages,\n\t\t      struct vm_area_struct **vmas, int *nonblocking)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long vm_flags;\n\tint i;\n\n\t/* calculate required read or write permissions.\n\t * If FOLL_FORCE is set, we only require the \"MAY\" flags.\n\t */\n\tvm_flags  = (foll_flags & FOLL_WRITE) ?\n\t\t\t(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= (foll_flags & FOLL_FORCE) ?\n\t\t\t(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tvma = find_vma(mm, start);\n\t\tif (!vma)\n\t\t\tgoto finish_or_fault;\n\n\t\t/* protect what we can, including chardevs */\n\t\tif ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||\n\t\t    !(vm_flags & vma->vm_flags))\n\t\t\tgoto finish_or_fault;\n\n\t\tif (pages) {\n\t\t\tpages[i] = virt_to_page(start);\n\t\t\tif (pages[i])\n\t\t\t\tget_page(pages[i]);\n\t\t}\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t}\n\n\treturn i;\n\nfinish_or_fault:\n\treturn i ? : -EFAULT;\n}"
  },
  {
    "function_name": "kobjsize",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "72-111",
    "snippet": "unsigned int kobjsize(const void *objp)\n{\n\tstruct page *page;\n\n\t/*\n\t * If the object we have should not have ksize performed on it,\n\t * return size of 0\n\t */\n\tif (!objp || !virt_addr_valid(objp))\n\t\treturn 0;\n\n\tpage = virt_to_head_page(objp);\n\n\t/*\n\t * If the allocator sets PageSlab, we know the pointer came from\n\t * kmalloc().\n\t */\n\tif (PageSlab(page))\n\t\treturn ksize(objp);\n\n\t/*\n\t * If it's not a compound page, see if we have a matching VMA\n\t * region. This test is intentionally done in reverse order,\n\t * so if there's no VMA, we still fall through and hand back\n\t * PAGE_SIZE for 0-order pages.\n\t */\n\tif (!PageCompound(page)) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tvma = find_vma(current->mm, (unsigned long)objp);\n\t\tif (vma)\n\t\t\treturn vma->vm_end - vma->vm_start;\n\t}\n\n\t/*\n\t * The ksize() function is only guaranteed to work for pointers\n\t * returned by kmalloc(). So handle arbitrary pointers here.\n\t */\n\treturn PAGE_SIZE << compound_order(page);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "compound_order",
          "args": [
            "page"
          ],
          "line": 110
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_vma",
          "args": [
            "current->mm",
            "(unsigned long)objp"
          ],
          "line": 101
        },
        "resolved": true,
        "details": {
          "function_name": "find_vma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
          "lines": "775-796",
          "snippet": "struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"internal.h\"",
            "#include <asm/mmu_context.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/tlb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/printk.h>",
            "#include <linux/audit.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/personality.h>",
            "#include <linux/mount.h>",
            "#include <linux/compiler.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/file.h>",
            "#include <linux/swap.h>",
            "#include <linux/mman.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/mm.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t/* check the cache first */\n\tvma = vmacache_find(mm, addr);\n\tif (likely(vma))\n\t\treturn vma;\n\n\t/* trawl the list (there may be multiple mappings in which addr\n\t * resides) */\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tif (vma->vm_start > addr)\n\t\t\treturn NULL;\n\t\tif (vma->vm_end > addr) {\n\t\t\tvmacache_update(addr, vma);\n\t\t\treturn vma;\n\t\t}\n\t}\n\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "PageCompound",
          "args": [
            "page"
          ],
          "line": 98
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ksize",
          "args": [
            "objp"
          ],
          "line": 90
        },
        "resolved": true,
        "details": {
          "function_name": "ksize",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/slab.c",
          "lines": "4465-4480",
          "snippet": "size_t ksize(const void *objp)\n{\n\tsize_t size;\n\n\tBUG_ON(!objp);\n\tif (unlikely(objp == ZERO_SIZE_PTR))\n\t\treturn 0;\n\n\tsize = virt_to_cache(objp)->object_size;\n\t/* We assume that ksize callers could use the whole allocated area,\n\t * so we need to unpoison this area.\n\t */\n\tkasan_unpoison_shadow(objp, size);\n\n\treturn size;\n}",
          "includes": [
            "#include\t\"slab.h\"",
            "#include\t\"internal.h\"",
            "#include <trace/events/kmem.h>",
            "#include\t<asm/page.h>",
            "#include\t<asm/tlbflush.h>",
            "#include\t<asm/cacheflush.h>",
            "#include\t<net/sock.h>",
            "#include\t<linux/sched/task_stack.h>",
            "#include\t<linux/prefetch.h>",
            "#include\t<linux/memory.h>",
            "#include\t<linux/debugobjects.h>",
            "#include\t<linux/reciprocal_div.h>",
            "#include\t<linux/rtmutex.h>",
            "#include\t<linux/fault-inject.h>",
            "#include\t<linux/mutex.h>",
            "#include\t<linux/mempolicy.h>",
            "#include\t<linux/kmemleak.h>",
            "#include\t<linux/nodemask.h>",
            "#include\t<linux/uaccess.h>",
            "#include\t<linux/string.h>",
            "#include\t<linux/rcupdate.h>",
            "#include\t<linux/module.h>",
            "#include\t<linux/sysctl.h>",
            "#include\t<linux/cpu.h>",
            "#include\t<linux/kallsyms.h>",
            "#include\t<linux/notifier.h>",
            "#include\t<linux/seq_file.h>",
            "#include\t<linux/proc_fs.h>",
            "#include\t<linux/cpuset.h>",
            "#include\t<linux/compiler.h>",
            "#include\t<linux/init.h>",
            "#include\t<linux/interrupt.h>",
            "#include\t<linux/cache.h>",
            "#include\t<linux/swap.h>",
            "#include\t<linux/poison.h>",
            "#include\t<linux/mm.h>",
            "#include\t<linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include\t\"slab.h\"\n#include\t\"internal.h\"\n#include <trace/events/kmem.h>\n#include\t<asm/page.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/cacheflush.h>\n#include\t<net/sock.h>\n#include\t<linux/sched/task_stack.h>\n#include\t<linux/prefetch.h>\n#include\t<linux/memory.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/mutex.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/string.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/module.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/cpu.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/notifier.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/compiler.h>\n#include\t<linux/init.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/cache.h>\n#include\t<linux/swap.h>\n#include\t<linux/poison.h>\n#include\t<linux/mm.h>\n#include\t<linux/slab.h>\n\nsize_t ksize(const void *objp)\n{\n\tsize_t size;\n\n\tBUG_ON(!objp);\n\tif (unlikely(objp == ZERO_SIZE_PTR))\n\t\treturn 0;\n\n\tsize = virt_to_cache(objp)->object_size;\n\t/* We assume that ksize callers could use the whole allocated area,\n\t * so we need to unpoison this area.\n\t */\n\tkasan_unpoison_shadow(objp, size);\n\n\treturn size;\n}"
        }
      },
      {
        "call_info": {
          "callee": "PageSlab",
          "args": [
            "page"
          ],
          "line": 89
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "virt_to_head_page",
          "args": [
            "objp"
          ],
          "line": 83
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "virt_addr_valid",
          "args": [
            "objp"
          ],
          "line": 80
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nunsigned int kobjsize(const void *objp)\n{\n\tstruct page *page;\n\n\t/*\n\t * If the object we have should not have ksize performed on it,\n\t * return size of 0\n\t */\n\tif (!objp || !virt_addr_valid(objp))\n\t\treturn 0;\n\n\tpage = virt_to_head_page(objp);\n\n\t/*\n\t * If the allocator sets PageSlab, we know the pointer came from\n\t * kmalloc().\n\t */\n\tif (PageSlab(page))\n\t\treturn ksize(objp);\n\n\t/*\n\t * If it's not a compound page, see if we have a matching VMA\n\t * region. This test is intentionally done in reverse order,\n\t * so if there's no VMA, we still fall through and hand back\n\t * PAGE_SIZE for 0-order pages.\n\t */\n\tif (!PageCompound(page)) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tvma = find_vma(current->mm, (unsigned long)objp);\n\t\tif (vma)\n\t\t\treturn vma->vm_end - vma->vm_start;\n\t}\n\n\t/*\n\t * The ksize() function is only guaranteed to work for pointers\n\t * returned by kmalloc(). So handle arbitrary pointers here.\n\t */\n\treturn PAGE_SIZE << compound_order(page);\n}"
  },
  {
    "function_name": "brk",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "483-506",
    "snippet": "SYSCALL_DEFINE1(brk, unsigned long, brk)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif (brk < mm->start_brk || brk > mm->context.end_brk)\n\t\treturn mm->brk;\n\n\tif (mm->brk == brk)\n\t\treturn mm->brk;\n\n\t/*\n\t * Always allow shrinking brk\n\t */\n\tif (brk <= mm->brk) {\n\t\tmm->brk = brk;\n\t\treturn brk;\n\t}\n\n\t/*\n\t * Ok, looks good - let it rip.\n\t */\n\tflush_icache_range(mm->brk, brk);\n\treturn mm->brk = brk;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nSYSCALL_DEFINE1(brk, unsigned long, brk)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif (brk < mm->start_brk || brk > mm->context.end_brk)\n\t\treturn mm->brk;\n\n\tif (mm->brk == brk)\n\t\treturn mm->brk;\n\n\t/*\n\t * Always allow shrinking brk\n\t */\n\tif (brk <= mm->brk) {\n\t\tmm->brk = brk;\n\t\treturn brk;\n\t}\n\n\t/*\n\t * Ok, looks good - let it rip.\n\t */\n\tflush_icache_range(mm->brk, brk);\n\treturn mm->brk = brk;\n}"
  },
  {
    "function_name": "mmap_pgoff",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1415-1420",
    "snippet": "SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,\n\t\tunsigned long, prot, unsigned long, flags,\n\t\tunsigned long, fd, unsigned long, pgoff)\n{\n\treturn ksys_mmap_pgoff(addr, len, prot, flags, fd, pgoff);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nSYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,\n\t\tunsigned long, prot, unsigned long, flags,\n\t\tunsigned long, fd, unsigned long, pgoff)\n{\n\treturn ksys_mmap_pgoff(addr, len, prot, flags, fd, pgoff);\n}"
  },
  {
    "function_name": "old_mmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1432-1443",
    "snippet": "SYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)\n{\n\tstruct mmap_arg_struct a;\n\n\tif (copy_from_user(&a, arg, sizeof(a)))\n\t\treturn -EFAULT;\n\tif (offset_in_page(a.offset))\n\t\treturn -EINVAL;\n\n\treturn ksys_mmap_pgoff(a.addr, a.len, a.prot, a.flags, a.fd,\n\t\t\t       a.offset >> PAGE_SHIFT);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nSYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)\n{\n\tstruct mmap_arg_struct a;\n\n\tif (copy_from_user(&a, arg, sizeof(a)))\n\t\treturn -EFAULT;\n\tif (offset_in_page(a.offset))\n\t\treturn -EINVAL;\n\n\treturn ksys_mmap_pgoff(a.addr, a.len, a.prot, a.flags, a.fd,\n\t\t\t       a.offset >> PAGE_SHIFT);\n}"
  },
  {
    "function_name": "munmap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1624-1627",
    "snippet": "SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)\n{\n\treturn vm_munmap(addr, len);\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nSYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)\n{\n\treturn vm_munmap(addr, len);\n}"
  },
  {
    "function_name": "mremap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18397/repo/mm/nommu.c",
    "lines": "1700-1710",
    "snippet": "SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tunsigned long ret;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mremap(addr, old_len, new_len, flags, new_addr);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}",
    "includes": [
      "#include \"internal.h\"",
      "#include <asm/mmu_context.h>",
      "#include <asm/tlbflush.h>",
      "#include <asm/tlb.h>",
      "#include <linux/uaccess.h>",
      "#include <linux/printk.h>",
      "#include <linux/audit.h>",
      "#include <linux/syscalls.h>",
      "#include <linux/security.h>",
      "#include <linux/personality.h>",
      "#include <linux/mount.h>",
      "#include <linux/compiler.h>",
      "#include <linux/backing-dev.h>",
      "#include <linux/blkdev.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/pagemap.h>",
      "#include <linux/highmem.h>",
      "#include <linux/file.h>",
      "#include <linux/swap.h>",
      "#include <linux/mman.h>",
      "#include <linux/vmacache.h>",
      "#include <linux/sched/mm.h>",
      "#include <linux/mm.h>",
      "#include <linux/export.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"internal.h\"\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n#include <linux/printk.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/personality.h>\n#include <linux/mount.h>\n#include <linux/compiler.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/vmacache.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tunsigned long ret;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mremap(addr, old_len, new_len, flags, new_addr);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}"
  }
]