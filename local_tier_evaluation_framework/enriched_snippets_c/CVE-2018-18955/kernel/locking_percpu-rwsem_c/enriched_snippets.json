[
  {
    "function_name": "percpu_up_write",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
    "lines": "166-191",
    "snippet": "void percpu_up_write(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * Signal the writer is done, no fast path yet.\n\t *\n\t * One reason that we cannot just immediately flip to readers_fast is\n\t * that new readers might fail to see the results of this writer's\n\t * critical section.\n\t *\n\t * Therefore we force it through the slow path which guarantees an\n\t * acquire and thereby guarantees the critical section's consistency.\n\t */\n\tsmp_store_release(&sem->readers_block, 0);\n\n\t/*\n\t * Release the write lock, this will allow readers back in the game.\n\t */\n\tup_write(&sem->rw_sem);\n\n\t/*\n\t * Once this completes (at least one RCU-sched grace period hence) the\n\t * reader fast path will be available again. Safe to use outside the\n\t * exclusive write lock because its counting.\n\t */\n\trcu_sync_exit(&sem->rss);\n}",
    "includes": [
      "#include <linux/errno.h>",
      "#include <linux/sched.h>",
      "#include <linux/rcupdate.h>",
      "#include <linux/percpu-rwsem.h>",
      "#include <linux/lockdep.h>",
      "#include <linux/percpu.h>",
      "#include <linux/rwsem.h>",
      "#include <linux/atomic.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_sync_exit",
          "args": [
            "&sem->rss"
          ],
          "line": 190
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_sync_exit",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/sync.c",
          "lines": "206-218",
          "snippet": "void rcu_sync_exit(struct rcu_sync *rsp)\n{\n\tspin_lock_irq(&rsp->rss_lock);\n\tif (!--rsp->gp_count) {\n\t\tif (rsp->cb_state == CB_IDLE) {\n\t\t\trsp->cb_state = CB_PENDING;\n\t\t\tgp_ops[rsp->gp_type].call(&rsp->cb_head, rcu_sync_func);\n\t\t} else if (rsp->cb_state == CB_PENDING) {\n\t\t\trsp->cb_state = CB_REPLAY;\n\t\t}\n\t}\n\tspin_unlock_irq(&rsp->rss_lock);\n}",
          "includes": [
            "#include <linux/sched.h>",
            "#include <linux/rcu_sync.h>"
          ],
          "macros_used": [
            "#define\trss_lock\tgp_wait.lock"
          ],
          "globals_used": [
            "static const struct {\n\tvoid (*sync)(void);\n\tvoid (*call)(struct rcu_head *, void (*)(struct rcu_head *));\n\tvoid (*wait)(void);\n#ifdef CONFIG_PROVE_RCU\n\tint  (*held)(void);\n#endif\n} gp_ops[] = {\n\t[RCU_SYNC] = {\n\t\t.sync = synchronize_rcu,\n\t\t.call = call_rcu,\n\t\t.wait = rcu_barrier,\n\t\t__INIT_HELD(rcu_read_lock_held)\n\t},\n\t[RCU_SCHED_SYNC] = {\n\t\t.sync = synchronize_sched,\n\t\t.call = call_rcu_sched,\n\t\t.wait = rcu_barrier_sched,\n\t\t__INIT_HELD(rcu_read_lock_sched_held)\n\t},\n\t[RCU_BH_SYNC] = {\n\t\t.sync = synchronize_rcu_bh,\n\t\t.call = call_rcu_bh,\n\t\t.wait = rcu_barrier_bh,\n\t\t__INIT_HELD(rcu_read_lock_bh_held)\n\t},\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched.h>\n#include <linux/rcu_sync.h>\n\n#define\trss_lock\tgp_wait.lock\n\nstatic const struct {\n\tvoid (*sync)(void);\n\tvoid (*call)(struct rcu_head *, void (*)(struct rcu_head *));\n\tvoid (*wait)(void);\n#ifdef CONFIG_PROVE_RCU\n\tint  (*held)(void);\n#endif\n} gp_ops[] = {\n\t[RCU_SYNC] = {\n\t\t.sync = synchronize_rcu,\n\t\t.call = call_rcu,\n\t\t.wait = rcu_barrier,\n\t\t__INIT_HELD(rcu_read_lock_held)\n\t},\n\t[RCU_SCHED_SYNC] = {\n\t\t.sync = synchronize_sched,\n\t\t.call = call_rcu_sched,\n\t\t.wait = rcu_barrier_sched,\n\t\t__INIT_HELD(rcu_read_lock_sched_held)\n\t},\n\t[RCU_BH_SYNC] = {\n\t\t.sync = synchronize_rcu_bh,\n\t\t.call = call_rcu_bh,\n\t\t.wait = rcu_barrier_bh,\n\t\t__INIT_HELD(rcu_read_lock_bh_held)\n\t},\n};\n\nvoid rcu_sync_exit(struct rcu_sync *rsp)\n{\n\tspin_lock_irq(&rsp->rss_lock);\n\tif (!--rsp->gp_count) {\n\t\tif (rsp->cb_state == CB_IDLE) {\n\t\t\trsp->cb_state = CB_PENDING;\n\t\t\tgp_ops[rsp->gp_type].call(&rsp->cb_head, rcu_sync_func);\n\t\t} else if (rsp->cb_state == CB_PENDING) {\n\t\t\trsp->cb_state = CB_REPLAY;\n\t\t}\n\t}\n\tspin_unlock_irq(&rsp->rss_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "up_write",
          "args": [
            "&sem->rw_sem"
          ],
          "line": 183
        },
        "resolved": true,
        "details": {
          "function_name": "percpu_up_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "166-191",
          "snippet": "void percpu_up_write(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * Signal the writer is done, no fast path yet.\n\t *\n\t * One reason that we cannot just immediately flip to readers_fast is\n\t * that new readers might fail to see the results of this writer's\n\t * critical section.\n\t *\n\t * Therefore we force it through the slow path which guarantees an\n\t * acquire and thereby guarantees the critical section's consistency.\n\t */\n\tsmp_store_release(&sem->readers_block, 0);\n\n\t/*\n\t * Release the write lock, this will allow readers back in the game.\n\t */\n\tup_write(&sem->rw_sem);\n\n\t/*\n\t * Once this completes (at least one RCU-sched grace period hence) the\n\t * reader fast path will be available again. Safe to use outside the\n\t * exclusive write lock because its counting.\n\t */\n\trcu_sync_exit(&sem->rss);\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "smp_store_release",
          "args": [
            "&sem->readers_block",
            "0"
          ],
          "line": 178
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid percpu_up_write(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * Signal the writer is done, no fast path yet.\n\t *\n\t * One reason that we cannot just immediately flip to readers_fast is\n\t * that new readers might fail to see the results of this writer's\n\t * critical section.\n\t *\n\t * Therefore we force it through the slow path which guarantees an\n\t * acquire and thereby guarantees the critical section's consistency.\n\t */\n\tsmp_store_release(&sem->readers_block, 0);\n\n\t/*\n\t * Release the write lock, this will allow readers back in the game.\n\t */\n\tup_write(&sem->rw_sem);\n\n\t/*\n\t * Once this completes (at least one RCU-sched grace period hence) the\n\t * reader fast path will be available again. Safe to use outside the\n\t * exclusive write lock because its counting.\n\t */\n\trcu_sync_exit(&sem->rss);\n}"
  },
  {
    "function_name": "percpu_down_write",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
    "lines": "140-163",
    "snippet": "void percpu_down_write(struct percpu_rw_semaphore *sem)\n{\n\t/* Notify readers to take the slow path. */\n\trcu_sync_enter(&sem->rss);\n\n\tdown_write(&sem->rw_sem);\n\n\t/*\n\t * Notify new readers to block; up until now, and thus throughout the\n\t * longish rcu_sync_enter() above, new readers could still come in.\n\t */\n\tWRITE_ONCE(sem->readers_block, 1);\n\n\tsmp_mb(); /* D matches A */\n\n\t/*\n\t * If they don't see our writer of readers_block, then we are\n\t * guaranteed to see their sem->read_count increment, and therefore\n\t * will wait for them.\n\t */\n\n\t/* Wait for all now active readers to complete. */\n\trcuwait_wait_event(&sem->writer, readers_active_check(sem));\n}",
    "includes": [
      "#include <linux/errno.h>",
      "#include <linux/sched.h>",
      "#include <linux/rcupdate.h>",
      "#include <linux/percpu-rwsem.h>",
      "#include <linux/lockdep.h>",
      "#include <linux/percpu.h>",
      "#include <linux/rwsem.h>",
      "#include <linux/atomic.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcuwait_wait_event",
          "args": [
            "&sem->writer",
            "readers_active_check(sem)"
          ],
          "line": 162
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "readers_active_check",
          "args": [
            "sem"
          ],
          "line": 162
        },
        "resolved": true,
        "details": {
          "function_name": "readers_active_check",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "125-138",
          "snippet": "static bool readers_active_check(struct percpu_rw_semaphore *sem)\n{\n\tif (per_cpu_sum(*sem->read_count) != 0)\n\t\treturn false;\n\n\t/*\n\t * If we observed the decrement; ensure we see the entire critical\n\t * section.\n\t */\n\n\tsmp_mb(); /* C matches B */\n\n\treturn true;\n}",
          "includes": [
            "#include <linux/errno.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/percpu.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nstatic bool readers_active_check(struct percpu_rw_semaphore *sem)\n{\n\tif (per_cpu_sum(*sem->read_count) != 0)\n\t\treturn false;\n\n\t/*\n\t * If we observed the decrement; ensure we see the entire critical\n\t * section.\n\t */\n\n\tsmp_mb(); /* C matches B */\n\n\treturn true;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 153
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "sem->readers_block",
            "1"
          ],
          "line": 151
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "down_write",
          "args": [
            "&sem->rw_sem"
          ],
          "line": 145
        },
        "resolved": true,
        "details": {
          "function_name": "percpu_down_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "140-163",
          "snippet": "void percpu_down_write(struct percpu_rw_semaphore *sem)\n{\n\t/* Notify readers to take the slow path. */\n\trcu_sync_enter(&sem->rss);\n\n\tdown_write(&sem->rw_sem);\n\n\t/*\n\t * Notify new readers to block; up until now, and thus throughout the\n\t * longish rcu_sync_enter() above, new readers could still come in.\n\t */\n\tWRITE_ONCE(sem->readers_block, 1);\n\n\tsmp_mb(); /* D matches A */\n\n\t/*\n\t * If they don't see our writer of readers_block, then we are\n\t * guaranteed to see their sem->read_count increment, and therefore\n\t * will wait for them.\n\t */\n\n\t/* Wait for all now active readers to complete. */\n\trcuwait_wait_event(&sem->writer, readers_active_check(sem));\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "rcu_sync_enter",
          "args": [
            "&sem->rss"
          ],
          "line": 143
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_sync_enter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/sync.c",
          "lines": "117-144",
          "snippet": "void rcu_sync_enter(struct rcu_sync *rsp)\n{\n\tbool need_wait, need_sync;\n\n\tspin_lock_irq(&rsp->rss_lock);\n\tneed_wait = rsp->gp_count++;\n\tneed_sync = rsp->gp_state == GP_IDLE;\n\tif (need_sync)\n\t\trsp->gp_state = GP_PENDING;\n\tspin_unlock_irq(&rsp->rss_lock);\n\n\tBUG_ON(need_wait && need_sync);\n\n\tif (need_sync) {\n\t\tgp_ops[rsp->gp_type].sync();\n\t\trsp->gp_state = GP_PASSED;\n\t\twake_up_all(&rsp->gp_wait);\n\t} else if (need_wait) {\n\t\twait_event(rsp->gp_wait, rsp->gp_state == GP_PASSED);\n\t} else {\n\t\t/*\n\t\t * Possible when there's a pending CB from a rcu_sync_exit().\n\t\t * Nobody has yet been allowed the 'fast' path and thus we can\n\t\t * avoid doing any sync(). The callback will get 'dropped'.\n\t\t */\n\t\tBUG_ON(rsp->gp_state != GP_PASSED);\n\t}\n}",
          "includes": [
            "#include <linux/sched.h>",
            "#include <linux/rcu_sync.h>"
          ],
          "macros_used": [
            "#define\trss_lock\tgp_wait.lock"
          ],
          "globals_used": [
            "static const struct {\n\tvoid (*sync)(void);\n\tvoid (*call)(struct rcu_head *, void (*)(struct rcu_head *));\n\tvoid (*wait)(void);\n#ifdef CONFIG_PROVE_RCU\n\tint  (*held)(void);\n#endif\n} gp_ops[] = {\n\t[RCU_SYNC] = {\n\t\t.sync = synchronize_rcu,\n\t\t.call = call_rcu,\n\t\t.wait = rcu_barrier,\n\t\t__INIT_HELD(rcu_read_lock_held)\n\t},\n\t[RCU_SCHED_SYNC] = {\n\t\t.sync = synchronize_sched,\n\t\t.call = call_rcu_sched,\n\t\t.wait = rcu_barrier_sched,\n\t\t__INIT_HELD(rcu_read_lock_sched_held)\n\t},\n\t[RCU_BH_SYNC] = {\n\t\t.sync = synchronize_rcu_bh,\n\t\t.call = call_rcu_bh,\n\t\t.wait = rcu_barrier_bh,\n\t\t__INIT_HELD(rcu_read_lock_bh_held)\n\t},\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched.h>\n#include <linux/rcu_sync.h>\n\n#define\trss_lock\tgp_wait.lock\n\nstatic const struct {\n\tvoid (*sync)(void);\n\tvoid (*call)(struct rcu_head *, void (*)(struct rcu_head *));\n\tvoid (*wait)(void);\n#ifdef CONFIG_PROVE_RCU\n\tint  (*held)(void);\n#endif\n} gp_ops[] = {\n\t[RCU_SYNC] = {\n\t\t.sync = synchronize_rcu,\n\t\t.call = call_rcu,\n\t\t.wait = rcu_barrier,\n\t\t__INIT_HELD(rcu_read_lock_held)\n\t},\n\t[RCU_SCHED_SYNC] = {\n\t\t.sync = synchronize_sched,\n\t\t.call = call_rcu_sched,\n\t\t.wait = rcu_barrier_sched,\n\t\t__INIT_HELD(rcu_read_lock_sched_held)\n\t},\n\t[RCU_BH_SYNC] = {\n\t\t.sync = synchronize_rcu_bh,\n\t\t.call = call_rcu_bh,\n\t\t.wait = rcu_barrier_bh,\n\t\t__INIT_HELD(rcu_read_lock_bh_held)\n\t},\n};\n\nvoid rcu_sync_enter(struct rcu_sync *rsp)\n{\n\tbool need_wait, need_sync;\n\n\tspin_lock_irq(&rsp->rss_lock);\n\tneed_wait = rsp->gp_count++;\n\tneed_sync = rsp->gp_state == GP_IDLE;\n\tif (need_sync)\n\t\trsp->gp_state = GP_PENDING;\n\tspin_unlock_irq(&rsp->rss_lock);\n\n\tBUG_ON(need_wait && need_sync);\n\n\tif (need_sync) {\n\t\tgp_ops[rsp->gp_type].sync();\n\t\trsp->gp_state = GP_PASSED;\n\t\twake_up_all(&rsp->gp_wait);\n\t} else if (need_wait) {\n\t\twait_event(rsp->gp_wait, rsp->gp_state == GP_PASSED);\n\t} else {\n\t\t/*\n\t\t * Possible when there's a pending CB from a rcu_sync_exit().\n\t\t * Nobody has yet been allowed the 'fast' path and thus we can\n\t\t * avoid doing any sync(). The callback will get 'dropped'.\n\t\t */\n\t\tBUG_ON(rsp->gp_state != GP_PASSED);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid percpu_down_write(struct percpu_rw_semaphore *sem)\n{\n\t/* Notify readers to take the slow path. */\n\trcu_sync_enter(&sem->rss);\n\n\tdown_write(&sem->rw_sem);\n\n\t/*\n\t * Notify new readers to block; up until now, and thus throughout the\n\t * longish rcu_sync_enter() above, new readers could still come in.\n\t */\n\tWRITE_ONCE(sem->readers_block, 1);\n\n\tsmp_mb(); /* D matches A */\n\n\t/*\n\t * If they don't see our writer of readers_block, then we are\n\t * guaranteed to see their sem->read_count increment, and therefore\n\t * will wait for them.\n\t */\n\n\t/* Wait for all now active readers to complete. */\n\trcuwait_wait_event(&sem->writer, readers_active_check(sem));\n}"
  },
  {
    "function_name": "readers_active_check",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
    "lines": "125-138",
    "snippet": "static bool readers_active_check(struct percpu_rw_semaphore *sem)\n{\n\tif (per_cpu_sum(*sem->read_count) != 0)\n\t\treturn false;\n\n\t/*\n\t * If we observed the decrement; ensure we see the entire critical\n\t * section.\n\t */\n\n\tsmp_mb(); /* C matches B */\n\n\treturn true;\n}",
    "includes": [
      "#include <linux/errno.h>",
      "#include <linux/sched.h>",
      "#include <linux/rcupdate.h>",
      "#include <linux/percpu-rwsem.h>",
      "#include <linux/lockdep.h>",
      "#include <linux/percpu.h>",
      "#include <linux/rwsem.h>",
      "#include <linux/atomic.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 135
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "per_cpu_sum",
          "args": [
            "*sem->read_count"
          ],
          "line": 127
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nstatic bool readers_active_check(struct percpu_rw_semaphore *sem)\n{\n\tif (per_cpu_sum(*sem->read_count) != 0)\n\t\treturn false;\n\n\t/*\n\t * If we observed the decrement; ensure we see the entire critical\n\t * section.\n\t */\n\n\tsmp_mb(); /* C matches B */\n\n\treturn true;\n}"
  },
  {
    "function_name": "__percpu_up_read",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
    "lines": "94-106",
    "snippet": "void __percpu_up_read(struct percpu_rw_semaphore *sem)\n{\n\tsmp_mb(); /* B matches C */\n\t/*\n\t * In other words, if they see our decrement (presumably to aggregate\n\t * zero, as that is the only time it matters) they will also see our\n\t * critical section.\n\t */\n\t__this_cpu_dec(*sem->read_count);\n\n\t/* Prod writer to recheck readers_active */\n\trcuwait_wake_up(&sem->writer);\n}",
    "includes": [
      "#include <linux/errno.h>",
      "#include <linux/sched.h>",
      "#include <linux/rcupdate.h>",
      "#include <linux/percpu-rwsem.h>",
      "#include <linux/lockdep.h>",
      "#include <linux/percpu.h>",
      "#include <linux/rwsem.h>",
      "#include <linux/atomic.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcuwait_wake_up",
          "args": [
            "&sem->writer"
          ],
          "line": 105
        },
        "resolved": true,
        "details": {
          "function_name": "rcuwait_wake_up",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "293-320",
          "snippet": "void rcuwait_wake_up(struct rcuwait *w)\n{\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\n\t/*\n\t * Order condition vs @task, such that everything prior to the load\n\t * of @task is visible. This is the condition as to why the user called\n\t * rcuwait_trywake() in the first place. Pairs with set_current_state()\n\t * barrier (A) in rcuwait_wait_event().\n\t *\n\t *    WAIT                WAKE\n\t *    [S] tsk = current\t  [S] cond = true\n\t *        MB (A)\t      MB (B)\n\t *    [L] cond\t\t  [L] tsk\n\t */\n\tsmp_rmb(); /* (B) */\n\n\t/*\n\t * Avoid using task_rcu_dereference() magic as long as we are careful,\n\t * see comment in rcuwait_wait_event() regarding ->exit_state.\n\t */\n\ttask = rcu_dereference(w->task);\n\tif (task)\n\t\twake_up_process(task);\n\trcu_read_unlock();\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nvoid rcuwait_wake_up(struct rcuwait *w)\n{\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\n\t/*\n\t * Order condition vs @task, such that everything prior to the load\n\t * of @task is visible. This is the condition as to why the user called\n\t * rcuwait_trywake() in the first place. Pairs with set_current_state()\n\t * barrier (A) in rcuwait_wait_event().\n\t *\n\t *    WAIT                WAKE\n\t *    [S] tsk = current\t  [S] cond = true\n\t *        MB (A)\t      MB (B)\n\t *    [L] cond\t\t  [L] tsk\n\t */\n\tsmp_rmb(); /* (B) */\n\n\t/*\n\t * Avoid using task_rcu_dereference() magic as long as we are careful,\n\t * see comment in rcuwait_wait_event() regarding ->exit_state.\n\t */\n\ttask = rcu_dereference(w->task);\n\tif (task)\n\t\twake_up_process(task);\n\trcu_read_unlock();\n}"
        }
      },
      {
        "call_info": {
          "callee": "__this_cpu_dec",
          "args": [
            "*sem->read_count"
          ],
          "line": 102
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 96
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid __percpu_up_read(struct percpu_rw_semaphore *sem)\n{\n\tsmp_mb(); /* B matches C */\n\t/*\n\t * In other words, if they see our decrement (presumably to aggregate\n\t * zero, as that is the only time it matters) they will also see our\n\t * critical section.\n\t */\n\t__this_cpu_dec(*sem->read_count);\n\n\t/* Prod writer to recheck readers_active */\n\trcuwait_wake_up(&sem->writer);\n}"
  },
  {
    "function_name": "__percpu_down_read",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
    "lines": "41-91",
    "snippet": "int __percpu_down_read(struct percpu_rw_semaphore *sem, int try)\n{\n\t/*\n\t * Due to having preemption disabled the decrement happens on\n\t * the same CPU as the increment, avoiding the\n\t * increment-on-one-CPU-and-decrement-on-another problem.\n\t *\n\t * If the reader misses the writer's assignment of readers_block, then\n\t * the writer is guaranteed to see the reader's increment.\n\t *\n\t * Conversely, any readers that increment their sem->read_count after\n\t * the writer looks are guaranteed to see the readers_block value,\n\t * which in turn means that they are guaranteed to immediately\n\t * decrement their sem->read_count, so that it doesn't matter that the\n\t * writer missed them.\n\t */\n\n\tsmp_mb(); /* A matches D */\n\n\t/*\n\t * If !readers_block the critical section starts here, matched by the\n\t * release in percpu_up_write().\n\t */\n\tif (likely(!smp_load_acquire(&sem->readers_block)))\n\t\treturn 1;\n\n\t/*\n\t * Per the above comment; we still have preemption disabled and\n\t * will thus decrement on the same CPU as we incremented.\n\t */\n\t__percpu_up_read(sem);\n\n\tif (try)\n\t\treturn 0;\n\n\t/*\n\t * We either call schedule() in the wait, or we'll fall through\n\t * and reschedule on the preempt_enable() in percpu_down_read().\n\t */\n\tpreempt_enable_no_resched();\n\n\t/*\n\t * Avoid lockdep for the down/up_read() we already have them.\n\t */\n\t__down_read(&sem->rw_sem);\n\tthis_cpu_inc(*sem->read_count);\n\t__up_read(&sem->rw_sem);\n\n\tpreempt_disable();\n\treturn 1;\n}",
    "includes": [
      "#include <linux/errno.h>",
      "#include <linux/sched.h>",
      "#include <linux/rcupdate.h>",
      "#include <linux/percpu-rwsem.h>",
      "#include <linux/lockdep.h>",
      "#include <linux/percpu.h>",
      "#include <linux/rwsem.h>",
      "#include <linux/atomic.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "preempt_disable",
          "args": [],
          "line": 89
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_preempt_disabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "3571-3576",
          "snippet": "void __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic void __sched;\nstatic void __sched;\n\nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "__up_read",
          "args": [
            "&sem->rw_sem"
          ],
          "line": 87
        },
        "resolved": true,
        "details": {
          "function_name": "__up_read",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-spinlock.c",
          "lines": "295-305",
          "snippet": "void __up_read(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (--sem->count == 0 && !list_empty(&sem->wait_list))\n\t\tsem = __rwsem_wake_one_writer(sem);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/rwsem.h>\n\nvoid __up_read(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (--sem->count == 0 && !list_empty(&sem->wait_list))\n\t\tsem = __rwsem_wake_one_writer(sem);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_cpu_inc",
          "args": [
            "*sem->read_count"
          ],
          "line": 86
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__down_read",
          "args": [
            "&sem->rw_sem"
          ],
          "line": 85
        },
        "resolved": true,
        "details": {
          "function_name": "__down_read_trylock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-spinlock.c",
          "lines": "191-208",
          "snippet": "int __down_read_trylock(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (sem->count >= 0 && list_empty(&sem->wait_list)) {\n\t\t/* granted */\n\t\tsem->count++;\n\t\tret = 1;\n\t}\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\n\treturn ret;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/rwsem.h>\n\nint __down_read_trylock(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (sem->count >= 0 && list_empty(&sem->wait_list)) {\n\t\t/* granted */\n\t\tsem->count++;\n\t\tret = 1;\n\t}\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "preempt_enable_no_resched",
          "args": [],
          "line": 80
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__percpu_up_read",
          "args": [
            "sem"
          ],
          "line": 71
        },
        "resolved": true,
        "details": {
          "function_name": "__percpu_up_read",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "94-106",
          "snippet": "void __percpu_up_read(struct percpu_rw_semaphore *sem)\n{\n\tsmp_mb(); /* B matches C */\n\t/*\n\t * In other words, if they see our decrement (presumably to aggregate\n\t * zero, as that is the only time it matters) they will also see our\n\t * critical section.\n\t */\n\t__this_cpu_dec(*sem->read_count);\n\n\t/* Prod writer to recheck readers_active */\n\trcuwait_wake_up(&sem->writer);\n}",
          "includes": [
            "#include <linux/errno.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/percpu.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid __percpu_up_read(struct percpu_rw_semaphore *sem)\n{\n\tsmp_mb(); /* B matches C */\n\t/*\n\t * In other words, if they see our decrement (presumably to aggregate\n\t * zero, as that is the only time it matters) they will also see our\n\t * critical section.\n\t */\n\t__this_cpu_dec(*sem->read_count);\n\n\t/* Prod writer to recheck readers_active */\n\trcuwait_wake_up(&sem->writer);\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!smp_load_acquire(&sem->readers_block)"
          ],
          "line": 64
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_load_acquire",
          "args": [
            "&sem->readers_block"
          ],
          "line": 64
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 58
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nint __percpu_down_read(struct percpu_rw_semaphore *sem, int try)\n{\n\t/*\n\t * Due to having preemption disabled the decrement happens on\n\t * the same CPU as the increment, avoiding the\n\t * increment-on-one-CPU-and-decrement-on-another problem.\n\t *\n\t * If the reader misses the writer's assignment of readers_block, then\n\t * the writer is guaranteed to see the reader's increment.\n\t *\n\t * Conversely, any readers that increment their sem->read_count after\n\t * the writer looks are guaranteed to see the readers_block value,\n\t * which in turn means that they are guaranteed to immediately\n\t * decrement their sem->read_count, so that it doesn't matter that the\n\t * writer missed them.\n\t */\n\n\tsmp_mb(); /* A matches D */\n\n\t/*\n\t * If !readers_block the critical section starts here, matched by the\n\t * release in percpu_up_write().\n\t */\n\tif (likely(!smp_load_acquire(&sem->readers_block)))\n\t\treturn 1;\n\n\t/*\n\t * Per the above comment; we still have preemption disabled and\n\t * will thus decrement on the same CPU as we incremented.\n\t */\n\t__percpu_up_read(sem);\n\n\tif (try)\n\t\treturn 0;\n\n\t/*\n\t * We either call schedule() in the wait, or we'll fall through\n\t * and reschedule on the preempt_enable() in percpu_down_read().\n\t */\n\tpreempt_enable_no_resched();\n\n\t/*\n\t * Avoid lockdep for the down/up_read() we already have them.\n\t */\n\t__down_read(&sem->rw_sem);\n\tthis_cpu_inc(*sem->read_count);\n\t__up_read(&sem->rw_sem);\n\n\tpreempt_disable();\n\treturn 1;\n}"
  },
  {
    "function_name": "percpu_free_rwsem",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
    "lines": "26-38",
    "snippet": "void percpu_free_rwsem(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * XXX: temporary kludge. The error path in alloc_super()\n\t * assumes that percpu_free_rwsem() is safe after kzalloc().\n\t */\n\tif (!sem->read_count)\n\t\treturn;\n\n\trcu_sync_dtor(&sem->rss);\n\tfree_percpu(sem->read_count);\n\tsem->read_count = NULL; /* catch use after free bugs */\n}",
    "includes": [
      "#include <linux/errno.h>",
      "#include <linux/sched.h>",
      "#include <linux/rcupdate.h>",
      "#include <linux/percpu-rwsem.h>",
      "#include <linux/lockdep.h>",
      "#include <linux/percpu.h>",
      "#include <linux/rwsem.h>",
      "#include <linux/atomic.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "free_percpu",
          "args": [
            "sem->read_count"
          ],
          "line": 36
        },
        "resolved": true,
        "details": {
          "function_name": "bpf_array_free_percpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/bpf/arraymap.c",
          "lines": "27-35",
          "snippet": "static void bpf_array_free_percpu(struct bpf_array *array)\n{\n\tint i;\n\n\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\tfree_percpu(array->pptrs[i]);\n\t\tcond_resched();\n\t}\n}",
          "includes": [
            "#include \"map_in_map.h\"",
            "#include <uapi/linux/btf.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/filter.h>",
            "#include <linux/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/btf.h>",
            "#include <linux/bpf.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"map_in_map.h\"\n#include <uapi/linux/btf.h>\n#include <linux/perf_event.h>\n#include <linux/filter.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/btf.h>\n#include <linux/bpf.h>\n\nstatic void bpf_array_free_percpu(struct bpf_array *array)\n{\n\tint i;\n\n\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\tfree_percpu(array->pptrs[i]);\n\t\tcond_resched();\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_sync_dtor",
          "args": [
            "&sem->rss"
          ],
          "line": 35
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_sync_dtor",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/sync.c",
          "lines": "224-240",
          "snippet": "void rcu_sync_dtor(struct rcu_sync *rsp)\n{\n\tint cb_state;\n\n\tBUG_ON(rsp->gp_count);\n\n\tspin_lock_irq(&rsp->rss_lock);\n\tif (rsp->cb_state == CB_REPLAY)\n\t\trsp->cb_state = CB_PENDING;\n\tcb_state = rsp->cb_state;\n\tspin_unlock_irq(&rsp->rss_lock);\n\n\tif (cb_state != CB_IDLE) {\n\t\tgp_ops[rsp->gp_type].wait();\n\t\tBUG_ON(rsp->cb_state != CB_IDLE);\n\t}\n}",
          "includes": [
            "#include <linux/sched.h>",
            "#include <linux/rcu_sync.h>"
          ],
          "macros_used": [
            "#define\trss_lock\tgp_wait.lock"
          ],
          "globals_used": [
            "static const struct {\n\tvoid (*sync)(void);\n\tvoid (*call)(struct rcu_head *, void (*)(struct rcu_head *));\n\tvoid (*wait)(void);\n#ifdef CONFIG_PROVE_RCU\n\tint  (*held)(void);\n#endif\n} gp_ops[] = {\n\t[RCU_SYNC] = {\n\t\t.sync = synchronize_rcu,\n\t\t.call = call_rcu,\n\t\t.wait = rcu_barrier,\n\t\t__INIT_HELD(rcu_read_lock_held)\n\t},\n\t[RCU_SCHED_SYNC] = {\n\t\t.sync = synchronize_sched,\n\t\t.call = call_rcu_sched,\n\t\t.wait = rcu_barrier_sched,\n\t\t__INIT_HELD(rcu_read_lock_sched_held)\n\t},\n\t[RCU_BH_SYNC] = {\n\t\t.sync = synchronize_rcu_bh,\n\t\t.call = call_rcu_bh,\n\t\t.wait = rcu_barrier_bh,\n\t\t__INIT_HELD(rcu_read_lock_bh_held)\n\t},\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched.h>\n#include <linux/rcu_sync.h>\n\n#define\trss_lock\tgp_wait.lock\n\nstatic const struct {\n\tvoid (*sync)(void);\n\tvoid (*call)(struct rcu_head *, void (*)(struct rcu_head *));\n\tvoid (*wait)(void);\n#ifdef CONFIG_PROVE_RCU\n\tint  (*held)(void);\n#endif\n} gp_ops[] = {\n\t[RCU_SYNC] = {\n\t\t.sync = synchronize_rcu,\n\t\t.call = call_rcu,\n\t\t.wait = rcu_barrier,\n\t\t__INIT_HELD(rcu_read_lock_held)\n\t},\n\t[RCU_SCHED_SYNC] = {\n\t\t.sync = synchronize_sched,\n\t\t.call = call_rcu_sched,\n\t\t.wait = rcu_barrier_sched,\n\t\t__INIT_HELD(rcu_read_lock_sched_held)\n\t},\n\t[RCU_BH_SYNC] = {\n\t\t.sync = synchronize_rcu_bh,\n\t\t.call = call_rcu_bh,\n\t\t.wait = rcu_barrier_bh,\n\t\t__INIT_HELD(rcu_read_lock_bh_held)\n\t},\n};\n\nvoid rcu_sync_dtor(struct rcu_sync *rsp)\n{\n\tint cb_state;\n\n\tBUG_ON(rsp->gp_count);\n\n\tspin_lock_irq(&rsp->rss_lock);\n\tif (rsp->cb_state == CB_REPLAY)\n\t\trsp->cb_state = CB_PENDING;\n\tcb_state = rsp->cb_state;\n\tspin_unlock_irq(&rsp->rss_lock);\n\n\tif (cb_state != CB_IDLE) {\n\t\tgp_ops[rsp->gp_type].wait();\n\t\tBUG_ON(rsp->cb_state != CB_IDLE);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid percpu_free_rwsem(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * XXX: temporary kludge. The error path in alloc_super()\n\t * assumes that percpu_free_rwsem() is safe after kzalloc().\n\t */\n\tif (!sem->read_count)\n\t\treturn;\n\n\trcu_sync_dtor(&sem->rss);\n\tfree_percpu(sem->read_count);\n\tsem->read_count = NULL; /* catch use after free bugs */\n}"
  },
  {
    "function_name": "__percpu_init_rwsem",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
    "lines": "10-23",
    "snippet": "int __percpu_init_rwsem(struct percpu_rw_semaphore *sem,\n\t\t\tconst char *name, struct lock_class_key *rwsem_key)\n{\n\tsem->read_count = alloc_percpu(int);\n\tif (unlikely(!sem->read_count))\n\t\treturn -ENOMEM;\n\n\t/* ->rw_sem represents the whole percpu_rw_semaphore for lockdep */\n\trcu_sync_init(&sem->rss, RCU_SCHED_SYNC);\n\t__init_rwsem(&sem->rw_sem, name, rwsem_key);\n\trcuwait_init(&sem->writer);\n\tsem->readers_block = 0;\n\treturn 0;\n}",
    "includes": [
      "#include <linux/errno.h>",
      "#include <linux/sched.h>",
      "#include <linux/rcupdate.h>",
      "#include <linux/percpu-rwsem.h>",
      "#include <linux/lockdep.h>",
      "#include <linux/percpu.h>",
      "#include <linux/rwsem.h>",
      "#include <linux/atomic.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcuwait_init",
          "args": [
            "&sem->writer"
          ],
          "line": 20
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__init_rwsem",
          "args": [
            "&sem->rw_sem",
            "name",
            "rwsem_key"
          ],
          "line": 19
        },
        "resolved": true,
        "details": {
          "function_name": "__init_rwsem",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-spinlock.c",
          "lines": "41-54",
          "snippet": "void __init_rwsem(struct rw_semaphore *sem, const char *name,\n\t\t  struct lock_class_key *key)\n{\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t/*\n\t * Make sure we are not reinitializing a held semaphore:\n\t */\n\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));\n\tlockdep_init_map(&sem->dep_map, name, key, 0);\n#endif\n\tsem->count = 0;\n\traw_spin_lock_init(&sem->wait_lock);\n\tINIT_LIST_HEAD(&sem->wait_list);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/rwsem.h>\n\nvoid __init_rwsem(struct rw_semaphore *sem, const char *name,\n\t\t  struct lock_class_key *key)\n{\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t/*\n\t * Make sure we are not reinitializing a held semaphore:\n\t */\n\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));\n\tlockdep_init_map(&sem->dep_map, name, key, 0);\n#endif\n\tsem->count = 0;\n\traw_spin_lock_init(&sem->wait_lock);\n\tINIT_LIST_HEAD(&sem->wait_list);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_sync_init",
          "args": [
            "&sem->rss",
            "RCU_SCHED_SYNC"
          ],
          "line": 18
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_sync_init",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/sync.c",
          "lines": "80-85",
          "snippet": "void rcu_sync_init(struct rcu_sync *rsp, enum rcu_sync_type type)\n{\n\tmemset(rsp, 0, sizeof(*rsp));\n\tinit_waitqueue_head(&rsp->gp_wait);\n\trsp->gp_type = type;\n}",
          "includes": [
            "#include <linux/sched.h>",
            "#include <linux/rcu_sync.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched.h>\n#include <linux/rcu_sync.h>\n\nvoid rcu_sync_init(struct rcu_sync *rsp, enum rcu_sync_type type)\n{\n\tmemset(rsp, 0, sizeof(*rsp));\n\tinit_waitqueue_head(&rsp->gp_wait);\n\trsp->gp_type = type;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!sem->read_count"
          ],
          "line": 14
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "alloc_percpu",
          "args": [
            "int"
          ],
          "line": 13
        },
        "resolved": true,
        "details": {
          "function_name": "bpf_array_alloc_percpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/bpf/arraymap.c",
          "lines": "37-54",
          "snippet": "static int bpf_array_alloc_percpu(struct bpf_array *array)\n{\n\tvoid __percpu *ptr;\n\tint i;\n\n\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\tptr = __alloc_percpu_gfp(array->elem_size, 8,\n\t\t\t\t\t GFP_USER | __GFP_NOWARN);\n\t\tif (!ptr) {\n\t\t\tbpf_array_free_percpu(array);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tarray->pptrs[i] = ptr;\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"map_in_map.h\"",
            "#include <uapi/linux/btf.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/filter.h>",
            "#include <linux/mm.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/btf.h>",
            "#include <linux/bpf.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"map_in_map.h\"\n#include <uapi/linux/btf.h>\n#include <linux/perf_event.h>\n#include <linux/filter.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/btf.h>\n#include <linux/bpf.h>\n\nstatic int bpf_array_alloc_percpu(struct bpf_array *array)\n{\n\tvoid __percpu *ptr;\n\tint i;\n\n\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\tptr = __alloc_percpu_gfp(array->elem_size, 8,\n\t\t\t\t\t GFP_USER | __GFP_NOWARN);\n\t\tif (!ptr) {\n\t\t\tbpf_array_free_percpu(array);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tarray->pptrs[i] = ptr;\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nint __percpu_init_rwsem(struct percpu_rw_semaphore *sem,\n\t\t\tconst char *name, struct lock_class_key *rwsem_key)\n{\n\tsem->read_count = alloc_percpu(int);\n\tif (unlikely(!sem->read_count))\n\t\treturn -ENOMEM;\n\n\t/* ->rw_sem represents the whole percpu_rw_semaphore for lockdep */\n\trcu_sync_init(&sem->rss, RCU_SCHED_SYNC);\n\t__init_rwsem(&sem->rw_sem, name, rwsem_key);\n\trcuwait_init(&sem->writer);\n\tsem->readers_block = 0;\n\treturn 0;\n}"
  }
]