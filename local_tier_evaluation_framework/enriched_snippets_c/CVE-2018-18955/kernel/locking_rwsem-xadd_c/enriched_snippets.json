[
  {
    "function_name": "rwsem_downgrade_wake",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "703-717",
    "snippet": "rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tDEFINE_WAKE_Q(wake_q);\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (!list_empty(&sem->wait_list))\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_READ_OWNED, &wake_q);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\twake_up_q(&wake_q);\n\n\treturn sem;\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "&wake_q"
          ],
          "line": 714
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_q",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "423-443",
          "snippet": "void wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&sem->wait_lock",
            "flags"
          ],
          "line": 713
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rwsem_mark_wake",
          "args": [
            "sem",
            "RWSEM_WAKE_READ_OWNED",
            "&wake_q"
          ],
          "line": 711
        },
        "resolved": true,
        "details": {
          "function_name": "__rwsem_mark_wake",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "127-220",
          "snippet": "static void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&sem->wait_list"
          ],
          "line": 710
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&sem->wait_lock",
            "flags"
          ],
          "line": 708
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 706
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nrw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tDEFINE_WAKE_Q(wake_q);\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (!list_empty(&sem->wait_list))\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_READ_OWNED, &wake_q);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\twake_up_q(&wake_q);\n\n\treturn sem;\n}"
  },
  {
    "function_name": "rwsem_wake",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "622-694",
    "snippet": "rw_semaphore *rwsem_wake(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tDEFINE_WAKE_Q(wake_q);\n\n\t/*\n\t* __rwsem_down_write_failed_common(sem)\n\t*   rwsem_optimistic_spin(sem)\n\t*     osq_unlock(sem->osq)\n\t*   ...\n\t*   atomic_long_add_return(&sem->count)\n\t*\n\t*      - VS -\n\t*\n\t*              __up_write()\n\t*                if (atomic_long_sub_return_release(&sem->count) < 0)\n\t*                  rwsem_wake(sem)\n\t*                    osq_is_locked(&sem->osq)\n\t*\n\t* And __up_write() must observe !osq_is_locked() when it observes the\n\t* atomic_long_add_return() in order to not miss a wakeup.\n\t*\n\t* This boils down to:\n\t*\n\t* [S.rel] X = 1                [RmW] r0 = (Y += 0)\n\t*         MB                         RMB\n\t* [RmW]   Y += 1               [L]   r1 = X\n\t*\n\t* exists (r0=1 /\\ r1=0)\n\t*/\n\tsmp_rmb();\n\n\t/*\n\t * If a spinner is present, it is not necessary to do the wakeup.\n\t * Try to do wakeup only if the trylock succeeds to minimize\n\t * spinlock contention which may introduce too much delay in the\n\t * unlock operation.\n\t *\n\t *    spinning writer\t\tup_write/up_read caller\n\t *    ---------------\t\t-----------------------\n\t * [S]   osq_unlock()\t\t[L]   osq\n\t *\t MB\t\t\t      RMB\n\t * [RmW] rwsem_try_write_lock() [RmW] spin_trylock(wait_lock)\n\t *\n\t * Here, it is important to make sure that there won't be a missed\n\t * wakeup while the rwsem is free and the only spinning writer goes\n\t * to sleep without taking the rwsem. Even when the spinning writer\n\t * is just going to break out of the waiting loop, it will still do\n\t * a trylock in rwsem_down_write_failed() before sleeping. IOW, if\n\t * rwsem_has_spinner() is true, it will guarantee at least one\n\t * trylock attempt on the rwsem later on.\n\t */\n\tif (rwsem_has_spinner(sem)) {\n\t\t/*\n\t\t * The smp_rmb() here is to make sure that the spinner\n\t\t * state is consulted before reading the wait_lock.\n\t\t */\n\t\tsmp_rmb();\n\t\tif (!raw_spin_trylock_irqsave(&sem->wait_lock, flags))\n\t\t\treturn sem;\n\t\tgoto locked;\n\t}\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\nlocked:\n\n\tif (!list_empty(&sem->wait_list))\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\twake_up_q(&wake_q);\n\n\treturn sem;\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "&wake_q"
          ],
          "line": 691
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_q",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "423-443",
          "snippet": "void wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&sem->wait_lock",
            "flags"
          ],
          "line": 690
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rwsem_mark_wake",
          "args": [
            "sem",
            "RWSEM_WAKE_ANY",
            "&wake_q"
          ],
          "line": 688
        },
        "resolved": true,
        "details": {
          "function_name": "__rwsem_mark_wake",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "127-220",
          "snippet": "static void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&sem->wait_list"
          ],
          "line": 687
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&sem->wait_lock",
            "flags"
          ],
          "line": 684
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_trylock_irqsave",
          "args": [
            "&sem->wait_lock",
            "flags"
          ],
          "line": 680
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 679
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwsem_has_spinner",
          "args": [
            "sem"
          ],
          "line": 674
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_has_spinner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "496-499",
          "snippet": "static inline bool rwsem_has_spinner(struct rw_semaphore *sem)\n{\n\treturn false;\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_has_spinner(struct rw_semaphore *sem)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 652
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 625
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nrw_semaphore *rwsem_wake(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tDEFINE_WAKE_Q(wake_q);\n\n\t/*\n\t* __rwsem_down_write_failed_common(sem)\n\t*   rwsem_optimistic_spin(sem)\n\t*     osq_unlock(sem->osq)\n\t*   ...\n\t*   atomic_long_add_return(&sem->count)\n\t*\n\t*      - VS -\n\t*\n\t*              __up_write()\n\t*                if (atomic_long_sub_return_release(&sem->count) < 0)\n\t*                  rwsem_wake(sem)\n\t*                    osq_is_locked(&sem->osq)\n\t*\n\t* And __up_write() must observe !osq_is_locked() when it observes the\n\t* atomic_long_add_return() in order to not miss a wakeup.\n\t*\n\t* This boils down to:\n\t*\n\t* [S.rel] X = 1                [RmW] r0 = (Y += 0)\n\t*         MB                         RMB\n\t* [RmW]   Y += 1               [L]   r1 = X\n\t*\n\t* exists (r0=1 /\\ r1=0)\n\t*/\n\tsmp_rmb();\n\n\t/*\n\t * If a spinner is present, it is not necessary to do the wakeup.\n\t * Try to do wakeup only if the trylock succeeds to minimize\n\t * spinlock contention which may introduce too much delay in the\n\t * unlock operation.\n\t *\n\t *    spinning writer\t\tup_write/up_read caller\n\t *    ---------------\t\t-----------------------\n\t * [S]   osq_unlock()\t\t[L]   osq\n\t *\t MB\t\t\t      RMB\n\t * [RmW] rwsem_try_write_lock() [RmW] spin_trylock(wait_lock)\n\t *\n\t * Here, it is important to make sure that there won't be a missed\n\t * wakeup while the rwsem is free and the only spinning writer goes\n\t * to sleep without taking the rwsem. Even when the spinning writer\n\t * is just going to break out of the waiting loop, it will still do\n\t * a trylock in rwsem_down_write_failed() before sleeping. IOW, if\n\t * rwsem_has_spinner() is true, it will guarantee at least one\n\t * trylock attempt on the rwsem later on.\n\t */\n\tif (rwsem_has_spinner(sem)) {\n\t\t/*\n\t\t * The smp_rmb() here is to make sure that the spinner\n\t\t * state is consulted before reading the wait_lock.\n\t\t */\n\t\tsmp_rmb();\n\t\tif (!raw_spin_trylock_irqsave(&sem->wait_lock, flags))\n\t\t\treturn sem;\n\t\tgoto locked;\n\t}\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\nlocked:\n\n\tif (!list_empty(&sem->wait_list))\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\twake_up_q(&wake_q);\n\n\treturn sem;\n}"
  },
  {
    "function_name": "__rwsem_down_write_failed_common",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "505-601",
    "snippet": "static inline struct rw_semaphore *\n__rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)\n{\n\tlong count;\n\tbool waiting = true; /* any queued threads before us */\n\tstruct rwsem_waiter waiter;\n\tstruct rw_semaphore *ret = sem;\n\tDEFINE_WAKE_Q(wake_q);\n\n\t/* undo write bias from down_write operation, stop active locking */\n\tcount = atomic_long_sub_return(RWSEM_ACTIVE_WRITE_BIAS, &sem->count);\n\n\t/* do optimistic spinning and steal lock if possible */\n\tif (rwsem_optimistic_spin(sem))\n\t\treturn sem;\n\n\t/*\n\t * Optimistic spinning failed, proceed to the slowpath\n\t * and block until we can acquire the sem.\n\t */\n\twaiter.task = current;\n\twaiter.type = RWSEM_WAITING_FOR_WRITE;\n\n\traw_spin_lock_irq(&sem->wait_lock);\n\n\t/* account for this before adding a new element to the list */\n\tif (list_empty(&sem->wait_list))\n\t\twaiting = false;\n\n\tlist_add_tail(&waiter.list, &sem->wait_list);\n\n\t/* we're now waiting on the lock, but no longer actively locking */\n\tif (waiting) {\n\t\tcount = atomic_long_read(&sem->count);\n\n\t\t/*\n\t\t * If there were already threads queued before us and there are\n\t\t * no active writers, the lock must be read owned; so we try to\n\t\t * wake any read locks that were queued ahead of us.\n\t\t */\n\t\tif (count > RWSEM_WAITING_BIAS) {\n\t\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_READERS, &wake_q);\n\t\t\t/*\n\t\t\t * The wakeup is normally called _after_ the wait_lock\n\t\t\t * is released, but given that we are proactively waking\n\t\t\t * readers we can deal with the wake_q overhead as it is\n\t\t\t * similar to releasing and taking the wait_lock again\n\t\t\t * for attempting rwsem_try_write_lock().\n\t\t\t */\n\t\t\twake_up_q(&wake_q);\n\n\t\t\t/*\n\t\t\t * Reinitialize wake_q after use.\n\t\t\t */\n\t\t\twake_q_init(&wake_q);\n\t\t}\n\n\t} else\n\t\tcount = atomic_long_add_return(RWSEM_WAITING_BIAS, &sem->count);\n\n\t/* wait until we successfully acquire the lock */\n\tset_current_state(state);\n\twhile (true) {\n\t\tif (rwsem_try_write_lock(count, sem))\n\t\t\tbreak;\n\t\traw_spin_unlock_irq(&sem->wait_lock);\n\n\t\t/* Block until there are no active lockers. */\n\t\tdo {\n\t\t\tif (signal_pending_state(state, current))\n\t\t\t\tgoto out_nolock;\n\n\t\t\tschedule();\n\t\t\tset_current_state(state);\n\t\t} while ((count = atomic_long_read(&sem->count)) & RWSEM_ACTIVE_MASK);\n\n\t\traw_spin_lock_irq(&sem->wait_lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tlist_del(&waiter.list);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\n\treturn ret;\n\nout_nolock:\n\t__set_current_state(TASK_RUNNING);\n\traw_spin_lock_irq(&sem->wait_lock);\n\tlist_del(&waiter.list);\n\tif (list_empty(&sem->wait_list))\n\t\tatomic_long_add(-RWSEM_WAITING_BIAS, &sem->count);\n\telse\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\twake_up_q(&wake_q);\n\n\treturn ERR_PTR(-EINTR);\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "ERR_PTR",
          "args": [
            "-EINTR"
          ],
          "line": 600
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "&wake_q"
          ],
          "line": 598
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_q",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "423-443",
          "snippet": "void wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&sem->wait_lock"
          ],
          "line": 597
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rwsem_mark_wake",
          "args": [
            "sem",
            "RWSEM_WAKE_ANY",
            "&wake_q"
          ],
          "line": 596
        },
        "resolved": true,
        "details": {
          "function_name": "__rwsem_mark_wake",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "127-220",
          "snippet": "static void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_add",
          "args": [
            "-RWSEM_WAITING_BIAS",
            "&sem->count"
          ],
          "line": 594
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&sem->wait_list"
          ],
          "line": 593
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_del",
          "args": [
            "&waiter.list"
          ],
          "line": 592
        },
        "resolved": true,
        "details": {
          "function_name": "list_del_leaf_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "446-448",
          "snippet": "static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&sem->wait_lock"
          ],
          "line": 591
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 590
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 583
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_read",
          "args": [
            "&sem->count"
          ],
          "line": 579
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 578
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedule",
          "args": [],
          "line": 577
        },
        "resolved": true,
        "details": {
          "function_name": "audit_schedule_prune",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/audit_tree.c",
          "lines": "919-922",
          "snippet": "static void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}",
          "includes": [
            "#include <linux/slab.h>",
            "#include <linux/refcount.h>",
            "#include <linux/kthread.h>",
            "#include <linux/mount.h>",
            "#include <linux/namei.h>",
            "#include <linux/fsnotify_backend.h>",
            "#include \"audit.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct task_struct *prune_thread;",
            "static void audit_schedule_prune(void);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/slab.h>\n#include <linux/refcount.h>\n#include <linux/kthread.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/fsnotify_backend.h>\n#include \"audit.h\"\n\nstatic struct task_struct *prune_thread;\nstatic void audit_schedule_prune(void);\n\nstatic void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}"
        }
      },
      {
        "call_info": {
          "callee": "signal_pending_state",
          "args": [
            "state",
            "current"
          ],
          "line": 574
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwsem_try_write_lock",
          "args": [
            "count",
            "sem"
          ],
          "line": 568
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_try_write_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "313-336",
          "snippet": "static inline bool rwsem_try_write_lock(long count, struct rw_semaphore *sem)\n{\n\t/*\n\t * Avoid trying to acquire write lock if count isn't RWSEM_WAITING_BIAS.\n\t */\n\tif (count != RWSEM_WAITING_BIAS)\n\t\treturn false;\n\n\t/*\n\t * Acquire the lock by trying to set it to ACTIVE_WRITE_BIAS. If there\n\t * are other tasks on the wait list, we need to add on WAITING_BIAS.\n\t */\n\tcount = list_is_singular(&sem->wait_list) ?\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS :\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS + RWSEM_WAITING_BIAS;\n\n\tif (atomic_long_cmpxchg_acquire(&sem->count, RWSEM_WAITING_BIAS, count)\n\t\t\t\t\t\t\t== RWSEM_WAITING_BIAS) {\n\t\trwsem_set_owner(sem);\n\t\treturn true;\n\t}\n\n\treturn false;\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_try_write_lock(long count, struct rw_semaphore *sem)\n{\n\t/*\n\t * Avoid trying to acquire write lock if count isn't RWSEM_WAITING_BIAS.\n\t */\n\tif (count != RWSEM_WAITING_BIAS)\n\t\treturn false;\n\n\t/*\n\t * Acquire the lock by trying to set it to ACTIVE_WRITE_BIAS. If there\n\t * are other tasks on the wait list, we need to add on WAITING_BIAS.\n\t */\n\tcount = list_is_singular(&sem->wait_list) ?\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS :\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS + RWSEM_WAITING_BIAS;\n\n\tif (atomic_long_cmpxchg_acquire(&sem->count, RWSEM_WAITING_BIAS, count)\n\t\t\t\t\t\t\t== RWSEM_WAITING_BIAS) {\n\t\trwsem_set_owner(sem);\n\t\treturn true;\n\t}\n\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 566
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_add_return",
          "args": [
            "RWSEM_WAITING_BIAS",
            "&sem->count"
          ],
          "line": 563
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_q_init",
          "args": [
            "&wake_q"
          ],
          "line": 559
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_read",
          "args": [
            "&sem->count"
          ],
          "line": 538
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_add_tail",
          "args": [
            "&waiter.list",
            "&sem->wait_list"
          ],
          "line": 534
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwsem_optimistic_spin",
          "args": [
            "sem"
          ],
          "line": 518
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_optimistic_spin",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "491-494",
          "snippet": "static bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\treturn false;\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_sub_return",
          "args": [
            "RWSEM_ACTIVE_WRITE_BIAS",
            "&sem->count"
          ],
          "line": 515
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 512
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline struct rw_semaphore *\n__rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)\n{\n\tlong count;\n\tbool waiting = true; /* any queued threads before us */\n\tstruct rwsem_waiter waiter;\n\tstruct rw_semaphore *ret = sem;\n\tDEFINE_WAKE_Q(wake_q);\n\n\t/* undo write bias from down_write operation, stop active locking */\n\tcount = atomic_long_sub_return(RWSEM_ACTIVE_WRITE_BIAS, &sem->count);\n\n\t/* do optimistic spinning and steal lock if possible */\n\tif (rwsem_optimistic_spin(sem))\n\t\treturn sem;\n\n\t/*\n\t * Optimistic spinning failed, proceed to the slowpath\n\t * and block until we can acquire the sem.\n\t */\n\twaiter.task = current;\n\twaiter.type = RWSEM_WAITING_FOR_WRITE;\n\n\traw_spin_lock_irq(&sem->wait_lock);\n\n\t/* account for this before adding a new element to the list */\n\tif (list_empty(&sem->wait_list))\n\t\twaiting = false;\n\n\tlist_add_tail(&waiter.list, &sem->wait_list);\n\n\t/* we're now waiting on the lock, but no longer actively locking */\n\tif (waiting) {\n\t\tcount = atomic_long_read(&sem->count);\n\n\t\t/*\n\t\t * If there were already threads queued before us and there are\n\t\t * no active writers, the lock must be read owned; so we try to\n\t\t * wake any read locks that were queued ahead of us.\n\t\t */\n\t\tif (count > RWSEM_WAITING_BIAS) {\n\t\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_READERS, &wake_q);\n\t\t\t/*\n\t\t\t * The wakeup is normally called _after_ the wait_lock\n\t\t\t * is released, but given that we are proactively waking\n\t\t\t * readers we can deal with the wake_q overhead as it is\n\t\t\t * similar to releasing and taking the wait_lock again\n\t\t\t * for attempting rwsem_try_write_lock().\n\t\t\t */\n\t\t\twake_up_q(&wake_q);\n\n\t\t\t/*\n\t\t\t * Reinitialize wake_q after use.\n\t\t\t */\n\t\t\twake_q_init(&wake_q);\n\t\t}\n\n\t} else\n\t\tcount = atomic_long_add_return(RWSEM_WAITING_BIAS, &sem->count);\n\n\t/* wait until we successfully acquire the lock */\n\tset_current_state(state);\n\twhile (true) {\n\t\tif (rwsem_try_write_lock(count, sem))\n\t\t\tbreak;\n\t\traw_spin_unlock_irq(&sem->wait_lock);\n\n\t\t/* Block until there are no active lockers. */\n\t\tdo {\n\t\t\tif (signal_pending_state(state, current))\n\t\t\t\tgoto out_nolock;\n\n\t\t\tschedule();\n\t\t\tset_current_state(state);\n\t\t} while ((count = atomic_long_read(&sem->count)) & RWSEM_ACTIVE_MASK);\n\n\t\traw_spin_lock_irq(&sem->wait_lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tlist_del(&waiter.list);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\n\treturn ret;\n\nout_nolock:\n\t__set_current_state(TASK_RUNNING);\n\traw_spin_lock_irq(&sem->wait_lock);\n\tlist_del(&waiter.list);\n\tif (list_empty(&sem->wait_list))\n\t\tatomic_long_add(-RWSEM_WAITING_BIAS, &sem->count);\n\telse\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\twake_up_q(&wake_q);\n\n\treturn ERR_PTR(-EINTR);\n}"
  },
  {
    "function_name": "rwsem_has_spinner",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "496-499",
    "snippet": "static inline bool rwsem_has_spinner(struct rw_semaphore *sem)\n{\n\treturn false;\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_has_spinner(struct rw_semaphore *sem)\n{\n\treturn false;\n}"
  },
  {
    "function_name": "rwsem_optimistic_spin",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "491-494",
    "snippet": "static bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\treturn false;\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\treturn false;\n}"
  },
  {
    "function_name": "rwsem_has_spinner",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "485-488",
    "snippet": "static inline bool rwsem_has_spinner(struct rw_semaphore *sem)\n{\n\treturn osq_is_locked(&sem->osq);\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "osq_is_locked",
          "args": [
            "&sem->osq"
          ],
          "line": 487
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_has_spinner(struct rw_semaphore *sem)\n{\n\treturn osq_is_locked(&sem->osq);\n}"
  },
  {
    "function_name": "rwsem_optimistic_spin",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "430-480",
    "snippet": "static bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\tbool taken = false;\n\n\tpreempt_disable();\n\n\t/* sem->wait_lock should not be held when doing optimistic spinning */\n\tif (!rwsem_can_spin_on_owner(sem))\n\t\tgoto done;\n\n\tif (!osq_lock(&sem->osq))\n\t\tgoto done;\n\n\t/*\n\t * Optimistically spin on the owner field and attempt to acquire the\n\t * lock whenever the owner changes. Spinning will be stopped when:\n\t *  1) the owning writer isn't running; or\n\t *  2) readers own the lock as we can't determine if they are\n\t *     actively running or not.\n\t */\n\twhile (rwsem_spin_on_owner(sem)) {\n\t\t/*\n\t\t * Try to acquire the lock\n\t\t */\n\t\tif (rwsem_try_write_lock_unqueued(sem)) {\n\t\t\ttaken = true;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * When there's no owner, we might have preempted between the\n\t\t * owner acquiring the lock and setting the owner field. If\n\t\t * we're an RT task that will live-lock because we won't let\n\t\t * the owner complete.\n\t\t */\n\t\tif (!sem->owner && (need_resched() || rt_task(current)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * The cpu_relax() call is a compiler barrier which forces\n\t\t * everything in this loop to be re-loaded. We don't need\n\t\t * memory barriers as we'll eventually observe the right\n\t\t * values at the cost of a few extra spins.\n\t\t */\n\t\tcpu_relax();\n\t}\n\tosq_unlock(&sem->osq);\ndone:\n\tpreempt_enable();\n\treturn taken;\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "preempt_enable",
          "args": [],
          "line": 478
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "osq_unlock",
          "args": [
            "&sem->osq"
          ],
          "line": 476
        },
        "resolved": true,
        "details": {
          "function_name": "osq_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/osq_lock.c",
          "lines": "206-231",
          "snippet": "void osq_unlock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\n\t/*\n\t * Fast path for the uncontended case.\n\t */\n\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,\n\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))\n\t\treturn;\n\n\t/*\n\t * Second most likely case.\n\t */\n\tnode = this_cpu_ptr(&osq_node);\n\tnext = xchg(&node->next, NULL);\n\tif (next) {\n\t\tWRITE_ONCE(next->locked, 1);\n\t\treturn;\n\t}\n\n\tnext = osq_wait_next(lock, node, NULL);\n\tif (next)\n\t\tWRITE_ONCE(next->locked, 1);\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);\n\nvoid osq_unlock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\n\t/*\n\t * Fast path for the uncontended case.\n\t */\n\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,\n\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))\n\t\treturn;\n\n\t/*\n\t * Second most likely case.\n\t */\n\tnode = this_cpu_ptr(&osq_node);\n\tnext = xchg(&node->next, NULL);\n\tif (next) {\n\t\tWRITE_ONCE(next->locked, 1);\n\t\treturn;\n\t}\n\n\tnext = osq_wait_next(lock, node, NULL);\n\tif (next)\n\t\tWRITE_ONCE(next->locked, 1);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 474
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_task",
          "args": [
            "current"
          ],
          "line": 465
        },
        "resolved": true,
        "details": {
          "function_name": "tg_has_rt_tasks",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/rt.c",
          "lines": "2416-2432",
          "snippet": "static inline int tg_has_rt_tasks(struct task_group *tg)\n{\n\tstruct task_struct *g, *p;\n\n\t/*\n\t * Autogroups do not have RT tasks; see autogroup_create().\n\t */\n\tif (task_group_is_autogroup(tg))\n\t\treturn 0;\n\n\tfor_each_process_thread(g, p) {\n\t\tif (rt_task(p) && task_group(p) == tg)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"pelt.h\"",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"pelt.h\"\n#include \"sched.h\"\n\nstatic inline int tg_has_rt_tasks(struct task_group *tg)\n{\n\tstruct task_struct *g, *p;\n\n\t/*\n\t * Autogroups do not have RT tasks; see autogroup_create().\n\t */\n\tif (task_group_is_autogroup(tg))\n\t\treturn 0;\n\n\tfor_each_process_thread(g, p) {\n\t\tif (rt_task(p) && task_group(p) == tg)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "need_resched",
          "args": [],
          "line": 465
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwsem_try_write_lock_unqueued",
          "args": [
            "sem"
          ],
          "line": 454
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_try_write_lock_unqueued",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "342-359",
          "snippet": "static inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)\n{\n\tlong old, count = atomic_long_read(&sem->count);\n\n\twhile (true) {\n\t\tif (!(count == 0 || count == RWSEM_WAITING_BIAS))\n\t\t\treturn false;\n\n\t\told = atomic_long_cmpxchg_acquire(&sem->count, count,\n\t\t\t\t      count + RWSEM_ACTIVE_WRITE_BIAS);\n\t\tif (old == count) {\n\t\t\trwsem_set_owner(sem);\n\t\t\treturn true;\n\t\t}\n\n\t\tcount = old;\n\t}\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)\n{\n\tlong old, count = atomic_long_read(&sem->count);\n\n\twhile (true) {\n\t\tif (!(count == 0 || count == RWSEM_WAITING_BIAS))\n\t\t\treturn false;\n\n\t\told = atomic_long_cmpxchg_acquire(&sem->count, count,\n\t\t\t\t      count + RWSEM_ACTIVE_WRITE_BIAS);\n\t\tif (old == count) {\n\t\t\trwsem_set_owner(sem);\n\t\t\treturn true;\n\t\t}\n\n\t\tcount = old;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rwsem_spin_on_owner",
          "args": [
            "sem"
          ],
          "line": 450
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_spin_on_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "393-428",
          "snippet": "static noinline bool rwsem_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner = READ_ONCE(sem->owner);\n\n\tif (!is_rwsem_owner_spinnable(owner))\n\t\treturn false;\n\n\trcu_read_lock();\n\twhile (owner && (READ_ONCE(sem->owner) == owner)) {\n\t\t/*\n\t\t * Ensure we emit the owner->on_cpu, dereference _after_\n\t\t * checking sem->owner still matches owner, if that fails,\n\t\t * owner might point to free()d memory, if it still matches,\n\t\t * the rcu_read_lock() ensures the memory stays valid.\n\t\t */\n\t\tbarrier();\n\n\t\t/*\n\t\t * abort spinning when need_resched or owner is not running or\n\t\t * owner's cpu is preempted.\n\t\t */\n\t\tif (need_resched() || !owner_on_cpu(owner)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * If there is a new owner or the owner is not set, we continue\n\t * spinning.\n\t */\n\treturn is_rwsem_owner_spinnable(READ_ONCE(sem->owner));\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic noinline bool rwsem_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner = READ_ONCE(sem->owner);\n\n\tif (!is_rwsem_owner_spinnable(owner))\n\t\treturn false;\n\n\trcu_read_lock();\n\twhile (owner && (READ_ONCE(sem->owner) == owner)) {\n\t\t/*\n\t\t * Ensure we emit the owner->on_cpu, dereference _after_\n\t\t * checking sem->owner still matches owner, if that fails,\n\t\t * owner might point to free()d memory, if it still matches,\n\t\t * the rcu_read_lock() ensures the memory stays valid.\n\t\t */\n\t\tbarrier();\n\n\t\t/*\n\t\t * abort spinning when need_resched or owner is not running or\n\t\t * owner's cpu is preempted.\n\t\t */\n\t\tif (need_resched() || !owner_on_cpu(owner)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * If there is a new owner or the owner is not set, we continue\n\t * spinning.\n\t */\n\treturn is_rwsem_owner_spinnable(READ_ONCE(sem->owner));\n}"
        }
      },
      {
        "call_info": {
          "callee": "osq_lock",
          "args": [
            "&sem->osq"
          ],
          "line": 440
        },
        "resolved": true,
        "details": {
          "function_name": "osq_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/osq_lock.c",
          "lines": "90-204",
          "snippet": "bool osq_lock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);\n\tstruct optimistic_spin_node *prev, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\tnode->locked = 0;\n\tnode->next = NULL;\n\tnode->cpu = curr;\n\n\t/*\n\t * We need both ACQUIRE (pairs with corresponding RELEASE in\n\t * unlock() uncontended, or fastpath) and RELEASE (to publish\n\t * the node fields we just initialised) semantics when updating\n\t * the lock tail.\n\t */\n\told = atomic_xchg(&lock->tail, curr);\n\tif (old == OSQ_UNLOCKED_VAL)\n\t\treturn true;\n\n\tprev = decode_cpu(old);\n\tnode->prev = prev;\n\n\t/*\n\t * osq_lock()\t\t\tunqueue\n\t *\n\t * node->prev = prev\t\tosq_wait_next()\n\t * WMB\t\t\t\tMB\n\t * prev->next = node\t\tnext->prev = prev // unqueue-C\n\t *\n\t * Here 'node->prev' and 'next->prev' are the same variable and we need\n\t * to ensure these stores happen in-order to avoid corrupting the list.\n\t */\n\tsmp_wmb();\n\n\tWRITE_ONCE(prev->next, node);\n\n\t/*\n\t * Normally @prev is untouchable after the above store; because at that\n\t * moment unlock can proceed and wipe the node element from stack.\n\t *\n\t * However, since our nodes are static per-cpu storage, we're\n\t * guaranteed their existence -- this allows us to apply\n\t * cmpxchg in an attempt to undo our queueing.\n\t */\n\n\twhile (!READ_ONCE(node->locked)) {\n\t\t/*\n\t\t * If we need to reschedule bail... so we can block.\n\t\t * Use vcpu_is_preempted() to avoid waiting for a preempted\n\t\t * lock holder:\n\t\t */\n\t\tif (need_resched() || vcpu_is_preempted(node_cpu(node->prev)))\n\t\t\tgoto unqueue;\n\n\t\tcpu_relax();\n\t}\n\treturn true;\n\nunqueue:\n\t/*\n\t * Step - A  -- stabilize @prev\n\t *\n\t * Undo our @prev->next assignment; this will make @prev's\n\t * unlock()/unqueue() wait for a next pointer since @lock points to us\n\t * (or later).\n\t */\n\n\tfor (;;) {\n\t\tif (prev->next == node &&\n\t\t    cmpxchg(&prev->next, node, NULL) == node)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We can only fail the cmpxchg() racing against an unlock(),\n\t\t * in which case we should observe @node->locked becomming\n\t\t * true.\n\t\t */\n\t\tif (smp_load_acquire(&node->locked))\n\t\t\treturn true;\n\n\t\tcpu_relax();\n\n\t\t/*\n\t\t * Or we race against a concurrent unqueue()'s step-B, in which\n\t\t * case its step-C will write us a new @node->prev pointer.\n\t\t */\n\t\tprev = READ_ONCE(node->prev);\n\t}\n\n\t/*\n\t * Step - B -- stabilize @next\n\t *\n\t * Similar to unlock(), wait for @node->next or move @lock from @node\n\t * back to @prev.\n\t */\n\n\tnext = osq_wait_next(lock, node, prev);\n\tif (!next)\n\t\treturn false;\n\n\t/*\n\t * Step - C -- unlink\n\t *\n\t * @prev is stable because its still waiting for a new @prev->next\n\t * pointer, @next is stable because our @node->next pointer is NULL and\n\t * it will wait in Step-A.\n\t */\n\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n\n\treturn false;\n}",
          "includes": [
            "#include <linux/osq_lock.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/osq_lock.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);\n\nbool osq_lock(struct optimistic_spin_queue *lock)\n{\n\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);\n\tstruct optimistic_spin_node *prev, *next;\n\tint curr = encode_cpu(smp_processor_id());\n\tint old;\n\n\tnode->locked = 0;\n\tnode->next = NULL;\n\tnode->cpu = curr;\n\n\t/*\n\t * We need both ACQUIRE (pairs with corresponding RELEASE in\n\t * unlock() uncontended, or fastpath) and RELEASE (to publish\n\t * the node fields we just initialised) semantics when updating\n\t * the lock tail.\n\t */\n\told = atomic_xchg(&lock->tail, curr);\n\tif (old == OSQ_UNLOCKED_VAL)\n\t\treturn true;\n\n\tprev = decode_cpu(old);\n\tnode->prev = prev;\n\n\t/*\n\t * osq_lock()\t\t\tunqueue\n\t *\n\t * node->prev = prev\t\tosq_wait_next()\n\t * WMB\t\t\t\tMB\n\t * prev->next = node\t\tnext->prev = prev // unqueue-C\n\t *\n\t * Here 'node->prev' and 'next->prev' are the same variable and we need\n\t * to ensure these stores happen in-order to avoid corrupting the list.\n\t */\n\tsmp_wmb();\n\n\tWRITE_ONCE(prev->next, node);\n\n\t/*\n\t * Normally @prev is untouchable after the above store; because at that\n\t * moment unlock can proceed and wipe the node element from stack.\n\t *\n\t * However, since our nodes are static per-cpu storage, we're\n\t * guaranteed their existence -- this allows us to apply\n\t * cmpxchg in an attempt to undo our queueing.\n\t */\n\n\twhile (!READ_ONCE(node->locked)) {\n\t\t/*\n\t\t * If we need to reschedule bail... so we can block.\n\t\t * Use vcpu_is_preempted() to avoid waiting for a preempted\n\t\t * lock holder:\n\t\t */\n\t\tif (need_resched() || vcpu_is_preempted(node_cpu(node->prev)))\n\t\t\tgoto unqueue;\n\n\t\tcpu_relax();\n\t}\n\treturn true;\n\nunqueue:\n\t/*\n\t * Step - A  -- stabilize @prev\n\t *\n\t * Undo our @prev->next assignment; this will make @prev's\n\t * unlock()/unqueue() wait for a next pointer since @lock points to us\n\t * (or later).\n\t */\n\n\tfor (;;) {\n\t\tif (prev->next == node &&\n\t\t    cmpxchg(&prev->next, node, NULL) == node)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We can only fail the cmpxchg() racing against an unlock(),\n\t\t * in which case we should observe @node->locked becomming\n\t\t * true.\n\t\t */\n\t\tif (smp_load_acquire(&node->locked))\n\t\t\treturn true;\n\n\t\tcpu_relax();\n\n\t\t/*\n\t\t * Or we race against a concurrent unqueue()'s step-B, in which\n\t\t * case its step-C will write us a new @node->prev pointer.\n\t\t */\n\t\tprev = READ_ONCE(node->prev);\n\t}\n\n\t/*\n\t * Step - B -- stabilize @next\n\t *\n\t * Similar to unlock(), wait for @node->next or move @lock from @node\n\t * back to @prev.\n\t */\n\n\tnext = osq_wait_next(lock, node, prev);\n\tif (!next)\n\t\treturn false;\n\n\t/*\n\t * Step - C -- unlink\n\t *\n\t * @prev is stable because its still waiting for a new @prev->next\n\t * pointer, @next is stable because our @node->next pointer is NULL and\n\t * it will wait in Step-A.\n\t */\n\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rwsem_can_spin_on_owner",
          "args": [
            "sem"
          ],
          "line": 437
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_can_spin_on_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "370-388",
          "snippet": "static inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner;\n\tbool ret = true;\n\n\tBUILD_BUG_ON(!rwsem_has_anonymous_owner(RWSEM_OWNER_UNKNOWN));\n\n\tif (need_resched())\n\t\treturn false;\n\n\trcu_read_lock();\n\towner = READ_ONCE(sem->owner);\n\tif (owner) {\n\t\tret = is_rwsem_owner_spinnable(owner) &&\n\t\t      owner_on_cpu(owner);\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner;\n\tbool ret = true;\n\n\tBUILD_BUG_ON(!rwsem_has_anonymous_owner(RWSEM_OWNER_UNKNOWN));\n\n\tif (need_resched())\n\t\treturn false;\n\n\trcu_read_lock();\n\towner = READ_ONCE(sem->owner);\n\tif (owner) {\n\t\tret = is_rwsem_owner_spinnable(owner) &&\n\t\t      owner_on_cpu(owner);\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "preempt_disable",
          "args": [],
          "line": 434
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_preempt_disabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "3571-3576",
          "snippet": "void __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic void __sched;\nstatic void __sched;\n\nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\tbool taken = false;\n\n\tpreempt_disable();\n\n\t/* sem->wait_lock should not be held when doing optimistic spinning */\n\tif (!rwsem_can_spin_on_owner(sem))\n\t\tgoto done;\n\n\tif (!osq_lock(&sem->osq))\n\t\tgoto done;\n\n\t/*\n\t * Optimistically spin on the owner field and attempt to acquire the\n\t * lock whenever the owner changes. Spinning will be stopped when:\n\t *  1) the owning writer isn't running; or\n\t *  2) readers own the lock as we can't determine if they are\n\t *     actively running or not.\n\t */\n\twhile (rwsem_spin_on_owner(sem)) {\n\t\t/*\n\t\t * Try to acquire the lock\n\t\t */\n\t\tif (rwsem_try_write_lock_unqueued(sem)) {\n\t\t\ttaken = true;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * When there's no owner, we might have preempted between the\n\t\t * owner acquiring the lock and setting the owner field. If\n\t\t * we're an RT task that will live-lock because we won't let\n\t\t * the owner complete.\n\t\t */\n\t\tif (!sem->owner && (need_resched() || rt_task(current)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * The cpu_relax() call is a compiler barrier which forces\n\t\t * everything in this loop to be re-loaded. We don't need\n\t\t * memory barriers as we'll eventually observe the right\n\t\t * values at the cost of a few extra spins.\n\t\t */\n\t\tcpu_relax();\n\t}\n\tosq_unlock(&sem->osq);\ndone:\n\tpreempt_enable();\n\treturn taken;\n}"
  },
  {
    "function_name": "rwsem_spin_on_owner",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "393-428",
    "snippet": "static noinline bool rwsem_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner = READ_ONCE(sem->owner);\n\n\tif (!is_rwsem_owner_spinnable(owner))\n\t\treturn false;\n\n\trcu_read_lock();\n\twhile (owner && (READ_ONCE(sem->owner) == owner)) {\n\t\t/*\n\t\t * Ensure we emit the owner->on_cpu, dereference _after_\n\t\t * checking sem->owner still matches owner, if that fails,\n\t\t * owner might point to free()d memory, if it still matches,\n\t\t * the rcu_read_lock() ensures the memory stays valid.\n\t\t */\n\t\tbarrier();\n\n\t\t/*\n\t\t * abort spinning when need_resched or owner is not running or\n\t\t * owner's cpu is preempted.\n\t\t */\n\t\tif (need_resched() || !owner_on_cpu(owner)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * If there is a new owner or the owner is not set, we continue\n\t * spinning.\n\t */\n\treturn is_rwsem_owner_spinnable(READ_ONCE(sem->owner));\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "is_rwsem_owner_spinnable",
          "args": [
            "READ_ONCE(sem->owner)"
          ],
          "line": 427
        },
        "resolved": true,
        "details": {
          "function_name": "is_rwsem_owner_spinnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem.h",
          "lines": "80-83",
          "snippet": "static inline bool is_rwsem_owner_spinnable(struct task_struct *owner)\n{\n\treturn !((unsigned long)owner & RWSEM_ANONYMOUSLY_OWNED);\n}",
          "includes": [],
          "macros_used": [
            "#define RWSEM_ANONYMOUSLY_OWNED\t(1UL << 1)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#define RWSEM_ANONYMOUSLY_OWNED\t(1UL << 1)\n\nstatic inline bool is_rwsem_owner_spinnable(struct task_struct *owner)\n{\n\treturn !((unsigned long)owner & RWSEM_ANONYMOUSLY_OWNED);\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "sem->owner"
          ],
          "line": 427
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 421
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 419
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "owner_on_cpu",
          "args": [
            "owner"
          ],
          "line": 414
        },
        "resolved": true,
        "details": {
          "function_name": "owner_on_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "361-368",
          "snippet": "static inline bool owner_on_cpu(struct task_struct *owner)\n{\n\t/*\n\t * As lock holder preemption issue, we both skip spinning if\n\t * task is not on cpu or its cpu is preempted\n\t */\n\treturn owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool owner_on_cpu(struct task_struct *owner)\n{\n\t/*\n\t * As lock holder preemption issue, we both skip spinning if\n\t * task is not on cpu or its cpu is preempted\n\t */\n\treturn owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));\n}"
        }
      },
      {
        "call_info": {
          "callee": "need_resched",
          "args": [],
          "line": 414
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "barrier",
          "args": [],
          "line": 408
        },
        "resolved": true,
        "details": {
          "function_name": "membarrier_register_global_expedited",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/membarrier.c",
          "lines": "189-219",
          "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tif (atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1) {\n\t\t/*\n\t\t * For single mm user, single threaded process, we can\n\t\t * simply issue a memory barrier after setting\n\t\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED to guarantee that\n\t\t * no memory access following registration is reordered\n\t\t * before registration.\n\t\t */\n\t\tsmp_mb();\n\t} else {\n\t\t/*\n\t\t * For multi-mm user threads, we need to ensure all\n\t\t * future scheduler executions will observe the new\n\t\t * thread flag state for this mm.\n\t\t */\n\t\tsynchronize_sched();\n\t}\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tif (atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1) {\n\t\t/*\n\t\t * For single mm user, single threaded process, we can\n\t\t * simply issue a memory barrier after setting\n\t\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED to guarantee that\n\t\t * no memory access following registration is reordered\n\t\t * before registration.\n\t\t */\n\t\tsmp_mb();\n\t} else {\n\t\t/*\n\t\t * For multi-mm user threads, we need to ensure all\n\t\t * future scheduler executions will observe the new\n\t\t * thread flag state for this mm.\n\t\t */\n\t\tsynchronize_sched();\n\t}\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "sem->owner"
          ],
          "line": 401
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 400
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "sem->owner"
          ],
          "line": 395
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic noinline bool rwsem_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner = READ_ONCE(sem->owner);\n\n\tif (!is_rwsem_owner_spinnable(owner))\n\t\treturn false;\n\n\trcu_read_lock();\n\twhile (owner && (READ_ONCE(sem->owner) == owner)) {\n\t\t/*\n\t\t * Ensure we emit the owner->on_cpu, dereference _after_\n\t\t * checking sem->owner still matches owner, if that fails,\n\t\t * owner might point to free()d memory, if it still matches,\n\t\t * the rcu_read_lock() ensures the memory stays valid.\n\t\t */\n\t\tbarrier();\n\n\t\t/*\n\t\t * abort spinning when need_resched or owner is not running or\n\t\t * owner's cpu is preempted.\n\t\t */\n\t\tif (need_resched() || !owner_on_cpu(owner)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * If there is a new owner or the owner is not set, we continue\n\t * spinning.\n\t */\n\treturn is_rwsem_owner_spinnable(READ_ONCE(sem->owner));\n}"
  },
  {
    "function_name": "rwsem_can_spin_on_owner",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "370-388",
    "snippet": "static inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner;\n\tbool ret = true;\n\n\tBUILD_BUG_ON(!rwsem_has_anonymous_owner(RWSEM_OWNER_UNKNOWN));\n\n\tif (need_resched())\n\t\treturn false;\n\n\trcu_read_lock();\n\towner = READ_ONCE(sem->owner);\n\tif (owner) {\n\t\tret = is_rwsem_owner_spinnable(owner) &&\n\t\t      owner_on_cpu(owner);\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 386
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "owner_on_cpu",
          "args": [
            "owner"
          ],
          "line": 384
        },
        "resolved": true,
        "details": {
          "function_name": "owner_on_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "361-368",
          "snippet": "static inline bool owner_on_cpu(struct task_struct *owner)\n{\n\t/*\n\t * As lock holder preemption issue, we both skip spinning if\n\t * task is not on cpu or its cpu is preempted\n\t */\n\treturn owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool owner_on_cpu(struct task_struct *owner)\n{\n\t/*\n\t * As lock holder preemption issue, we both skip spinning if\n\t * task is not on cpu or its cpu is preempted\n\t */\n\treturn owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));\n}"
        }
      },
      {
        "call_info": {
          "callee": "is_rwsem_owner_spinnable",
          "args": [
            "owner"
          ],
          "line": 383
        },
        "resolved": true,
        "details": {
          "function_name": "is_rwsem_owner_spinnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem.h",
          "lines": "80-83",
          "snippet": "static inline bool is_rwsem_owner_spinnable(struct task_struct *owner)\n{\n\treturn !((unsigned long)owner & RWSEM_ANONYMOUSLY_OWNED);\n}",
          "includes": [],
          "macros_used": [
            "#define RWSEM_ANONYMOUSLY_OWNED\t(1UL << 1)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#define RWSEM_ANONYMOUSLY_OWNED\t(1UL << 1)\n\nstatic inline bool is_rwsem_owner_spinnable(struct task_struct *owner)\n{\n\treturn !((unsigned long)owner & RWSEM_ANONYMOUSLY_OWNED);\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "sem->owner"
          ],
          "line": 381
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 380
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "need_resched",
          "args": [],
          "line": 377
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUILD_BUG_ON",
          "args": [
            "!rwsem_has_anonymous_owner(RWSEM_OWNER_UNKNOWN)"
          ],
          "line": 375
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rwsem_has_anonymous_owner",
          "args": [
            "RWSEM_OWNER_UNKNOWN"
          ],
          "line": 375
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_has_anonymous_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem.h",
          "lines": "88-91",
          "snippet": "static inline bool rwsem_has_anonymous_owner(struct task_struct *owner)\n{\n\treturn (unsigned long)owner & RWSEM_ANONYMOUSLY_OWNED;\n}",
          "includes": [],
          "macros_used": [
            "#define RWSEM_ANONYMOUSLY_OWNED\t(1UL << 1)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#define RWSEM_ANONYMOUSLY_OWNED\t(1UL << 1)\n\nstatic inline bool rwsem_has_anonymous_owner(struct task_struct *owner)\n{\n\treturn (unsigned long)owner & RWSEM_ANONYMOUSLY_OWNED;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner;\n\tbool ret = true;\n\n\tBUILD_BUG_ON(!rwsem_has_anonymous_owner(RWSEM_OWNER_UNKNOWN));\n\n\tif (need_resched())\n\t\treturn false;\n\n\trcu_read_lock();\n\towner = READ_ONCE(sem->owner);\n\tif (owner) {\n\t\tret = is_rwsem_owner_spinnable(owner) &&\n\t\t      owner_on_cpu(owner);\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}"
  },
  {
    "function_name": "owner_on_cpu",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "361-368",
    "snippet": "static inline bool owner_on_cpu(struct task_struct *owner)\n{\n\t/*\n\t * As lock holder preemption issue, we both skip spinning if\n\t * task is not on cpu or its cpu is preempted\n\t */\n\treturn owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "vcpu_is_preempted",
          "args": [
            "task_cpu(owner)"
          ],
          "line": 367
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_cpu",
          "args": [
            "owner"
          ],
          "line": 367
        },
        "resolved": true,
        "details": {
          "function_name": "ignore_task_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/ftrace.c",
          "lines": "6542-6556",
          "snippet": "static void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/suspend.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_MUTEX(ftrace_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nstatic DEFINE_MUTEX(ftrace_lock);\n\nstatic void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool owner_on_cpu(struct task_struct *owner)\n{\n\t/*\n\t * As lock holder preemption issue, we both skip spinning if\n\t * task is not on cpu or its cpu is preempted\n\t */\n\treturn owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));\n}"
  },
  {
    "function_name": "rwsem_try_write_lock_unqueued",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "342-359",
    "snippet": "static inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)\n{\n\tlong old, count = atomic_long_read(&sem->count);\n\n\twhile (true) {\n\t\tif (!(count == 0 || count == RWSEM_WAITING_BIAS))\n\t\t\treturn false;\n\n\t\told = atomic_long_cmpxchg_acquire(&sem->count, count,\n\t\t\t\t      count + RWSEM_ACTIVE_WRITE_BIAS);\n\t\tif (old == count) {\n\t\t\trwsem_set_owner(sem);\n\t\t\treturn true;\n\t\t}\n\n\t\tcount = old;\n\t}\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rwsem_set_owner",
          "args": [
            "sem"
          ],
          "line": 353
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_set_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem.h",
          "lines": "112-114",
          "snippet": "static inline void rwsem_set_owner(struct rw_semaphore *sem)\n{\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void rwsem_set_owner(struct rw_semaphore *sem)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_cmpxchg_acquire",
          "args": [
            "&sem->count",
            "count",
            "count + RWSEM_ACTIVE_WRITE_BIAS"
          ],
          "line": 350
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_read",
          "args": [
            "&sem->count"
          ],
          "line": 344
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)\n{\n\tlong old, count = atomic_long_read(&sem->count);\n\n\twhile (true) {\n\t\tif (!(count == 0 || count == RWSEM_WAITING_BIAS))\n\t\t\treturn false;\n\n\t\told = atomic_long_cmpxchg_acquire(&sem->count, count,\n\t\t\t\t      count + RWSEM_ACTIVE_WRITE_BIAS);\n\t\tif (old == count) {\n\t\t\trwsem_set_owner(sem);\n\t\t\treturn true;\n\t\t}\n\n\t\tcount = old;\n\t}\n}"
  },
  {
    "function_name": "rwsem_try_write_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "313-336",
    "snippet": "static inline bool rwsem_try_write_lock(long count, struct rw_semaphore *sem)\n{\n\t/*\n\t * Avoid trying to acquire write lock if count isn't RWSEM_WAITING_BIAS.\n\t */\n\tif (count != RWSEM_WAITING_BIAS)\n\t\treturn false;\n\n\t/*\n\t * Acquire the lock by trying to set it to ACTIVE_WRITE_BIAS. If there\n\t * are other tasks on the wait list, we need to add on WAITING_BIAS.\n\t */\n\tcount = list_is_singular(&sem->wait_list) ?\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS :\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS + RWSEM_WAITING_BIAS;\n\n\tif (atomic_long_cmpxchg_acquire(&sem->count, RWSEM_WAITING_BIAS, count)\n\t\t\t\t\t\t\t== RWSEM_WAITING_BIAS) {\n\t\trwsem_set_owner(sem);\n\t\treturn true;\n\t}\n\n\treturn false;\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rwsem_set_owner",
          "args": [
            "sem"
          ],
          "line": 331
        },
        "resolved": true,
        "details": {
          "function_name": "rwsem_set_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem.h",
          "lines": "112-114",
          "snippet": "static inline void rwsem_set_owner(struct rw_semaphore *sem)\n{\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void rwsem_set_owner(struct rw_semaphore *sem)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_cmpxchg_acquire",
          "args": [
            "&sem->count",
            "RWSEM_WAITING_BIAS",
            "count"
          ],
          "line": 329
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_is_singular",
          "args": [
            "&sem->wait_list"
          ],
          "line": 325
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline bool rwsem_try_write_lock(long count, struct rw_semaphore *sem)\n{\n\t/*\n\t * Avoid trying to acquire write lock if count isn't RWSEM_WAITING_BIAS.\n\t */\n\tif (count != RWSEM_WAITING_BIAS)\n\t\treturn false;\n\n\t/*\n\t * Acquire the lock by trying to set it to ACTIVE_WRITE_BIAS. If there\n\t * are other tasks on the wait list, we need to add on WAITING_BIAS.\n\t */\n\tcount = list_is_singular(&sem->wait_list) ?\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS :\n\t\t\tRWSEM_ACTIVE_WRITE_BIAS + RWSEM_WAITING_BIAS;\n\n\tif (atomic_long_cmpxchg_acquire(&sem->count, RWSEM_WAITING_BIAS, count)\n\t\t\t\t\t\t\t== RWSEM_WAITING_BIAS) {\n\t\trwsem_set_owner(sem);\n\t\treturn true;\n\t}\n\n\treturn false;\n}"
  },
  {
    "function_name": "__rwsem_down_read_failed_common",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "225-292",
    "snippet": "static inline struct rw_semaphore __sched *\n__rwsem_down_read_failed_common(struct rw_semaphore *sem, int state)\n{\n\tlong count, adjustment = -RWSEM_ACTIVE_READ_BIAS;\n\tstruct rwsem_waiter waiter;\n\tDEFINE_WAKE_Q(wake_q);\n\n\twaiter.task = current;\n\twaiter.type = RWSEM_WAITING_FOR_READ;\n\n\traw_spin_lock_irq(&sem->wait_lock);\n\tif (list_empty(&sem->wait_list)) {\n\t\t/*\n\t\t * In case the wait queue is empty and the lock isn't owned\n\t\t * by a writer, this reader can exit the slowpath and return\n\t\t * immediately as its RWSEM_ACTIVE_READ_BIAS has already\n\t\t * been set in the count.\n\t\t */\n\t\tif (atomic_long_read(&sem->count) >= 0) {\n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\treturn sem;\n\t\t}\n\t\tadjustment += RWSEM_WAITING_BIAS;\n\t}\n\tlist_add_tail(&waiter.list, &sem->wait_list);\n\n\t/* we're now waiting on the lock, but no longer actively locking */\n\tcount = atomic_long_add_return(adjustment, &sem->count);\n\n\t/*\n\t * If there are no active locks, wake the front queued process(es).\n\t *\n\t * If there are no writers and we are first in the queue,\n\t * wake our own waiter to join the existing active readers !\n\t */\n\tif (count == RWSEM_WAITING_BIAS ||\n\t    (count > RWSEM_WAITING_BIAS &&\n\t     adjustment != -RWSEM_ACTIVE_READ_BIAS))\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);\n\n\traw_spin_unlock_irq(&sem->wait_lock);\n\twake_up_q(&wake_q);\n\n\t/* wait to be given the lock */\n\twhile (true) {\n\t\tset_current_state(state);\n\t\tif (!waiter.task)\n\t\t\tbreak;\n\t\tif (signal_pending_state(state, current)) {\n\t\t\traw_spin_lock_irq(&sem->wait_lock);\n\t\t\tif (waiter.task)\n\t\t\t\tgoto out_nolock;\n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\tbreak;\n\t\t}\n\t\tschedule();\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn sem;\nout_nolock:\n\tlist_del(&waiter.list);\n\tif (list_empty(&sem->wait_list))\n\t\tatomic_long_add(-RWSEM_WAITING_BIAS, &sem->count);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\t__set_current_state(TASK_RUNNING);\n\treturn ERR_PTR(-EINTR);\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "ERR_PTR",
          "args": [
            "-EINTR"
          ],
          "line": 291
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 290
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&sem->wait_lock"
          ],
          "line": 289
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_add",
          "args": [
            "-RWSEM_WAITING_BIAS",
            "&sem->count"
          ],
          "line": 288
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&sem->wait_list"
          ],
          "line": 287
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_del",
          "args": [
            "&waiter.list"
          ],
          "line": 286
        },
        "resolved": true,
        "details": {
          "function_name": "list_del_leaf_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "446-448",
          "snippet": "static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 283
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedule",
          "args": [],
          "line": 280
        },
        "resolved": true,
        "details": {
          "function_name": "audit_schedule_prune",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/audit_tree.c",
          "lines": "919-922",
          "snippet": "static void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}",
          "includes": [
            "#include <linux/slab.h>",
            "#include <linux/refcount.h>",
            "#include <linux/kthread.h>",
            "#include <linux/mount.h>",
            "#include <linux/namei.h>",
            "#include <linux/fsnotify_backend.h>",
            "#include \"audit.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct task_struct *prune_thread;",
            "static void audit_schedule_prune(void);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/slab.h>\n#include <linux/refcount.h>\n#include <linux/kthread.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/fsnotify_backend.h>\n#include \"audit.h\"\n\nstatic struct task_struct *prune_thread;\nstatic void audit_schedule_prune(void);\n\nstatic void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&sem->wait_lock"
          ],
          "line": 274
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "signal_pending_state",
          "args": [
            "state",
            "current"
          ],
          "line": 273
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 270
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "&wake_q"
          ],
          "line": 266
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_q",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "423-443",
          "snippet": "void wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rwsem_mark_wake",
          "args": [
            "sem",
            "RWSEM_WAKE_ANY",
            "&wake_q"
          ],
          "line": 263
        },
        "resolved": true,
        "details": {
          "function_name": "__rwsem_mark_wake",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
          "lines": "127-220",
          "snippet": "static void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/osq_lock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/init.h>",
            "#include <linux/rwsem.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible struct",
            "__visible\nstruct",
            "__visible\nstruct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_add_return",
          "args": [
            "adjustment",
            "&sem->count"
          ],
          "line": 252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_add_tail",
          "args": [
            "&waiter.list",
            "&sem->wait_list"
          ],
          "line": 249
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_read",
          "args": [
            "&sem->count"
          ],
          "line": 243
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 230
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic inline struct rw_semaphore __sched *\n__rwsem_down_read_failed_common(struct rw_semaphore *sem, int state)\n{\n\tlong count, adjustment = -RWSEM_ACTIVE_READ_BIAS;\n\tstruct rwsem_waiter waiter;\n\tDEFINE_WAKE_Q(wake_q);\n\n\twaiter.task = current;\n\twaiter.type = RWSEM_WAITING_FOR_READ;\n\n\traw_spin_lock_irq(&sem->wait_lock);\n\tif (list_empty(&sem->wait_list)) {\n\t\t/*\n\t\t * In case the wait queue is empty and the lock isn't owned\n\t\t * by a writer, this reader can exit the slowpath and return\n\t\t * immediately as its RWSEM_ACTIVE_READ_BIAS has already\n\t\t * been set in the count.\n\t\t */\n\t\tif (atomic_long_read(&sem->count) >= 0) {\n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\treturn sem;\n\t\t}\n\t\tadjustment += RWSEM_WAITING_BIAS;\n\t}\n\tlist_add_tail(&waiter.list, &sem->wait_list);\n\n\t/* we're now waiting on the lock, but no longer actively locking */\n\tcount = atomic_long_add_return(adjustment, &sem->count);\n\n\t/*\n\t * If there are no active locks, wake the front queued process(es).\n\t *\n\t * If there are no writers and we are first in the queue,\n\t * wake our own waiter to join the existing active readers !\n\t */\n\tif (count == RWSEM_WAITING_BIAS ||\n\t    (count > RWSEM_WAITING_BIAS &&\n\t     adjustment != -RWSEM_ACTIVE_READ_BIAS))\n\t\t__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);\n\n\traw_spin_unlock_irq(&sem->wait_lock);\n\twake_up_q(&wake_q);\n\n\t/* wait to be given the lock */\n\twhile (true) {\n\t\tset_current_state(state);\n\t\tif (!waiter.task)\n\t\t\tbreak;\n\t\tif (signal_pending_state(state, current)) {\n\t\t\traw_spin_lock_irq(&sem->wait_lock);\n\t\t\tif (waiter.task)\n\t\t\t\tgoto out_nolock;\n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\tbreak;\n\t\t}\n\t\tschedule();\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn sem;\nout_nolock:\n\tlist_del(&waiter.list);\n\tif (list_empty(&sem->wait_list))\n\t\tatomic_long_add(-RWSEM_WAITING_BIAS, &sem->count);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\t__set_current_state(TASK_RUNNING);\n\treturn ERR_PTR(-EINTR);\n}"
  },
  {
    "function_name": "__rwsem_mark_wake",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "127-220",
    "snippet": "static void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_long_add",
          "args": [
            "adjustment",
            "&sem->count"
          ],
          "line": 219
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&sem->wait_list"
          ],
          "line": 213
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_store_release",
          "args": [
            "&waiter->task",
            "NULL"
          ],
          "line": 209
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_del",
          "args": [
            "&waiter->list"
          ],
          "line": 202
        },
        "resolved": true,
        "details": {
          "function_name": "list_del_leaf_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "446-448",
          "snippet": "static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_q_add",
          "args": [
            "wake_q",
            "tsk"
          ],
          "line": 201
        },
        "resolved": true,
        "details": {
          "function_name": "wake_q_add",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "399-421",
          "snippet": "void wake_q_add(struct wake_q_head *head, struct task_struct *task)\n{\n\tstruct wake_q_node *node = &task->wake_q;\n\n\t/*\n\t * Atomically grab the task, if ->wake_q is !nil already it means\n\t * its already queued (either by us or someone else) and will get the\n\t * wakeup due to that.\n\t *\n\t * This cmpxchg() executes a full barrier, which pairs with the full\n\t * barrier executed by the wakeup in wake_up_q().\n\t */\n\tif (cmpxchg(&node->next, NULL, WAKE_Q_TAIL))\n\t\treturn;\n\n\tget_task_struct(task);\n\n\t/*\n\t * The head is context local, there can be no concurrency.\n\t */\n\t*head->lastp = node;\n\thead->lastp = &node->next;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid wake_q_add(struct wake_q_head *head, struct task_struct *task)\n{\n\tstruct wake_q_node *node = &task->wake_q;\n\n\t/*\n\t * Atomically grab the task, if ->wake_q is !nil already it means\n\t * its already queued (either by us or someone else) and will get the\n\t * wakeup due to that.\n\t *\n\t * This cmpxchg() executes a full barrier, which pairs with the full\n\t * barrier executed by the wakeup in wake_up_q().\n\t */\n\tif (cmpxchg(&node->next, NULL, WAKE_Q_TAIL))\n\t\treturn;\n\n\tget_task_struct(task);\n\n\t/*\n\t * The head is context local, there can be no concurrency.\n\t */\n\t*head->lastp = node;\n\thead->lastp = &node->next;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_for_each_entry_safe",
          "args": [
            "waiter",
            "tmp",
            "&sem->wait_list",
            "list"
          ],
          "line": 192
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rwsem_set_reader_owned",
          "args": [
            "sem",
            "waiter->task"
          ],
          "line": 183
        },
        "resolved": true,
        "details": {
          "function_name": "__rwsem_set_reader_owned",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem.h",
          "lines": "120-123",
          "snippet": "static inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,\n\t\t\t\t\t   struct task_struct *owner)\n{\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,\n\t\t\t\t\t   struct task_struct *owner)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_long_add_return",
          "args": [
            "-adjustment",
            "&sem->count"
          ],
          "line": 171
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "oldcount < RWSEM_WAITING_BIAS"
          ],
          "line": 164
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_fetch_add",
          "args": [
            "adjustment",
            "&sem->count"
          ],
          "line": 163
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_first_entry",
          "args": [
            "&sem->wait_list",
            "structrwsem_waiter",
            "list"
          ],
          "line": 138
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nstatic void __rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t      enum rwsem_wake_type wake_type,\n\t\t\t      struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\n\t/*\n\t * Take a peek at the queue head waiter such that we can determine\n\t * the wakeup(s) to perform.\n\t */\n\twaiter = list_first_entry(&sem->wait_list, struct rwsem_waiter, list);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t/*\n\t\t\t * Mark writer at the front of the queue for wakeup.\n\t\t\t * Until the task is actually later awoken later by\n\t\t\t * the caller, other writers are able to steal it.\n\t\t\t * Readers, on the other hand, will block as they\n\t\t\t * will notice the queued writer.\n\t\t\t */\n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Writers might steal the lock before we grant it to the next reader.\n\t * We prefer to do the first reader grant before counting readers\n\t * so we can bail out early if a writer stole the lock.\n\t */\n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tadjustment = RWSEM_ACTIVE_READ_BIAS;\n try_reader_grant:\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount < RWSEM_WAITING_BIAS)) {\n\t\t\t/*\n\t\t\t * If the count is still less than RWSEM_WAITING_BIAS\n\t\t\t * after removing the adjustment, it is assumed that\n\t\t\t * a writer has stolen the lock. We have to undo our\n\t\t\t * reader grant.\n\t\t\t */\n\t\t\tif (atomic_long_add_return(-adjustment, &sem->count) <\n\t\t\t    RWSEM_WAITING_BIAS)\n\t\t\t\treturn;\n\n\t\t\t/* Last active locker left. Retry waking readers. */\n\t\t\tgoto try_reader_grant;\n\t\t}\n\t\t/*\n\t\t * It is not really necessary to set it to reader-owned here,\n\t\t * but it gives the spinners an early indication that the\n\t\t * readers now have the lock.\n\t\t */\n\t\t__rwsem_set_reader_owned(sem, waiter->task);\n\t}\n\n\t/*\n\t * Grant an infinite number of read locks to the readers at the front\n\t * of the queue. We know that woken will be at least 1 as we accounted\n\t * for above. Note we increment the 'active part' of the count by the\n\t * number of readers before waking any processes up.\n\t */\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tstruct task_struct *tsk;\n\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tbreak;\n\n\t\twoken++;\n\t\ttsk = waiter->task;\n\n\t\twake_q_add(wake_q, tsk);\n\t\tlist_del(&waiter->list);\n\t\t/*\n\t\t * Ensure that the last operation is setting the reader\n\t\t * waiter to nil such that rwsem_down_read_failed() cannot\n\t\t * race with do_exit() by always holding a reference count\n\t\t * to the task to wakeup.\n\t\t */\n\t\tsmp_store_release(&waiter->task, NULL);\n\t}\n\n\tadjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;\n\tif (list_empty(&sem->wait_list)) {\n\t\t/* hit end of list above */\n\t\tadjustment -= RWSEM_WAITING_BIAS;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n}"
  },
  {
    "function_name": "__init_rwsem",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem-xadd.c",
    "lines": "76-93",
    "snippet": "void __init_rwsem(struct rw_semaphore *sem, const char *name,\n\t\t  struct lock_class_key *key)\n{\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t/*\n\t * Make sure we are not reinitializing a held semaphore:\n\t */\n\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));\n\tlockdep_init_map(&sem->dep_map, name, key, 0);\n#endif\n\tatomic_long_set(&sem->count, RWSEM_UNLOCKED_VALUE);\n\traw_spin_lock_init(&sem->wait_lock);\n\tINIT_LIST_HEAD(&sem->wait_list);\n#ifdef CONFIG_RWSEM_SPIN_ON_OWNER\n\tsem->owner = NULL;\n\tosq_lock_init(&sem->osq);\n#endif\n}",
    "includes": [
      "#include \"rwsem.h\"",
      "#include <linux/osq_lock.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/init.h>",
      "#include <linux/rwsem.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible struct",
      "__visible\nstruct",
      "__visible\nstruct"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "osq_lock_init",
          "args": [
            "&sem->osq"
          ],
          "line": 91
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "INIT_LIST_HEAD",
          "args": [
            "&sem->wait_list"
          ],
          "line": 88
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_init",
          "args": [
            "&sem->wait_lock"
          ],
          "line": 87
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_set",
          "args": [
            "&sem->count",
            "RWSEM_UNLOCKED_VALUE"
          ],
          "line": 86
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_init_map",
          "args": [
            "&sem->dep_map",
            "name",
            "key",
            "0"
          ],
          "line": 84
        },
        "resolved": true,
        "details": {
          "function_name": "lockdep_init_map",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/lockdep.c",
          "lines": "3148-3152",
          "snippet": "void lockdep_init_map(struct lockdep_map *lock, const char *name,\n\t\t      struct lock_class_key *key, int subclass)\n{\n\t__lockdep_init_map(lock, name, key, subclass);\n}",
          "includes": [
            "#include \"lockdep_states.h\"",
            "#include \"lockdep_states.h\"",
            "#include \"lockdep_states.h\"",
            "#include \"lockdep_states.h\"",
            "#include <trace/events/lock.h>",
            "#include \"lockdep_internals.h\"",
            "#include <asm/sections.h>",
            "#include <linux/nmi.h>",
            "#include <linux/jhash.h>",
            "#include <linux/random.h>",
            "#include <linux/gfp.h>",
            "#include <linux/bitops.h>",
            "#include <linux/stringify.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/hash.h>",
            "#include <linux/utsname.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/module.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/mutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"lockdep_states.h\"\n#include \"lockdep_states.h\"\n#include \"lockdep_states.h\"\n#include \"lockdep_states.h\"\n#include <trace/events/lock.h>\n#include \"lockdep_internals.h\"\n#include <asm/sections.h>\n#include <linux/nmi.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <linux/gfp.h>\n#include <linux/bitops.h>\n#include <linux/stringify.h>\n#include <linux/ftrace.h>\n#include <linux/hash.h>\n#include <linux/utsname.h>\n#include <linux/irqflags.h>\n#include <linux/debug_locks.h>\n#include <linux/stacktrace.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n\nvoid lockdep_init_map(struct lockdep_map *lock, const char *name,\n\t\t      struct lock_class_key *key, int subclass)\n{\n\t__lockdep_init_map(lock, name, key, subclass);\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_check_no_locks_freed",
          "args": [
            "(void *)sem",
            "sizeof(*sem)"
          ],
          "line": 83
        },
        "resolved": true,
        "details": {
          "function_name": "debug_check_no_locks_freed",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/lockdep.c",
          "lines": "4322-4344",
          "snippet": "void debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tunsigned long flags;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\n\t\tif (not_in_range(mem_from, mem_len, hlock->instance,\n\t\t\t\t\tsizeof(*hlock->instance)))\n\t\t\tcontinue;\n\n\t\tprint_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);\n\t\tbreak;\n\t}\n\traw_local_irq_restore(flags);\n}",
          "includes": [
            "#include \"lockdep_states.h\"",
            "#include \"lockdep_states.h\"",
            "#include \"lockdep_states.h\"",
            "#include \"lockdep_states.h\"",
            "#include <trace/events/lock.h>",
            "#include \"lockdep_internals.h\"",
            "#include <asm/sections.h>",
            "#include <linux/nmi.h>",
            "#include <linux/jhash.h>",
            "#include <linux/random.h>",
            "#include <linux/gfp.h>",
            "#include <linux/bitops.h>",
            "#include <linux/stringify.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/hash.h>",
            "#include <linux/utsname.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/stacktrace.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/module.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/mutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"lockdep_states.h\"\n#include \"lockdep_states.h\"\n#include \"lockdep_states.h\"\n#include \"lockdep_states.h\"\n#include <trace/events/lock.h>\n#include \"lockdep_internals.h\"\n#include <asm/sections.h>\n#include <linux/nmi.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <linux/gfp.h>\n#include <linux/bitops.h>\n#include <linux/stringify.h>\n#include <linux/ftrace.h>\n#include <linux/hash.h>\n#include <linux/utsname.h>\n#include <linux/irqflags.h>\n#include <linux/debug_locks.h>\n#include <linux/stacktrace.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n\nvoid debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tunsigned long flags;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\n\t\tif (not_in_range(mem_from, mem_len, hlock->instance,\n\t\t\t\t\tsizeof(*hlock->instance)))\n\t\t\tcontinue;\n\n\t\tprint_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);\n\t\tbreak;\n\t}\n\traw_local_irq_restore(flags);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/osq_lock.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/rwsem.h>\n\n__visible struct;\n__visible struct;\n__visible struct;\n__visible struct;\n__visible\nstruct;\n__visible\nstruct;\n\nvoid __init_rwsem(struct rw_semaphore *sem, const char *name,\n\t\t  struct lock_class_key *key)\n{\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t/*\n\t * Make sure we are not reinitializing a held semaphore:\n\t */\n\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));\n\tlockdep_init_map(&sem->dep_map, name, key, 0);\n#endif\n\tatomic_long_set(&sem->count, RWSEM_UNLOCKED_VALUE);\n\traw_spin_lock_init(&sem->wait_lock);\n\tINIT_LIST_HEAD(&sem->wait_list);\n#ifdef CONFIG_RWSEM_SPIN_ON_OWNER\n\tsem->owner = NULL;\n\tosq_lock_init(&sem->osq);\n#endif\n}"
  }
]