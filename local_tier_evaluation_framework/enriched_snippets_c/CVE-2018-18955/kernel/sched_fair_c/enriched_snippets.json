[
  {
    "function_name": "set_skip_buddy",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6511-10299",
    "snippet": "static void set_skip_buddy(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se)\n\t\tcfs_rq_of(se)->skip = se;\n}\n\n/*\n * Preempt the current task with a newly woken task if needed:\n */\nstatic void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct sched_entity *se = &curr->se, *pse = &p->se;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tint scale = cfs_rq->nr_running >= sched_nr_latency;\n\tint next_buddy_marked = 0;\n\n\tif (unlikely(se == pse))\n\t\treturn;\n\n\t/*\n\t * This is possible from callers such as attach_tasks(), in which we\n\t * unconditionally check_prempt_curr() after an enqueue (which may have\n\t * lead to a throttle).  This both saves work and prevents false\n\t * next-buddy nomination below.\n\t */\n\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))\n\t\treturn;\n\n\tif (sched_feat(NEXT_BUDDY) && scale && !(wake_flags & WF_FORK)) {\n\t\tset_next_buddy(pse);\n\t\tnext_buddy_marked = 1;\n\t}\n\n\t/*\n\t * We can come here with TIF_NEED_RESCHED already set from new task\n\t * wake up path.\n\t *\n\t * Note: this also catches the edge-case of curr being in a throttled\n\t * group (e.g. via set_curr_task), since update_curr() (in the\n\t * enqueue of curr) will have resulted in resched being set.  This\n\t * prevents us from potentially nominating it as a false LAST_BUDDY\n\t * below.\n\t */\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\t/* Idle tasks are by definition preempted by non-idle tasks. */\n\tif (unlikely(curr->policy == SCHED_IDLE) &&\n\t    likely(p->policy != SCHED_IDLE))\n\t\tgoto preempt;\n\n\t/*\n\t * Batch and idle tasks do not preempt non-idle tasks (their preemption\n\t * is driven by the tick):\n\t */\n\tif (unlikely(p->policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION))\n\t\treturn;\n\n\tfind_matching_se(&se, &pse);\n\tupdate_curr(cfs_rq_of(se));\n\tBUG_ON(!pse);\n\tif (wakeup_preempt_entity(se, pse) == 1) {\n\t\t/*\n\t\t * Bias pick_next to pick the sched entity that is\n\t\t * triggering this preemption.\n\t\t */\n\t\tif (!next_buddy_marked)\n\t\t\tset_next_buddy(pse);\n\t\tgoto preempt;\n\t}\n\n\treturn;\n\npreempt:\n\tresched_curr(rq);\n\t/*\n\t * Only set the backward buddy when the current task is still\n\t * on the rq. This can happen when a wakeup gets interleaved\n\t * with schedule on the ->pre_schedule() or idle_balance()\n\t * point, either of which can * drop the rq lock.\n\t *\n\t * Also, during early boot the idle thread is in the fair class,\n\t * for obvious reasons its a bad idea to schedule back to it.\n\t */\n\tif (unlikely(!se->on_rq || curr == rq->idle))\n\t\treturn;\n\n\tif (sched_feat(LAST_BUDDY) && scale && entity_is_task(se))\n\t\tset_last_buddy(se);\n}\n\nstatic struct task_struct *\npick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tstruct sched_entity *se;\n\tstruct task_struct *p;\n\tint new_tasks;\n\nagain:\n\tif (!cfs_rq->nr_running)\n\t\tgoto idle;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (prev->sched_class != &fair_sched_class)\n\t\tgoto simple;\n\n\t/*\n\t * Because of the set_next_buddy() in dequeue_task_fair() it is rather\n\t * likely that a next task is from the same cgroup as the current.\n\t *\n\t * Therefore attempt to avoid putting and setting the entire cgroup\n\t * hierarchy, only change the part that actually changes.\n\t */\n\n\tdo {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\n\t\t/*\n\t\t * Since we got here without doing put_prev_entity() we also\n\t\t * have to consider cfs_rq->curr. If it is still a runnable\n\t\t * entity, update_curr() will update its vruntime, otherwise\n\t\t * forget we've ever seen it.\n\t\t */\n\t\tif (curr) {\n\t\t\tif (curr->on_rq)\n\t\t\t\tupdate_curr(cfs_rq);\n\t\t\telse\n\t\t\t\tcurr = NULL;\n\n\t\t\t/*\n\t\t\t * This call to check_cfs_rq_runtime() will do the\n\t\t\t * throttle and dequeue its entity in the parent(s).\n\t\t\t * Therefore the nr_running test will indeed\n\t\t\t * be correct.\n\t\t\t */\n\t\t\tif (unlikely(check_cfs_rq_runtime(cfs_rq))) {\n\t\t\t\tcfs_rq = &rq->cfs;\n\n\t\t\t\tif (!cfs_rq->nr_running)\n\t\t\t\t\tgoto idle;\n\n\t\t\t\tgoto simple;\n\t\t\t}\n\t\t}\n\n\t\tse = pick_next_entity(cfs_rq, curr);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\n\t/*\n\t * Since we haven't yet done put_prev_entity and if the selected task\n\t * is a different task than we started out with, try and touch the\n\t * least amount of cfs_rqs.\n\t */\n\tif (prev != p) {\n\t\tstruct sched_entity *pse = &prev->se;\n\n\t\twhile (!(cfs_rq = is_same_group(se, pse))) {\n\t\t\tint se_depth = se->depth;\n\t\t\tint pse_depth = pse->depth;\n\n\t\t\tif (se_depth <= pse_depth) {\n\t\t\t\tput_prev_entity(cfs_rq_of(pse), pse);\n\t\t\t\tpse = parent_entity(pse);\n\t\t\t}\n\t\t\tif (se_depth >= pse_depth) {\n\t\t\t\tset_next_entity(cfs_rq_of(se), se);\n\t\t\t\tse = parent_entity(se);\n\t\t\t}\n\t\t}\n\n\t\tput_prev_entity(cfs_rq, pse);\n\t\tset_next_entity(cfs_rq, se);\n\t}\n\n\tgoto done;\nsimple:\n#endif\n\n\tput_prev_task(rq, prev);\n\n\tdo {\n\t\tse = pick_next_entity(cfs_rq, NULL);\n\t\tset_next_entity(cfs_rq, se);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\ndone: __maybe_unused;\n#ifdef CONFIG_SMP\n\t/*\n\t * Move the next running task to the front of\n\t * the list, so our cfs_tasks list becomes MRU\n\t * one.\n\t */\n\tlist_move(&p->se.group_node, &rq->cfs_tasks);\n#endif\n\n\tif (hrtick_enabled(rq))\n\t\thrtick_start_fair(rq, p);\n\n\tupdate_misfit_status(p, rq);\n\n\treturn p;\n\nidle:\n\tupdate_misfit_status(NULL, rq);\n\tnew_tasks = idle_balance(rq, rf);\n\n\t/*\n\t * Because idle_balance() releases (and re-acquires) rq->lock, it is\n\t * possible for any higher priority task to appear. In that case we\n\t * must re-start the pick_next_entity() loop.\n\t */\n\tif (new_tasks < 0)\n\t\treturn RETRY_TASK;\n\n\tif (new_tasks > 0)\n\t\tgoto again;\n\n\treturn NULL;\n}\n\n/*\n * Account for a descheduled task:\n */\nstatic void put_prev_task_fair(struct rq *rq, struct task_struct *prev)\n{\n\tstruct sched_entity *se = &prev->se;\n\tstruct cfs_rq *cfs_rq;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tput_prev_entity(cfs_rq, se);\n\t}\n}\n\n/*\n * sched_yield() is very simple\n *\n * The magic of dealing with the ->skip buddy is in pick_next_entity.\n */\nstatic void yield_task_fair(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tstruct sched_entity *se = &curr->se;\n\n\t/*\n\t * Are we the only task in the tree?\n\t */\n\tif (unlikely(rq->nr_running == 1))\n\t\treturn;\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (curr->policy != SCHED_BATCH) {\n\t\tupdate_rq_clock(rq);\n\t\t/*\n\t\t * Update run-time statistics of the 'current'.\n\t\t */\n\t\tupdate_curr(cfs_rq);\n\t\t/*\n\t\t * Tell update_rq_clock() that we've just updated,\n\t\t * so we don't do microscopic update in schedule()\n\t\t * and double the fastpath cost.\n\t\t */\n\t\trq_clock_skip_update(rq);\n\t}\n\n\tset_skip_buddy(se);\n}\n\nstatic bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/* throttled hierarchies are not runnable */\n\tif (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))\n\t\treturn false;\n\n\t/* Tell the scheduler that we'd really like pse to run next. */\n\tset_next_buddy(se);\n\n\tyield_task_fair(rq);\n\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\n/**************************************************\n * Fair scheduling class load-balancing methods.\n *\n * BASICS\n *\n * The purpose of load-balancing is to achieve the same basic fairness the\n * per-CPU scheduler provides, namely provide a proportional amount of compute\n * time to each task. This is expressed in the following equation:\n *\n *   W_i,n/P_i == W_j,n/P_j for all i,j                               (1)\n *\n * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight\n * W_i,0 is defined as:\n *\n *   W_i,0 = \\Sum_j w_i,j                                             (2)\n *\n * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight\n * is derived from the nice value as per sched_prio_to_weight[].\n *\n * The weight average is an exponential decay average of the instantaneous\n * weight:\n *\n *   W'_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0               (3)\n *\n * C_i is the compute capacity of CPU i, typically it is the\n * fraction of 'recent' time available for SCHED_OTHER task execution. But it\n * can also include other factors [XXX].\n *\n * To achieve this balance we define a measure of imbalance which follows\n * directly from (1):\n *\n *   imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j }    (4)\n *\n * We them move tasks around to minimize the imbalance. In the continuous\n * function space it is obvious this converges, in the discrete case we get\n * a few fun cases generally called infeasible weight scenarios.\n *\n * [XXX expand on:\n *     - infeasible weights;\n *     - local vs global optima in the discrete case. ]\n *\n *\n * SCHED DOMAINS\n *\n * In order to solve the imbalance equation (4), and avoid the obvious O(n^2)\n * for all i,j solution, we create a tree of CPUs that follows the hardware\n * topology where each level pairs two lower groups (or better). This results\n * in O(log n) layers. Furthermore we reduce the number of CPUs going up the\n * tree to only the first of the previous level and we decrease the frequency\n * of load-balance at each level inv. proportional to the number of CPUs in\n * the groups.\n *\n * This yields:\n *\n *     log_2 n     1     n\n *   \\Sum       { --- * --- * 2^i } = O(n)                            (5)\n *     i = 0      2^i   2^i\n *                               `- size of each group\n *         |         |     `- number of CPUs doing load-balance\n *         |         `- freq\n *         `- sum over all levels\n *\n * Coupled with a limit on how many tasks we can migrate every balance pass,\n * this makes (5) the runtime complexity of the balancer.\n *\n * An important property here is that each CPU is still (indirectly) connected\n * to every other CPU in at most O(log n) steps:\n *\n * The adjacency matrix of the resulting graph is given by:\n *\n *             log_2 n\n *   A_i,j = \\Union     (i % 2^k == 0) && i / 2^(k+1) == j / 2^(k+1)  (6)\n *             k = 0\n *\n * And you'll find that:\n *\n *   A^(log_2 n)_i,j != 0  for all i,j                                (7)\n *\n * Showing there's indeed a path between every CPU in at most O(log n) steps.\n * The task movement gives a factor of O(m), giving a convergence complexity\n * of:\n *\n *   O(nm log n),  n := nr_cpus, m := nr_tasks                        (8)\n *\n *\n * WORK CONSERVING\n *\n * In order to avoid CPUs going idle while there's still work to do, new idle\n * balancing is more aggressive and has the newly idle CPU iterate up the domain\n * tree itself instead of relying on other CPUs to bring it work.\n *\n * This adds some complexity to both (5) and (8) but it reduces the total idle\n * time.\n *\n * [XXX more?]\n *\n *\n * CGROUPS\n *\n * Cgroups make a horror show out of (2), instead of a simple sum we get:\n *\n *                                s_k,i\n *   W_i,0 = \\Sum_j \\Prod_k w_k * -----                               (9)\n *                                 S_k\n *\n * Where\n *\n *   s_k,i = \\Sum_j w_i,j,k  and  S_k = \\Sum_i s_k,i                 (10)\n *\n * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i.\n *\n * The big problem is S_k, its a global sum needed to compute a local (W_i)\n * property.\n *\n * [XXX write more on how we solve this.. _after_ merging pjt's patches that\n *      rewrite all of this once again.]\n */\n\nstatic unsigned long __read_mostly max_load_balance_interval = HZ/10;\n\nenum fbq_type { regular, remote, all };\n\nenum group_type {\n\tgroup_other = 0,\n\tgroup_misfit_task,\n\tgroup_imbalanced,\n\tgroup_overloaded,\n};\n\n#define LBF_ALL_PINNED\t0x01\n#define LBF_NEED_BREAK\t0x02\n#define LBF_DST_PINNED  0x04\n#define LBF_SOME_PINNED\t0x08\n#define LBF_NOHZ_STATS\t0x10\n#define LBF_NOHZ_AGAIN\t0x20\n\nstruct lb_env {\n\tstruct sched_domain\t*sd;\n\n\tstruct rq\t\t*src_rq;\n\tint\t\t\tsrc_cpu;\n\n\tint\t\t\tdst_cpu;\n\tstruct rq\t\t*dst_rq;\n\n\tstruct cpumask\t\t*dst_grpmask;\n\tint\t\t\tnew_dst_cpu;\n\tenum cpu_idle_type\tidle;\n\tlong\t\t\timbalance;\n\t/* The set of CPUs under consideration for load-balancing */\n\tstruct cpumask\t\t*cpus;\n\n\tunsigned int\t\tflags;\n\n\tunsigned int\t\tloop;\n\tunsigned int\t\tloop_break;\n\tunsigned int\t\tloop_max;\n\n\tenum fbq_type\t\tfbq_type;\n\tenum group_type\t\tsrc_grp_type;\n\tstruct list_head\ttasks;\n};\n\n/*\n * Is this task likely cache-hot:\n */\nstatic int task_hot(struct task_struct *p, struct lb_env *env)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(p->policy == SCHED_IDLE))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = rq_clock_task(env->src_rq) - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns 1, if task migration degrades locality\n * Returns 0, if task migration improves locality i.e migration preferred.\n * Returns -1, if task migration is not affected by locality.\n */\nstatic int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)\n{\n\tstruct numa_group *numa_group = rcu_dereference(p->numa_group);\n\tunsigned long src_weight, dst_weight;\n\tint src_nid, dst_nid, dist;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn -1;\n\n\tif (!p->numa_faults || !(env->sd->flags & SD_NUMA))\n\t\treturn -1;\n\n\tsrc_nid = cpu_to_node(env->src_cpu);\n\tdst_nid = cpu_to_node(env->dst_cpu);\n\n\tif (src_nid == dst_nid)\n\t\treturn -1;\n\n\t/* Migrating away from the preferred node is always bad. */\n\tif (src_nid == p->numa_preferred_nid) {\n\t\tif (env->src_rq->nr_running > env->src_rq->nr_preferred_running)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\t/* Encourage migration to the preferred node. */\n\tif (dst_nid == p->numa_preferred_nid)\n\t\treturn 0;\n\n\t/* Leaving a core idle is often worse than degrading locality. */\n\tif (env->idle == CPU_IDLE)\n\t\treturn -1;\n\n\tdist = node_distance(src_nid, dst_nid);\n\tif (numa_group) {\n\t\tsrc_weight = group_weight(p, src_nid, dist);\n\t\tdst_weight = group_weight(p, dst_nid, dist);\n\t} else {\n\t\tsrc_weight = task_weight(p, src_nid, dist);\n\t\tdst_weight = task_weight(p, dst_nid, dist);\n\t}\n\n\treturn dst_weight < src_weight;\n}\n\n#else\nstatic inline int migrate_degrades_locality(struct task_struct *p,\n\t\t\t\t\t     struct lb_env *env)\n{\n\treturn -1;\n}\n#endif\n\n/*\n * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?\n */\nstatic\nint can_migrate_task(struct task_struct *p, struct lb_env *env)\n{\n\tint tsk_cache_hot;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\t/*\n\t * We do not migrate tasks that are:\n\t * 1) throttled_lb_pair, or\n\t * 2) cannot be migrated to this CPU due to cpus_allowed, or\n\t * 3) running (obviously), or\n\t * 4) are cache-hot on their current CPU.\n\t */\n\tif (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(env->dst_cpu, &p->cpus_allowed)) {\n\t\tint cpu;\n\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_affine);\n\n\t\tenv->flags |= LBF_SOME_PINNED;\n\n\t\t/*\n\t\t * Remember if this task can be migrated to any other CPU in\n\t\t * our sched_group. We may want to revisit it if we couldn't\n\t\t * meet load balance goals by pulling other tasks on src_cpu.\n\t\t *\n\t\t * Avoid computing new_dst_cpu for NEWLY_IDLE or if we have\n\t\t * already computed one in current iteration.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE || (env->flags & LBF_DST_PINNED))\n\t\t\treturn 0;\n\n\t\t/* Prevent to re-select dst_cpu via env's CPUs: */\n\t\tfor_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {\n\t\t\tif (cpumask_test_cpu(cpu, &p->cpus_allowed)) {\n\t\t\t\tenv->flags |= LBF_DST_PINNED;\n\t\t\t\tenv->new_dst_cpu = cpu;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* Record that we found atleast one task that could run on dst_cpu */\n\tenv->flags &= ~LBF_ALL_PINNED;\n\n\tif (task_running(env->src_rq, p)) {\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_running);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Aggressive migration if:\n\t * 1) destination numa is preferred\n\t * 2) task is cache cold, or\n\t * 3) too many balance attempts have failed.\n\t */\n\ttsk_cache_hot = migrate_degrades_locality(p, env);\n\tif (tsk_cache_hot == -1)\n\t\ttsk_cache_hot = task_hot(p, env);\n\n\tif (tsk_cache_hot <= 0 ||\n\t    env->sd->nr_balance_failed > env->sd->cache_nice_tries) {\n\t\tif (tsk_cache_hot == 1) {\n\t\t\tschedstat_inc(env->sd->lb_hot_gained[env->idle]);\n\t\t\tschedstat_inc(p->se.statistics.nr_forced_migrations);\n\t\t}\n\t\treturn 1;\n\t}\n\n\tschedstat_inc(p->se.statistics.nr_failed_migrations_hot);\n\treturn 0;\n}\n\n/*\n * detach_task() -- detach the task for the migration specified in env\n */\nstatic void detach_task(struct task_struct *p, struct lb_env *env)\n{\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\tdeactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);\n\tset_task_cpu(p, env->dst_cpu);\n}\n\n/*\n * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as\n * part of active balancing operations within \"domain\".\n *\n * Returns a task if successful and NULL otherwise.\n */\nstatic struct task_struct *detach_one_task(struct lb_env *env)\n{\n\tstruct task_struct *p;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tlist_for_each_entry_reverse(p,\n\t\t\t&env->src_rq->cfs_tasks, se.group_node) {\n\t\tif (!can_migrate_task(p, env))\n\t\t\tcontinue;\n\n\t\tdetach_task(p, env);\n\n\t\t/*\n\t\t * Right now, this is only the second place where\n\t\t * lb_gained[env->idle] is updated (other is detach_tasks)\n\t\t * so we can safely collect stats here rather than\n\t\t * inside detach_tasks().\n\t\t */\n\t\tschedstat_inc(env->sd->lb_gained[env->idle]);\n\t\treturn p;\n\t}\n\treturn NULL;\n}\n\nstatic const unsigned int sched_nr_migrate_break = 32;\n\n/*\n * detach_tasks() -- tries to detach up to imbalance weighted load from\n * busiest_rq, as part of a balancing operation within domain \"sd\".\n *\n * Returns number of detached tasks if successful and 0 otherwise.\n */\nstatic int detach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->src_rq->cfs_tasks;\n\tstruct task_struct *p;\n\tunsigned long load;\n\tint detached = 0;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (env->imbalance <= 0)\n\t\treturn 0;\n\n\twhile (!list_empty(tasks)) {\n\t\t/*\n\t\t * We don't want to steal all, otherwise we may be treated likewise,\n\t\t * which could at worst lead to a livelock crash.\n\t\t */\n\t\tif (env->idle != CPU_NOT_IDLE && env->src_rq->nr_running <= 1)\n\t\t\tbreak;\n\n\t\tp = list_last_entry(tasks, struct task_struct, se.group_node);\n\n\t\tenv->loop++;\n\t\t/* We've more or less seen every task there is, call it quits */\n\t\tif (env->loop > env->loop_max)\n\t\t\tbreak;\n\n\t\t/* take a breather every nr_migrate tasks */\n\t\tif (env->loop > env->loop_break) {\n\t\t\tenv->loop_break += sched_nr_migrate_break;\n\t\t\tenv->flags |= LBF_NEED_BREAK;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!can_migrate_task(p, env))\n\t\t\tgoto next;\n\n\t\tload = task_h_load(p);\n\n\t\tif (sched_feat(LB_MIN) && load < 16 && !env->sd->nr_balance_failed)\n\t\t\tgoto next;\n\n\t\tif ((load / 2) > env->imbalance)\n\t\t\tgoto next;\n\n\t\tdetach_task(p, env);\n\t\tlist_add(&p->se.group_node, &env->tasks);\n\n\t\tdetached++;\n\t\tenv->imbalance -= load;\n\n#ifdef CONFIG_PREEMPT\n\t\t/*\n\t\t * NEWIDLE balancing is a source of latency, so preemptible\n\t\t * kernels will stop after the first task is detached to minimize\n\t\t * the critical section.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE)\n\t\t\tbreak;\n#endif\n\n\t\t/*\n\t\t * We only want to steal up to the prescribed amount of\n\t\t * weighted load.\n\t\t */\n\t\tif (env->imbalance <= 0)\n\t\t\tbreak;\n\n\t\tcontinue;\nnext:\n\t\tlist_move(&p->se.group_node, tasks);\n\t}\n\n\t/*\n\t * Right now, this is one of only two places we collect this stat\n\t * so we can safely collect detach_one_task() stats here rather\n\t * than inside detach_one_task().\n\t */\n\tschedstat_add(env->sd->lb_gained[env->idle], detached);\n\n\treturn detached;\n}\n\n/*\n * attach_task() -- attach the task detached by detach_task() to its new rq.\n */\nstatic void attach_task(struct rq *rq, struct task_struct *p)\n{\n\tlockdep_assert_held(&rq->lock);\n\n\tBUG_ON(task_rq(p) != rq);\n\tactivate_task(rq, p, ENQUEUE_NOCLOCK);\n\tp->on_rq = TASK_ON_RQ_QUEUED;\n\tcheck_preempt_curr(rq, p, 0);\n}\n\n/*\n * attach_one_task() -- attaches the task returned from detach_one_task() to\n * its new rq.\n */\nstatic void attach_one_task(struct rq *rq, struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\tattach_task(rq, p);\n\trq_unlock(rq, &rf);\n}\n\n/*\n * attach_tasks() -- attaches all tasks detached by detach_tasks() to their\n * new rq.\n */\nstatic void attach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->tasks;\n\tstruct task_struct *p;\n\tstruct rq_flags rf;\n\n\trq_lock(env->dst_rq, &rf);\n\tupdate_rq_clock(env->dst_rq);\n\n\twhile (!list_empty(tasks)) {\n\t\tp = list_first_entry(tasks, struct task_struct, se.group_node);\n\t\tlist_del_init(&p->se.group_node);\n\n\t\tattach_task(env->dst_rq, p);\n\t}\n\n\trq_unlock(env->dst_rq, &rf);\n}\n\nstatic inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->avg.load_avg)\n\t\treturn true;\n\n\tif (cfs_rq->avg.util_avg)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool others_have_blocked(struct rq *rq)\n{\n\tif (READ_ONCE(rq->avg_rt.util_avg))\n\t\treturn true;\n\n\tif (READ_ONCE(rq->avg_dl.util_avg))\n\t\treturn true;\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tif (READ_ONCE(rq->avg_irq.util_avg))\n\t\treturn true;\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->load.weight)\n\t\treturn false;\n\n\tif (cfs_rq->avg.load_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.util_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.runnable_load_sum)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq, *pos;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\tbool done = true;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\t/*\n\t * Iterates the task_group tree in a bottom up fashion, see\n\t * list_add_leaf_cfs_rq() for details.\n\t */\n\tfor_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {\n\t\tstruct sched_entity *se;\n\n\t\t/* throttled entities do not contribute to load */\n\t\tif (throttled_hierarchy(cfs_rq))\n\t\t\tcontinue;\n\n\t\tif (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))\n\t\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t\t/* Propagate pending load changes to the parent, if any: */\n\t\tse = cfs_rq->tg->se[cpu];\n\t\tif (se && !skip_blocked_update(se))\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, 0);\n\n\t\t/*\n\t\t * There can be a lot of idle CPU cgroups.  Don't let fully\n\t\t * decayed cfs_rqs linger on the list.\n\t\t */\n\t\tif (cfs_rq_is_decayed(cfs_rq))\n\t\t\tlist_del_leaf_cfs_rq(cfs_rq);\n\n\t\t/* Don't need periodic decay once load/util_avg are null */\n\t\tif (cfs_rq_has_blocked(cfs_rq))\n\t\t\tdone = false;\n\t}\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n\t/* Don't need periodic decay once load/util_avg are null */\n\tif (others_have_blocked(rq))\n\t\tdone = false;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (done)\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n/*\n * Compute the hierarchical load factor for cfs_rq and all its ascendants.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];\n\tunsigned long now = jiffies;\n\tunsigned long load;\n\n\tif (cfs_rq->last_h_load_update == now)\n\t\treturn;\n\n\tcfs_rq->h_load_next = NULL;\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_load_next = se;\n\t\tif (cfs_rq->last_h_load_update == now)\n\t\t\tbreak;\n\t}\n\n\tif (!se) {\n\t\tcfs_rq->h_load = cfs_rq_load_avg(cfs_rq);\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n\n\twhile ((se = cfs_rq->h_load_next) != NULL) {\n\t\tload = cfs_rq->h_load;\n\t\tload = div64_ul(load * se->avg.load_avg,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n\t\tcfs_rq = group_cfs_rq(se);\n\t\tcfs_rq->h_load = load;\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n\n\tupdate_cfs_rq_h_load(cfs_rq);\n\treturn div64_ul(p->se.avg.load_avg * cfs_rq->h_load,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n}\n#else\nstatic inline void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\tupdate_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq);\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (!cfs_rq_has_blocked(cfs_rq) && !others_have_blocked(rq))\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\treturn p->se.avg.load_avg;\n}\n#endif\n\n/********** Helpers for find_busiest_group ************************/\n\n/*\n * sg_lb_stats - stats of a sched_group required for load_balancing\n */\nstruct sg_lb_stats {\n\tunsigned long avg_load; /*Avg load across the CPUs of the group */\n\tunsigned long group_load; /* Total load over the CPUs of the group */\n\tunsigned long sum_weighted_load; /* Weighted load of group's tasks */\n\tunsigned long load_per_task;\n\tunsigned long group_capacity;\n\tunsigned long group_util; /* Total utilization of the group */\n\tunsigned int sum_nr_running; /* Nr tasks running in the group */\n\tunsigned int idle_cpus;\n\tunsigned int group_weight;\n\tenum group_type group_type;\n\tint group_no_capacity;\n\tunsigned long group_misfit_task_load; /* A CPU has a task too big for its capacity */\n#ifdef CONFIG_NUMA_BALANCING\n\tunsigned int nr_numa_running;\n\tunsigned int nr_preferred_running;\n#endif\n};\n\n/*\n * sd_lb_stats - Structure to store the statistics of a sched_domain\n *\t\t during load balancing.\n */\nstruct sd_lb_stats {\n\tstruct sched_group *busiest;\t/* Busiest group in this sd */\n\tstruct sched_group *local;\t/* Local group in this sd */\n\tunsigned long total_running;\n\tunsigned long total_load;\t/* Total load of all groups in sd */\n\tunsigned long total_capacity;\t/* Total capacity of all groups in sd */\n\tunsigned long avg_load;\t/* Average load across all groups in sd */\n\n\tstruct sg_lb_stats busiest_stat;/* Statistics of the busiest group */\n\tstruct sg_lb_stats local_stat;\t/* Statistics of the local group */\n};\n\nstatic inline void init_sd_lb_stats(struct sd_lb_stats *sds)\n{\n\t/*\n\t * Skimp on the clearing to avoid duplicate work. We can avoid clearing\n\t * local_stat because update_sg_lb_stats() does a full clear/assignment.\n\t * We must however clear busiest_stat::avg_load because\n\t * update_sd_pick_busiest() reads this before assignment.\n\t */\n\t*sds = (struct sd_lb_stats){\n\t\t.busiest = NULL,\n\t\t.local = NULL,\n\t\t.total_running = 0UL,\n\t\t.total_load = 0UL,\n\t\t.total_capacity = 0UL,\n\t\t.busiest_stat = {\n\t\t\t.avg_load = 0UL,\n\t\t\t.sum_nr_running = 0,\n\t\t\t.group_type = group_other,\n\t\t},\n\t};\n}\n\n/**\n * get_sd_load_idx - Obtain the load index for a given sched domain.\n * @sd: The sched_domain whose load_idx is to be obtained.\n * @idle: The idle status of the CPU for whose sd load_idx is obtained.\n *\n * Return: The load index.\n */\nstatic inline int get_sd_load_idx(struct sched_domain *sd,\n\t\t\t\t\tenum cpu_idle_type idle)\n{\n\tint load_idx;\n\n\tswitch (idle) {\n\tcase CPU_NOT_IDLE:\n\t\tload_idx = sd->busy_idx;\n\t\tbreak;\n\n\tcase CPU_NEWLY_IDLE:\n\t\tload_idx = sd->newidle_idx;\n\t\tbreak;\n\tdefault:\n\t\tload_idx = sd->idle_idx;\n\t\tbreak;\n\t}\n\n\treturn load_idx;\n}\n\nstatic unsigned long scale_rt_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long max = arch_scale_cpu_capacity(sd, cpu);\n\tunsigned long used, free;\n\tunsigned long irq;\n\n\tirq = cpu_util_irq(rq);\n\n\tif (unlikely(irq >= max))\n\t\treturn 1;\n\n\tused = READ_ONCE(rq->avg_rt.util_avg);\n\tused += READ_ONCE(rq->avg_dl.util_avg);\n\n\tif (unlikely(used >= max))\n\t\treturn 1;\n\n\tfree = max - used;\n\n\treturn scale_irq_capacity(free, irq, max);\n}\n\nstatic void update_cpu_capacity(struct sched_domain *sd, int cpu)\n{\n\tunsigned long capacity = scale_rt_capacity(sd, cpu);\n\tstruct sched_group *sdg = sd->groups;\n\n\tcpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(sd, cpu);\n\n\tif (!capacity)\n\t\tcapacity = 1;\n\n\tcpu_rq(cpu)->cpu_capacity = capacity;\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = capacity;\n\tsdg->sgc->max_capacity = capacity;\n}\n\nvoid update_group_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_domain *child = sd->child;\n\tstruct sched_group *group, *sdg = sd->groups;\n\tunsigned long capacity, min_capacity, max_capacity;\n\tunsigned long interval;\n\n\tinterval = msecs_to_jiffies(sd->balance_interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\tsdg->sgc->next_update = jiffies + interval;\n\n\tif (!child) {\n\t\tupdate_cpu_capacity(sd, cpu);\n\t\treturn;\n\t}\n\n\tcapacity = 0;\n\tmin_capacity = ULONG_MAX;\n\tmax_capacity = 0;\n\n\tif (child->flags & SD_OVERLAP) {\n\t\t/*\n\t\t * SD_OVERLAP domains cannot assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tfor_each_cpu(cpu, sched_group_span(sdg)) {\n\t\t\tstruct sched_group_capacity *sgc;\n\t\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\t\t/*\n\t\t\t * build_sched_domains() -> init_sched_groups_capacity()\n\t\t\t * gets here before we've attached the domains to the\n\t\t\t * runqueues.\n\t\t\t *\n\t\t\t * Use capacity_of(), which is set irrespective of domains\n\t\t\t * in update_cpu_capacity().\n\t\t\t *\n\t\t\t * This avoids capacity from being 0 and\n\t\t\t * causing divide-by-zero issues on boot.\n\t\t\t */\n\t\t\tif (unlikely(!rq->sd)) {\n\t\t\t\tcapacity += capacity_of(cpu);\n\t\t\t} else {\n\t\t\t\tsgc = rq->sd->groups->sgc;\n\t\t\t\tcapacity += sgc->capacity;\n\t\t\t}\n\n\t\t\tmin_capacity = min(capacity, min_capacity);\n\t\t\tmax_capacity = max(capacity, max_capacity);\n\t\t}\n\t} else  {\n\t\t/*\n\t\t * !SD_OVERLAP domains can assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tgroup = child->groups;\n\t\tdo {\n\t\t\tstruct sched_group_capacity *sgc = group->sgc;\n\n\t\t\tcapacity += sgc->capacity;\n\t\t\tmin_capacity = min(sgc->min_capacity, min_capacity);\n\t\t\tmax_capacity = max(sgc->max_capacity, max_capacity);\n\t\t\tgroup = group->next;\n\t\t} while (group != child->groups);\n\t}\n\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = min_capacity;\n\tsdg->sgc->max_capacity = max_capacity;\n}\n\n/*\n * Check whether the capacity of the rq has been noticeably reduced by side\n * activity. The imbalance_pct is used for the threshold.\n * Return true is the capacity is reduced\n */\nstatic inline int\ncheck_cpu_capacity(struct rq *rq, struct sched_domain *sd)\n{\n\treturn ((rq->cpu_capacity * sd->imbalance_pct) <\n\t\t\t\t(rq->cpu_capacity_orig * 100));\n}\n\n/*\n * Group imbalance indicates (and tries to solve) the problem where balancing\n * groups is inadequate due to ->cpus_allowed constraints.\n *\n * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a\n * cpumask covering 1 CPU of the first group and 3 CPUs of the second group.\n * Something like:\n *\n *\t{ 0 1 2 3 } { 4 5 6 7 }\n *\t        *     * * *\n *\n * If we were to balance group-wise we'd place two tasks in the first group and\n * two tasks in the second group. Clearly this is undesired as it will overload\n * cpu 3 and leave one of the CPUs in the second group unused.\n *\n * The current solution to this issue is detecting the skew in the first group\n * by noticing the lower domain failed to reach balance and had difficulty\n * moving tasks due to affinity constraints.\n *\n * When this is so detected; this group becomes a candidate for busiest; see\n * update_sd_pick_busiest(). And calculate_imbalance() and\n * find_busiest_group() avoid some of the usual balance conditions to allow it\n * to create an effective group imbalance.\n *\n * This is a somewhat tricky proposition since the next run might not find the\n * group imbalance and decide the groups need to be balanced again. A most\n * subtle and fragile situation.\n */\n\nstatic inline int sg_imbalanced(struct sched_group *group)\n{\n\treturn group->sgc->imbalance;\n}\n\n/*\n * group_has_capacity returns true if the group has spare capacity that could\n * be used by some tasks.\n * We consider that a group has spare capacity if the  * number of task is\n * smaller than the number of CPUs or if the utilization is lower than the\n * available capacity for CFS tasks.\n * For the latter, we use a threshold to stabilize the state, to take into\n * account the variance of the tasks' load and to return true if the available\n * capacity in meaningful for the load balancer.\n * As an example, an available capacity of 1% can appear but it doesn't make\n * any benefit for the load balance.\n */\nstatic inline bool\ngroup_has_capacity(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running < sgs->group_weight)\n\t\treturn true;\n\n\tif ((sgs->group_capacity * 100) >\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *  group_is_overloaded returns true if the group has more tasks than it can\n *  handle.\n *  group_is_overloaded is not equals to !group_has_capacity because a group\n *  with the exact right number of tasks, has no more spare capacity but is not\n *  overloaded so both group_has_capacity and group_is_overloaded return\n *  false.\n */\nstatic inline bool\ngroup_is_overloaded(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running <= sgs->group_weight)\n\t\treturn false;\n\n\tif ((sgs->group_capacity * 100) <\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * group_smaller_min_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_min_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->min_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->min_capacity * 1024;\n}\n\n/*\n * group_smaller_max_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity_orig than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_max_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->max_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->max_capacity * 1024;\n}\n\nstatic inline enum\ngroup_type group_classify(struct sched_group *group,\n\t\t\t  struct sg_lb_stats *sgs)\n{\n\tif (sgs->group_no_capacity)\n\t\treturn group_overloaded;\n\n\tif (sg_imbalanced(group))\n\t\treturn group_imbalanced;\n\n\tif (sgs->group_misfit_task_load)\n\t\treturn group_misfit_task;\n\n\treturn group_other;\n}\n\nstatic bool update_nohz_stats(struct rq *rq, bool force)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned int cpu = rq->cpu;\n\n\tif (!rq->has_blocked_load)\n\t\treturn false;\n\n\tif (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))\n\t\treturn false;\n\n\tif (!force && !time_after(jiffies, rq->last_blocked_load_update_tick))\n\t\treturn true;\n\n\tupdate_blocked_averages(cpu);\n\n\treturn rq->has_blocked_load;\n#else\n\treturn false;\n#endif\n}\n\n/**\n * update_sg_lb_stats - Update sched_group's statistics for load balancing.\n * @env: The load balancing environment.\n * @group: sched_group whose statistics are to be updated.\n * @load_idx: Load index of sched_domain of this_cpu for load calc.\n * @local_group: Does group contain this_cpu.\n * @sgs: variable to hold the statistics for this group.\n * @overload: Indicate pullable load (e.g. >1 runnable task).\n */\nstatic inline void update_sg_lb_stats(struct lb_env *env,\n\t\t\tstruct sched_group *group, int load_idx,\n\t\t\tint local_group, struct sg_lb_stats *sgs,\n\t\t\tbool *overload)\n{\n\tunsigned long load;\n\tint i, nr_running;\n\n\tmemset(sgs, 0, sizeof(*sgs));\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tstruct rq *rq = cpu_rq(i);\n\n\t\tif ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))\n\t\t\tenv->flags |= LBF_NOHZ_AGAIN;\n\n\t\t/* Bias balancing toward CPUs of our domain: */\n\t\tif (local_group)\n\t\t\tload = target_load(i, load_idx);\n\t\telse\n\t\t\tload = source_load(i, load_idx);\n\n\t\tsgs->group_load += load;\n\t\tsgs->group_util += cpu_util(i);\n\t\tsgs->sum_nr_running += rq->cfs.h_nr_running;\n\n\t\tnr_running = rq->nr_running;\n\t\tif (nr_running > 1)\n\t\t\t*overload = true;\n\n#ifdef CONFIG_NUMA_BALANCING\n\t\tsgs->nr_numa_running += rq->nr_numa_running;\n\t\tsgs->nr_preferred_running += rq->nr_preferred_running;\n#endif\n\t\tsgs->sum_weighted_load += weighted_cpuload(rq);\n\t\t/*\n\t\t * No need to call idle_cpu() if nr_running is not 0\n\t\t */\n\t\tif (!nr_running && idle_cpu(i))\n\t\t\tsgs->idle_cpus++;\n\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    sgs->group_misfit_task_load < rq->misfit_task_load) {\n\t\t\tsgs->group_misfit_task_load = rq->misfit_task_load;\n\t\t\t*overload = 1;\n\t\t}\n\t}\n\n\t/* Adjust by relative CPU capacity of the group */\n\tsgs->group_capacity = group->sgc->capacity;\n\tsgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;\n\n\tif (sgs->sum_nr_running)\n\t\tsgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;\n\n\tsgs->group_weight = group->group_weight;\n\n\tsgs->group_no_capacity = group_is_overloaded(env, sgs);\n\tsgs->group_type = group_classify(group, sgs);\n}\n\n/**\n * update_sd_pick_busiest - return 1 on busiest group\n * @env: The load balancing environment.\n * @sds: sched_domain statistics\n * @sg: sched_group candidate to be checked for being the busiest\n * @sgs: sched_group statistics\n *\n * Determine if @sg is a busier group than the previously selected\n * busiest group.\n *\n * Return: %true if @sg is a busier group than the previously selected\n * busiest group. %false otherwise.\n */\nstatic bool update_sd_pick_busiest(struct lb_env *env,\n\t\t\t\t   struct sd_lb_stats *sds,\n\t\t\t\t   struct sched_group *sg,\n\t\t\t\t   struct sg_lb_stats *sgs)\n{\n\tstruct sg_lb_stats *busiest = &sds->busiest_stat;\n\n\t/*\n\t * Don't try to pull misfit tasks we can't help.\n\t * We can use max_capacity here as reduction in capacity on some\n\t * CPUs in the group should either be possible to resolve\n\t * internally or be covered by avg_load imbalance (eventually).\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    (!group_smaller_max_cpu_capacity(sg, sds->local) ||\n\t     !group_has_capacity(env, &sds->local_stat)))\n\t\treturn false;\n\n\tif (sgs->group_type > busiest->group_type)\n\t\treturn true;\n\n\tif (sgs->group_type < busiest->group_type)\n\t\treturn false;\n\n\tif (sgs->avg_load <= busiest->avg_load)\n\t\treturn false;\n\n\tif (!(env->sd->flags & SD_ASYM_CPUCAPACITY))\n\t\tgoto asym_packing;\n\n\t/*\n\t * Candidate sg has no more than one task per CPU and\n\t * has higher per-CPU capacity. Migrating tasks to less\n\t * capable CPUs may harm throughput. Maximize throughput,\n\t * power/energy consequences are not considered.\n\t */\n\tif (sgs->sum_nr_running <= sgs->group_weight &&\n\t    group_smaller_min_cpu_capacity(sds->local, sg))\n\t\treturn false;\n\n\t/*\n\t * If we have more than one misfit sg go with the biggest misfit.\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    sgs->group_misfit_task_load < busiest->group_misfit_task_load)\n\t\treturn false;\n\nasym_packing:\n\t/* This is the busiest node in its class. */\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn true;\n\n\t/* No ASYM_PACKING if target CPU is already busy */\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn true;\n\t/*\n\t * ASYM_PACKING needs to move all the work to the highest\n\t * prority CPUs in the group, therefore mark all groups\n\t * of lower priority than ourself as busy.\n\t */\n\tif (sgs->sum_nr_running &&\n\t    sched_asym_prefer(env->dst_cpu, sg->asym_prefer_cpu)) {\n\t\tif (!sds->busiest)\n\t\t\treturn true;\n\n\t\t/* Prefer to move from lowest priority CPU's work */\n\t\tif (sched_asym_prefer(sds->busiest->asym_prefer_cpu,\n\t\t\t\t      sg->asym_prefer_cpu))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running > sgs->nr_numa_running)\n\t\treturn regular;\n\tif (sgs->sum_nr_running > sgs->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\tif (rq->nr_running > rq->nr_numa_running)\n\t\treturn regular;\n\tif (rq->nr_running > rq->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n#else\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\treturn regular;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n/**\n * update_sd_lb_stats - Update sched_domain's statistics for load balancing.\n * @env: The load balancing environment.\n * @sds: variable to hold the statistics for this sched_domain.\n */\nstatic inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tstruct sched_domain *child = env->sd->child;\n\tstruct sched_group *sg = env->sd->groups;\n\tstruct sg_lb_stats *local = &sds->local_stat;\n\tstruct sg_lb_stats tmp_sgs;\n\tint load_idx;\n\tbool overload = false;\n\tbool prefer_sibling = child && child->flags & SD_PREFER_SIBLING;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))\n\t\tenv->flags |= LBF_NOHZ_STATS;\n#endif\n\n\tload_idx = get_sd_load_idx(env->sd, env->idle);\n\n\tdo {\n\t\tstruct sg_lb_stats *sgs = &tmp_sgs;\n\t\tint local_group;\n\n\t\tlocal_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));\n\t\tif (local_group) {\n\t\t\tsds->local = sg;\n\t\t\tsgs = local;\n\n\t\t\tif (env->idle != CPU_NEWLY_IDLE ||\n\t\t\t    time_after_eq(jiffies, sg->sgc->next_update))\n\t\t\t\tupdate_group_capacity(env->sd, env->dst_cpu);\n\t\t}\n\n\t\tupdate_sg_lb_stats(env, sg, load_idx, local_group, sgs,\n\t\t\t\t\t\t&overload);\n\n\t\tif (local_group)\n\t\t\tgoto next_group;\n\n\t\t/*\n\t\t * In case the child domain prefers tasks go to siblings\n\t\t * first, lower the sg capacity so that we'll try\n\t\t * and move all the excess tasks away. We lower the capacity\n\t\t * of a group only if the local group has the capacity to fit\n\t\t * these excess tasks. The extra check prevents the case where\n\t\t * you always pull from the heaviest group when it is already\n\t\t * under-utilized (possible with a large weight task outweighs\n\t\t * the tasks on the system).\n\t\t */\n\t\tif (prefer_sibling && sds->local &&\n\t\t    group_has_capacity(env, local) &&\n\t\t    (sgs->sum_nr_running > local->sum_nr_running + 1)) {\n\t\t\tsgs->group_no_capacity = 1;\n\t\t\tsgs->group_type = group_classify(sg, sgs);\n\t\t}\n\n\t\tif (update_sd_pick_busiest(env, sds, sg, sgs)) {\n\t\t\tsds->busiest = sg;\n\t\t\tsds->busiest_stat = *sgs;\n\t\t}\n\nnext_group:\n\t\t/* Now, start updating sd_lb_stats */\n\t\tsds->total_running += sgs->sum_nr_running;\n\t\tsds->total_load += sgs->group_load;\n\t\tsds->total_capacity += sgs->group_capacity;\n\n\t\tsg = sg->next;\n\t} while (sg != env->sd->groups);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif ((env->flags & LBF_NOHZ_AGAIN) &&\n\t    cpumask_subset(nohz.idle_cpus_mask, sched_domain_span(env->sd))) {\n\n\t\tWRITE_ONCE(nohz.next_blocked,\n\t\t\t   jiffies + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\t}\n#endif\n\n\tif (env->sd->flags & SD_NUMA)\n\t\tenv->fbq_type = fbq_classify_group(&sds->busiest_stat);\n\n\tif (!env->sd->parent) {\n\t\t/* update overload indicator if we are at root domain */\n\t\tif (READ_ONCE(env->dst_rq->rd->overload) != overload)\n\t\t\tWRITE_ONCE(env->dst_rq->rd->overload, overload);\n\t}\n}\n\n/**\n * check_asym_packing - Check to see if the group is packed into the\n *\t\t\tsched domain.\n *\n * This is primarily intended to used at the sibling level.  Some\n * cores like POWER7 prefer to use lower numbered SMT threads.  In the\n * case of POWER7, it can move to lower SMT modes only when higher\n * threads are idle.  When in lower SMT modes, the threads will\n * perform better since they share less core resources.  Hence when we\n * have idle threads, we want them to be the higher ones.\n *\n * This packing function is run on idle threads.  It checks to see if\n * the busiest CPU in this domain (core in the P7 case) has a higher\n * CPU number than the packing function is being run on.  Here we are\n * assuming lower CPU number will be equivalent to lower a SMT thread\n * number.\n *\n * Return: 1 when packing is required and a task should be moved to\n * this CPU.  The amount of the imbalance is returned in env->imbalance.\n *\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain which is to be packed\n */\nstatic int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tint busiest_cpu;\n\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn 0;\n\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn 0;\n\n\tif (!sds->busiest)\n\t\treturn 0;\n\n\tbusiest_cpu = sds->busiest->asym_prefer_cpu;\n\tif (sched_asym_prefer(busiest_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tenv->imbalance = DIV_ROUND_CLOSEST(\n\t\tsds->busiest_stat.avg_load * sds->busiest_stat.group_capacity,\n\t\tSCHED_CAPACITY_SCALE);\n\n\treturn 1;\n}\n\n/**\n * fix_small_imbalance - Calculate the minor imbalance that exists\n *\t\t\tamongst the groups of a sched_domain, during\n *\t\t\tload balancing.\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline\nvoid fix_small_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long tmp, capa_now = 0, capa_move = 0;\n\tunsigned int imbn = 2;\n\tunsigned long scaled_busy_load_per_task;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (!local->sum_nr_running)\n\t\tlocal->load_per_task = cpu_avg_load_per_task(env->dst_cpu);\n\telse if (busiest->load_per_task > local->load_per_task)\n\t\timbn = 1;\n\n\tscaled_busy_load_per_task =\n\t\t(busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\tbusiest->group_capacity;\n\n\tif (busiest->avg_load + scaled_busy_load_per_task >=\n\t    local->avg_load + (scaled_busy_load_per_task * imbn)) {\n\t\tenv->imbalance = busiest->load_per_task;\n\t\treturn;\n\t}\n\n\t/*\n\t * OK, we don't have enough imbalance to justify moving tasks,\n\t * however we may be able to increase total CPU capacity used by\n\t * moving them.\n\t */\n\n\tcapa_now += busiest->group_capacity *\n\t\t\tmin(busiest->load_per_task, busiest->avg_load);\n\tcapa_now += local->group_capacity *\n\t\t\tmin(local->load_per_task, local->avg_load);\n\tcapa_now /= SCHED_CAPACITY_SCALE;\n\n\t/* Amount of load we'd subtract */\n\tif (busiest->avg_load > scaled_busy_load_per_task) {\n\t\tcapa_move += busiest->group_capacity *\n\t\t\t    min(busiest->load_per_task,\n\t\t\t\tbusiest->avg_load - scaled_busy_load_per_task);\n\t}\n\n\t/* Amount of load we'd add */\n\tif (busiest->avg_load * busiest->group_capacity <\n\t    busiest->load_per_task * SCHED_CAPACITY_SCALE) {\n\t\ttmp = (busiest->avg_load * busiest->group_capacity) /\n\t\t      local->group_capacity;\n\t} else {\n\t\ttmp = (busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\t      local->group_capacity;\n\t}\n\tcapa_move += local->group_capacity *\n\t\t    min(local->load_per_task, local->avg_load + tmp);\n\tcapa_move /= SCHED_CAPACITY_SCALE;\n\n\t/* Move if we gain throughput */\n\tif (capa_move > capa_now)\n\t\tenv->imbalance = busiest->load_per_task;\n}\n\n/**\n * calculate_imbalance - Calculate the amount of imbalance present within the\n *\t\t\t groups of a given sched_domain during load balance.\n * @env: load balance environment\n * @sds: statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long max_pull, load_above_capacity = ~0UL;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (busiest->group_type == group_imbalanced) {\n\t\t/*\n\t\t * In the group_imb case we cannot rely on group-wide averages\n\t\t * to ensure CPU-load equilibrium, look at wider averages. XXX\n\t\t */\n\t\tbusiest->load_per_task =\n\t\t\tmin(busiest->load_per_task, sds->avg_load);\n\t}\n\n\t/*\n\t * Avg load of busiest sg can be less and avg load of local sg can\n\t * be greater than avg load across all sgs of sd because avg load\n\t * factors in sg capacity and sgs with smaller group_type are\n\t * skipped when updating the busiest sg:\n\t */\n\tif (busiest->group_type != group_misfit_task &&\n\t    (busiest->avg_load <= sds->avg_load ||\n\t     local->avg_load >= sds->avg_load)) {\n\t\tenv->imbalance = 0;\n\t\treturn fix_small_imbalance(env, sds);\n\t}\n\n\t/*\n\t * If there aren't any idle CPUs, avoid creating some.\n\t */\n\tif (busiest->group_type == group_overloaded &&\n\t    local->group_type   == group_overloaded) {\n\t\tload_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;\n\t\tif (load_above_capacity > busiest->group_capacity) {\n\t\t\tload_above_capacity -= busiest->group_capacity;\n\t\t\tload_above_capacity *= scale_load_down(NICE_0_LOAD);\n\t\t\tload_above_capacity /= busiest->group_capacity;\n\t\t} else\n\t\t\tload_above_capacity = ~0UL;\n\t}\n\n\t/*\n\t * We're trying to get all the CPUs to the average_load, so we don't\n\t * want to push ourselves above the average load, nor do we wish to\n\t * reduce the max loaded CPU below the average load. At the same time,\n\t * we also don't want to reduce the group load below the group\n\t * capacity. Thus we look for the minimum possible imbalance.\n\t */\n\tmax_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);\n\n\t/* How much load to actually move to equalise the imbalance */\n\tenv->imbalance = min(\n\t\tmax_pull * busiest->group_capacity,\n\t\t(sds->avg_load - local->avg_load) * local->group_capacity\n\t) / SCHED_CAPACITY_SCALE;\n\n\t/* Boost imbalance to allow misfit task to be balanced. */\n\tif (busiest->group_type == group_misfit_task) {\n\t\tenv->imbalance = max_t(long, env->imbalance,\n\t\t\t\t       busiest->group_misfit_task_load);\n\t}\n\n\t/*\n\t * if *imbalance is less than the average load per runnable task\n\t * there is no guarantee that any tasks will be moved so we'll have\n\t * a think about bumping its value to force at least one task to be\n\t * moved\n\t */\n\tif (env->imbalance < busiest->load_per_task)\n\t\treturn fix_small_imbalance(env, sds);\n}\n\n/******* find_busiest_group() helpers end here *********************/\n\n/**\n * find_busiest_group - Returns the busiest group within the sched_domain\n * if there is an imbalance.\n *\n * Also calculates the amount of weighted load which should be moved\n * to restore balance.\n *\n * @env: The load balancing environment.\n *\n * Return:\t- The busiest group if imbalance exists.\n */\nstatic struct sched_group *find_busiest_group(struct lb_env *env)\n{\n\tstruct sg_lb_stats *local, *busiest;\n\tstruct sd_lb_stats sds;\n\n\tinit_sd_lb_stats(&sds);\n\n\t/*\n\t * Compute the various statistics relavent for load balancing at\n\t * this level.\n\t */\n\tupdate_sd_lb_stats(env, &sds);\n\tlocal = &sds.local_stat;\n\tbusiest = &sds.busiest_stat;\n\n\t/* ASYM feature bypasses nice load balance check */\n\tif (check_asym_packing(env, &sds))\n\t\treturn sds.busiest;\n\n\t/* There is no busy sibling group to pull tasks from */\n\tif (!sds.busiest || busiest->sum_nr_running == 0)\n\t\tgoto out_balanced;\n\n\t/* XXX broken for overlapping NUMA groups */\n\tsds.avg_load = (SCHED_CAPACITY_SCALE * sds.total_load)\n\t\t\t\t\t\t/ sds.total_capacity;\n\n\t/*\n\t * If the busiest group is imbalanced the below checks don't\n\t * work because they assume all things are equal, which typically\n\t * isn't true due to cpus_allowed constraints and the like.\n\t */\n\tif (busiest->group_type == group_imbalanced)\n\t\tgoto force_balance;\n\n\t/*\n\t * When dst_cpu is idle, prevent SMP nice and/or asymmetric group\n\t * capacities from resulting in underutilization due to avg_load.\n\t */\n\tif (env->idle != CPU_NOT_IDLE && group_has_capacity(env, local) &&\n\t    busiest->group_no_capacity)\n\t\tgoto force_balance;\n\n\t/* Misfit tasks should be dealt with regardless of the avg load */\n\tif (busiest->group_type == group_misfit_task)\n\t\tgoto force_balance;\n\n\t/*\n\t * If the local group is busier than the selected busiest group\n\t * don't try and pull any tasks.\n\t */\n\tif (local->avg_load >= busiest->avg_load)\n\t\tgoto out_balanced;\n\n\t/*\n\t * Don't pull any tasks if this group is already above the domain\n\t * average load.\n\t */\n\tif (local->avg_load >= sds.avg_load)\n\t\tgoto out_balanced;\n\n\tif (env->idle == CPU_IDLE) {\n\t\t/*\n\t\t * This CPU is idle. If the busiest group is not overloaded\n\t\t * and there is no imbalance between this and busiest group\n\t\t * wrt idle CPUs, it is balanced. The imbalance becomes\n\t\t * significant if the diff is greater than 1 otherwise we\n\t\t * might end up to just move the imbalance on another group\n\t\t */\n\t\tif ((busiest->group_type != group_overloaded) &&\n\t\t\t\t(local->idle_cpus <= (busiest->idle_cpus + 1)))\n\t\t\tgoto out_balanced;\n\t} else {\n\t\t/*\n\t\t * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use\n\t\t * imbalance_pct to be conservative.\n\t\t */\n\t\tif (100 * busiest->avg_load <=\n\t\t\t\tenv->sd->imbalance_pct * local->avg_load)\n\t\t\tgoto out_balanced;\n\t}\n\nforce_balance:\n\t/* Looks like there is an imbalance. Compute it */\n\tenv->src_grp_type = busiest->group_type;\n\tcalculate_imbalance(env, &sds);\n\treturn env->imbalance ? sds.busiest : NULL;\n\nout_balanced:\n\tenv->imbalance = 0;\n\treturn NULL;\n}\n\n/*\n * find_busiest_queue - find the busiest runqueue among the CPUs in the group.\n */\nstatic struct rq *find_busiest_queue(struct lb_env *env,\n\t\t\t\t     struct sched_group *group)\n{\n\tstruct rq *busiest = NULL, *rq;\n\tunsigned long busiest_load = 0, busiest_capacity = 1;\n\tint i;\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tunsigned long capacity, wl;\n\t\tenum fbq_type rt;\n\n\t\trq = cpu_rq(i);\n\t\trt = fbq_classify_rq(rq);\n\n\t\t/*\n\t\t * We classify groups/runqueues into three groups:\n\t\t *  - regular: there are !numa tasks\n\t\t *  - remote:  there are numa tasks that run on the 'wrong' node\n\t\t *  - all:     there is no distinction\n\t\t *\n\t\t * In order to avoid migrating ideally placed numa tasks,\n\t\t * ignore those when there's better options.\n\t\t *\n\t\t * If we ignore the actual busiest queue to migrate another\n\t\t * task, the next balance pass can still reduce the busiest\n\t\t * queue by moving tasks around inside the node.\n\t\t *\n\t\t * If we cannot move enough load due to this classification\n\t\t * the next pass will adjust the group classification and\n\t\t * allow migration of more tasks.\n\t\t *\n\t\t * Both cases only affect the total convergence complexity.\n\t\t */\n\t\tif (rt > env->fbq_type)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains with misfit tasks we simply\n\t\t * seek the \"biggest\" misfit task.\n\t\t */\n\t\tif (env->src_grp_type == group_misfit_task) {\n\t\t\tif (rq->misfit_task_load > busiest_load) {\n\t\t\t\tbusiest_load = rq->misfit_task_load;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tcapacity = capacity_of(i);\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains, don't pick a CPU that could\n\t\t * eventually lead to active_balancing high->low capacity.\n\t\t * Higher per-CPU capacity is considered better than balancing\n\t\t * average load.\n\t\t */\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    capacity_of(env->dst_cpu) < capacity &&\n\t\t    rq->nr_running == 1)\n\t\t\tcontinue;\n\n\t\twl = weighted_cpuload(rq);\n\n\t\t/*\n\t\t * When comparing with imbalance, use weighted_cpuload()\n\t\t * which is not scaled with the CPU capacity.\n\t\t */\n\n\t\tif (rq->nr_running == 1 && wl > env->imbalance &&\n\t\t    !check_cpu_capacity(rq, env->sd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For the load comparisons with the other CPU's, consider\n\t\t * the weighted_cpuload() scaled with the CPU capacity, so\n\t\t * that the load can be moved away from the CPU that is\n\t\t * potentially running at a lower capacity.\n\t\t *\n\t\t * Thus we're looking for max(wl_i / capacity_i), crosswise\n\t\t * multiplication to rid ourselves of the division works out\n\t\t * to: wl_i * capacity_j > wl_j * capacity_i;  where j is\n\t\t * our previous maximum.\n\t\t */\n\t\tif (wl * busiest_capacity > busiest_load * capacity) {\n\t\t\tbusiest_load = wl;\n\t\t\tbusiest_capacity = capacity;\n\t\t\tbusiest = rq;\n\t\t}\n\t}\n\n\treturn busiest;\n}\n\n/*\n * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but\n * so long as it is large enough.\n */\n#define MAX_PINNED_INTERVAL\t512\n\nstatic int need_active_balance(struct lb_env *env)\n{\n\tstruct sched_domain *sd = env->sd;\n\n\tif (env->idle == CPU_NEWLY_IDLE) {\n\n\t\t/*\n\t\t * ASYM_PACKING needs to force migrate tasks from busy but\n\t\t * lower priority CPUs in order to pack all tasks in the\n\t\t * highest priority CPUs.\n\t\t */\n\t\tif ((sd->flags & SD_ASYM_PACKING) &&\n\t\t    sched_asym_prefer(env->dst_cpu, env->src_cpu))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task.\n\t * It's worth migrating the task if the src_cpu's capacity is reduced\n\t * because of other sched_class or IRQs if more capacity stays\n\t * available on dst_cpu.\n\t */\n\tif ((env->idle != CPU_NOT_IDLE) &&\n\t    (env->src_rq->cfs.h_nr_running == 1)) {\n\t\tif ((check_cpu_capacity(env->src_rq, sd)) &&\n\t\t    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))\n\t\t\treturn 1;\n\t}\n\n\tif (env->src_grp_type == group_misfit_task)\n\t\treturn 1;\n\n\treturn unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);\n}\n\nstatic int active_load_balance_cpu_stop(void *data);\n\nstatic int should_we_balance(struct lb_env *env)\n{\n\tstruct sched_group *sg = env->sd->groups;\n\tint cpu, balance_cpu = -1;\n\n\t/*\n\t * Ensure the balancing environment is consistent; can happen\n\t * when the softirq triggers 'during' hotplug.\n\t */\n\tif (!cpumask_test_cpu(env->dst_cpu, env->cpus))\n\t\treturn 0;\n\n\t/*\n\t * In the newly idle case, we will allow all the CPUs\n\t * to do the newly idle load balance.\n\t */\n\tif (env->idle == CPU_NEWLY_IDLE)\n\t\treturn 1;\n\n\t/* Try to find first idle CPU */\n\tfor_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {\n\t\tif (!idle_cpu(cpu))\n\t\t\tcontinue;\n\n\t\tbalance_cpu = cpu;\n\t\tbreak;\n\t}\n\n\tif (balance_cpu == -1)\n\t\tbalance_cpu = group_balance_cpu(sg);\n\n\t/*\n\t * First idle CPU or the first CPU(busiest) in this sched group\n\t * is eligible for doing load balancing at this and above domains.\n\t */\n\treturn balance_cpu == env->dst_cpu;\n}\n\n/*\n * Check this_cpu to ensure it is balanced within domain. Attempt to move\n * tasks if there is an imbalance.\n */\nstatic int load_balance(int this_cpu, struct rq *this_rq,\n\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n\t\t\tint *continue_balancing)\n{\n\tint ld_moved, cur_ld_moved, active_balance = 0;\n\tstruct sched_domain *sd_parent = sd->parent;\n\tstruct sched_group *group;\n\tstruct rq *busiest;\n\tstruct rq_flags rf;\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);\n\n\tstruct lb_env env = {\n\t\t.sd\t\t= sd,\n\t\t.dst_cpu\t= this_cpu,\n\t\t.dst_rq\t\t= this_rq,\n\t\t.dst_grpmask    = sched_group_span(sd->groups),\n\t\t.idle\t\t= idle,\n\t\t.loop_break\t= sched_nr_migrate_break,\n\t\t.cpus\t\t= cpus,\n\t\t.fbq_type\t= all,\n\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),\n\t};\n\n\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);\n\n\tschedstat_inc(sd->lb_count[idle]);\n\nredo:\n\tif (!should_we_balance(&env)) {\n\t\t*continue_balancing = 0;\n\t\tgoto out_balanced;\n\t}\n\n\tgroup = find_busiest_group(&env);\n\tif (!group) {\n\t\tschedstat_inc(sd->lb_nobusyg[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tbusiest = find_busiest_queue(&env, group);\n\tif (!busiest) {\n\t\tschedstat_inc(sd->lb_nobusyq[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tBUG_ON(busiest == env.dst_rq);\n\n\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);\n\n\tenv.src_cpu = busiest->cpu;\n\tenv.src_rq = busiest;\n\n\tld_moved = 0;\n\tif (busiest->nr_running > 1) {\n\t\t/*\n\t\t * Attempt to move tasks. If find_busiest_group has found\n\t\t * an imbalance but busiest->nr_running <= 1, the group is\n\t\t * still unbalanced. ld_moved simply stays zero, so it is\n\t\t * correctly treated as an imbalance.\n\t\t */\n\t\tenv.flags |= LBF_ALL_PINNED;\n\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);\n\nmore_balance:\n\t\trq_lock_irqsave(busiest, &rf);\n\t\tupdate_rq_clock(busiest);\n\n\t\t/*\n\t\t * cur_ld_moved - load moved in current iteration\n\t\t * ld_moved     - cumulative load moved across iterations\n\t\t */\n\t\tcur_ld_moved = detach_tasks(&env);\n\n\t\t/*\n\t\t * We've detached some tasks from busiest_rq. Every\n\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely\n\t\t * unlock busiest->lock, and we are able to be sure\n\t\t * that nobody can manipulate the tasks in parallel.\n\t\t * See task_rq_lock() family for the details.\n\t\t */\n\n\t\trq_unlock(busiest, &rf);\n\n\t\tif (cur_ld_moved) {\n\t\t\tattach_tasks(&env);\n\t\t\tld_moved += cur_ld_moved;\n\t\t}\n\n\t\tlocal_irq_restore(rf.flags);\n\n\t\tif (env.flags & LBF_NEED_BREAK) {\n\t\t\tenv.flags &= ~LBF_NEED_BREAK;\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to\n\t\t * us and move them to an alternate dst_cpu in our sched_group\n\t\t * where they can run. The upper limit on how many times we\n\t\t * iterate on same src_cpu is dependent on number of CPUs in our\n\t\t * sched_group.\n\t\t *\n\t\t * This changes load balance semantics a bit on who can move\n\t\t * load to a given_cpu. In addition to the given_cpu itself\n\t\t * (or a ilb_cpu acting on its behalf where given_cpu is\n\t\t * nohz-idle), we now have balance_cpu in a position to move\n\t\t * load to given_cpu. In rare situations, this may cause\n\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding\n\t\t * _independently_ and at _same_ time to move some load to\n\t\t * given_cpu) causing exceess load to be moved to given_cpu.\n\t\t * This however should not happen so much in practice and\n\t\t * moreover subsequent load balance cycles should correct the\n\t\t * excess load moved.\n\t\t */\n\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {\n\n\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */\n\t\t\tcpumask_clear_cpu(env.dst_cpu, env.cpus);\n\n\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);\n\t\t\tenv.dst_cpu\t = env.new_dst_cpu;\n\t\t\tenv.flags\t&= ~LBF_DST_PINNED;\n\t\t\tenv.loop\t = 0;\n\t\t\tenv.loop_break\t = sched_nr_migrate_break;\n\n\t\t\t/*\n\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we\n\t\t\t * need to continue with same src_cpu.\n\t\t\t */\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * We failed to reach balance because of affinity.\n\t\t */\n\t\tif (sd_parent) {\n\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)\n\t\t\t\t*group_imbalance = 1;\n\t\t}\n\n\t\t/* All tasks on this runqueue were pinned by CPU affinity */\n\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {\n\t\t\tcpumask_clear_cpu(cpu_of(busiest), cpus);\n\t\t\t/*\n\t\t\t * Attempting to continue load balancing at the current\n\t\t\t * sched_domain level only makes sense if there are\n\t\t\t * active CPUs remaining as possible busiest CPUs to\n\t\t\t * pull load from which are not contained within the\n\t\t\t * destination group that is receiving any migrated\n\t\t\t * load.\n\t\t\t */\n\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {\n\t\t\t\tenv.loop = 0;\n\t\t\t\tenv.loop_break = sched_nr_migrate_break;\n\t\t\t\tgoto redo;\n\t\t\t}\n\t\t\tgoto out_all_pinned;\n\t\t}\n\t}\n\n\tif (!ld_moved) {\n\t\tschedstat_inc(sd->lb_failed[idle]);\n\t\t/*\n\t\t * Increment the failure counter only on periodic balance.\n\t\t * We do not want newidle balance, which can be very\n\t\t * frequent, pollute the failure counter causing\n\t\t * excessive cache_hot migrations and active balances.\n\t\t */\n\t\tif (idle != CPU_NEWLY_IDLE)\n\t\t\tsd->nr_balance_failed++;\n\n\t\tif (need_active_balance(&env)) {\n\t\t\tunsigned long flags;\n\n\t\t\traw_spin_lock_irqsave(&busiest->lock, flags);\n\n\t\t\t/*\n\t\t\t * Don't kick the active_load_balance_cpu_stop,\n\t\t\t * if the curr task on busiest CPU can't be\n\t\t\t * moved to this_cpu:\n\t\t\t */\n\t\t\tif (!cpumask_test_cpu(this_cpu, &busiest->curr->cpus_allowed)) {\n\t\t\t\traw_spin_unlock_irqrestore(&busiest->lock,\n\t\t\t\t\t\t\t    flags);\n\t\t\t\tenv.flags |= LBF_ALL_PINNED;\n\t\t\t\tgoto out_one_pinned;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * ->active_balance synchronizes accesses to\n\t\t\t * ->active_balance_work.  Once set, it's cleared\n\t\t\t * only after active load balance is finished.\n\t\t\t */\n\t\t\tif (!busiest->active_balance) {\n\t\t\t\tbusiest->active_balance = 1;\n\t\t\t\tbusiest->push_cpu = this_cpu;\n\t\t\t\tactive_balance = 1;\n\t\t\t}\n\t\t\traw_spin_unlock_irqrestore(&busiest->lock, flags);\n\n\t\t\tif (active_balance) {\n\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),\n\t\t\t\t\tactive_load_balance_cpu_stop, busiest,\n\t\t\t\t\t&busiest->active_balance_work);\n\t\t\t}\n\n\t\t\t/* We've kicked active balancing, force task migration. */\n\t\t\tsd->nr_balance_failed = sd->cache_nice_tries+1;\n\t\t}\n\t} else\n\t\tsd->nr_balance_failed = 0;\n\n\tif (likely(!active_balance)) {\n\t\t/* We were unbalanced, so reset the balancing interval */\n\t\tsd->balance_interval = sd->min_interval;\n\t} else {\n\t\t/*\n\t\t * If we've begun active balancing, start to back off. This\n\t\t * case may not be covered by the all_pinned logic if there\n\t\t * is only 1 task on the busy runqueue (because we don't call\n\t\t * detach_tasks).\n\t\t */\n\t\tif (sd->balance_interval < sd->max_interval)\n\t\t\tsd->balance_interval *= 2;\n\t}\n\n\tgoto out;\n\nout_balanced:\n\t/*\n\t * We reach balance although we may have faced some affinity\n\t * constraints. Clear the imbalance flag if it was set.\n\t */\n\tif (sd_parent) {\n\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\tif (*group_imbalance)\n\t\t\t*group_imbalance = 0;\n\t}\n\nout_all_pinned:\n\t/*\n\t * We reach balance because all tasks are pinned at this level so\n\t * we can't migrate them. Let the imbalance flag set so parent level\n\t * can try to migrate them.\n\t */\n\tschedstat_inc(sd->lb_balanced[idle]);\n\n\tsd->nr_balance_failed = 0;\n\nout_one_pinned:\n\t/* tune up the balancing interval */\n\tif (((env.flags & LBF_ALL_PINNED) &&\n\t\t\tsd->balance_interval < MAX_PINNED_INTERVAL) ||\n\t\t\t(sd->balance_interval < sd->max_interval))\n\t\tsd->balance_interval *= 2;\n\n\tld_moved = 0;\nout:\n\treturn ld_moved;\n}\n\nstatic inline unsigned long\nget_sd_balance_interval(struct sched_domain *sd, int cpu_busy)\n{\n\tunsigned long interval = sd->balance_interval;\n\n\tif (cpu_busy)\n\t\tinterval *= sd->busy_factor;\n\n\t/* scale ms to jiffies */\n\tinterval = msecs_to_jiffies(interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\n\treturn interval;\n}\n\nstatic inline void\nupdate_next_balance(struct sched_domain *sd, unsigned long *next_balance)\n{\n\tunsigned long interval, next;\n\n\t/* used by idle balance, so cpu_busy = 0 */\n\tinterval = get_sd_balance_interval(sd, 0);\n\tnext = sd->last_balance + interval;\n\n\tif (time_after(*next_balance, next))\n\t\t*next_balance = next;\n}\n\n/*\n * active_load_balance_cpu_stop is run by the CPU stopper. It pushes\n * running tasks off the busiest CPU onto idle CPUs. It requires at\n * least 1 task to be running on each physical CPU where possible, and\n * avoids physical / logical imbalances.\n */\nstatic int active_load_balance_cpu_stop(void *data)\n{\n\tstruct rq *busiest_rq = data;\n\tint busiest_cpu = cpu_of(busiest_rq);\n\tint target_cpu = busiest_rq->push_cpu;\n\tstruct rq *target_rq = cpu_rq(target_cpu);\n\tstruct sched_domain *sd;\n\tstruct task_struct *p = NULL;\n\tstruct rq_flags rf;\n\n\trq_lock_irq(busiest_rq, &rf);\n\t/*\n\t * Between queueing the stop-work and running it is a hole in which\n\t * CPUs can become inactive. We should not move tasks from or to\n\t * inactive CPUs.\n\t */\n\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu))\n\t\tgoto out_unlock;\n\n\t/* Make sure the requested CPU hasn't gone down in the meantime: */\n\tif (unlikely(busiest_cpu != smp_processor_id() ||\n\t\t     !busiest_rq->active_balance))\n\t\tgoto out_unlock;\n\n\t/* Is there any task to move? */\n\tif (busiest_rq->nr_running <= 1)\n\t\tgoto out_unlock;\n\n\t/*\n\t * This condition is \"impossible\", if it occurs\n\t * we need to fix it. Originally reported by\n\t * Bjorn Helgaas on a 128-CPU setup.\n\t */\n\tBUG_ON(busiest_rq == target_rq);\n\n\t/* Search for an sd spanning us and the target CPU. */\n\trcu_read_lock();\n\tfor_each_domain(target_cpu, sd) {\n\t\tif ((sd->flags & SD_LOAD_BALANCE) &&\n\t\t    cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))\n\t\t\t\tbreak;\n\t}\n\n\tif (likely(sd)) {\n\t\tstruct lb_env env = {\n\t\t\t.sd\t\t= sd,\n\t\t\t.dst_cpu\t= target_cpu,\n\t\t\t.dst_rq\t\t= target_rq,\n\t\t\t.src_cpu\t= busiest_rq->cpu,\n\t\t\t.src_rq\t\t= busiest_rq,\n\t\t\t.idle\t\t= CPU_IDLE,\n\t\t\t/*\n\t\t\t * can_migrate_task() doesn't need to compute new_dst_cpu\n\t\t\t * for active balancing. Since we have CPU_IDLE, but no\n\t\t\t * @dst_grpmask we need to make that test go away with lying\n\t\t\t * about DST_PINNED.\n\t\t\t */\n\t\t\t.flags\t\t= LBF_DST_PINNED,\n\t\t};\n\n\t\tschedstat_inc(sd->alb_count);\n\t\tupdate_rq_clock(busiest_rq);\n\n\t\tp = detach_one_task(&env);\n\t\tif (p) {\n\t\t\tschedstat_inc(sd->alb_pushed);\n\t\t\t/* Active balancing done, reset the failure counter. */\n\t\t\tsd->nr_balance_failed = 0;\n\t\t} else {\n\t\t\tschedstat_inc(sd->alb_failed);\n\t\t}\n\t}\n\trcu_read_unlock();\nout_unlock:\n\tbusiest_rq->active_balance = 0;\n\trq_unlock(busiest_rq, &rf);\n\n\tif (p)\n\t\tattach_one_task(target_rq, p);\n\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\nstatic DEFINE_SPINLOCK(balancing);\n\n/*\n * Scale the max load_balance interval with the number of CPUs in the system.\n * This trades load-balance latency on larger machines for less cross talk.\n */\nvoid update_max_interval(void)\n{\n\tmax_load_balance_interval = HZ*num_online_cpus()/10;\n}\n\n/*\n * It checks each scheduling domain to see if it is due to be balanced,\n * and initiates a balancing operation if so.\n *\n * Balancing parameters are set up in init_sched_domains.\n */\nstatic void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)\n{\n\tint continue_balancing = 1;\n\tint cpu = rq->cpu;\n\tunsigned long interval;\n\tstruct sched_domain *sd;\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long next_balance = jiffies + 60*HZ;\n\tint update_next_balance = 0;\n\tint need_serialize, need_decay = 0;\n\tu64 max_cost = 0;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\t/*\n\t\t * Decay the newidle max times here because this is a regular\n\t\t * visit to all the domains. Decay ~1% per second.\n\t\t */\n\t\tif (time_after(jiffies, sd->next_decay_max_lb_cost)) {\n\t\t\tsd->max_newidle_lb_cost =\n\t\t\t\t(sd->max_newidle_lb_cost * 253) / 256;\n\t\t\tsd->next_decay_max_lb_cost = jiffies + HZ;\n\t\t\tneed_decay = 1;\n\t\t}\n\t\tmax_cost += sd->max_newidle_lb_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Stop the load balance at this level. There is another\n\t\t * CPU in our sched group which is doing load balancing more\n\t\t * actively.\n\t\t */\n\t\tif (!continue_balancing) {\n\t\t\tif (need_decay)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\n\t\tneed_serialize = sd->flags & SD_SERIALIZE;\n\t\tif (need_serialize) {\n\t\t\tif (!spin_trylock(&balancing))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (time_after_eq(jiffies, sd->last_balance + interval)) {\n\t\t\tif (load_balance(cpu, rq, sd, idle, &continue_balancing)) {\n\t\t\t\t/*\n\t\t\t\t * The LBF_DST_PINNED logic could have changed\n\t\t\t\t * env->dst_cpu, so we can't know our idle\n\t\t\t\t * state even if we migrated tasks. Update it.\n\t\t\t\t */\n\t\t\t\tidle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;\n\t\t\t}\n\t\t\tsd->last_balance = jiffies;\n\t\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\t\t}\n\t\tif (need_serialize)\n\t\t\tspin_unlock(&balancing);\nout:\n\t\tif (time_after(next_balance, sd->last_balance + interval)) {\n\t\t\tnext_balance = sd->last_balance + interval;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\tif (need_decay) {\n\t\t/*\n\t\t * Ensure the rq-wide value also decays but keep it at a\n\t\t * reasonable floor to avoid funnies with rq->avg_idle.\n\t\t */\n\t\trq->max_idle_balance_cost =\n\t\t\tmax((u64)sysctl_sched_migration_cost, max_cost);\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the cpu is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance)) {\n\t\trq->next_balance = next_balance;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\t\t/*\n\t\t * If this CPU has been elected to perform the nohz idle\n\t\t * balance. Other idle CPUs have already rebalanced with\n\t\t * nohz_idle_balance() and nohz.next_balance has been\n\t\t * updated accordingly. This CPU is now running the idle load\n\t\t * balance for itself and we need to update the\n\t\t * nohz.next_balance accordingly.\n\t\t */\n\t\tif ((idle == CPU_IDLE) && time_after(nohz.next_balance, rq->next_balance))\n\t\t\tnohz.next_balance = rq->next_balance;\n#endif\n\t}\n}\n\nstatic inline int on_null_domain(struct rq *rq)\n{\n\treturn unlikely(!rcu_dereference_sched(rq->sd));\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * idle load balancing details\n * - When one of the busy CPUs notice that there may be an idle rebalancing\n *   needed, they will kick the idle load balancer, which then does idle\n *   load balancing for all the idle CPUs.\n */\n\nstatic inline int find_new_ilb(void)\n{\n\tint ilb = cpumask_first(nohz.idle_cpus_mask);\n\n\tif (ilb < nr_cpu_ids && idle_cpu(ilb))\n\t\treturn ilb;\n\n\treturn nr_cpu_ids;\n}\n\n/*\n * Kick a CPU to do the nohz balancing, if it is time for it. We pick the\n * nohz_load_balancer CPU (if there is one) otherwise fallback to any idle\n * CPU (if there is one).\n */\nstatic void kick_ilb(unsigned int flags)\n{\n\tint ilb_cpu;\n\n\tnohz.next_balance++;\n\n\tilb_cpu = find_new_ilb();\n\n\tif (ilb_cpu >= nr_cpu_ids)\n\t\treturn;\n\n\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));\n\tif (flags & NOHZ_KICK_MASK)\n\t\treturn;\n\n\t/*\n\t * Use smp_send_reschedule() instead of resched_cpu().\n\t * This way we generate a sched IPI on the target CPU which\n\t * is idle. And the softirq performing nohz idle load balance\n\t * will be run before returning from the IPI.\n\t */\n\tsmp_send_reschedule(ilb_cpu);\n}\n\n/*\n * Current heuristic for kicking the idle load balancer in the presence\n * of an idle cpu in the system.\n *   - This rq has more than one task.\n *   - This rq has at least one CFS task and the capacity of the CPU is\n *     significantly reduced because of RT tasks or IRQs.\n *   - At parent of LLC scheduler domain level, this cpu's scheduler group has\n *     multiple busy cpu.\n *   - For SD_ASYM_PACKING, if the lower numbered cpu's in the scheduler\n *     domain span are idle.\n */\nstatic void nohz_balancer_kick(struct rq *rq)\n{\n\tunsigned long now = jiffies;\n\tstruct sched_domain_shared *sds;\n\tstruct sched_domain *sd;\n\tint nr_busy, i, cpu = rq->cpu;\n\tunsigned int flags = 0;\n\n\tif (unlikely(rq->idle_balance))\n\t\treturn;\n\n\t/*\n\t * We may be recently in ticked or tickless idle mode. At the first\n\t * busy tick after returning from idle, we will update the busy stats.\n\t */\n\tnohz_balance_exit_idle(rq);\n\n\t/*\n\t * None are in tickless mode and hence no need for NOHZ idle load\n\t * balancing.\n\t */\n\tif (likely(!atomic_read(&nohz.nr_cpus)))\n\t\treturn;\n\n\tif (READ_ONCE(nohz.has_blocked) &&\n\t    time_after(now, READ_ONCE(nohz.next_blocked)))\n\t\tflags = NOHZ_STATS_KICK;\n\n\tif (time_before(now, nohz.next_balance))\n\t\tgoto out;\n\n\tif (rq->nr_running >= 2 || rq->misfit_task_load) {\n\t\tflags = NOHZ_KICK_MASK;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds) {\n\t\t/*\n\t\t * XXX: write a coherent comment on why we do this.\n\t\t * See also: http://lkml.kernel.org/r/20111202010832.602203411@sbsiddha-desk.sc.intel.com\n\t\t */\n\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);\n\t\tif (nr_busy > 1) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\n\t}\n\n\tsd = rcu_dereference(rq->sd);\n\tif (sd) {\n\t\tif ((rq->cfs.h_nr_running >= 1) &&\n\t\t\t\tcheck_cpu_capacity(rq, sd)) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_asym, cpu));\n\tif (sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\t\tif (i == cpu ||\n\t\t\t    !cpumask_test_cpu(i, nohz.idle_cpus_mask))\n\t\t\t\tcontinue;\n\n\t\t\tif (sched_asym_prefer(i, cpu)) {\n\t\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\nout:\n\tif (flags)\n\t\tkick_ilb(flags);\n}\n\nstatic void set_cpu_sd_state_busy(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || !sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 0;\n\n\tatomic_inc(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\nvoid nohz_balance_exit_idle(struct rq *rq)\n{\n\tSCHED_WARN_ON(rq != this_rq());\n\n\tif (likely(!rq->nohz_tick_stopped))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 0;\n\tcpumask_clear_cpu(rq->cpu, nohz.idle_cpus_mask);\n\tatomic_dec(&nohz.nr_cpus);\n\n\tset_cpu_sd_state_busy(rq->cpu);\n}\n\nstatic void set_cpu_sd_state_idle(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 1;\n\n\tatomic_dec(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\n/*\n * This routine will record that the CPU is going idle with tick stopped.\n * This info will be used in performing idle load balancing in the future.\n */\nvoid nohz_balance_enter_idle(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tSCHED_WARN_ON(cpu != smp_processor_id());\n\n\t/* If this CPU is going down, then nothing needs to be done: */\n\tif (!cpu_active(cpu))\n\t\treturn;\n\n\t/* Spare idle load balancing on CPUs that don't want to be disturbed: */\n\tif (!housekeeping_cpu(cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/*\n\t * Can be set safely without rq->lock held\n\t * If a clear happens, it will have evaluated last additions because\n\t * rq->lock is held during the check and the clear\n\t */\n\trq->has_blocked_load = 1;\n\n\t/*\n\t * The tick is still stopped but load could have been added in the\n\t * meantime. We set the nohz.has_blocked flag to trig a check of the\n\t * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear\n\t * of nohz.has_blocked can only happen after checking the new load\n\t */\n\tif (rq->nohz_tick_stopped)\n\t\tgoto out;\n\n\t/* If we're a completely isolated CPU, we don't play: */\n\tif (on_null_domain(rq))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 1;\n\n\tcpumask_set_cpu(cpu, nohz.idle_cpus_mask);\n\tatomic_inc(&nohz.nr_cpus);\n\n\t/*\n\t * Ensures that if nohz_idle_balance() fails to observe our\n\t * @idle_cpus_mask store, it must observe the @has_blocked\n\t * store.\n\t */\n\tsmp_mb__after_atomic();\n\n\tset_cpu_sd_state_idle(cpu);\n\nout:\n\t/*\n\t * Each time a cpu enter idle, we assume that it has blocked load and\n\t * enable the periodic update of the load of idle cpus\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 1);\n}\n\n/*\n * Internal function that runs load balance for all idle cpus. The load balance\n * can be a simple update of blocked load or a complete load balance with\n * tasks movement depending of flags.\n * The function returns false if the loop has stopped before running\n * through all idle CPUs.\n */\nstatic bool _nohz_idle_balance(struct rq *this_rq, unsigned int flags,\n\t\t\t       enum cpu_idle_type idle)\n{\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long now = jiffies;\n\tunsigned long next_balance = now + 60*HZ;\n\tbool has_blocked_load = false;\n\tint update_next_balance = 0;\n\tint this_cpu = this_rq->cpu;\n\tint balance_cpu;\n\tint ret = false;\n\tstruct rq *rq;\n\n\tSCHED_WARN_ON((flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK);\n\n\t/*\n\t * We assume there will be no idle load after this update and clear\n\t * the has_blocked flag. If a cpu enters idle in the mean time, it will\n\t * set the has_blocked flag and trig another update of idle load.\n\t * Because a cpu that becomes idle, is added to idle_cpus_mask before\n\t * setting the flag, we are sure to not clear the state and not\n\t * check the load of an idle cpu.\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 0);\n\n\t/*\n\t * Ensures that if we miss the CPU, we must see the has_blocked\n\t * store from nohz_balance_enter_idle().\n\t */\n\tsmp_mb();\n\n\tfor_each_cpu(balance_cpu, nohz.idle_cpus_mask) {\n\t\tif (balance_cpu == this_cpu || !idle_cpu(balance_cpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If this CPU gets work to do, stop the load balancing\n\t\t * work being done for other CPUs. Next load\n\t\t * balancing owner will pick it up.\n\t\t */\n\t\tif (need_resched()) {\n\t\t\thas_blocked_load = true;\n\t\t\tgoto abort;\n\t\t}\n\n\t\trq = cpu_rq(balance_cpu);\n\n\t\thas_blocked_load |= update_nohz_stats(rq, true);\n\n\t\t/*\n\t\t * If time for next balance is due,\n\t\t * do the balance.\n\t\t */\n\t\tif (time_after_eq(jiffies, rq->next_balance)) {\n\t\t\tstruct rq_flags rf;\n\n\t\t\trq_lock_irqsave(rq, &rf);\n\t\t\tupdate_rq_clock(rq);\n\t\t\tcpu_load_update_idle(rq);\n\t\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\t\tif (flags & NOHZ_BALANCE_KICK)\n\t\t\t\trebalance_domains(rq, CPU_IDLE);\n\t\t}\n\n\t\tif (time_after(next_balance, rq->next_balance)) {\n\t\t\tnext_balance = rq->next_balance;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\n\t/* Newly idle CPU doesn't need an update */\n\tif (idle != CPU_NEWLY_IDLE) {\n\t\tupdate_blocked_averages(this_cpu);\n\t\thas_blocked_load |= this_rq->has_blocked_load;\n\t}\n\n\tif (flags & NOHZ_BALANCE_KICK)\n\t\trebalance_domains(this_rq, CPU_IDLE);\n\n\tWRITE_ONCE(nohz.next_blocked,\n\t\tnow + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\n\t/* The full idle balance loop has been done */\n\tret = true;\n\nabort:\n\t/* There is still blocked load, enable periodic update */\n\tif (has_blocked_load)\n\t\tWRITE_ONCE(nohz.has_blocked, 1);\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the CPU is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance))\n\t\tnohz.next_balance = next_balance;\n\n\treturn ret;\n}\n\n/*\n * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the\n * rebalancing for all the cpus for whom scheduler ticks are stopped.\n */\nstatic bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\tint this_cpu = this_rq->cpu;\n\tunsigned int flags;\n\n\tif (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\tif (idle != CPU_IDLE) {\n\t\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\t\treturn false;\n\t}\n\n\t/*\n\t * barrier, pairs with nohz_balance_enter_idle(), ensures ...\n\t */\n\tflags = atomic_fetch_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\tif (!(flags & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\t_nohz_idle_balance(this_rq, flags, idle);\n\n\treturn true;\n}\n\nstatic void nohz_newidle_balance(struct rq *this_rq)\n{\n\tint this_cpu = this_rq->cpu;\n\n\t/*\n\t * This CPU doesn't want to be disturbed by scheduler\n\t * housekeeping\n\t */\n\tif (!housekeeping_cpu(this_cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/* Will wake up very soon. No time for doing anything else*/\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost)\n\t\treturn;\n\n\t/* Don't need to update blocked load of idle CPUs*/\n\tif (!READ_ONCE(nohz.has_blocked) ||\n\t    time_before(jiffies, READ_ONCE(nohz.next_blocked)))\n\t\treturn;\n\n\traw_spin_unlock(&this_rq->lock);\n\t/*\n\t * This CPU is going to be idle and blocked load of idle CPUs\n\t * need to be updated. Run the ilb locally as it is a good\n\t * candidate for ilb instead of waking up another idle CPU.\n\t * Kick an normal ilb if we failed to do the update.\n\t */\n\tif (!_nohz_idle_balance(this_rq, NOHZ_STATS_KICK, CPU_NEWLY_IDLE))\n\t\tkick_ilb(NOHZ_STATS_KICK);\n\traw_spin_lock(&this_rq->lock);\n}\n\n#else /* !CONFIG_NO_HZ_COMMON */\nstatic inline void nohz_balancer_kick(struct rq *rq) { }\n\nstatic inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\treturn false;\n}\n\nstatic inline void nohz_newidle_balance(struct rq *this_rq) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\n/*\n * idle_balance is called by schedule() if this_cpu is about to become\n * idle. Attempts to pull tasks from other CPUs.\n */\nstatic int idle_balance(struct rq *this_rq, struct rq_flags *rf)\n{\n\tunsigned long next_balance = jiffies + HZ;\n\tint this_cpu = this_rq->cpu;\n\tstruct sched_domain *sd;\n\tint pulled_task = 0;\n\tu64 curr_cost = 0;\n\n\t/*\n\t * We must set idle_stamp _before_ calling idle_balance(), such that we\n\t * measure the duration of idle_balance() as idle time.\n\t */\n\tthis_rq->idle_stamp = rq_clock(this_rq);\n\n\t/*\n\t * Do not pull tasks towards !active CPUs...\n\t */\n\tif (!cpu_active(this_cpu))\n\t\treturn 0;\n\n\t/*\n\t * This is OK, because current is on_cpu, which avoids it being picked\n\t * for load-balance and preemption/IRQs are still disabled avoiding\n\t * further scheduler activity on it and we're being very careful to\n\t * re-start the picking loop.\n\t */\n\trq_unpin_lock(this_rq, rf);\n\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost ||\n\t    !READ_ONCE(this_rq->rd->overload)) {\n\n\t\trcu_read_lock();\n\t\tsd = rcu_dereference_check_sched_domain(this_rq->sd);\n\t\tif (sd)\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\trcu_read_unlock();\n\n\t\tnohz_newidle_balance(this_rq);\n\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock(&this_rq->lock);\n\n\tupdate_blocked_averages(this_cpu);\n\trcu_read_lock();\n\tfor_each_domain(this_cpu, sd) {\n\t\tint continue_balancing = 1;\n\t\tu64 t0, domain_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\tif (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sd->flags & SD_BALANCE_NEWIDLE) {\n\t\t\tt0 = sched_clock_cpu(this_cpu);\n\n\t\t\tpulled_task = load_balance(this_cpu, this_rq,\n\t\t\t\t\t\t   sd, CPU_NEWLY_IDLE,\n\t\t\t\t\t\t   &continue_balancing);\n\n\t\t\tdomain_cost = sched_clock_cpu(this_cpu) - t0;\n\t\t\tif (domain_cost > sd->max_newidle_lb_cost)\n\t\t\t\tsd->max_newidle_lb_cost = domain_cost;\n\n\t\t\tcurr_cost += domain_cost;\n\t\t}\n\n\t\tupdate_next_balance(sd, &next_balance);\n\n\t\t/*\n\t\t * Stop searching for tasks to pull if there are\n\t\t * now runnable tasks on this rq.\n\t\t */\n\t\tif (pulled_task || this_rq->nr_running > 0)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\traw_spin_lock(&this_rq->lock);\n\n\tif (curr_cost > this_rq->max_idle_balance_cost)\n\t\tthis_rq->max_idle_balance_cost = curr_cost;\n\nout:\n\t/*\n\t * While browsing the domains, we released the rq lock, a task could\n\t * have been enqueued in the meantime. Since we're not going idle,\n\t * pretend we pulled a task.\n\t */\n\tif (this_rq->cfs.h_nr_running && !pulled_task)\n\t\tpulled_task = 1;\n\n\t/* Move the next balance forward */\n\tif (time_after(this_rq->next_balance, next_balance))\n\t\tthis_rq->next_balance = next_balance;\n\n\t/* Is there a task of a high priority class? */\n\tif (this_rq->nr_running != this_rq->cfs.h_nr_running)\n\t\tpulled_task = -1;\n\n\tif (pulled_task)\n\t\tthis_rq->idle_stamp = 0;\n\n\trq_repin_lock(this_rq, rf);\n\n\treturn pulled_task;\n}\n\n/*\n * run_rebalance_domains is triggered when needed from the scheduler tick.\n * Also triggered for nohz idle balancing (with nohz_balancing_kick set).\n */\nstatic __latent_entropy void run_rebalance_domains(struct softirq_action *h)\n{\n\tstruct rq *this_rq = this_rq();\n\tenum cpu_idle_type idle = this_rq->idle_balance ?\n\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;\n\n\t/*\n\t * If this CPU has a pending nohz_balance_kick, then do the\n\t * balancing on behalf of the other idle CPUs whose ticks are\n\t * stopped. Do nohz_idle_balance *before* rebalance_domains to\n\t * give the idle CPUs a chance to load balance. Else we may\n\t * load balance only within the local sched_domain hierarchy\n\t * and abort nohz_idle_balance altogether if we pull some load.\n\t */\n\tif (nohz_idle_balance(this_rq, idle))\n\t\treturn;\n\n\t/* normal load balance */\n\tupdate_blocked_averages(this_rq->cpu);\n\trebalance_domains(this_rq, idle);\n}\n\n/*\n * Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing.\n */\nvoid trigger_load_balance(struct rq *rq)\n{\n\t/* Don't need to rebalance while attached to NULL domain */\n\tif (unlikely(on_null_domain(rq)))\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->next_balance))\n\t\traise_softirq(SCHED_SOFTIRQ);\n\n\tnohz_balancer_kick(rq);\n}\n\nstatic void rq_online_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\tupdate_runtime_enabled(rq);\n}\n\nstatic void rq_offline_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\t/* Ensure any throttled groups are reachable by pick_next_task */\n\tunthrottle_offline_cfs_rqs(rq);\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * scheduler tick hitting a task of our scheduling class.\n *\n * NOTE: This function can be called remotely by the tick offload that\n * goes along full dynticks. Therefore no local assumption can be made\n * and everything must be accessed through the @rq and @curr passed in\n * parameters.\n */\nstatic void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tentity_tick(cfs_rq, se, queued);\n\t}\n\n\tif (static_branch_unlikely(&sched_numa_balancing))\n\t\ttask_tick_numa(rq, curr);\n\n\tupdate_misfit_status(curr, rq);\n}\n\n/*\n * called on fork with the child task as argument from the parent's context\n *  - child not yet on the tasklist\n *  - preemption disabled\n */\nstatic void task_fork_fair(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se, *curr;\n\tstruct rq *rq = this_rq();\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\tcfs_rq = task_cfs_rq(current);\n\tcurr = cfs_rq->curr;\n\tif (curr) {\n\t\tupdate_curr(cfs_rq);\n\t\tse->vruntime = curr->vruntime;\n\t}\n\tplace_entity(cfs_rq, se, 1);\n\n\tif (sysctl_sched_child_runs_first && curr && entity_before(curr, se)) {\n\t\t/*\n\t\t * Upon rescheduling, sched_class::put_prev_task() will place\n\t\t * 'current' within the tree based on its new key value.\n\t\t */\n\t\tswap(curr->vruntime, se->vruntime);\n\t\tresched_curr(rq);\n\t}\n\n\tse->vruntime -= cfs_rq->min_vruntime;\n\trq_unlock(rq, &rf);\n}\n\n/*\n * Priority of the task has changed. Check to see if we preempt\n * the current task.\n */\nstatic void\nprio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tif (!task_on_rq_queued(p))\n\t\treturn;\n\n\t/*\n\t * Reschedule if we are currently running on this runqueue and\n\t * our priority decreased, or if we are not currently running on\n\t * this runqueue and our priority is higher than the current's\n\t */\n\tif (rq->curr == p) {\n\t\tif (p->prio > oldprio)\n\t\t\tresched_curr(rq);\n\t} else\n\t\tcheck_preempt_curr(rq, p, 0);\n}\n\nstatic inline bool vruntime_normalized(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases,\n\t * the dequeue_entity(.flags=0) will already have normalized the\n\t * vruntime.\n\t */\n\tif (p->on_rq)\n\t\treturn true;\n\n\t/*\n\t * When !on_rq, vruntime of the task has usually NOT been normalized.\n\t * But there are some cases where it has already been normalized:\n\t *\n\t * - A forked child which is waiting for being woken up by\n\t *   wake_up_new_task().\n\t * - A task which has been woken up by try_to_wake_up() and\n\t *   waiting for actually being woken up by sched_ttwu_pending().\n\t */\n\tif (!se->sum_exec_runtime ||\n\t    (p->state == TASK_WAKING && p->sched_remote_wakeup))\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n/*\n * Propagate the changes of the sched_entity across the tg tree to make it\n * visible to the root\n */\nstatic void propagate_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq;\n\n\t/* Start to propagate at parent */\n\tse = se->parent;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n}\n#else\nstatic void propagate_entity_cfs_rq(struct sched_entity *se) { }\n#endif\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t/* Catch up with the cfs_rq and remove our load when we leave */\n\tupdate_load_avg(cfs_rq, se, 0);\n\tdetach_entity_load_avg(cfs_rq, se);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void attach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/*\n\t * Since the real-depth could have been changed (only FAIR\n\t * class maintain depth value), reset depth properly.\n\t */\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n#endif\n\n\t/* Synchronize entity with its cfs_rq */\n\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);\n\tattach_entity_load_avg(cfs_rq, se, 0);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void detach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tif (!vruntime_normalized(p)) {\n\t\t/*\n\t\t * Fix up our vruntime so that the current sleep doesn't\n\t\t * cause 'unlimited' sleep bonus.\n\t\t */\n\t\tplace_entity(cfs_rq, se, 0);\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\t}\n\n\tdetach_entity_cfs_rq(se);\n}\n\nstatic void attach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tattach_entity_cfs_rq(se);\n\n\tif (!vruntime_normalized(p))\n\t\tse->vruntime += cfs_rq->min_vruntime;\n}\n\nstatic void switched_from_fair(struct rq *rq, struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n}\n\nstatic void switched_to_fair(struct rq *rq, struct task_struct *p)\n{\n\tattach_task_cfs_rq(p);\n\n\tif (task_on_rq_queued(p)) {\n\t\t/*\n\t\t * We were most likely switched from sched_rt, so\n\t\t * kick off the schedule if running, otherwise just see\n\t\t * if we can still preempt the current task.\n\t\t */\n\t\tif (rq->curr == p)\n\t\t\tresched_curr(rq);\n\t\telse\n\t\t\tcheck_preempt_curr(rq, p, 0);\n\t}\n}\n\n/* Account for a task changing its policy or group.\n *\n * This routine is mostly called to set cfs_rq->curr field when a task\n * migrates between groups/classes.\n */\nstatic void set_curr_task_fair(struct rq *rq)\n{\n\tstruct sched_entity *se = &rq->curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\tset_next_entity(cfs_rq, se);\n\t\t/* ensure bandwidth has been allocated on our new cfs_rq */\n\t\taccount_cfs_rq_runtime(cfs_rq, 0);\n\t}\n}\n\nvoid init_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT_CACHED;\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n#ifndef CONFIG_64BIT\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n#ifdef CONFIG_SMP\n\traw_spin_lock_init(&cfs_rq->removed.lock);\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void task_set_group_fair(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tset_task_rq(p, task_cpu(p));\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n}\n\nstatic void task_move_group_fair(struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n\tset_task_rq(p, task_cpu(p));\n\n#ifdef CONFIG_SMP\n\t/* Tell se's cfs_rq has been changed -- migrated */\n\tp->se.avg.last_update_time = 0;\n#endif\n\tattach_task_cfs_rq(p);\n}\n\nstatic void task_change_group_fair(struct task_struct *p, int type)\n{\n\tswitch (type) {\n\tcase TASK_SET_GROUP:\n\t\ttask_set_group_fair(p);\n\t\tbreak;\n\n\tcase TASK_MOVE_GROUP:\n\t\ttask_move_group_fair(p);\n\t\tbreak;\n\t}\n}\n\nvoid free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct sched_entity *se;\n\tstruct cfs_rq *cfs_rq;\n\tint i;\n\n\ttg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tinit_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_cfs_rq(cfs_rq);\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t\tinit_entity_runnable_average(se);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nvoid online_fair_sched_group(struct task_group *tg)\n{\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\t\tse = tg->se[i];\n\n\t\traw_spin_lock_irq(&rq->lock);\n\t\tupdate_rq_clock(rq);\n\t\tattach_entity_cfs_rq(se);\n\t\tsync_throttle(tg, i);\n\t\traw_spin_unlock_irq(&rq->lock);\n\t}\n}\n\nvoid unregister_fair_sched_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (tg->se[cpu])\n\t\t\tremove_entity_load_avg(tg->se[cpu]);\n\n\t\t/*\n\t\t * Only empty task groups can be destroyed; so we can speculatively\n\t\t * check on_list without danger of it being re-added.\n\t\t */\n\t\tif (!tg->cfs_rq[cpu]->on_list)\n\t\t\tcontinue;\n\n\t\trq = cpu_rq(cpu);\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n}\n\nvoid init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\tstruct sched_entity *se, int cpu,\n\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tcfs_rq->tg = tg;\n\tcfs_rq->rq = rq;\n\tinit_cfs_rq_runtime(cfs_rq);\n\n\ttg->cfs_rq[cpu] = cfs_rq;\n\ttg->se[cpu] = se;\n\n\t/* se could be NULL for root_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent) {\n\t\tse->cfs_rq = &rq->cfs;\n\t\tse->depth = 0;\n\t} else {\n\t\tse->cfs_rq = parent->my_q;\n\t\tse->depth = parent->depth + 1;\n\t}\n\n\tse->my_q = cfs_rq;\n\t/* guarantee group entities always have weight */\n\tupdate_load_set(&se->load, NICE_0_LOAD);\n\tse->parent = parent;\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES));\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg->se[i];\n\t\tstruct rq_flags rf;\n\n\t\t/* Propagate contribution to hierarchy */\n\t\trq_lock_irqsave(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_sched_entity(se) {\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG);\n\t\t\tupdate_cfs_group(se);\n\t\t}\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n#else /* CONFIG_FAIR_GROUP_SCHED */\n\nvoid free_fair_sched_group(struct task_group *tg) { }\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nvoid online_fair_sched_group(struct task_group *tg) { }\n\nvoid unregister_fair_sched_group(struct task_group *tg) { }\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\nstatic unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task)\n{\n\tstruct sched_entity *se = &task->se;\n\tunsigned int rr_interval = 0;\n\n\t/*\n\t * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise\n\t * idle runqueue:\n\t */\n\tif (rq->cfs.load.weight)\n\t\trr_interval = NS_TO_JIFFIES(sched_slice(cfs_rq_of(se), se));\n\n\treturn rr_interval;\n}\n\n/*\n * All the scheduling class methods:\n */\nconst struct sched_class fair_sched_class = {\n\t.next\t\t\t= &idle_sched_class,\n\t.enqueue_task\t\t= enqueue_task_fair,\n\t.dequeue_task\t\t= dequeue_task_fair,\n\t.yield_task\t\t= yield_task_fair,\n\t.yield_to_task\t\t= yield_to_task_fair,\n\n\t.check_preempt_curr\t= check_preempt_wakeup,\n\n\t.pick_next_task\t\t= pick_next_task_fair,\n\t.put_prev_task\t\t= put_prev_task_fair,\n\n#ifdef CONFIG_SMP\n\t.select_task_rq\t\t= select_task_rq_fair,\n\t.migrate_task_rq\t= migrate_task_rq_fair,\n\n\t.rq_online\t\t= rq_online_fair,\n\t.rq_offline\t\t= rq_offline_fair,\n\n\t.task_dead\t\t= task_dead_fair,\n\t.set_cpus_allowed\t= set_cpus_allowed_common,\n#endif\n\n\t.set_curr_task          = set_curr_task_fair,\n\t.task_tick\t\t= task_tick_fair,\n\t.task_fork\t\t= task_fork_fair,\n\n\t.prio_changed\t\t= prio_changed_fair,\n\t.switched_from\t\t= switched_from_fair,\n\t.switched_to\t\t= switched_to_fair,\n\n\t.get_rr_interval\t= get_rr_interval_fair,\n\n\t.update_curr\t\t= update_curr_fair,\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t.task_change_group\t= task_change_group_fair,\n#endif\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nvoid print_cfs_stats(struct seq_file *m, int cpu)\n{\n\tstruct cfs_rq *cfs_rq, *pos;\n\n\trcu_read_lock();\n\tfor_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)\n\t\tprint_cfs_rq(m, cpu, cfs_rq);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nvoid show_numa_stats(struct task_struct *p, struct seq_file *m)\n{\n\tint node;\n\tunsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0;\n\n\tfor_each_online_node(node) {\n\t\tif (p->numa_faults) {\n\t\t\ttsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t\t\ttpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tif (p->numa_group) {\n\t\t\tgsf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 0)],\n\t\t\tgpf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tprint_numa_stats(m, node, tsf, tpf, gsf, gpf);\n\t}\n}\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_SCHED_DEBUG */\n\n__init void init_sched_fair_class(void)\n{\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tnohz.next_balance = jiffies;\n\tnohz.next_blocked = jiffies;\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n#endif\n#endif /* SMP */\n\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define MAX_PINNED_INTERVAL\t512",
      "#define LBF_NOHZ_AGAIN\t0x20",
      "#define LBF_NOHZ_STATS\t0x10",
      "#define LBF_SOME_PINNED\t0x08",
      "#define LBF_DST_PINNED  0x04",
      "#define LBF_NEED_BREAK\t0x02",
      "#define LBF_ALL_PINNED\t0x01",
      "#define SKIP_AGE_LOAD\t0x0",
      "#define UPDATE_TG\t0x0",
      "#define SKIP_AGE_LOAD\t0x2",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static unsigned int sched_nr_latency = 8;",
      "unsigned int sysctl_sched_child_runs_first",
      "const_debug unsigned int sysctl_sched_migration_cost\t= 500000UL;",
      "unsigned int capacity_margin\t\t\t\t= 1280;",
      "const struct sched_class fair_sched_class;",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "zalloc_cpumask_var",
          "args": [
            "&nohz.idle_cpus_mask",
            "GFP_NOWAIT"
          ],
          "line": 10295
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "open_softirq",
          "args": [
            "SCHED_SOFTIRQ",
            "run_rebalance_domains"
          ],
          "line": 10290
        },
        "resolved": true,
        "details": {
          "function_name": "open_softirq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/softirq.c",
          "lines": "455-458",
          "snippet": "void open_softirq(int nr, void (*action)(struct softirq_action *))\n{\n\tsoftirq_vec[nr].action = action;\n}",
          "includes": [
            "#include <trace/events/irq.h>",
            "#include <linux/irq.h>",
            "#include <linux/tick.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/smp.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/kthread.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cpu.h>",
            "#include <linux/percpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static struct softirq_action softirq_vec[NR_SOFTIRQS]"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/irq.h>\n#include <linux/irq.h>\n#include <linux/tick.h>\n#include <linux/smpboot.h>\n#include <linux/smp.h>\n#include <linux/ftrace.h>\n#include <linux/rcupdate.h>\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/cpu.h>\n#include <linux/percpu.h>\n#include <linux/notifier.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/kernel_stat.h>\n#include <linux/export.h>\n\nstatic struct softirq_action softirq_vec[NR_SOFTIRQS];\n\nvoid open_softirq(int nr, void (*action)(struct softirq_action *))\n{\n\tsoftirq_vec[nr].action = action;\n}"
        }
      },
      {
        "call_info": {
          "callee": "print_numa_stats",
          "args": [
            "m",
            "node",
            "tsf",
            "tpf",
            "gsf",
            "gpf"
          ],
          "line": 10281
        },
        "resolved": true,
        "details": {
          "function_name": "print_numa_stats",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "841-847",
          "snippet": "void print_numa_stats(struct seq_file *m, int node, unsigned long tsf,\n\t\tunsigned long tpf, unsigned long gsf, unsigned long gpf)\n{\n\tSEQ_printf(m, \"numa_faults node=%d \", node);\n\tSEQ_printf(m, \"task_private=%lu task_shared=%lu \", tpf, tsf);\n\tSEQ_printf(m, \"group_private=%lu group_shared=%lu\\n\", gpf, gsf);\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nvoid print_numa_stats(struct seq_file *m, int node, unsigned long tsf,\n\t\tunsigned long tpf, unsigned long gsf, unsigned long gpf)\n{\n\tSEQ_printf(m, \"numa_faults node=%d \", node);\n\tSEQ_printf(m, \"task_private=%lu task_shared=%lu \", tpf, tsf);\n\tSEQ_printf(m, \"group_private=%lu group_shared=%lu\\n\", gpf, gsf);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_MEM",
            "node",
            "1"
          ],
          "line": 10279
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 10263
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "print_cfs_rq",
          "args": [
            "m",
            "cpu",
            "cfs_rq"
          ],
          "line": 10262
        },
        "resolved": true,
        "details": {
          "function_name": "print_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "496-572",
          "snippet": "void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)\n{\n\ts64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1,\n\t\tspread, rq0_min_vruntime, spread0;\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct sched_entity *last;\n\tunsigned long flags;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"cfs_rq[%d]:%s\\n\", cpu, task_group_path(cfs_rq->tg));\n#else\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"cfs_rq[%d]:\\n\", cpu);\n#endif\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"exec_clock\",\n\t\t\tSPLIT_NS(cfs_rq->exec_clock));\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\tif (rb_first_cached(&cfs_rq->tasks_timeline))\n\t\tMIN_vruntime = (__pick_first_entity(cfs_rq))->vruntime;\n\tlast = __pick_last_entity(cfs_rq);\n\tif (last)\n\t\tmax_vruntime = last->vruntime;\n\tmin_vruntime = cfs_rq->min_vruntime;\n\trq0_min_vruntime = cpu_rq(0)->cfs.min_vruntime;\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"MIN_vruntime\",\n\t\t\tSPLIT_NS(MIN_vruntime));\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"min_vruntime\",\n\t\t\tSPLIT_NS(min_vruntime));\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"max_vruntime\",\n\t\t\tSPLIT_NS(max_vruntime));\n\tspread = max_vruntime - MIN_vruntime;\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"spread\",\n\t\t\tSPLIT_NS(spread));\n\tspread0 = min_vruntime - rq0_min_vruntime;\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"spread0\",\n\t\t\tSPLIT_NS(spread0));\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"nr_spread_over\",\n\t\t\tcfs_rq->nr_spread_over);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"nr_running\", cfs_rq->nr_running);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"load\", cfs_rq->load.weight);\n#ifdef CONFIG_SMP\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"runnable_weight\", cfs_rq->runnable_weight);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"load_avg\",\n\t\t\tcfs_rq->avg.load_avg);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"runnable_load_avg\",\n\t\t\tcfs_rq->avg.runnable_load_avg);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"util_avg\",\n\t\t\tcfs_rq->avg.util_avg);\n\tSEQ_printf(m, \"  .%-30s: %u\\n\", \"util_est_enqueued\",\n\t\t\tcfs_rq->avg.util_est.enqueued);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.load_avg\",\n\t\t\tcfs_rq->removed.load_avg);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.util_avg\",\n\t\t\tcfs_rq->removed.util_avg);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.runnable_sum\",\n\t\t\tcfs_rq->removed.runnable_sum);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"tg_load_avg_contrib\",\n\t\t\tcfs_rq->tg_load_avg_contrib);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"tg_load_avg\",\n\t\t\tatomic_long_read(&cfs_rq->tg->load_avg));\n#endif\n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"throttled\",\n\t\t\tcfs_rq->throttled);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"throttle_count\",\n\t\t\tcfs_rq->throttle_count);\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tprint_cfs_group_stats(m, cpu, cfs_rq->tg);\n#endif\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nvoid print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)\n{\n\ts64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1,\n\t\tspread, rq0_min_vruntime, spread0;\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct sched_entity *last;\n\tunsigned long flags;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"cfs_rq[%d]:%s\\n\", cpu, task_group_path(cfs_rq->tg));\n#else\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"cfs_rq[%d]:\\n\", cpu);\n#endif\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"exec_clock\",\n\t\t\tSPLIT_NS(cfs_rq->exec_clock));\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\tif (rb_first_cached(&cfs_rq->tasks_timeline))\n\t\tMIN_vruntime = (__pick_first_entity(cfs_rq))->vruntime;\n\tlast = __pick_last_entity(cfs_rq);\n\tif (last)\n\t\tmax_vruntime = last->vruntime;\n\tmin_vruntime = cfs_rq->min_vruntime;\n\trq0_min_vruntime = cpu_rq(0)->cfs.min_vruntime;\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"MIN_vruntime\",\n\t\t\tSPLIT_NS(MIN_vruntime));\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"min_vruntime\",\n\t\t\tSPLIT_NS(min_vruntime));\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"max_vruntime\",\n\t\t\tSPLIT_NS(max_vruntime));\n\tspread = max_vruntime - MIN_vruntime;\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"spread\",\n\t\t\tSPLIT_NS(spread));\n\tspread0 = min_vruntime - rq0_min_vruntime;\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"spread0\",\n\t\t\tSPLIT_NS(spread0));\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"nr_spread_over\",\n\t\t\tcfs_rq->nr_spread_over);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"nr_running\", cfs_rq->nr_running);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"load\", cfs_rq->load.weight);\n#ifdef CONFIG_SMP\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"runnable_weight\", cfs_rq->runnable_weight);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"load_avg\",\n\t\t\tcfs_rq->avg.load_avg);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"runnable_load_avg\",\n\t\t\tcfs_rq->avg.runnable_load_avg);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"util_avg\",\n\t\t\tcfs_rq->avg.util_avg);\n\tSEQ_printf(m, \"  .%-30s: %u\\n\", \"util_est_enqueued\",\n\t\t\tcfs_rq->avg.util_est.enqueued);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.load_avg\",\n\t\t\tcfs_rq->removed.load_avg);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.util_avg\",\n\t\t\tcfs_rq->removed.util_avg);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.runnable_sum\",\n\t\t\tcfs_rq->removed.runnable_sum);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"tg_load_avg_contrib\",\n\t\t\tcfs_rq->tg_load_avg_contrib);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"tg_load_avg\",\n\t\t\tatomic_long_read(&cfs_rq->tg->load_avg));\n#endif\n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"throttled\",\n\t\t\tcfs_rq->throttled);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"throttle_count\",\n\t\t\tcfs_rq->throttle_count);\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tprint_cfs_group_stats(m, cpu, cfs_rq->tg);\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_leaf_cfs_rq_safe",
          "args": [
            "cpu_rq(cpu)",
            "cfs_rq",
            "pos"
          ],
          "line": 10261
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 10261
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 10260
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "NS_TO_JIFFIES",
          "args": [
            "sched_slice(cfs_rq_of(se), se)"
          ],
          "line": 10207
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_slice",
          "args": [
            "cfs_rq_of(se)",
            "se"
          ],
          "line": 10207
        },
        "resolved": true,
        "details": {
          "function_name": "sched_slice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "658-678",
          "snippet": "static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 10207
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&shares_mutex"
          ],
          "line": 10178
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_unlock_irqrestore",
          "args": [
            "rq",
            "&rf"
          ],
          "line": 10174
        },
        "resolved": true,
        "details": {
          "function_name": "rq_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1138-1144",
          "snippet": "static inline void\nrq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)\n\t__releases(rq->lock)\n{\n\trq_unpin_lock(rq, rf);\n\traw_spin_unlock_irqrestore(&rq->lock, rf->flags);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);",
            "extern void activate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\nextern void activate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline void\nrq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)\n\t__releases(rq->lock)\n{\n\trq_unpin_lock(rq, rf);\n\traw_spin_unlock_irqrestore(&rq->lock, rf->flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_group",
          "args": [
            "se"
          ],
          "line": 10172
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3009-3011",
          "snippet": "static inline void update_cfs_group(struct sched_entity *se)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq_of(se)",
            "se",
            "UPDATE_TG"
          ],
          "line": 10171
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_rq_clock",
          "args": [
            "rq"
          ],
          "line": 10169
        },
        "resolved": true,
        "details": {
          "function_name": "update_rq_clock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "185-205",
          "snippet": "void update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (rq->clock_update_flags & RQCF_ACT_SKIP)\n\t\treturn;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (sched_feat(WARN_DOUBLE_CLOCK))\n\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);\n\trq->clock_update_flags |= RQCF_UPDATED;\n#endif\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\tif (delta < 0)\n\t\treturn;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (rq->clock_update_flags & RQCF_ACT_SKIP)\n\t\treturn;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (sched_feat(WARN_DOUBLE_CLOCK))\n\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);\n\trq->clock_update_flags |= RQCF_UPDATED;\n#endif\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\tif (delta < 0)\n\t\treturn;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_lock_irqsave",
          "args": [
            "rq",
            "&rf"
          ],
          "line": 10168
        },
        "resolved": true,
        "details": {
          "function_name": "rq_lock_irqsave",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1106-1112",
          "snippet": "static inline void\nrq_lock_irqsave(struct rq *rq, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\traw_spin_lock_irqsave(&rq->lock, rf->flags);\n\trq_pin_lock(rq, rf);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);",
            "extern void activate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\nextern void activate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline void\nrq_lock_irqsave(struct rq *rq, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\traw_spin_lock_irqsave(&rq->lock, rf->flags);\n\trq_pin_lock(rq, rf);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "i"
          ],
          "line": 10163
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&shares_mutex"
          ],
          "line": 10157
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "clamp",
          "args": [
            "shares",
            "scale_load(MIN_SHARES)",
            "scale_load(MAX_SHARES)"
          ],
          "line": 10155
        },
        "resolved": true,
        "details": {
          "function_name": "wq_clamp_max_active",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/workqueue.c",
          "lines": "4004-4014",
          "snippet": "static int wq_clamp_max_active(int max_active, unsigned int flags,\n\t\t\t       const char *name)\n{\n\tint lim = flags & WQ_UNBOUND ? WQ_UNBOUND_MAX_ACTIVE : WQ_MAX_ACTIVE;\n\n\tif (max_active < 1 || max_active > lim)\n\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",\n\t\t\tmax_active, name, 1, lim);\n\n\treturn clamp_val(max_active, 1, lim);\n}",
          "includes": [
            "#include <trace/events/workqueue.h>",
            "#include \"workqueue_internal.h\"",
            "#include <linux/nmi.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/rculist.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/jhash.h>",
            "#include <linux/idr.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/freezer.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/kthread.h>",
            "#include <linux/notifier.h>",
            "#include <linux/cpu.h>",
            "#include <linux/slab.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/completion.h>",
            "#include <linux/signal.h>",
            "#include <linux/init.h>",
            "#include <linux/sched.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/workqueue.h>\n#include \"workqueue_internal.h\"\n#include <linux/nmi.h>\n#include <linux/sched/isolation.h>\n#include <linux/uaccess.h>\n#include <linux/moduleparam.h>\n#include <linux/nodemask.h>\n#include <linux/rculist.h>\n#include <linux/hashtable.h>\n#include <linux/jhash.h>\n#include <linux/idr.h>\n#include <linux/lockdep.h>\n#include <linux/debug_locks.h>\n#include <linux/freezer.h>\n#include <linux/mempolicy.h>\n#include <linux/hardirq.h>\n#include <linux/kthread.h>\n#include <linux/notifier.h>\n#include <linux/cpu.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/completion.h>\n#include <linux/signal.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nstatic int wq_clamp_max_active(int max_active, unsigned int flags,\n\t\t\t       const char *name)\n{\n\tint lim = flags & WQ_UNBOUND ? WQ_UNBOUND_MAX_ACTIVE : WQ_MAX_ACTIVE;\n\n\tif (max_active < 1 || max_active > lim)\n\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",\n\t\t\tmax_active, name, 1, lim);\n\n\treturn clamp_val(max_active, 1, lim);\n}"
        }
      },
      {
        "call_info": {
          "callee": "scale_load",
          "args": [
            "MAX_SHARES"
          ],
          "line": 10155
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_load",
          "args": [
            "MIN_SHARES"
          ],
          "line": 10155
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_load_set",
          "args": [
            "&se->load",
            "NICE_0_LOAD"
          ],
          "line": 10139
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "133-137",
          "snippet": "static inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "init_cfs_rq_runtime",
          "args": [
            "cfs_rq"
          ],
          "line": 10120
        },
        "resolved": true,
        "details": {
          "function_name": "init_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5010-5010",
          "snippet": "static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 10116
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&rq->lock",
            "flags"
          ],
          "line": 10108
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_del_leaf_cfs_rq",
          "args": [
            "tg->cfs_rq[cpu]"
          ],
          "line": 10107
        },
        "resolved": true,
        "details": {
          "function_name": "list_del_leaf_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "446-448",
          "snippet": "static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&rq->lock",
            "flags"
          ],
          "line": 10106
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 10104
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "remove_entity_load_avg",
          "args": [
            "tg->se[cpu]"
          ],
          "line": 10095
        },
        "resolved": true,
        "details": {
          "function_name": "remove_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3738-3738",
          "snippet": "static inline void remove_entity_load_avg(struct sched_entity *se) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void remove_entity_load_avg(struct sched_entity *se) {}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&rq->lock"
          ],
          "line": 10083
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sync_throttle",
          "args": [
            "tg",
            "i"
          ],
          "line": 10082
        },
        "resolved": true,
        "details": {
          "function_name": "sync_throttle",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4988-4988",
          "snippet": "static inline void sync_throttle(struct task_group *tg, int cpu) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void sync_throttle(struct task_group *tg, int cpu) {}"
        }
      },
      {
        "call_info": {
          "callee": "attach_entity_cfs_rq",
          "args": [
            "se"
          ],
          "line": 10081
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&rq->lock"
          ],
          "line": 10079
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "i"
          ],
          "line": 10076
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "cfs_rq"
          ],
          "line": 10064
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/params.c",
          "lines": "73-86",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "init_entity_runnable_average",
          "args": [
            "se"
          ],
          "line": 10058
        },
        "resolved": true,
        "details": {
          "function_name": "init_entity_runnable_average",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "789-791",
          "snippet": "void init_entity_runnable_average(struct sched_entity *se)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "init_tg_cfs_entry",
          "args": [
            "tg",
            "cfs_rq",
            "se",
            "i",
            "parent->se[i]"
          ],
          "line": 10057
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kzalloc_node",
          "args": [
            "sizeof(struct sched_entity)",
            "GFP_KERNEL",
            "cpu_to_node(i)"
          ],
          "line": 10051
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "i"
          ],
          "line": 10052
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kzalloc_node",
          "args": [
            "sizeof(struct cfs_rq)",
            "GFP_KERNEL",
            "cpu_to_node(i)"
          ],
          "line": 10046
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "i"
          ],
          "line": 10047
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "init_cfs_bandwidth",
          "args": [
            "tg_cfs_bandwidth(tg)"
          ],
          "line": 10043
        },
        "resolved": true,
        "details": {
          "function_name": "init_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5007-5007",
          "snippet": "void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}"
        }
      },
      {
        "call_info": {
          "callee": "tg_cfs_bandwidth",
          "args": [
            "tg"
          ],
          "line": 10043
        },
        "resolved": true,
        "details": {
          "function_name": "tg_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5013-5016",
          "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "kcalloc",
          "args": [
            "nr_cpu_ids",
            "sizeof(se)",
            "GFP_KERNEL"
          ],
          "line": 10037
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kcalloc",
          "args": [
            "nr_cpu_ids",
            "sizeof(cfs_rq)",
            "GFP_KERNEL"
          ],
          "line": 10034
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "destroy_cfs_bandwidth",
          "args": [
            "tg_cfs_bandwidth(tg)"
          ],
          "line": 10015
        },
        "resolved": true,
        "details": {
          "function_name": "destroy_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5017-5017",
          "snippet": "static inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}"
        }
      },
      {
        "call_info": {
          "callee": "task_move_group_fair",
          "args": [
            "p"
          ],
          "line": 10006
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_set_group_fair",
          "args": [
            "p"
          ],
          "line": 10002
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "attach_task_cfs_rq",
          "args": [
            "p"
          ],
          "line": 9995
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_task_rq",
          "args": [
            "p",
            "task_cpu(p)"
          ],
          "line": 9989
        },
        "resolved": true,
        "details": {
          "function_name": "set_task_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1420-1420",
          "snippet": "static inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void __dl_clear_params(struct task_struct *p);",
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nvoid __dl_clear_params(struct task_struct *p);\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void resched_cpu(int cpu);\n\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }"
        }
      },
      {
        "call_info": {
          "callee": "task_cpu",
          "args": [
            "p"
          ],
          "line": 9989
        },
        "resolved": true,
        "details": {
          "function_name": "ignore_task_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/ftrace.c",
          "lines": "6542-6556",
          "snippet": "static void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/suspend.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_MUTEX(ftrace_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nstatic DEFINE_MUTEX(ftrace_lock);\n\nstatic void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}"
        }
      },
      {
        "call_info": {
          "callee": "detach_task_cfs_rq",
          "args": [
            "p"
          ],
          "line": 9988
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_init",
          "args": [
            "&cfs_rq->removed.lock"
          ],
          "line": 9973
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "-(1LL << 20)"
          ],
          "line": 9968
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "account_cfs_rq_runtime",
          "args": [
            "cfs_rq",
            "0"
          ],
          "line": 9961
        },
        "resolved": true,
        "details": {
          "function_name": "account_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4985-4985",
          "snippet": "static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}"
        }
      },
      {
        "call_info": {
          "callee": "set_next_entity",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 9959
        },
        "resolved": true,
        "details": {
          "function_name": "set_next_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4050-4080",
          "snippet": "static void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/* 'current' is not kept within the tree. */\n\tif (se->on_rq) {\n\t\t/*\n\t\t * Any task has to be enqueued before it get to execute on\n\t\t * a CPU. So account for the time it spent waiting on the\n\t\t * runqueue.\n\t\t */\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n\n\tupdate_stats_curr_start(cfs_rq, se);\n\tcfs_rq->curr = se;\n\n\t/*\n\t * Track our maximum slice length, if the CPU's load is at\n\t * least twice that of our own weight (i.e. dont track it\n\t * when there are only lesser-weight tasks around):\n\t */\n\tif (schedstat_enabled() && rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {\n\t\tschedstat_set(se->statistics.slice_max,\n\t\t\tmax((u64)schedstat_val(se->statistics.slice_max),\n\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime));\n\t}\n\n\tse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define UPDATE_TG\t0x0",
            "#define UPDATE_TG\t0x1"
          ],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/* 'current' is not kept within the tree. */\n\tif (se->on_rq) {\n\t\t/*\n\t\t * Any task has to be enqueued before it get to execute on\n\t\t * a CPU. So account for the time it spent waiting on the\n\t\t * runqueue.\n\t\t */\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n\n\tupdate_stats_curr_start(cfs_rq, se);\n\tcfs_rq->curr = se;\n\n\t/*\n\t * Track our maximum slice length, if the CPU's load is at\n\t * least twice that of our own weight (i.e. dont track it\n\t * when there are only lesser-weight tasks around):\n\t */\n\tif (schedstat_enabled() && rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {\n\t\tschedstat_set(se->statistics.slice_max,\n\t\t\tmax((u64)schedstat_val(se->statistics.slice_max),\n\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime));\n\t}\n\n\tse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "check_preempt_curr",
          "args": [
            "rq",
            "p",
            "0"
          ],
          "line": 9943
        },
        "resolved": true,
        "details": {
          "function_name": "check_preempt_curr_idle",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/idle.c",
          "lines": "371-374",
          "snippet": "static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int flags)\n{\n\tresched_curr(rq);\n}",
          "includes": [
            "#include <trace/events/power.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/power.h>\n#include \"sched.h\"\n\nstatic void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int flags)\n{\n\tresched_curr(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "resched_curr",
          "args": [
            "rq"
          ],
          "line": 9941
        },
        "resolved": true,
        "details": {
          "function_name": "resched_curr",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "452-474",
          "snippet": "void resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_on_rq_queued",
          "args": [
            "p"
          ],
          "line": 9934
        },
        "resolved": true,
        "details": {
          "function_name": "task_on_rq_queued",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1535-1538",
          "snippet": "static inline int task_on_rq_queued(struct task_struct *p)\n{\n\treturn p->on_rq == TASK_ON_RQ_QUEUED;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [
            "#define TASK_ON_RQ_QUEUED\t1"
          ],
          "globals_used": [
            "void __dl_clear_params(struct task_struct *p);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\n#define TASK_ON_RQ_QUEUED\t1\n\nvoid __dl_clear_params(struct task_struct *p);\n\nstatic inline int task_on_rq_queued(struct task_struct *p)\n{\n\treturn p->on_rq == TASK_ON_RQ_QUEUED;\n}"
        }
      },
      {
        "call_info": {
          "callee": "attach_task_cfs_rq",
          "args": [
            "p"
          ],
          "line": 9932
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "detach_task_cfs_rq",
          "args": [
            "p"
          ],
          "line": 9927
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vruntime_normalized",
          "args": [
            "p"
          ],
          "line": 9921
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "attach_entity_cfs_rq",
          "args": [
            "se"
          ],
          "line": 9919
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "detach_entity_cfs_rq",
          "args": [
            "se"
          ],
          "line": 9911
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "place_entity",
          "args": [
            "cfs_rq",
            "se",
            "0"
          ],
          "line": 9907
        },
        "resolved": true,
        "details": {
          "function_name": "place_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3773-3803",
          "snippet": "static void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}"
        }
      },
      {
        "call_info": {
          "callee": "vruntime_normalized",
          "args": [
            "p"
          ],
          "line": 9902
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "propagate_entity_cfs_rq",
          "args": [
            "se"
          ],
          "line": 9894
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_tg_load_avg",
          "args": [
            "cfs_rq",
            "false"
          ],
          "line": 9893
        },
        "resolved": true,
        "details": {
          "function_name": "update_tg_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3344-3344",
          "snippet": "static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}"
        }
      },
      {
        "call_info": {
          "callee": "attach_entity_load_avg",
          "args": [
            "cfs_rq",
            "se",
            "0"
          ],
          "line": 9892
        },
        "resolved": true,
        "details": {
          "function_name": "attach_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3740-3741",
          "snippet": "static inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "ATTACH_AGE_LOAD"
          ],
          "line": 9891
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "propagate_entity_cfs_rq",
          "args": [
            "se"
          ],
          "line": 9875
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "detach_entity_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 9873
        },
        "resolved": true,
        "details": {
          "function_name": "detach_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3742-3743",
          "snippet": "static inline void\ndetach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndetach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 9857
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_unlock",
          "args": [
            "rq",
            "&rf"
          ],
          "line": 9789
        },
        "resolved": true,
        "details": {
          "function_name": "double_rq_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2053-2060",
          "snippet": "static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "swap",
          "args": [
            "curr->vruntime",
            "se->vruntime"
          ],
          "line": 9784
        },
        "resolved": true,
        "details": {
          "function_name": "__migrate_swap_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "1185-1215",
          "snippet": "static void __migrate_swap_task(struct task_struct *p, int cpu)\n{\n\tif (task_on_rq_queued(p)) {\n\t\tstruct rq *src_rq, *dst_rq;\n\t\tstruct rq_flags srf, drf;\n\n\t\tsrc_rq = task_rq(p);\n\t\tdst_rq = cpu_rq(cpu);\n\n\t\trq_pin_lock(src_rq, &srf);\n\t\trq_pin_lock(dst_rq, &drf);\n\n\t\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\t\tdeactivate_task(src_rq, p, 0);\n\t\tset_task_cpu(p, cpu);\n\t\tactivate_task(dst_rq, p, 0);\n\t\tp->on_rq = TASK_ON_RQ_QUEUED;\n\t\tcheck_preempt_curr(dst_rq, p, 0);\n\n\t\trq_unpin_lock(dst_rq, &drf);\n\t\trq_unpin_lock(src_rq, &srf);\n\n\t} else {\n\t\t/*\n\t\t * Task isn't running anymore; make it appear like we migrated\n\t\t * it before it went to sleep. This means on wakeup we make the\n\t\t * previous CPU our target instead of where it really is.\n\t\t */\n\t\tp->wake_cpu = cpu;\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nstatic void __migrate_swap_task(struct task_struct *p, int cpu)\n{\n\tif (task_on_rq_queued(p)) {\n\t\tstruct rq *src_rq, *dst_rq;\n\t\tstruct rq_flags srf, drf;\n\n\t\tsrc_rq = task_rq(p);\n\t\tdst_rq = cpu_rq(cpu);\n\n\t\trq_pin_lock(src_rq, &srf);\n\t\trq_pin_lock(dst_rq, &drf);\n\n\t\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\t\tdeactivate_task(src_rq, p, 0);\n\t\tset_task_cpu(p, cpu);\n\t\tactivate_task(dst_rq, p, 0);\n\t\tp->on_rq = TASK_ON_RQ_QUEUED;\n\t\tcheck_preempt_curr(dst_rq, p, 0);\n\n\t\trq_unpin_lock(dst_rq, &drf);\n\t\trq_unpin_lock(src_rq, &srf);\n\n\t} else {\n\t\t/*\n\t\t * Task isn't running anymore; make it appear like we migrated\n\t\t * it before it went to sleep. This means on wakeup we make the\n\t\t * previous CPU our target instead of where it really is.\n\t\t */\n\t\tp->wake_cpu = cpu;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_before",
          "args": [
            "curr",
            "se"
          ],
          "line": 9779
        },
        "resolved": true,
        "details": {
          "function_name": "entity_before",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "490-494",
          "snippet": "static inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_curr",
          "args": [
            "cfs_rq"
          ],
          "line": 9774
        },
        "resolved": true,
        "details": {
          "function_name": "update_curr_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "838-841",
          "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_cfs_rq",
          "args": [
            "current"
          ],
          "line": 9771
        },
        "resolved": true,
        "details": {
          "function_name": "task_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "423-426",
          "snippet": "static inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn &task_rq(p)->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn &task_rq(p)->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_lock",
          "args": [
            "rq",
            "&rf"
          ],
          "line": 9768
        },
        "resolved": true,
        "details": {
          "function_name": "task_rq_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "90-126",
          "snippet": "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, rf->flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\t/*\n\t\t *\tmove_queued_task()\t\ttask_rq_lock()\n\t\t *\n\t\t *\tACQUIRE (rq->lock)\n\t\t *\t[S] ->on_rq = MIGRATING\t\t[L] rq = task_rq()\n\t\t *\tWMB (__set_task_cpu())\t\tACQUIRE (rq->lock);\n\t\t *\t[S] ->cpu = new_cpu\t\t[L] task_rq()\n\t\t *\t\t\t\t\t[L] ->on_rq\n\t\t *\tRELEASE (rq->lock)\n\t\t *\n\t\t * If we observe the old CPU in task_rq_lock, the acquire of\n\t\t * the old rq->lock will fully serialize against the stores.\n\t\t *\n\t\t * If we observe the new CPU in task_rq_lock, the acquire will\n\t\t * pair with the WMB to ensure we must then also see migrating.\n\t\t */\n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, rf->flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\t/*\n\t\t *\tmove_queued_task()\t\ttask_rq_lock()\n\t\t *\n\t\t *\tACQUIRE (rq->lock)\n\t\t *\t[S] ->on_rq = MIGRATING\t\t[L] rq = task_rq()\n\t\t *\tWMB (__set_task_cpu())\t\tACQUIRE (rq->lock);\n\t\t *\t[S] ->cpu = new_cpu\t\t[L] task_rq()\n\t\t *\t\t\t\t\t[L] ->on_rq\n\t\t *\tRELEASE (rq->lock)\n\t\t *\n\t\t * If we observe the old CPU in task_rq_lock, the acquire of\n\t\t * the old rq->lock will fully serialize against the stores.\n\t\t *\n\t\t * If we observe the new CPU in task_rq_lock, the acquire will\n\t\t * pair with the WMB to ensure we must then also see migrating.\n\t\t */\n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_rq",
          "args": [],
          "line": 9765
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_misfit_status",
          "args": [
            "curr",
            "rq"
          ],
          "line": 9753
        },
        "resolved": true,
        "details": {
          "function_name": "update_misfit_status",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3756-3756",
          "snippet": "static inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}"
        }
      },
      {
        "call_info": {
          "callee": "task_tick_numa",
          "args": [
            "rq",
            "curr"
          ],
          "line": 9751
        },
        "resolved": true,
        "details": {
          "function_name": "task_tick_numa",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2650-2652",
          "snippet": "static void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "static_branch_unlikely",
          "args": [
            "&sched_numa_balancing"
          ],
          "line": 9750
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "entity_tick",
          "args": [
            "cfs_rq",
            "se",
            "queued"
          ],
          "line": 9747
        },
        "resolved": true,
        "details": {
          "function_name": "entity_tick",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4169-4202",
          "snippet": "static void\nentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Ensure that runnable average is periodically updated.\n\t */\n\tupdate_load_avg(cfs_rq, curr, UPDATE_TG);\n\tupdate_cfs_group(curr);\n\n#ifdef CONFIG_SCHED_HRTICK\n\t/*\n\t * queued ticks are scheduled to match the slice, so don't bother\n\t * validating it and just reschedule.\n\t */\n\tif (queued) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\treturn;\n\t}\n\t/*\n\t * don't let the period tick interfere with the hrtick preemption\n\t */\n\tif (!sched_feat(DOUBLE_TICK) &&\n\t\t\thrtimer_active(&rq_of(cfs_rq)->hrtick_timer))\n\t\treturn;\n#endif\n\n\tif (cfs_rq->nr_running > 1)\n\t\tcheck_preempt_tick(cfs_rq, curr);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define UPDATE_TG\t0x0",
            "#define UPDATE_TG\t0x1"
          ],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void\nentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Ensure that runnable average is periodically updated.\n\t */\n\tupdate_load_avg(cfs_rq, curr, UPDATE_TG);\n\tupdate_cfs_group(curr);\n\n#ifdef CONFIG_SCHED_HRTICK\n\t/*\n\t * queued ticks are scheduled to match the slice, so don't bother\n\t * validating it and just reschedule.\n\t */\n\tif (queued) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\treturn;\n\t}\n\t/*\n\t * don't let the period tick interfere with the hrtick preemption\n\t */\n\tif (!sched_feat(DOUBLE_TICK) &&\n\t\t\thrtimer_active(&rq_of(cfs_rq)->hrtick_timer))\n\t\treturn;\n#endif\n\n\tif (cfs_rq->nr_running > 1)\n\t\tcheck_preempt_tick(cfs_rq, curr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unthrottle_offline_cfs_rqs",
          "args": [
            "rq"
          ],
          "line": 9727
        },
        "resolved": true,
        "details": {
          "function_name": "unthrottle_offline_cfs_rqs",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5019-5019",
          "snippet": "static inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}"
        }
      },
      {
        "call_info": {
          "callee": "update_sysctl",
          "args": [],
          "line": 9724
        },
        "resolved": true,
        "details": {
          "function_name": "update_sysctl",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "169-179",
          "snippet": "static void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_runtime_enabled",
          "args": [
            "rq"
          ],
          "line": 9719
        },
        "resolved": true,
        "details": {
          "function_name": "update_runtime_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5018-5018",
          "snippet": "static inline void update_runtime_enabled(struct rq *rq) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_runtime_enabled(struct rq *rq) {}"
        }
      },
      {
        "call_info": {
          "callee": "nohz_balancer_kick",
          "args": [
            "rq"
          ],
          "line": 9712
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raise_softirq",
          "args": [
            "SCHED_SOFTIRQ"
          ],
          "line": 9710
        },
        "resolved": true,
        "details": {
          "function_name": "__raise_softirq_irqoff",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/softirq.c",
          "lines": "449-453",
          "snippet": "void __raise_softirq_irqoff(unsigned int nr)\n{\n\ttrace_softirq_raise(nr);\n\tor_softirq_pending(1UL << nr);\n}",
          "includes": [
            "#include <trace/events/irq.h>",
            "#include <linux/irq.h>",
            "#include <linux/tick.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/smp.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/kthread.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cpu.h>",
            "#include <linux/percpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/irq.h>\n#include <linux/irq.h>\n#include <linux/tick.h>\n#include <linux/smpboot.h>\n#include <linux/smp.h>\n#include <linux/ftrace.h>\n#include <linux/rcupdate.h>\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/cpu.h>\n#include <linux/percpu.h>\n#include <linux/notifier.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/kernel_stat.h>\n#include <linux/export.h>\n\nvoid __raise_softirq_irqoff(unsigned int nr)\n{\n\ttrace_softirq_raise(nr);\n\tor_softirq_pending(1UL << nr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "time_after_eq",
          "args": [
            "jiffies",
            "rq->next_balance"
          ],
          "line": 9709
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "on_null_domain(rq)"
          ],
          "line": 9706
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "on_null_domain",
          "args": [
            "rq"
          ],
          "line": 9706
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rebalance_domains",
          "args": [
            "this_rq",
            "idle"
          ],
          "line": 9697
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_blocked_averages",
          "args": [
            "this_rq->cpu"
          ],
          "line": 9696
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "nohz_idle_balance",
          "args": [
            "this_rq",
            "idle"
          ],
          "line": 9692
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "this_rq",
          "args": [],
          "line": 9680
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_repin_lock",
          "args": [
            "this_rq",
            "rf"
          ],
          "line": 9669
        },
        "resolved": true,
        "details": {
          "function_name": "rq_repin_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1070-1080",
          "snippet": "static inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)\n{\n\tlockdep_repin_lock(&rq->lock, rf->cookie);\n\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * Restore the value we stashed in @rf for this pin context.\n\t */\n\trq->clock_update_flags |= rf->clock_update_flags;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)\n{\n\tlockdep_repin_lock(&rq->lock, rf->cookie);\n\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * Restore the value we stashed in @rf for this pin context.\n\t */\n\trq->clock_update_flags |= rf->clock_update_flags;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "this_rq->next_balance",
            "next_balance"
          ],
          "line": 9659
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&this_rq->lock"
          ],
          "line": 9644
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_next_balance",
          "args": [
            "sd",
            "&next_balance"
          ],
          "line": 9633
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_clock_cpu",
          "args": [
            "this_cpu"
          ],
          "line": 9626
        },
        "resolved": true,
        "details": {
          "function_name": "sched_clock_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/clock.c",
          "lines": "460-466",
          "snippet": "u64 sched_clock_cpu(int cpu)\n{\n\tif (!static_branch_unlikely(&sched_clock_running))\n\t\treturn 0;\n\n\treturn sched_clock();\n}",
          "includes": [
            "#include <linux/sched_clock.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_STATIC_KEY_FALSE(sched_clock_running);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched_clock.h>\n#include \"sched.h\"\n\nstatic DEFINE_STATIC_KEY_FALSE(sched_clock_running);\n\nu64 sched_clock_cpu(int cpu)\n{\n\tif (!static_branch_unlikely(&sched_clock_running))\n\t\treturn 0;\n\n\treturn sched_clock();\n}"
        }
      },
      {
        "call_info": {
          "callee": "load_balance",
          "args": [
            "this_cpu",
            "this_rq",
            "sd",
            "CPU_NEWLY_IDLE",
            "&continue_balancing"
          ],
          "line": 9622
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_next_balance",
          "args": [
            "sd",
            "&next_balance"
          ],
          "line": 9615
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_domain",
          "args": [
            "this_cpu",
            "sd"
          ],
          "line": 9607
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_blocked_averages",
          "args": [
            "this_cpu"
          ],
          "line": 9605
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&this_rq->lock"
          ],
          "line": 9603
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "nohz_newidle_balance",
          "args": [
            "this_rq"
          ],
          "line": 9598
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_next_balance",
          "args": [
            "sd",
            "&next_balance"
          ],
          "line": 9595
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_dereference_check_sched_domain",
          "args": [
            "this_rq->sd"
          ],
          "line": 9593
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "this_rq->rd->overload"
          ],
          "line": 9590
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_unpin_lock",
          "args": [
            "this_rq",
            "rf"
          ],
          "line": 9587
        },
        "resolved": true,
        "details": {
          "function_name": "rq_unpin_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1060-1068",
          "snippet": "static inline void rq_unpin_lock(struct rq *rq, struct rq_flags *rf)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\tif (rq->clock_update_flags > RQCF_ACT_SKIP)\n\t\trf->clock_update_flags = RQCF_UPDATED;\n#endif\n\n\tlockdep_unpin_lock(&rq->lock, rf->cookie);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [
            "#define RQCF_UPDATED\t\t0x04",
            "#define RQCF_ACT_SKIP\t\t0x02"
          ],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\n#define RQCF_UPDATED\t\t0x04\n#define RQCF_ACT_SKIP\t\t0x02\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void rq_unpin_lock(struct rq *rq, struct rq_flags *rf)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\tif (rq->clock_update_flags > RQCF_ACT_SKIP)\n\t\trf->clock_update_flags = RQCF_UPDATED;\n#endif\n\n\tlockdep_unpin_lock(&rq->lock, rf->cookie);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_active",
          "args": [
            "this_cpu"
          ],
          "line": 9578
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "this_rq"
          ],
          "line": 9573
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "kick_ilb",
          "args": [
            "NOHZ_STATS_KICK"
          ],
          "line": 9542
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "_nohz_idle_balance",
          "args": [
            "this_rq",
            "NOHZ_STATS_KICK",
            "CPU_NEWLY_IDLE"
          ],
          "line": 9541
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_before",
          "args": [
            "jiffies",
            "READ_ONCE(nohz.next_blocked)"
          ],
          "line": 9531
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "nohz.next_blocked"
          ],
          "line": 9531
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "nohz.has_blocked"
          ],
          "line": 9530
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "housekeeping_cpu",
          "args": [
            "this_cpu",
            "HK_FLAG_SCHED"
          ],
          "line": 9522
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "_nohz_idle_balance",
          "args": [
            "this_rq",
            "flags",
            "idle"
          ],
          "line": 9509
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_fetch_andnot",
          "args": [
            "NOHZ_KICK_MASK",
            "nohz_flags(this_cpu)"
          ],
          "line": 9505
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "nohz_flags",
          "args": [
            "this_cpu"
          ],
          "line": 9505
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_andnot",
          "args": [
            "NOHZ_KICK_MASK",
            "nohz_flags(this_cpu)"
          ],
          "line": 9498
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "nohz_flags",
          "args": [
            "this_cpu"
          ],
          "line": 9498
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "nohz_flags(this_cpu)"
          ],
          "line": 9494
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "nohz_flags",
          "args": [
            "this_cpu"
          ],
          "line": 9494
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "update_next_balance"
          ],
          "line": 9479
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "nohz.has_blocked",
            "1"
          ],
          "line": 9472
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "nohz.next_blocked",
            "now + msecs_to_jiffies(LOAD_AVG_PERIOD)"
          ],
          "line": 9463
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "msecs_to_jiffies",
          "args": [
            "LOAD_AVG_PERIOD"
          ],
          "line": 9464
        },
        "resolved": true,
        "details": {
          "function_name": "__msecs_to_jiffies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/time.c",
          "lines": "565-573",
          "snippet": "unsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}",
          "includes": [
            "#include \"timekeeping.h\"",
            "#include <generated/timeconst.h>",
            "#include <asm/unistd.h>",
            "#include <linux/compat.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/math64.h>",
            "#include <linux/fs.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/errno.h>",
            "#include <linux/timekeeper_internal.h>",
            "#include <linux/capability.h>",
            "#include <linux/timex.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"timekeeping.h\"\n#include <generated/timeconst.h>\n#include <asm/unistd.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/math64.h>\n#include <linux/fs.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/errno.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/capability.h>\n#include <linux/timex.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nunsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rebalance_domains",
          "args": [
            "this_rq",
            "CPU_IDLE"
          ],
          "line": 9461
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_blocked_averages",
          "args": [
            "this_cpu"
          ],
          "line": 9456
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "next_balance",
            "rq->next_balance"
          ],
          "line": 9448
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rebalance_domains",
          "args": [
            "rq",
            "CPU_IDLE"
          ],
          "line": 9445
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_load_update_idle",
          "args": [
            "rq"
          ],
          "line": 9441
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_load_update_idle",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5400-5409",
          "snippet": "static void cpu_load_update_idle(struct rq *this_rq)\n{\n\t/*\n\t * bail if there's load or we're actually up-to-date.\n\t */\n\tif (weighted_cpuload(this_rq))\n\t\treturn;\n\n\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void cpu_load_update_idle(struct rq *this_rq)\n{\n\t/*\n\t * bail if there's load or we're actually up-to-date.\n\t */\n\tif (weighted_cpuload(this_rq))\n\t\treturn;\n\n\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "time_after_eq",
          "args": [
            "jiffies",
            "rq->next_balance"
          ],
          "line": 9436
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_nohz_stats",
          "args": [
            "rq",
            "true"
          ],
          "line": 9430
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "balance_cpu"
          ],
          "line": 9428
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "need_resched",
          "args": [],
          "line": 9423
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "idle_cpu",
          "args": [
            "balance_cpu"
          ],
          "line": 9415
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "balance_cpu",
            "nohz.idle_cpus_mask"
          ],
          "line": 9414
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 9412
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "nohz.has_blocked",
            "0"
          ],
          "line": 9406
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "(flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK"
          ],
          "line": 9396
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "nohz.has_blocked",
            "1"
          ],
          "line": 9373
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_cpu_sd_state_idle",
          "args": [
            "cpu"
          ],
          "line": 9366
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb__after_atomic",
          "args": [],
          "line": 9364
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_inc",
          "args": [
            "&nohz.nr_cpus"
          ],
          "line": 9357
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_set_cpu",
          "args": [
            "cpu",
            "nohz.idle_cpus_mask"
          ],
          "line": 9356
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "on_null_domain",
          "args": [
            "rq"
          ],
          "line": 9351
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "housekeeping_cpu",
          "args": [
            "cpu",
            "HK_FLAG_SCHED"
          ],
          "line": 9331
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_active",
          "args": [
            "cpu"
          ],
          "line": 9327
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "cpu != smp_processor_id()"
          ],
          "line": 9324
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 9324
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 9322
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_dec",
          "args": [
            "&sd->shared->nr_busy_cpus"
          ],
          "line": 9311
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "per_cpu(sd_llc, cpu)"
          ],
          "line": 9305
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "per_cpu",
          "args": [
            "sd_llc",
            "cpu"
          ],
          "line": 9305
        },
        "resolved": true,
        "details": {
          "function_name": "kdb_per_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/debug/kdb/kdb_main.c",
          "lines": "2575-2640",
          "snippet": "static int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}",
          "includes": [
            "#include \"kdb_private.h\"",
            "#include <linux/slab.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/time.h>",
            "#include <linux/nmi.h>",
            "#include <linux/delay.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kdb.h>",
            "#include <linux/kgdb.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/atomic.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/utsname.h>",
            "#include <linux/smp.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched.h>",
            "#include <linux/reboot.h>",
            "#include <linux/kmsg_dump.h>",
            "#include <linux/kernel.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/ctype.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"kdb_private.h\"\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/proc_fs.h>\n#include <linux/kdebug.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/ptrace.h>\n#include <linux/time.h>\n#include <linux/nmi.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n#include <linux/kdb.h>\n#include <linux/kgdb.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/atomic.h>\n#include <linux/vmalloc.h>\n#include <linux/utsname.h>\n#include <linux/smp.h>\n#include <linux/sysrq.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched.h>\n#include <linux/reboot.h>\n#include <linux/kmsg_dump.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ctype.h>\n\nstatic int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_cpu_sd_state_busy",
          "args": [
            "rq->cpu"
          ],
          "line": 9297
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_dec",
          "args": [
            "&nohz.nr_cpus"
          ],
          "line": 9295
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_clear_cpu",
          "args": [
            "rq->cpu",
            "nohz.idle_cpus_mask"
          ],
          "line": 9294
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!rq->nohz_tick_stopped"
          ],
          "line": 9290
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "rq != this_rq()"
          ],
          "line": 9288
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "this_rq",
          "args": [],
          "line": 9288
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_inc",
          "args": [
            "&sd->shared->nr_busy_cpus"
          ],
          "line": 9281
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kick_ilb",
          "args": [
            "flags"
          ],
          "line": 9267
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_asym_prefer",
          "args": [
            "i",
            "cpu"
          ],
          "line": 9257
        },
        "resolved": true,
        "details": {
          "function_name": "sched_asym_prefer",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "701-704",
          "snippet": "static inline bool sched_asym_prefer(int a, int b)\n{\n\treturn arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nstatic inline bool sched_asym_prefer(int a, int b)\n{\n\treturn arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "i",
            "nohz.idle_cpus_mask"
          ],
          "line": 9254
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "i",
            "sched_domain_span(sd)"
          ],
          "line": 9252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "sd"
          ],
          "line": 9252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "check_cpu_capacity",
          "args": [
            "rq",
            "sd"
          ],
          "line": 9244
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&sds->nr_busy_cpus"
          ],
          "line": 9233
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_before",
          "args": [
            "now",
            "nohz.next_balance"
          ],
          "line": 9218
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "now",
            "READ_ONCE(nohz.next_blocked)"
          ],
          "line": 9215
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "nohz.next_blocked"
          ],
          "line": 9215
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "nohz.has_blocked"
          ],
          "line": 9214
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!atomic_read(&nohz.nr_cpus)"
          ],
          "line": 9211
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&nohz.nr_cpus"
          ],
          "line": 9211
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "nohz_balance_exit_idle",
          "args": [
            "rq"
          ],
          "line": 9205
        },
        "resolved": true,
        "details": {
          "function_name": "nohz_balance_exit_idle",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2105-2105",
          "snippet": "static inline void nohz_balance_exit_idle(struct rq *rq) { }",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void nohz_balance_exit_idle(struct rq *rq) { }"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "rq->idle_balance"
          ],
          "line": 9198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_send_reschedule",
          "args": [
            "ilb_cpu"
          ],
          "line": 9176
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_fetch_or",
          "args": [
            "flags",
            "nohz_flags(ilb_cpu)"
          ],
          "line": 9166
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "nohz_flags",
          "args": [
            "ilb_cpu"
          ],
          "line": 9166
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_new_ilb",
          "args": [],
          "line": 9161
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_first",
          "args": [
            "nohz.idle_cpus_mask"
          ],
          "line": 9142
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!rcu_dereference_sched(rq->sd)"
          ],
          "line": 9129
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_dereference_sched",
          "args": [
            "rq->sd"
          ],
          "line": 9129
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "nohz.next_balance",
            "rq->next_balance"
          ],
          "line": 9121
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "update_next_balance"
          ],
          "line": 9109
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "(u64)sysctl_sched_migration_cost",
            "max_cost"
          ],
          "line": 9100
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "next_balance",
            "sd->last_balance + interval"
          ],
          "line": 9089
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&balancing"
          ],
          "line": 9087
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_queued_spin_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "546-560",
          "snippet": "__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_sd_balance_interval",
          "args": [
            "sd",
            "idle != CPU_IDLE"
          ],
          "line": 9084
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "load_balance",
          "args": [
            "cpu",
            "rq",
            "sd",
            "idle",
            "&continue_balancing"
          ],
          "line": 9075
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_after_eq",
          "args": [
            "jiffies",
            "sd->last_balance + interval"
          ],
          "line": 9074
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_trylock",
          "args": [
            "&balancing"
          ],
          "line": 9070
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_trylock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "134-137",
          "snippet": "int __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)\n{\n\treturn __raw_spin_trylock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nint __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)\n{\n\treturn __raw_spin_trylock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_sd_balance_interval",
          "args": [
            "sd",
            "idle != CPU_IDLE"
          ],
          "line": 9066
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "jiffies",
            "sd->next_decay_max_lb_cost"
          ],
          "line": 9044
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_domain",
          "args": [
            "cpu",
            "sd"
          ],
          "line": 9039
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "num_online_cpus",
          "args": [],
          "line": 9017
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "local_irq_enable",
          "args": [],
          "line": 9004
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "attach_one_task",
          "args": [
            "target_rq",
            "p"
          ],
          "line": 9002
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->alb_failed"
          ],
          "line": 8993
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->alb_pushed"
          ],
          "line": 8989
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "detach_one_task",
          "args": [
            "&env"
          ],
          "line": 8987
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->alb_count"
          ],
          "line": 8984
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "sd"
          ],
          "line": 8967
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "busiest_cpu",
            "sched_domain_span(sd)"
          ],
          "line": 8963
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "sd"
          ],
          "line": 8963
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_domain",
          "args": [
            "target_cpu",
            "sd"
          ],
          "line": 8961
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "busiest_rq == target_rq"
          ],
          "line": 8957
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "busiest_cpu != smp_processor_id() ||\n\t\t     !busiest_rq->active_balance"
          ],
          "line": 8944
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 8944
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_active",
          "args": [
            "target_cpu"
          ],
          "line": 8940
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_active",
          "args": [
            "busiest_cpu"
          ],
          "line": 8940
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_lock_irq",
          "args": [
            "busiest_rq",
            "&rf"
          ],
          "line": 8934
        },
        "resolved": true,
        "details": {
          "function_name": "rq_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1114-1120",
          "snippet": "static inline void\nrq_lock_irq(struct rq *rq, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\traw_spin_lock_irq(&rq->lock);\n\trq_pin_lock(rq, rf);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void\nrq_lock_irq(struct rq *rq, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\traw_spin_lock_irq(&rq->lock);\n\trq_pin_lock(rq, rf);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "target_cpu"
          ],
          "line": 8929
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "busiest_rq"
          ],
          "line": 8927
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "*next_balance",
            "next"
          ],
          "line": 8914
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_sd_balance_interval",
          "args": [
            "sd",
            "0"
          ],
          "line": 8911
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->lb_balanced[idle]"
          ],
          "line": 8874
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!active_balance"
          ],
          "line": 8840
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "stop_one_cpu_nowait",
          "args": [
            "cpu_of(busiest)",
            "active_load_balance_cpu_stop",
            "busiest",
            "&busiest->active_balance_work"
          ],
          "line": 8829
        },
        "resolved": true,
        "details": {
          "function_name": "stop_one_cpu_nowait",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/stop_machine.c",
          "lines": "356-361",
          "snippet": "bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\n\t\t\tstruct cpu_stop_work *work_buf)\n{\n\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };\n\treturn cpu_stop_queue_work(cpu, work_buf);\n}",
          "includes": [
            "#include <linux/sched/wake_q.h>",
            "#include <linux/nmi.h>",
            "#include <linux/atomic.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/sched.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/kthread.h>",
            "#include <linux/init.h>",
            "#include <linux/cpu.h>",
            "#include <linux/completion.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/wake_q.h>\n#include <linux/nmi.h>\n#include <linux/atomic.h>\n#include <linux/smpboot.h>\n#include <linux/kallsyms.h>\n#include <linux/interrupt.h>\n#include <linux/stop_machine.h>\n#include <linux/sched.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/kthread.h>\n#include <linux/init.h>\n#include <linux/cpu.h>\n#include <linux/completion.h>\n\nbool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\n\t\t\tstruct cpu_stop_work *work_buf)\n{\n\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };\n\treturn cpu_stop_queue_work(cpu, work_buf);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "this_cpu",
            "&busiest->curr->cpus_allowed"
          ],
          "line": 8809
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "need_active_balance",
          "args": [
            "&env"
          ],
          "line": 8799
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->lb_failed[idle]"
          ],
          "line": 8789
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_subset",
          "args": [
            "cpus",
            "env.dst_grpmask"
          ],
          "line": 8779
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_clear_cpu",
          "args": [
            "cpu_of(busiest)",
            "cpus"
          ],
          "line": 8770
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "env.flags & LBF_ALL_PINNED"
          ],
          "line": 8769
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "env.new_dst_cpu"
          ],
          "line": 8745
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_clear_cpu",
          "args": [
            "env.dst_cpu",
            "env.cpus"
          ],
          "line": 8743
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "local_irq_restore",
          "args": [
            "rf.flags"
          ],
          "line": 8714
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "attach_tasks",
          "args": [
            "&env"
          ],
          "line": 8710
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "detach_tasks",
          "args": [
            "&env"
          ],
          "line": 8697
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "sysctl_sched_nr_migrate",
            "busiest->nr_running"
          ],
          "line": 8687
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_add",
          "args": [
            "sd->lb_imbalance[idle]",
            "env.imbalance"
          ],
          "line": 8673
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "busiest == env.dst_rq"
          ],
          "line": 8671
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->lb_nobusyq[idle]"
          ],
          "line": 8667
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_busiest_queue",
          "args": [
            "&env",
            "group"
          ],
          "line": 8665
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->lb_nobusyg[idle]"
          ],
          "line": 8661
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_busiest_group",
          "args": [
            "&env"
          ],
          "line": 8659
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "should_we_balance",
          "args": [
            "&env"
          ],
          "line": 8654
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->lb_count[idle]"
          ],
          "line": 8651
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_and",
          "args": [
            "cpus",
            "sched_domain_span(sd)",
            "cpu_active_mask"
          ],
          "line": 8649
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "sd"
          ],
          "line": 8649
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "LIST_HEAD_INIT",
          "args": [
            "env.tasks"
          ],
          "line": 8646
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_group_span",
          "args": [
            "sd->groups"
          ],
          "line": 8641
        },
        "resolved": true,
        "details": {
          "function_name": "sched_group_span",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1330-1333",
          "snippet": "static inline struct cpumask *sched_group_span(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->cpumask);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nstatic inline struct cpumask *sched_group_span(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->cpumask);\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_cpu_cpumask_var_ptr",
          "args": [
            "load_balance_mask"
          ],
          "line": 8635
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_balance_cpu",
          "args": [
            "sg"
          ],
          "line": 8613
        },
        "resolved": true,
        "details": {
          "function_name": "group_balance_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/topology.c",
          "lines": "501-504",
          "snippet": "int group_balance_cpu(struct sched_group *sg)\n{\n\treturn cpumask_first(group_balance_mask(sg));\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nint group_balance_cpu(struct sched_group *sg)\n{\n\treturn cpumask_first(group_balance_mask(sg));\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_cpu_and",
          "args": [
            "cpu",
            "group_balance_mask(sg)",
            "env->cpus"
          ],
          "line": 8604
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_balance_mask",
          "args": [
            "sg"
          ],
          "line": 8604
        },
        "resolved": true,
        "details": {
          "function_name": "group_balance_mask",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1338-1341",
          "snippet": "static inline struct cpumask *group_balance_mask(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->sgc->cpumask);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nstatic inline struct cpumask *group_balance_mask(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->sgc->cpumask);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "env->dst_cpu",
            "env->cpus"
          ],
          "line": 8593
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "sd->nr_balance_failed > sd->cache_nice_tries+2"
          ],
          "line": 8579
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "capacity_of",
          "args": [
            "env->dst_cpu"
          ],
          "line": 8572
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5509-5512",
          "snippet": "static unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}"
        }
      },
      {
        "call_info": {
          "callee": "check_cpu_capacity",
          "args": [
            "env->src_rq",
            "sd"
          ],
          "line": 8571
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "check_cpu_capacity",
          "args": [
            "rq",
            "env->sd"
          ],
          "line": 8517
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "rq"
          ],
          "line": 8509
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "fbq_classify_rq",
          "args": [
            "rq"
          ],
          "line": 8459
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "i"
          ],
          "line": 8458
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu_and",
          "args": [
            "i",
            "sched_group_span(group)",
            "env->cpus"
          ],
          "line": 8454
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "calculate_imbalance",
          "args": [
            "env",
            "&sds"
          ],
          "line": 8436
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_has_capacity",
          "args": [
            "env",
            "local"
          ],
          "line": 8390
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "check_asym_packing",
          "args": [
            "env",
            "&sds"
          ],
          "line": 8367
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_sd_lb_stats",
          "args": [
            "env",
            "&sds"
          ],
          "line": 8362
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "init_sd_lb_stats",
          "args": [
            "&sds"
          ],
          "line": 8356
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "fix_small_imbalance",
          "args": [
            "env",
            "sds"
          ],
          "line": 8335
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max_t",
          "args": [
            "long",
            "env->imbalance",
            "busiest->group_misfit_task_load"
          ],
          "line": 8324
        },
        "resolved": true,
        "details": {
          "function_name": "update_max_tr_single",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace.c",
          "lines": "1397-1431",
          "snippet": "void\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}",
          "includes": [
            "#include \"trace_selftest.c\"",
            "#include \"trace_output.h\"",
            "#include \"trace.h\"",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/trace.h>",
            "#include <linux/fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/poll.h>",
            "#include <linux/init.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/mount.h>",
            "#include <linux/string.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/splice.h>",
            "#include <linux/percpu.h>",
            "#include <linux/module.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/linkage.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/notifier.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/writeback.h>",
            "#include <linux/stacktrace.h>",
            "#include <generated/utsrelease.h>",
            "#include <linux/ring_buffer.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_selftest.c\"\n#include \"trace_output.h\"\n#include \"trace.h\"\n#include <linux/sched/rt.h>\n#include <linux/sched/clock.h>\n#include <linux/trace.h>\n#include <linux/fs.h>\n#include <linux/nmi.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/rwsem.h>\n#include <linux/mount.h>\n#include <linux/string.h>\n#include <linux/kdebug.h>\n#include <linux/splice.h>\n#include <linux/percpu.h>\n#include <linux/module.h>\n#include <linux/ftrace.h>\n#include <linux/vmalloc.h>\n#include <linux/uaccess.h>\n#include <linux/linkage.h>\n#include <linux/hardirq.h>\n#include <linux/pagemap.h>\n#include <linux/tracefs.h>\n#include <linux/debugfs.h>\n#include <linux/irqflags.h>\n#include <linux/notifier.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/writeback.h>\n#include <linux/stacktrace.h>\n#include <generated/utsrelease.h>\n#include <linux/ring_buffer.h>\n\nstatic __always_inline struct;\n\nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "NICE_0_LOAD"
          ],
          "line": 8301
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "fix_small_imbalance",
          "args": [
            "env",
            "sds"
          ],
          "line": 8290
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_avg_load_per_task",
          "args": [
            "env->dst_cpu"
          ],
          "line": 8206
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_avg_load_per_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5519-5529",
          "snippet": "static unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = READ_ONCE(rq->cfs.h_nr_running);\n\tunsigned long load_avg = weighted_cpuload(rq);\n\n\tif (nr_running)\n\t\treturn load_avg / nr_running;\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = READ_ONCE(rq->cfs.h_nr_running);\n\tunsigned long load_avg = weighted_cpuload(rq);\n\n\tif (nr_running)\n\t\treturn load_avg / nr_running;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DIV_ROUND_CLOSEST",
          "args": [
            "sds->busiest_stat.avg_load * sds->busiest_stat.group_capacity",
            "SCHED_CAPACITY_SCALE"
          ],
          "line": 8180
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "env->dst_rq->rd->overload",
            "overload"
          ],
          "line": 8136
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "env->dst_rq->rd->overload"
          ],
          "line": 8135
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "fbq_classify_group",
          "args": [
            "&sds->busiest_stat"
          ],
          "line": 8131
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "nohz.next_blocked",
            "jiffies + msecs_to_jiffies(LOAD_AVG_PERIOD)"
          ],
          "line": 8125
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_subset",
          "args": [
            "nohz.idle_cpus_mask",
            "sched_domain_span(env->sd)"
          ],
          "line": 8123
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "env->sd"
          ],
          "line": 8123
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_sd_pick_busiest",
          "args": [
            "env",
            "sds",
            "sg",
            "sgs"
          ],
          "line": 8107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_classify",
          "args": [
            "sg",
            "sgs"
          ],
          "line": 8104
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_has_capacity",
          "args": [
            "env",
            "local"
          ],
          "line": 8101
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_sg_lb_stats",
          "args": [
            "env",
            "sg",
            "load_idx",
            "local_group",
            "sgs",
            "&overload"
          ],
          "line": 8084
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_group_capacity",
          "args": [
            "env->sd",
            "env->dst_cpu"
          ],
          "line": 8081
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_after_eq",
          "args": [
            "jiffies",
            "sg->sgc->next_update"
          ],
          "line": 8080
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "env->dst_cpu",
            "sched_group_span(sg)"
          ],
          "line": 8074
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_sd_load_idx",
          "args": [
            "env->sd",
            "env->idle"
          ],
          "line": 8068
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "nohz.has_blocked"
          ],
          "line": 8064
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_smaller_min_cpu_capacity",
          "args": [
            "sds->local",
            "sg"
          ],
          "line": 7981
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_has_capacity",
          "args": [
            "env",
            "&sds->local_stat"
          ],
          "line": 7959
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_smaller_max_cpu_capacity",
          "args": [
            "sg",
            "sds->local"
          ],
          "line": 7958
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_classify",
          "args": [
            "group",
            "sgs"
          ],
          "line": 7928
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_is_overloaded",
          "args": [
            "env",
            "sgs"
          ],
          "line": 7927
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_util",
          "args": [
            "i"
          ],
          "line": 7893
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_util",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6204-6216",
          "snippet": "static inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}"
        }
      },
      {
        "call_info": {
          "callee": "source_load",
          "args": [
            "i",
            "load_idx"
          ],
          "line": 7890
        },
        "resolved": true,
        "details": {
          "function_name": "source_load",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5483-5492",
          "snippet": "static unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}"
        }
      },
      {
        "call_info": {
          "callee": "target_load",
          "args": [
            "i",
            "load_idx"
          ],
          "line": 7888
        },
        "resolved": true,
        "details": {
          "function_name": "target_load",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5498-5507",
          "snippet": "static unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_nohz_stats",
          "args": [
            "rq",
            "false"
          ],
          "line": 7883
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "i"
          ],
          "line": 7881
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu_and",
          "args": [
            "i",
            "sched_group_span(group)",
            "env->cpus"
          ],
          "line": 7880
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "sgs",
            "0",
            "sizeof(*sgs)"
          ],
          "line": 7878
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_blocked_averages",
          "args": [
            "cpu"
          ],
          "line": 7853
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "jiffies",
            "rq->last_blocked_load_update_tick"
          ],
          "line": 7850
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "nohz.idle_cpus_mask"
          ],
          "line": 7847
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sg_imbalanced",
          "args": [
            "group"
          ],
          "line": 7830
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!rq->sd"
          ],
          "line": 7677
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 7664
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "cpu",
            "sched_group_span(sdg)"
          ],
          "line": 7662
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_cpu_capacity",
          "args": [
            "sd",
            "cpu"
          ],
          "line": 7648
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 7630
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "arch_scale_cpu_capacity",
          "args": [
            "sd",
            "cpu"
          ],
          "line": 7625
        },
        "resolved": true,
        "details": {
          "function_name": "arch_scale_cpu_capacity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1872-1876",
          "snippet": "static __always_inline\nunsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void resched_cpu(int cpu);\n\nstatic __always_inline\nunsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 7625
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_rt_capacity",
          "args": [
            "sd",
            "cpu"
          ],
          "line": 7622
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_irq_capacity",
          "args": [
            "free",
            "irq",
            "max"
          ],
          "line": 7617
        },
        "resolved": true,
        "details": {
          "function_name": "scale_irq_capacity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2261-2265",
          "snippet": "static inline\nunsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)\n{\n\treturn util;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nstatic inline\nunsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)\n{\n\treturn util;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "used >= max"
          ],
          "line": 7612
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "rq->avg_dl.util_avg"
          ],
          "line": 7610
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "rq->avg_rt.util_avg"
          ],
          "line": 7609
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "irq >= max"
          ],
          "line": 7606
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_util_irq",
          "args": [
            "rq"
          ],
          "line": 7604
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_util_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2256-2259",
          "snippet": "static inline unsigned long cpu_util_irq(struct rq *rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline unsigned long cpu_util_irq(struct rq *rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 7599
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "others_have_blocked",
          "args": [
            "rq"
          ],
          "line": 7495
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_has_blocked",
          "args": [
            "cfs_rq"
          ],
          "line": 7495
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_irq_load_avg",
          "args": [
            "rq",
            "0"
          ],
          "line": 7492
        },
        "resolved": true,
        "details": {
          "function_name": "update_irq_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/pelt.h",
          "lines": "65-69",
          "snippet": "static inline int\nupdate_irq_load_avg(struct rq *rq, u64 running)\n{\n\treturn 0;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline int\nupdate_irq_load_avg(struct rq *rq, u64 running)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_dl_rq_load_avg",
          "args": [
            "rq_clock_task(rq)",
            "rq",
            "curr_class == &dl_sched_class"
          ],
          "line": 7491
        },
        "resolved": true,
        "details": {
          "function_name": "update_dl_rq_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/pelt.h",
          "lines": "59-63",
          "snippet": "static inline int\nupdate_dl_rq_load_avg(u64 now, struct rq *rq, int running)\n{\n\treturn 0;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline int\nupdate_dl_rq_load_avg(u64 now, struct rq *rq, int running)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_rt_rq_load_avg",
          "args": [
            "rq_clock_task(rq)",
            "rq",
            "curr_class == &rt_sched_class"
          ],
          "line": 7490
        },
        "resolved": true,
        "details": {
          "function_name": "update_rt_rq_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/pelt.h",
          "lines": "53-57",
          "snippet": "static inline int\nupdate_rt_rq_load_avg(u64 now, struct rq *rq, int running)\n{\n\treturn 0;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline int\nupdate_rt_rq_load_avg(u64 now, struct rq *rq, int running)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_rq_load_avg",
          "args": [
            "cfs_rq_clock_task(cfs_rq)",
            "cfs_rq"
          ],
          "line": 7487
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_rq_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3371-3413",
          "snippet": "static inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 7480
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "div64_ul",
          "args": [
            "p->se.avg.load_avg * cfs_rq->h_load",
            "cfs_rq_load_avg(cfs_rq) + 1"
          ],
          "line": 7474
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_load_avg",
          "args": [
            "cfs_rq"
          ],
          "line": 7475
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3591-3594",
          "snippet": "static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_rq_h_load",
          "args": [
            "cfs_rq"
          ],
          "line": 7473
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_cfs_rq",
          "args": [
            "se"
          ],
          "line": 7463
        },
        "resolved": true,
        "details": {
          "function_name": "group_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "437-440",
          "snippet": "static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "div64_ul",
          "args": [
            "load * se->avg.load_avg",
            "cfs_rq_load_avg(cfs_rq) + 1"
          ],
          "line": 7461
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "others_have_blocked",
          "args": [
            "rq"
          ],
          "line": 7420
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_has_blocked",
          "args": [
            "cfs_rq"
          ],
          "line": 7411
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_is_decayed",
          "args": [
            "cfs_rq"
          ],
          "line": 7407
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "skip_blocked_update",
          "args": [
            "se"
          ],
          "line": 7400
        },
        "resolved": true,
        "details": {
          "function_name": "skip_blocked_update",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3316-3340",
          "snippet": "static inline bool skip_blocked_update(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\n\t/*\n\t * If sched_entity still have not zero load or utilization, we have to\n\t * decay it:\n\t */\n\tif (se->avg.load_avg || se->avg.util_avg)\n\t\treturn false;\n\n\t/*\n\t * If there is a pending propagation, we have to update the load and\n\t * the utilization of the sched_entity:\n\t */\n\tif (gcfs_rq->propagate)\n\t\treturn false;\n\n\t/*\n\t * Otherwise, the load and the utilization of the sched_entity is\n\t * already zero and there is no pending propagation, so it will be a\n\t * waste of time to try to decay it:\n\t */\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline bool skip_blocked_update(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\n\t/*\n\t * If sched_entity still have not zero load or utilization, we have to\n\t * decay it:\n\t */\n\tif (se->avg.load_avg || se->avg.util_avg)\n\t\treturn false;\n\n\t/*\n\t * If there is a pending propagation, we have to update the load and\n\t * the utilization of the sched_entity:\n\t */\n\tif (gcfs_rq->propagate)\n\t\treturn false;\n\n\t/*\n\t * Otherwise, the load and the utilization of the sched_entity is\n\t * already zero and there is no pending propagation, so it will be a\n\t * waste of time to try to decay it:\n\t */\n\treturn true;\n}"
        }
      },
      {
        "call_info": {
          "callee": "throttled_hierarchy",
          "args": [
            "cfs_rq"
          ],
          "line": 7392
        },
        "resolved": true,
        "details": {
          "function_name": "throttled_hierarchy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4996-4999",
          "snippet": "static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_leaf_cfs_rq_safe",
          "args": [
            "rq",
            "cfs_rq",
            "pos"
          ],
          "line": 7388
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 7375
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "rq->avg_irq.util_avg"
          ],
          "line": 7347
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "rq->avg_dl.util_avg"
          ],
          "line": 7343
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "rq->avg_rt.util_avg"
          ],
          "line": 7340
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "attach_task",
          "args": [
            "env->dst_rq",
            "p"
          ],
          "line": 7321
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_attach_task_all",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "52-77",
          "snippet": "int cgroup_attach_task_all(struct task_struct *from, struct task_struct *tsk)\n{\n\tstruct cgroup_root *root;\n\tint retval = 0;\n\n\tmutex_lock(&cgroup_mutex);\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\tfor_each_root(root) {\n\t\tstruct cgroup *from_cgrp;\n\n\t\tif (root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\tfrom_cgrp = task_cgroup_from_root(from, root);\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\tretval = cgroup_attach_task(from_cgrp, tsk, false);\n\t\tif (retval)\n\t\t\tbreak;\n\t}\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tmutex_unlock(&cgroup_mutex);\n\n\treturn retval;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nint cgroup_attach_task_all(struct task_struct *from, struct task_struct *tsk)\n{\n\tstruct cgroup_root *root;\n\tint retval = 0;\n\n\tmutex_lock(&cgroup_mutex);\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\tfor_each_root(root) {\n\t\tstruct cgroup *from_cgrp;\n\n\t\tif (root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\tfrom_cgrp = task_cgroup_from_root(from, root);\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\tretval = cgroup_attach_task(from_cgrp, tsk, false);\n\t\tif (retval)\n\t\t\tbreak;\n\t}\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tmutex_unlock(&cgroup_mutex);\n\n\treturn retval;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_del_init",
          "args": [
            "&p->se.group_node"
          ],
          "line": 7319
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_first_entry",
          "args": [
            "tasks",
            "structtask_struct",
            "se.group_node"
          ],
          "line": 7318
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "tasks"
          ],
          "line": 7317
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "activate_task",
          "args": [
            "rq",
            "p",
            "ENQUEUE_NOCLOCK"
          ],
          "line": 7285
        },
        "resolved": true,
        "details": {
          "function_name": "deactivate_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "754-760",
          "snippet": "void deactivate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible++;\n\n\tdequeue_task(rq, p, flags);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid deactivate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible++;\n\n\tdequeue_task(rq, p, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "task_rq(p) != rq"
          ],
          "line": 7284
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_rq",
          "args": [
            "p"
          ],
          "line": 7284
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&rq->lock"
          ],
          "line": 7282
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_add",
          "args": [
            "env->sd->lb_gained[env->idle]",
            "detached"
          ],
          "line": 7272
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_move",
          "args": [
            "&p->se.group_node",
            "tasks"
          ],
          "line": 7264
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_add",
          "args": [
            "&p->se.group_node",
            "&env->tasks"
          ],
          "line": 7240
        },
        "resolved": true,
        "details": {
          "function_name": "list_add_event",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/events/core.c",
          "lines": "1662-1690",
          "snippet": "static void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\tevent->tstamp = perf_event_time(event);\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tevent->group_caps = event->event_caps;\n\t\tadd_event_to_groups(event, ctx);\n\t}\n\n\tlist_update_cgroup_event(event, ctx, true);\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n\n\tctx->generation++;\n}",
          "includes": [
            "#include <asm/irq_regs.h>",
            "#include \"internal.h\"",
            "#include <linux/mount.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/parser.h>",
            "#include <linux/namei.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf.h>",
            "#include <linux/compat.h>",
            "#include <linux/mman.h>",
            "#include <linux/module.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <linux/trace_events.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/anon_inodes.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/rculist.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/export.h>",
            "#include <linux/device.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/reboot.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/percpu.h>",
            "#include <linux/dcache.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/tick.h>",
            "#include <linux/hash.h>",
            "#include <linux/slab.h>",
            "#include <linux/poll.h>",
            "#include <linux/file.h>",
            "#include <linux/idr.h>",
            "#include <linux/smp.h>",
            "#include <linux/cpu.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void update_context_time(struct perf_event_context *ctx);",
            "static u64 perf_event_time(struct perf_event *event);",
            "static __must_check struct",
            "static void perf_log_itrace_start(struct perf_event *event);",
            "static void perf_event_free_filter(struct perf_event *event);",
            "static void perf_event_free_bpf_prog(struct perf_event *event);",
            "static int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr);",
            "static void perf_pmu_output_stop(struct perf_event *event);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <asm/irq_regs.h>\n#include \"internal.h\"\n#include <linux/mount.h>\n#include <linux/proc_ns.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/clock.h>\n#include <linux/parser.h>\n#include <linux/namei.h>\n#include <linux/filter.h>\n#include <linux/bpf.h>\n#include <linux/compat.h>\n#include <linux/mman.h>\n#include <linux/module.h>\n#include <linux/mm_types.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/trace_events.h>\n#include <linux/perf_event.h>\n#include <linux/cgroup.h>\n#include <linux/kernel_stat.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/rculist.h>\n#include <linux/hardirq.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <linux/device.h>\n#include <linux/vmstat.h>\n#include <linux/reboot.h>\n#include <linux/ptrace.h>\n#include <linux/percpu.h>\n#include <linux/dcache.h>\n#include <linux/sysfs.h>\n#include <linux/tick.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/idr.h>\n#include <linux/smp.h>\n#include <linux/cpu.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n\nstatic void update_context_time(struct perf_event_context *ctx);\nstatic u64 perf_event_time(struct perf_event *event);\nstatic __must_check struct;\nstatic void perf_log_itrace_start(struct perf_event *event);\nstatic void perf_event_free_filter(struct perf_event *event);\nstatic void perf_event_free_bpf_prog(struct perf_event *event);\nstatic int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr);\nstatic void perf_pmu_output_stop(struct perf_event *event);\n\nstatic void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\tevent->tstamp = perf_event_time(event);\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tevent->group_caps = event->event_caps;\n\t\tadd_event_to_groups(event, ctx);\n\t}\n\n\tlist_update_cgroup_event(event, ctx, true);\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n\n\tctx->generation++;\n}"
        }
      },
      {
        "call_info": {
          "callee": "detach_task",
          "args": [
            "p",
            "env"
          ],
          "line": 7239
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_h_load",
          "args": [
            "p"
          ],
          "line": 7231
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "can_migrate_task",
          "args": [
            "p",
            "env"
          ],
          "line": 7228
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_last_entry",
          "args": [
            "tasks",
            "structtask_struct",
            "se.group_node"
          ],
          "line": 7214
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&env->src_rq->lock"
          ],
          "line": 7201
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "env->sd->lb_gained[env->idle]"
          ],
          "line": 7180
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "detach_task",
          "args": [
            "p",
            "env"
          ],
          "line": 7172
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "can_migrate_task",
          "args": [
            "p",
            "env"
          ],
          "line": 7169
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_for_each_entry_reverse",
          "args": [
            "p",
            "&env->src_rq->cfs_tasks",
            "se.group_node"
          ],
          "line": 7167
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&env->src_rq->lock"
          ],
          "line": 7165
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_task_cpu",
          "args": [
            "p",
            "env->dst_cpu"
          ],
          "line": 7152
        },
        "resolved": true,
        "details": {
          "function_name": "set_task_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "1132-1182",
          "snippet": "void set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * We should never call set_task_cpu() on a blocked task,\n\t * ttwu() will sort out the placement.\n\t */\n\tWARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&\n\t\t\t!p->on_rq);\n\n\t/*\n\t * Migrating fair class task must have p->on_rq = TASK_ON_RQ_MIGRATING,\n\t * because schedstat_wait_{start,end} rebase migrating task's wait_start\n\t * time relying on p->on_rq.\n\t */\n\tWARN_ON_ONCE(p->state == TASK_RUNNING &&\n\t\t     p->sched_class == &fair_sched_class &&\n\t\t     (p->on_rq && !task_on_rq_migrating(p)));\n\n#ifdef CONFIG_LOCKDEP\n\t/*\n\t * The caller should hold either p->pi_lock or rq->lock, when changing\n\t * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks.\n\t *\n\t * sched_move_task() holds both and thus holding either pins the cgroup,\n\t * see task_group().\n\t *\n\t * Furthermore, all task_rq users should acquire both locks, see\n\t * task_rq_lock().\n\t */\n\tWARN_ON_ONCE(debug_locks && !(lockdep_is_held(&p->pi_lock) ||\n\t\t\t\t      lockdep_is_held(&task_rq(p)->lock)));\n#endif\n\t/*\n\t * Clearly, migrating tasks to offline CPUs is a fairly daft thing.\n\t */\n\tWARN_ON_ONCE(!cpu_online(new_cpu));\n#endif\n\n\ttrace_sched_migrate_task(p, new_cpu);\n\n\tif (task_cpu(p) != new_cpu) {\n\t\tif (p->sched_class->migrate_task_rq)\n\t\t\tp->sched_class->migrate_task_rq(p, new_cpu);\n\t\tp->se.nr_migrations++;\n\t\trseq_migrate(p);\n\t\tperf_event_task_migrate(p);\n\t}\n\n\t__set_task_cpu(p, new_cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * We should never call set_task_cpu() on a blocked task,\n\t * ttwu() will sort out the placement.\n\t */\n\tWARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&\n\t\t\t!p->on_rq);\n\n\t/*\n\t * Migrating fair class task must have p->on_rq = TASK_ON_RQ_MIGRATING,\n\t * because schedstat_wait_{start,end} rebase migrating task's wait_start\n\t * time relying on p->on_rq.\n\t */\n\tWARN_ON_ONCE(p->state == TASK_RUNNING &&\n\t\t     p->sched_class == &fair_sched_class &&\n\t\t     (p->on_rq && !task_on_rq_migrating(p)));\n\n#ifdef CONFIG_LOCKDEP\n\t/*\n\t * The caller should hold either p->pi_lock or rq->lock, when changing\n\t * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks.\n\t *\n\t * sched_move_task() holds both and thus holding either pins the cgroup,\n\t * see task_group().\n\t *\n\t * Furthermore, all task_rq users should acquire both locks, see\n\t * task_rq_lock().\n\t */\n\tWARN_ON_ONCE(debug_locks && !(lockdep_is_held(&p->pi_lock) ||\n\t\t\t\t      lockdep_is_held(&task_rq(p)->lock)));\n#endif\n\t/*\n\t * Clearly, migrating tasks to offline CPUs is a fairly daft thing.\n\t */\n\tWARN_ON_ONCE(!cpu_online(new_cpu));\n#endif\n\n\ttrace_sched_migrate_task(p, new_cpu);\n\n\tif (task_cpu(p) != new_cpu) {\n\t\tif (p->sched_class->migrate_task_rq)\n\t\t\tp->sched_class->migrate_task_rq(p, new_cpu);\n\t\tp->se.nr_migrations++;\n\t\trseq_migrate(p);\n\t\tperf_event_task_migrate(p);\n\t}\n\n\t__set_task_cpu(p, new_cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&env->src_rq->lock"
          ],
          "line": 7148
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "p->se.statistics.nr_failed_migrations_hot"
          ],
          "line": 7139
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "p->se.statistics.nr_forced_migrations"
          ],
          "line": 7134
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "env->sd->lb_hot_gained[env->idle]"
          ],
          "line": 7133
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_hot",
          "args": [
            "p",
            "env"
          ],
          "line": 7128
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "migrate_degrades_locality",
          "args": [
            "p",
            "env"
          ],
          "line": 7126
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "p->se.statistics.nr_failed_migrations_running"
          ],
          "line": 7116
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_running",
          "args": [
            "env->src_rq",
            "p"
          ],
          "line": 7115
        },
        "resolved": true,
        "details": {
          "function_name": "task_running",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1526-1533",
          "snippet": "static inline int task_running(struct rq *rq, struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->on_cpu;\n#else\n\treturn task_current(rq, p);\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void __dl_clear_params(struct task_struct *p);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nvoid __dl_clear_params(struct task_struct *p);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->on_cpu;\n#else\n\treturn task_current(rq, p);\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "&p->cpus_allowed"
          ],
          "line": 7102
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu_and",
          "args": [
            "cpu",
            "env->dst_grpmask",
            "env->cpus"
          ],
          "line": 7101
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "p->se.statistics.nr_failed_migrations_affine"
          ],
          "line": 7085
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "env->dst_cpu",
            "&p->cpus_allowed"
          ],
          "line": 7082
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "throttled_lb_pair",
          "args": [
            "task_group(p)",
            "env->src_cpu",
            "env->dst_cpu"
          ],
          "line": 7079
        },
        "resolved": true,
        "details": {
          "function_name": "throttled_lb_pair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5001-5005",
          "snippet": "static inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_group",
          "args": [
            "p"
          ],
          "line": 7079
        },
        "resolved": true,
        "details": {
          "function_name": "task_group_is_autogroup",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/autogroup.h",
          "lines": "44-47",
          "snippet": "static inline bool task_group_is_autogroup(struct task_group *tg)\n{\n\treturn 0;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline bool task_group_is_autogroup(struct task_group *tg)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&env->src_rq->lock"
          ],
          "line": 7070
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_weight",
          "args": [
            "p",
            "dst_nid",
            "dist"
          ],
          "line": 7048
        },
        "resolved": true,
        "details": {
          "function_name": "task_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1350-1367",
          "snippet": "static inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_weight",
          "args": [
            "p",
            "dst_nid",
            "dist"
          ],
          "line": 7045
        },
        "resolved": true,
        "details": {
          "function_name": "group_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1369-1386",
          "snippet": "static inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "node_distance",
          "args": [
            "src_nid",
            "dst_nid"
          ],
          "line": 7042
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "env->dst_cpu"
          ],
          "line": 7021
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "env->src_cpu"
          ],
          "line": 7020
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "static_branch_likely",
          "args": [
            "&sched_numa_balancing"
          ],
          "line": 7014
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "p->policy == SCHED_IDLE"
          ],
          "line": 6981
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&env->src_rq->lock"
          ],
          "line": 6976
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "yield_task_fair",
          "args": [
            "rq"
          ],
          "line": 6800
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_next_buddy",
          "args": [
            "se"
          ],
          "line": 6798
        },
        "resolved": true,
        "details": {
          "function_name": "set_next_buddy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6499-6509",
          "snippet": "static void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_skip_buddy",
          "args": [
            "se"
          ],
          "line": 6786
        },
        "resolved": true,
        "details": {
          "function_name": "set_skip_buddy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6511-10299",
          "snippet": "static void set_skip_buddy(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se)\n\t\tcfs_rq_of(se)->skip = se;\n}\n\n/*\n * Preempt the current task with a newly woken task if needed:\n */\nstatic void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct sched_entity *se = &curr->se, *pse = &p->se;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tint scale = cfs_rq->nr_running >= sched_nr_latency;\n\tint next_buddy_marked = 0;\n\n\tif (unlikely(se == pse))\n\t\treturn;\n\n\t/*\n\t * This is possible from callers such as attach_tasks(), in which we\n\t * unconditionally check_prempt_curr() after an enqueue (which may have\n\t * lead to a throttle).  This both saves work and prevents false\n\t * next-buddy nomination below.\n\t */\n\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))\n\t\treturn;\n\n\tif (sched_feat(NEXT_BUDDY) && scale && !(wake_flags & WF_FORK)) {\n\t\tset_next_buddy(pse);\n\t\tnext_buddy_marked = 1;\n\t}\n\n\t/*\n\t * We can come here with TIF_NEED_RESCHED already set from new task\n\t * wake up path.\n\t *\n\t * Note: this also catches the edge-case of curr being in a throttled\n\t * group (e.g. via set_curr_task), since update_curr() (in the\n\t * enqueue of curr) will have resulted in resched being set.  This\n\t * prevents us from potentially nominating it as a false LAST_BUDDY\n\t * below.\n\t */\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\t/* Idle tasks are by definition preempted by non-idle tasks. */\n\tif (unlikely(curr->policy == SCHED_IDLE) &&\n\t    likely(p->policy != SCHED_IDLE))\n\t\tgoto preempt;\n\n\t/*\n\t * Batch and idle tasks do not preempt non-idle tasks (their preemption\n\t * is driven by the tick):\n\t */\n\tif (unlikely(p->policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION))\n\t\treturn;\n\n\tfind_matching_se(&se, &pse);\n\tupdate_curr(cfs_rq_of(se));\n\tBUG_ON(!pse);\n\tif (wakeup_preempt_entity(se, pse) == 1) {\n\t\t/*\n\t\t * Bias pick_next to pick the sched entity that is\n\t\t * triggering this preemption.\n\t\t */\n\t\tif (!next_buddy_marked)\n\t\t\tset_next_buddy(pse);\n\t\tgoto preempt;\n\t}\n\n\treturn;\n\npreempt:\n\tresched_curr(rq);\n\t/*\n\t * Only set the backward buddy when the current task is still\n\t * on the rq. This can happen when a wakeup gets interleaved\n\t * with schedule on the ->pre_schedule() or idle_balance()\n\t * point, either of which can * drop the rq lock.\n\t *\n\t * Also, during early boot the idle thread is in the fair class,\n\t * for obvious reasons its a bad idea to schedule back to it.\n\t */\n\tif (unlikely(!se->on_rq || curr == rq->idle))\n\t\treturn;\n\n\tif (sched_feat(LAST_BUDDY) && scale && entity_is_task(se))\n\t\tset_last_buddy(se);\n}\n\nstatic struct task_struct *\npick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tstruct sched_entity *se;\n\tstruct task_struct *p;\n\tint new_tasks;\n\nagain:\n\tif (!cfs_rq->nr_running)\n\t\tgoto idle;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (prev->sched_class != &fair_sched_class)\n\t\tgoto simple;\n\n\t/*\n\t * Because of the set_next_buddy() in dequeue_task_fair() it is rather\n\t * likely that a next task is from the same cgroup as the current.\n\t *\n\t * Therefore attempt to avoid putting and setting the entire cgroup\n\t * hierarchy, only change the part that actually changes.\n\t */\n\n\tdo {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\n\t\t/*\n\t\t * Since we got here without doing put_prev_entity() we also\n\t\t * have to consider cfs_rq->curr. If it is still a runnable\n\t\t * entity, update_curr() will update its vruntime, otherwise\n\t\t * forget we've ever seen it.\n\t\t */\n\t\tif (curr) {\n\t\t\tif (curr->on_rq)\n\t\t\t\tupdate_curr(cfs_rq);\n\t\t\telse\n\t\t\t\tcurr = NULL;\n\n\t\t\t/*\n\t\t\t * This call to check_cfs_rq_runtime() will do the\n\t\t\t * throttle and dequeue its entity in the parent(s).\n\t\t\t * Therefore the nr_running test will indeed\n\t\t\t * be correct.\n\t\t\t */\n\t\t\tif (unlikely(check_cfs_rq_runtime(cfs_rq))) {\n\t\t\t\tcfs_rq = &rq->cfs;\n\n\t\t\t\tif (!cfs_rq->nr_running)\n\t\t\t\t\tgoto idle;\n\n\t\t\t\tgoto simple;\n\t\t\t}\n\t\t}\n\n\t\tse = pick_next_entity(cfs_rq, curr);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\n\t/*\n\t * Since we haven't yet done put_prev_entity and if the selected task\n\t * is a different task than we started out with, try and touch the\n\t * least amount of cfs_rqs.\n\t */\n\tif (prev != p) {\n\t\tstruct sched_entity *pse = &prev->se;\n\n\t\twhile (!(cfs_rq = is_same_group(se, pse))) {\n\t\t\tint se_depth = se->depth;\n\t\t\tint pse_depth = pse->depth;\n\n\t\t\tif (se_depth <= pse_depth) {\n\t\t\t\tput_prev_entity(cfs_rq_of(pse), pse);\n\t\t\t\tpse = parent_entity(pse);\n\t\t\t}\n\t\t\tif (se_depth >= pse_depth) {\n\t\t\t\tset_next_entity(cfs_rq_of(se), se);\n\t\t\t\tse = parent_entity(se);\n\t\t\t}\n\t\t}\n\n\t\tput_prev_entity(cfs_rq, pse);\n\t\tset_next_entity(cfs_rq, se);\n\t}\n\n\tgoto done;\nsimple:\n#endif\n\n\tput_prev_task(rq, prev);\n\n\tdo {\n\t\tse = pick_next_entity(cfs_rq, NULL);\n\t\tset_next_entity(cfs_rq, se);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\ndone: __maybe_unused;\n#ifdef CONFIG_SMP\n\t/*\n\t * Move the next running task to the front of\n\t * the list, so our cfs_tasks list becomes MRU\n\t * one.\n\t */\n\tlist_move(&p->se.group_node, &rq->cfs_tasks);\n#endif\n\n\tif (hrtick_enabled(rq))\n\t\thrtick_start_fair(rq, p);\n\n\tupdate_misfit_status(p, rq);\n\n\treturn p;\n\nidle:\n\tupdate_misfit_status(NULL, rq);\n\tnew_tasks = idle_balance(rq, rf);\n\n\t/*\n\t * Because idle_balance() releases (and re-acquires) rq->lock, it is\n\t * possible for any higher priority task to appear. In that case we\n\t * must re-start the pick_next_entity() loop.\n\t */\n\tif (new_tasks < 0)\n\t\treturn RETRY_TASK;\n\n\tif (new_tasks > 0)\n\t\tgoto again;\n\n\treturn NULL;\n}\n\n/*\n * Account for a descheduled task:\n */\nstatic void put_prev_task_fair(struct rq *rq, struct task_struct *prev)\n{\n\tstruct sched_entity *se = &prev->se;\n\tstruct cfs_rq *cfs_rq;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tput_prev_entity(cfs_rq, se);\n\t}\n}\n\n/*\n * sched_yield() is very simple\n *\n * The magic of dealing with the ->skip buddy is in pick_next_entity.\n */\nstatic void yield_task_fair(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tstruct sched_entity *se = &curr->se;\n\n\t/*\n\t * Are we the only task in the tree?\n\t */\n\tif (unlikely(rq->nr_running == 1))\n\t\treturn;\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (curr->policy != SCHED_BATCH) {\n\t\tupdate_rq_clock(rq);\n\t\t/*\n\t\t * Update run-time statistics of the 'current'.\n\t\t */\n\t\tupdate_curr(cfs_rq);\n\t\t/*\n\t\t * Tell update_rq_clock() that we've just updated,\n\t\t * so we don't do microscopic update in schedule()\n\t\t * and double the fastpath cost.\n\t\t */\n\t\trq_clock_skip_update(rq);\n\t}\n\n\tset_skip_buddy(se);\n}\n\nstatic bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/* throttled hierarchies are not runnable */\n\tif (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))\n\t\treturn false;\n\n\t/* Tell the scheduler that we'd really like pse to run next. */\n\tset_next_buddy(se);\n\n\tyield_task_fair(rq);\n\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\n/**************************************************\n * Fair scheduling class load-balancing methods.\n *\n * BASICS\n *\n * The purpose of load-balancing is to achieve the same basic fairness the\n * per-CPU scheduler provides, namely provide a proportional amount of compute\n * time to each task. This is expressed in the following equation:\n *\n *   W_i,n/P_i == W_j,n/P_j for all i,j                               (1)\n *\n * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight\n * W_i,0 is defined as:\n *\n *   W_i,0 = \\Sum_j w_i,j                                             (2)\n *\n * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight\n * is derived from the nice value as per sched_prio_to_weight[].\n *\n * The weight average is an exponential decay average of the instantaneous\n * weight:\n *\n *   W'_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0               (3)\n *\n * C_i is the compute capacity of CPU i, typically it is the\n * fraction of 'recent' time available for SCHED_OTHER task execution. But it\n * can also include other factors [XXX].\n *\n * To achieve this balance we define a measure of imbalance which follows\n * directly from (1):\n *\n *   imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j }    (4)\n *\n * We them move tasks around to minimize the imbalance. In the continuous\n * function space it is obvious this converges, in the discrete case we get\n * a few fun cases generally called infeasible weight scenarios.\n *\n * [XXX expand on:\n *     - infeasible weights;\n *     - local vs global optima in the discrete case. ]\n *\n *\n * SCHED DOMAINS\n *\n * In order to solve the imbalance equation (4), and avoid the obvious O(n^2)\n * for all i,j solution, we create a tree of CPUs that follows the hardware\n * topology where each level pairs two lower groups (or better). This results\n * in O(log n) layers. Furthermore we reduce the number of CPUs going up the\n * tree to only the first of the previous level and we decrease the frequency\n * of load-balance at each level inv. proportional to the number of CPUs in\n * the groups.\n *\n * This yields:\n *\n *     log_2 n     1     n\n *   \\Sum       { --- * --- * 2^i } = O(n)                            (5)\n *     i = 0      2^i   2^i\n *                               `- size of each group\n *         |         |     `- number of CPUs doing load-balance\n *         |         `- freq\n *         `- sum over all levels\n *\n * Coupled with a limit on how many tasks we can migrate every balance pass,\n * this makes (5) the runtime complexity of the balancer.\n *\n * An important property here is that each CPU is still (indirectly) connected\n * to every other CPU in at most O(log n) steps:\n *\n * The adjacency matrix of the resulting graph is given by:\n *\n *             log_2 n\n *   A_i,j = \\Union     (i % 2^k == 0) && i / 2^(k+1) == j / 2^(k+1)  (6)\n *             k = 0\n *\n * And you'll find that:\n *\n *   A^(log_2 n)_i,j != 0  for all i,j                                (7)\n *\n * Showing there's indeed a path between every CPU in at most O(log n) steps.\n * The task movement gives a factor of O(m), giving a convergence complexity\n * of:\n *\n *   O(nm log n),  n := nr_cpus, m := nr_tasks                        (8)\n *\n *\n * WORK CONSERVING\n *\n * In order to avoid CPUs going idle while there's still work to do, new idle\n * balancing is more aggressive and has the newly idle CPU iterate up the domain\n * tree itself instead of relying on other CPUs to bring it work.\n *\n * This adds some complexity to both (5) and (8) but it reduces the total idle\n * time.\n *\n * [XXX more?]\n *\n *\n * CGROUPS\n *\n * Cgroups make a horror show out of (2), instead of a simple sum we get:\n *\n *                                s_k,i\n *   W_i,0 = \\Sum_j \\Prod_k w_k * -----                               (9)\n *                                 S_k\n *\n * Where\n *\n *   s_k,i = \\Sum_j w_i,j,k  and  S_k = \\Sum_i s_k,i                 (10)\n *\n * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i.\n *\n * The big problem is S_k, its a global sum needed to compute a local (W_i)\n * property.\n *\n * [XXX write more on how we solve this.. _after_ merging pjt's patches that\n *      rewrite all of this once again.]\n */\n\nstatic unsigned long __read_mostly max_load_balance_interval = HZ/10;\n\nenum fbq_type { regular, remote, all };\n\nenum group_type {\n\tgroup_other = 0,\n\tgroup_misfit_task,\n\tgroup_imbalanced,\n\tgroup_overloaded,\n};\n\n#define LBF_ALL_PINNED\t0x01\n#define LBF_NEED_BREAK\t0x02\n#define LBF_DST_PINNED  0x04\n#define LBF_SOME_PINNED\t0x08\n#define LBF_NOHZ_STATS\t0x10\n#define LBF_NOHZ_AGAIN\t0x20\n\nstruct lb_env {\n\tstruct sched_domain\t*sd;\n\n\tstruct rq\t\t*src_rq;\n\tint\t\t\tsrc_cpu;\n\n\tint\t\t\tdst_cpu;\n\tstruct rq\t\t*dst_rq;\n\n\tstruct cpumask\t\t*dst_grpmask;\n\tint\t\t\tnew_dst_cpu;\n\tenum cpu_idle_type\tidle;\n\tlong\t\t\timbalance;\n\t/* The set of CPUs under consideration for load-balancing */\n\tstruct cpumask\t\t*cpus;\n\n\tunsigned int\t\tflags;\n\n\tunsigned int\t\tloop;\n\tunsigned int\t\tloop_break;\n\tunsigned int\t\tloop_max;\n\n\tenum fbq_type\t\tfbq_type;\n\tenum group_type\t\tsrc_grp_type;\n\tstruct list_head\ttasks;\n};\n\n/*\n * Is this task likely cache-hot:\n */\nstatic int task_hot(struct task_struct *p, struct lb_env *env)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(p->policy == SCHED_IDLE))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = rq_clock_task(env->src_rq) - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns 1, if task migration degrades locality\n * Returns 0, if task migration improves locality i.e migration preferred.\n * Returns -1, if task migration is not affected by locality.\n */\nstatic int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)\n{\n\tstruct numa_group *numa_group = rcu_dereference(p->numa_group);\n\tunsigned long src_weight, dst_weight;\n\tint src_nid, dst_nid, dist;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn -1;\n\n\tif (!p->numa_faults || !(env->sd->flags & SD_NUMA))\n\t\treturn -1;\n\n\tsrc_nid = cpu_to_node(env->src_cpu);\n\tdst_nid = cpu_to_node(env->dst_cpu);\n\n\tif (src_nid == dst_nid)\n\t\treturn -1;\n\n\t/* Migrating away from the preferred node is always bad. */\n\tif (src_nid == p->numa_preferred_nid) {\n\t\tif (env->src_rq->nr_running > env->src_rq->nr_preferred_running)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\t/* Encourage migration to the preferred node. */\n\tif (dst_nid == p->numa_preferred_nid)\n\t\treturn 0;\n\n\t/* Leaving a core idle is often worse than degrading locality. */\n\tif (env->idle == CPU_IDLE)\n\t\treturn -1;\n\n\tdist = node_distance(src_nid, dst_nid);\n\tif (numa_group) {\n\t\tsrc_weight = group_weight(p, src_nid, dist);\n\t\tdst_weight = group_weight(p, dst_nid, dist);\n\t} else {\n\t\tsrc_weight = task_weight(p, src_nid, dist);\n\t\tdst_weight = task_weight(p, dst_nid, dist);\n\t}\n\n\treturn dst_weight < src_weight;\n}\n\n#else\nstatic inline int migrate_degrades_locality(struct task_struct *p,\n\t\t\t\t\t     struct lb_env *env)\n{\n\treturn -1;\n}\n#endif\n\n/*\n * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?\n */\nstatic\nint can_migrate_task(struct task_struct *p, struct lb_env *env)\n{\n\tint tsk_cache_hot;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\t/*\n\t * We do not migrate tasks that are:\n\t * 1) throttled_lb_pair, or\n\t * 2) cannot be migrated to this CPU due to cpus_allowed, or\n\t * 3) running (obviously), or\n\t * 4) are cache-hot on their current CPU.\n\t */\n\tif (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(env->dst_cpu, &p->cpus_allowed)) {\n\t\tint cpu;\n\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_affine);\n\n\t\tenv->flags |= LBF_SOME_PINNED;\n\n\t\t/*\n\t\t * Remember if this task can be migrated to any other CPU in\n\t\t * our sched_group. We may want to revisit it if we couldn't\n\t\t * meet load balance goals by pulling other tasks on src_cpu.\n\t\t *\n\t\t * Avoid computing new_dst_cpu for NEWLY_IDLE or if we have\n\t\t * already computed one in current iteration.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE || (env->flags & LBF_DST_PINNED))\n\t\t\treturn 0;\n\n\t\t/* Prevent to re-select dst_cpu via env's CPUs: */\n\t\tfor_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {\n\t\t\tif (cpumask_test_cpu(cpu, &p->cpus_allowed)) {\n\t\t\t\tenv->flags |= LBF_DST_PINNED;\n\t\t\t\tenv->new_dst_cpu = cpu;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* Record that we found atleast one task that could run on dst_cpu */\n\tenv->flags &= ~LBF_ALL_PINNED;\n\n\tif (task_running(env->src_rq, p)) {\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_running);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Aggressive migration if:\n\t * 1) destination numa is preferred\n\t * 2) task is cache cold, or\n\t * 3) too many balance attempts have failed.\n\t */\n\ttsk_cache_hot = migrate_degrades_locality(p, env);\n\tif (tsk_cache_hot == -1)\n\t\ttsk_cache_hot = task_hot(p, env);\n\n\tif (tsk_cache_hot <= 0 ||\n\t    env->sd->nr_balance_failed > env->sd->cache_nice_tries) {\n\t\tif (tsk_cache_hot == 1) {\n\t\t\tschedstat_inc(env->sd->lb_hot_gained[env->idle]);\n\t\t\tschedstat_inc(p->se.statistics.nr_forced_migrations);\n\t\t}\n\t\treturn 1;\n\t}\n\n\tschedstat_inc(p->se.statistics.nr_failed_migrations_hot);\n\treturn 0;\n}\n\n/*\n * detach_task() -- detach the task for the migration specified in env\n */\nstatic void detach_task(struct task_struct *p, struct lb_env *env)\n{\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\tdeactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);\n\tset_task_cpu(p, env->dst_cpu);\n}\n\n/*\n * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as\n * part of active balancing operations within \"domain\".\n *\n * Returns a task if successful and NULL otherwise.\n */\nstatic struct task_struct *detach_one_task(struct lb_env *env)\n{\n\tstruct task_struct *p;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tlist_for_each_entry_reverse(p,\n\t\t\t&env->src_rq->cfs_tasks, se.group_node) {\n\t\tif (!can_migrate_task(p, env))\n\t\t\tcontinue;\n\n\t\tdetach_task(p, env);\n\n\t\t/*\n\t\t * Right now, this is only the second place where\n\t\t * lb_gained[env->idle] is updated (other is detach_tasks)\n\t\t * so we can safely collect stats here rather than\n\t\t * inside detach_tasks().\n\t\t */\n\t\tschedstat_inc(env->sd->lb_gained[env->idle]);\n\t\treturn p;\n\t}\n\treturn NULL;\n}\n\nstatic const unsigned int sched_nr_migrate_break = 32;\n\n/*\n * detach_tasks() -- tries to detach up to imbalance weighted load from\n * busiest_rq, as part of a balancing operation within domain \"sd\".\n *\n * Returns number of detached tasks if successful and 0 otherwise.\n */\nstatic int detach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->src_rq->cfs_tasks;\n\tstruct task_struct *p;\n\tunsigned long load;\n\tint detached = 0;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (env->imbalance <= 0)\n\t\treturn 0;\n\n\twhile (!list_empty(tasks)) {\n\t\t/*\n\t\t * We don't want to steal all, otherwise we may be treated likewise,\n\t\t * which could at worst lead to a livelock crash.\n\t\t */\n\t\tif (env->idle != CPU_NOT_IDLE && env->src_rq->nr_running <= 1)\n\t\t\tbreak;\n\n\t\tp = list_last_entry(tasks, struct task_struct, se.group_node);\n\n\t\tenv->loop++;\n\t\t/* We've more or less seen every task there is, call it quits */\n\t\tif (env->loop > env->loop_max)\n\t\t\tbreak;\n\n\t\t/* take a breather every nr_migrate tasks */\n\t\tif (env->loop > env->loop_break) {\n\t\t\tenv->loop_break += sched_nr_migrate_break;\n\t\t\tenv->flags |= LBF_NEED_BREAK;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!can_migrate_task(p, env))\n\t\t\tgoto next;\n\n\t\tload = task_h_load(p);\n\n\t\tif (sched_feat(LB_MIN) && load < 16 && !env->sd->nr_balance_failed)\n\t\t\tgoto next;\n\n\t\tif ((load / 2) > env->imbalance)\n\t\t\tgoto next;\n\n\t\tdetach_task(p, env);\n\t\tlist_add(&p->se.group_node, &env->tasks);\n\n\t\tdetached++;\n\t\tenv->imbalance -= load;\n\n#ifdef CONFIG_PREEMPT\n\t\t/*\n\t\t * NEWIDLE balancing is a source of latency, so preemptible\n\t\t * kernels will stop after the first task is detached to minimize\n\t\t * the critical section.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE)\n\t\t\tbreak;\n#endif\n\n\t\t/*\n\t\t * We only want to steal up to the prescribed amount of\n\t\t * weighted load.\n\t\t */\n\t\tif (env->imbalance <= 0)\n\t\t\tbreak;\n\n\t\tcontinue;\nnext:\n\t\tlist_move(&p->se.group_node, tasks);\n\t}\n\n\t/*\n\t * Right now, this is one of only two places we collect this stat\n\t * so we can safely collect detach_one_task() stats here rather\n\t * than inside detach_one_task().\n\t */\n\tschedstat_add(env->sd->lb_gained[env->idle], detached);\n\n\treturn detached;\n}\n\n/*\n * attach_task() -- attach the task detached by detach_task() to its new rq.\n */\nstatic void attach_task(struct rq *rq, struct task_struct *p)\n{\n\tlockdep_assert_held(&rq->lock);\n\n\tBUG_ON(task_rq(p) != rq);\n\tactivate_task(rq, p, ENQUEUE_NOCLOCK);\n\tp->on_rq = TASK_ON_RQ_QUEUED;\n\tcheck_preempt_curr(rq, p, 0);\n}\n\n/*\n * attach_one_task() -- attaches the task returned from detach_one_task() to\n * its new rq.\n */\nstatic void attach_one_task(struct rq *rq, struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\tattach_task(rq, p);\n\trq_unlock(rq, &rf);\n}\n\n/*\n * attach_tasks() -- attaches all tasks detached by detach_tasks() to their\n * new rq.\n */\nstatic void attach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->tasks;\n\tstruct task_struct *p;\n\tstruct rq_flags rf;\n\n\trq_lock(env->dst_rq, &rf);\n\tupdate_rq_clock(env->dst_rq);\n\n\twhile (!list_empty(tasks)) {\n\t\tp = list_first_entry(tasks, struct task_struct, se.group_node);\n\t\tlist_del_init(&p->se.group_node);\n\n\t\tattach_task(env->dst_rq, p);\n\t}\n\n\trq_unlock(env->dst_rq, &rf);\n}\n\nstatic inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->avg.load_avg)\n\t\treturn true;\n\n\tif (cfs_rq->avg.util_avg)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool others_have_blocked(struct rq *rq)\n{\n\tif (READ_ONCE(rq->avg_rt.util_avg))\n\t\treturn true;\n\n\tif (READ_ONCE(rq->avg_dl.util_avg))\n\t\treturn true;\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tif (READ_ONCE(rq->avg_irq.util_avg))\n\t\treturn true;\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->load.weight)\n\t\treturn false;\n\n\tif (cfs_rq->avg.load_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.util_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.runnable_load_sum)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq, *pos;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\tbool done = true;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\t/*\n\t * Iterates the task_group tree in a bottom up fashion, see\n\t * list_add_leaf_cfs_rq() for details.\n\t */\n\tfor_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {\n\t\tstruct sched_entity *se;\n\n\t\t/* throttled entities do not contribute to load */\n\t\tif (throttled_hierarchy(cfs_rq))\n\t\t\tcontinue;\n\n\t\tif (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))\n\t\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t\t/* Propagate pending load changes to the parent, if any: */\n\t\tse = cfs_rq->tg->se[cpu];\n\t\tif (se && !skip_blocked_update(se))\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, 0);\n\n\t\t/*\n\t\t * There can be a lot of idle CPU cgroups.  Don't let fully\n\t\t * decayed cfs_rqs linger on the list.\n\t\t */\n\t\tif (cfs_rq_is_decayed(cfs_rq))\n\t\t\tlist_del_leaf_cfs_rq(cfs_rq);\n\n\t\t/* Don't need periodic decay once load/util_avg are null */\n\t\tif (cfs_rq_has_blocked(cfs_rq))\n\t\t\tdone = false;\n\t}\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n\t/* Don't need periodic decay once load/util_avg are null */\n\tif (others_have_blocked(rq))\n\t\tdone = false;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (done)\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n/*\n * Compute the hierarchical load factor for cfs_rq and all its ascendants.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];\n\tunsigned long now = jiffies;\n\tunsigned long load;\n\n\tif (cfs_rq->last_h_load_update == now)\n\t\treturn;\n\n\tcfs_rq->h_load_next = NULL;\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_load_next = se;\n\t\tif (cfs_rq->last_h_load_update == now)\n\t\t\tbreak;\n\t}\n\n\tif (!se) {\n\t\tcfs_rq->h_load = cfs_rq_load_avg(cfs_rq);\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n\n\twhile ((se = cfs_rq->h_load_next) != NULL) {\n\t\tload = cfs_rq->h_load;\n\t\tload = div64_ul(load * se->avg.load_avg,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n\t\tcfs_rq = group_cfs_rq(se);\n\t\tcfs_rq->h_load = load;\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n\n\tupdate_cfs_rq_h_load(cfs_rq);\n\treturn div64_ul(p->se.avg.load_avg * cfs_rq->h_load,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n}\n#else\nstatic inline void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\tupdate_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq);\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (!cfs_rq_has_blocked(cfs_rq) && !others_have_blocked(rq))\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\treturn p->se.avg.load_avg;\n}\n#endif\n\n/********** Helpers for find_busiest_group ************************/\n\n/*\n * sg_lb_stats - stats of a sched_group required for load_balancing\n */\nstruct sg_lb_stats {\n\tunsigned long avg_load; /*Avg load across the CPUs of the group */\n\tunsigned long group_load; /* Total load over the CPUs of the group */\n\tunsigned long sum_weighted_load; /* Weighted load of group's tasks */\n\tunsigned long load_per_task;\n\tunsigned long group_capacity;\n\tunsigned long group_util; /* Total utilization of the group */\n\tunsigned int sum_nr_running; /* Nr tasks running in the group */\n\tunsigned int idle_cpus;\n\tunsigned int group_weight;\n\tenum group_type group_type;\n\tint group_no_capacity;\n\tunsigned long group_misfit_task_load; /* A CPU has a task too big for its capacity */\n#ifdef CONFIG_NUMA_BALANCING\n\tunsigned int nr_numa_running;\n\tunsigned int nr_preferred_running;\n#endif\n};\n\n/*\n * sd_lb_stats - Structure to store the statistics of a sched_domain\n *\t\t during load balancing.\n */\nstruct sd_lb_stats {\n\tstruct sched_group *busiest;\t/* Busiest group in this sd */\n\tstruct sched_group *local;\t/* Local group in this sd */\n\tunsigned long total_running;\n\tunsigned long total_load;\t/* Total load of all groups in sd */\n\tunsigned long total_capacity;\t/* Total capacity of all groups in sd */\n\tunsigned long avg_load;\t/* Average load across all groups in sd */\n\n\tstruct sg_lb_stats busiest_stat;/* Statistics of the busiest group */\n\tstruct sg_lb_stats local_stat;\t/* Statistics of the local group */\n};\n\nstatic inline void init_sd_lb_stats(struct sd_lb_stats *sds)\n{\n\t/*\n\t * Skimp on the clearing to avoid duplicate work. We can avoid clearing\n\t * local_stat because update_sg_lb_stats() does a full clear/assignment.\n\t * We must however clear busiest_stat::avg_load because\n\t * update_sd_pick_busiest() reads this before assignment.\n\t */\n\t*sds = (struct sd_lb_stats){\n\t\t.busiest = NULL,\n\t\t.local = NULL,\n\t\t.total_running = 0UL,\n\t\t.total_load = 0UL,\n\t\t.total_capacity = 0UL,\n\t\t.busiest_stat = {\n\t\t\t.avg_load = 0UL,\n\t\t\t.sum_nr_running = 0,\n\t\t\t.group_type = group_other,\n\t\t},\n\t};\n}\n\n/**\n * get_sd_load_idx - Obtain the load index for a given sched domain.\n * @sd: The sched_domain whose load_idx is to be obtained.\n * @idle: The idle status of the CPU for whose sd load_idx is obtained.\n *\n * Return: The load index.\n */\nstatic inline int get_sd_load_idx(struct sched_domain *sd,\n\t\t\t\t\tenum cpu_idle_type idle)\n{\n\tint load_idx;\n\n\tswitch (idle) {\n\tcase CPU_NOT_IDLE:\n\t\tload_idx = sd->busy_idx;\n\t\tbreak;\n\n\tcase CPU_NEWLY_IDLE:\n\t\tload_idx = sd->newidle_idx;\n\t\tbreak;\n\tdefault:\n\t\tload_idx = sd->idle_idx;\n\t\tbreak;\n\t}\n\n\treturn load_idx;\n}\n\nstatic unsigned long scale_rt_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long max = arch_scale_cpu_capacity(sd, cpu);\n\tunsigned long used, free;\n\tunsigned long irq;\n\n\tirq = cpu_util_irq(rq);\n\n\tif (unlikely(irq >= max))\n\t\treturn 1;\n\n\tused = READ_ONCE(rq->avg_rt.util_avg);\n\tused += READ_ONCE(rq->avg_dl.util_avg);\n\n\tif (unlikely(used >= max))\n\t\treturn 1;\n\n\tfree = max - used;\n\n\treturn scale_irq_capacity(free, irq, max);\n}\n\nstatic void update_cpu_capacity(struct sched_domain *sd, int cpu)\n{\n\tunsigned long capacity = scale_rt_capacity(sd, cpu);\n\tstruct sched_group *sdg = sd->groups;\n\n\tcpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(sd, cpu);\n\n\tif (!capacity)\n\t\tcapacity = 1;\n\n\tcpu_rq(cpu)->cpu_capacity = capacity;\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = capacity;\n\tsdg->sgc->max_capacity = capacity;\n}\n\nvoid update_group_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_domain *child = sd->child;\n\tstruct sched_group *group, *sdg = sd->groups;\n\tunsigned long capacity, min_capacity, max_capacity;\n\tunsigned long interval;\n\n\tinterval = msecs_to_jiffies(sd->balance_interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\tsdg->sgc->next_update = jiffies + interval;\n\n\tif (!child) {\n\t\tupdate_cpu_capacity(sd, cpu);\n\t\treturn;\n\t}\n\n\tcapacity = 0;\n\tmin_capacity = ULONG_MAX;\n\tmax_capacity = 0;\n\n\tif (child->flags & SD_OVERLAP) {\n\t\t/*\n\t\t * SD_OVERLAP domains cannot assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tfor_each_cpu(cpu, sched_group_span(sdg)) {\n\t\t\tstruct sched_group_capacity *sgc;\n\t\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\t\t/*\n\t\t\t * build_sched_domains() -> init_sched_groups_capacity()\n\t\t\t * gets here before we've attached the domains to the\n\t\t\t * runqueues.\n\t\t\t *\n\t\t\t * Use capacity_of(), which is set irrespective of domains\n\t\t\t * in update_cpu_capacity().\n\t\t\t *\n\t\t\t * This avoids capacity from being 0 and\n\t\t\t * causing divide-by-zero issues on boot.\n\t\t\t */\n\t\t\tif (unlikely(!rq->sd)) {\n\t\t\t\tcapacity += capacity_of(cpu);\n\t\t\t} else {\n\t\t\t\tsgc = rq->sd->groups->sgc;\n\t\t\t\tcapacity += sgc->capacity;\n\t\t\t}\n\n\t\t\tmin_capacity = min(capacity, min_capacity);\n\t\t\tmax_capacity = max(capacity, max_capacity);\n\t\t}\n\t} else  {\n\t\t/*\n\t\t * !SD_OVERLAP domains can assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tgroup = child->groups;\n\t\tdo {\n\t\t\tstruct sched_group_capacity *sgc = group->sgc;\n\n\t\t\tcapacity += sgc->capacity;\n\t\t\tmin_capacity = min(sgc->min_capacity, min_capacity);\n\t\t\tmax_capacity = max(sgc->max_capacity, max_capacity);\n\t\t\tgroup = group->next;\n\t\t} while (group != child->groups);\n\t}\n\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = min_capacity;\n\tsdg->sgc->max_capacity = max_capacity;\n}\n\n/*\n * Check whether the capacity of the rq has been noticeably reduced by side\n * activity. The imbalance_pct is used for the threshold.\n * Return true is the capacity is reduced\n */\nstatic inline int\ncheck_cpu_capacity(struct rq *rq, struct sched_domain *sd)\n{\n\treturn ((rq->cpu_capacity * sd->imbalance_pct) <\n\t\t\t\t(rq->cpu_capacity_orig * 100));\n}\n\n/*\n * Group imbalance indicates (and tries to solve) the problem where balancing\n * groups is inadequate due to ->cpus_allowed constraints.\n *\n * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a\n * cpumask covering 1 CPU of the first group and 3 CPUs of the second group.\n * Something like:\n *\n *\t{ 0 1 2 3 } { 4 5 6 7 }\n *\t        *     * * *\n *\n * If we were to balance group-wise we'd place two tasks in the first group and\n * two tasks in the second group. Clearly this is undesired as it will overload\n * cpu 3 and leave one of the CPUs in the second group unused.\n *\n * The current solution to this issue is detecting the skew in the first group\n * by noticing the lower domain failed to reach balance and had difficulty\n * moving tasks due to affinity constraints.\n *\n * When this is so detected; this group becomes a candidate for busiest; see\n * update_sd_pick_busiest(). And calculate_imbalance() and\n * find_busiest_group() avoid some of the usual balance conditions to allow it\n * to create an effective group imbalance.\n *\n * This is a somewhat tricky proposition since the next run might not find the\n * group imbalance and decide the groups need to be balanced again. A most\n * subtle and fragile situation.\n */\n\nstatic inline int sg_imbalanced(struct sched_group *group)\n{\n\treturn group->sgc->imbalance;\n}\n\n/*\n * group_has_capacity returns true if the group has spare capacity that could\n * be used by some tasks.\n * We consider that a group has spare capacity if the  * number of task is\n * smaller than the number of CPUs or if the utilization is lower than the\n * available capacity for CFS tasks.\n * For the latter, we use a threshold to stabilize the state, to take into\n * account the variance of the tasks' load and to return true if the available\n * capacity in meaningful for the load balancer.\n * As an example, an available capacity of 1% can appear but it doesn't make\n * any benefit for the load balance.\n */\nstatic inline bool\ngroup_has_capacity(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running < sgs->group_weight)\n\t\treturn true;\n\n\tif ((sgs->group_capacity * 100) >\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *  group_is_overloaded returns true if the group has more tasks than it can\n *  handle.\n *  group_is_overloaded is not equals to !group_has_capacity because a group\n *  with the exact right number of tasks, has no more spare capacity but is not\n *  overloaded so both group_has_capacity and group_is_overloaded return\n *  false.\n */\nstatic inline bool\ngroup_is_overloaded(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running <= sgs->group_weight)\n\t\treturn false;\n\n\tif ((sgs->group_capacity * 100) <\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * group_smaller_min_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_min_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->min_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->min_capacity * 1024;\n}\n\n/*\n * group_smaller_max_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity_orig than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_max_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->max_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->max_capacity * 1024;\n}\n\nstatic inline enum\ngroup_type group_classify(struct sched_group *group,\n\t\t\t  struct sg_lb_stats *sgs)\n{\n\tif (sgs->group_no_capacity)\n\t\treturn group_overloaded;\n\n\tif (sg_imbalanced(group))\n\t\treturn group_imbalanced;\n\n\tif (sgs->group_misfit_task_load)\n\t\treturn group_misfit_task;\n\n\treturn group_other;\n}\n\nstatic bool update_nohz_stats(struct rq *rq, bool force)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned int cpu = rq->cpu;\n\n\tif (!rq->has_blocked_load)\n\t\treturn false;\n\n\tif (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))\n\t\treturn false;\n\n\tif (!force && !time_after(jiffies, rq->last_blocked_load_update_tick))\n\t\treturn true;\n\n\tupdate_blocked_averages(cpu);\n\n\treturn rq->has_blocked_load;\n#else\n\treturn false;\n#endif\n}\n\n/**\n * update_sg_lb_stats - Update sched_group's statistics for load balancing.\n * @env: The load balancing environment.\n * @group: sched_group whose statistics are to be updated.\n * @load_idx: Load index of sched_domain of this_cpu for load calc.\n * @local_group: Does group contain this_cpu.\n * @sgs: variable to hold the statistics for this group.\n * @overload: Indicate pullable load (e.g. >1 runnable task).\n */\nstatic inline void update_sg_lb_stats(struct lb_env *env,\n\t\t\tstruct sched_group *group, int load_idx,\n\t\t\tint local_group, struct sg_lb_stats *sgs,\n\t\t\tbool *overload)\n{\n\tunsigned long load;\n\tint i, nr_running;\n\n\tmemset(sgs, 0, sizeof(*sgs));\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tstruct rq *rq = cpu_rq(i);\n\n\t\tif ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))\n\t\t\tenv->flags |= LBF_NOHZ_AGAIN;\n\n\t\t/* Bias balancing toward CPUs of our domain: */\n\t\tif (local_group)\n\t\t\tload = target_load(i, load_idx);\n\t\telse\n\t\t\tload = source_load(i, load_idx);\n\n\t\tsgs->group_load += load;\n\t\tsgs->group_util += cpu_util(i);\n\t\tsgs->sum_nr_running += rq->cfs.h_nr_running;\n\n\t\tnr_running = rq->nr_running;\n\t\tif (nr_running > 1)\n\t\t\t*overload = true;\n\n#ifdef CONFIG_NUMA_BALANCING\n\t\tsgs->nr_numa_running += rq->nr_numa_running;\n\t\tsgs->nr_preferred_running += rq->nr_preferred_running;\n#endif\n\t\tsgs->sum_weighted_load += weighted_cpuload(rq);\n\t\t/*\n\t\t * No need to call idle_cpu() if nr_running is not 0\n\t\t */\n\t\tif (!nr_running && idle_cpu(i))\n\t\t\tsgs->idle_cpus++;\n\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    sgs->group_misfit_task_load < rq->misfit_task_load) {\n\t\t\tsgs->group_misfit_task_load = rq->misfit_task_load;\n\t\t\t*overload = 1;\n\t\t}\n\t}\n\n\t/* Adjust by relative CPU capacity of the group */\n\tsgs->group_capacity = group->sgc->capacity;\n\tsgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;\n\n\tif (sgs->sum_nr_running)\n\t\tsgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;\n\n\tsgs->group_weight = group->group_weight;\n\n\tsgs->group_no_capacity = group_is_overloaded(env, sgs);\n\tsgs->group_type = group_classify(group, sgs);\n}\n\n/**\n * update_sd_pick_busiest - return 1 on busiest group\n * @env: The load balancing environment.\n * @sds: sched_domain statistics\n * @sg: sched_group candidate to be checked for being the busiest\n * @sgs: sched_group statistics\n *\n * Determine if @sg is a busier group than the previously selected\n * busiest group.\n *\n * Return: %true if @sg is a busier group than the previously selected\n * busiest group. %false otherwise.\n */\nstatic bool update_sd_pick_busiest(struct lb_env *env,\n\t\t\t\t   struct sd_lb_stats *sds,\n\t\t\t\t   struct sched_group *sg,\n\t\t\t\t   struct sg_lb_stats *sgs)\n{\n\tstruct sg_lb_stats *busiest = &sds->busiest_stat;\n\n\t/*\n\t * Don't try to pull misfit tasks we can't help.\n\t * We can use max_capacity here as reduction in capacity on some\n\t * CPUs in the group should either be possible to resolve\n\t * internally or be covered by avg_load imbalance (eventually).\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    (!group_smaller_max_cpu_capacity(sg, sds->local) ||\n\t     !group_has_capacity(env, &sds->local_stat)))\n\t\treturn false;\n\n\tif (sgs->group_type > busiest->group_type)\n\t\treturn true;\n\n\tif (sgs->group_type < busiest->group_type)\n\t\treturn false;\n\n\tif (sgs->avg_load <= busiest->avg_load)\n\t\treturn false;\n\n\tif (!(env->sd->flags & SD_ASYM_CPUCAPACITY))\n\t\tgoto asym_packing;\n\n\t/*\n\t * Candidate sg has no more than one task per CPU and\n\t * has higher per-CPU capacity. Migrating tasks to less\n\t * capable CPUs may harm throughput. Maximize throughput,\n\t * power/energy consequences are not considered.\n\t */\n\tif (sgs->sum_nr_running <= sgs->group_weight &&\n\t    group_smaller_min_cpu_capacity(sds->local, sg))\n\t\treturn false;\n\n\t/*\n\t * If we have more than one misfit sg go with the biggest misfit.\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    sgs->group_misfit_task_load < busiest->group_misfit_task_load)\n\t\treturn false;\n\nasym_packing:\n\t/* This is the busiest node in its class. */\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn true;\n\n\t/* No ASYM_PACKING if target CPU is already busy */\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn true;\n\t/*\n\t * ASYM_PACKING needs to move all the work to the highest\n\t * prority CPUs in the group, therefore mark all groups\n\t * of lower priority than ourself as busy.\n\t */\n\tif (sgs->sum_nr_running &&\n\t    sched_asym_prefer(env->dst_cpu, sg->asym_prefer_cpu)) {\n\t\tif (!sds->busiest)\n\t\t\treturn true;\n\n\t\t/* Prefer to move from lowest priority CPU's work */\n\t\tif (sched_asym_prefer(sds->busiest->asym_prefer_cpu,\n\t\t\t\t      sg->asym_prefer_cpu))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running > sgs->nr_numa_running)\n\t\treturn regular;\n\tif (sgs->sum_nr_running > sgs->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\tif (rq->nr_running > rq->nr_numa_running)\n\t\treturn regular;\n\tif (rq->nr_running > rq->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n#else\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\treturn regular;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n/**\n * update_sd_lb_stats - Update sched_domain's statistics for load balancing.\n * @env: The load balancing environment.\n * @sds: variable to hold the statistics for this sched_domain.\n */\nstatic inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tstruct sched_domain *child = env->sd->child;\n\tstruct sched_group *sg = env->sd->groups;\n\tstruct sg_lb_stats *local = &sds->local_stat;\n\tstruct sg_lb_stats tmp_sgs;\n\tint load_idx;\n\tbool overload = false;\n\tbool prefer_sibling = child && child->flags & SD_PREFER_SIBLING;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))\n\t\tenv->flags |= LBF_NOHZ_STATS;\n#endif\n\n\tload_idx = get_sd_load_idx(env->sd, env->idle);\n\n\tdo {\n\t\tstruct sg_lb_stats *sgs = &tmp_sgs;\n\t\tint local_group;\n\n\t\tlocal_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));\n\t\tif (local_group) {\n\t\t\tsds->local = sg;\n\t\t\tsgs = local;\n\n\t\t\tif (env->idle != CPU_NEWLY_IDLE ||\n\t\t\t    time_after_eq(jiffies, sg->sgc->next_update))\n\t\t\t\tupdate_group_capacity(env->sd, env->dst_cpu);\n\t\t}\n\n\t\tupdate_sg_lb_stats(env, sg, load_idx, local_group, sgs,\n\t\t\t\t\t\t&overload);\n\n\t\tif (local_group)\n\t\t\tgoto next_group;\n\n\t\t/*\n\t\t * In case the child domain prefers tasks go to siblings\n\t\t * first, lower the sg capacity so that we'll try\n\t\t * and move all the excess tasks away. We lower the capacity\n\t\t * of a group only if the local group has the capacity to fit\n\t\t * these excess tasks. The extra check prevents the case where\n\t\t * you always pull from the heaviest group when it is already\n\t\t * under-utilized (possible with a large weight task outweighs\n\t\t * the tasks on the system).\n\t\t */\n\t\tif (prefer_sibling && sds->local &&\n\t\t    group_has_capacity(env, local) &&\n\t\t    (sgs->sum_nr_running > local->sum_nr_running + 1)) {\n\t\t\tsgs->group_no_capacity = 1;\n\t\t\tsgs->group_type = group_classify(sg, sgs);\n\t\t}\n\n\t\tif (update_sd_pick_busiest(env, sds, sg, sgs)) {\n\t\t\tsds->busiest = sg;\n\t\t\tsds->busiest_stat = *sgs;\n\t\t}\n\nnext_group:\n\t\t/* Now, start updating sd_lb_stats */\n\t\tsds->total_running += sgs->sum_nr_running;\n\t\tsds->total_load += sgs->group_load;\n\t\tsds->total_capacity += sgs->group_capacity;\n\n\t\tsg = sg->next;\n\t} while (sg != env->sd->groups);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif ((env->flags & LBF_NOHZ_AGAIN) &&\n\t    cpumask_subset(nohz.idle_cpus_mask, sched_domain_span(env->sd))) {\n\n\t\tWRITE_ONCE(nohz.next_blocked,\n\t\t\t   jiffies + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\t}\n#endif\n\n\tif (env->sd->flags & SD_NUMA)\n\t\tenv->fbq_type = fbq_classify_group(&sds->busiest_stat);\n\n\tif (!env->sd->parent) {\n\t\t/* update overload indicator if we are at root domain */\n\t\tif (READ_ONCE(env->dst_rq->rd->overload) != overload)\n\t\t\tWRITE_ONCE(env->dst_rq->rd->overload, overload);\n\t}\n}\n\n/**\n * check_asym_packing - Check to see if the group is packed into the\n *\t\t\tsched domain.\n *\n * This is primarily intended to used at the sibling level.  Some\n * cores like POWER7 prefer to use lower numbered SMT threads.  In the\n * case of POWER7, it can move to lower SMT modes only when higher\n * threads are idle.  When in lower SMT modes, the threads will\n * perform better since they share less core resources.  Hence when we\n * have idle threads, we want them to be the higher ones.\n *\n * This packing function is run on idle threads.  It checks to see if\n * the busiest CPU in this domain (core in the P7 case) has a higher\n * CPU number than the packing function is being run on.  Here we are\n * assuming lower CPU number will be equivalent to lower a SMT thread\n * number.\n *\n * Return: 1 when packing is required and a task should be moved to\n * this CPU.  The amount of the imbalance is returned in env->imbalance.\n *\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain which is to be packed\n */\nstatic int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tint busiest_cpu;\n\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn 0;\n\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn 0;\n\n\tif (!sds->busiest)\n\t\treturn 0;\n\n\tbusiest_cpu = sds->busiest->asym_prefer_cpu;\n\tif (sched_asym_prefer(busiest_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tenv->imbalance = DIV_ROUND_CLOSEST(\n\t\tsds->busiest_stat.avg_load * sds->busiest_stat.group_capacity,\n\t\tSCHED_CAPACITY_SCALE);\n\n\treturn 1;\n}\n\n/**\n * fix_small_imbalance - Calculate the minor imbalance that exists\n *\t\t\tamongst the groups of a sched_domain, during\n *\t\t\tload balancing.\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline\nvoid fix_small_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long tmp, capa_now = 0, capa_move = 0;\n\tunsigned int imbn = 2;\n\tunsigned long scaled_busy_load_per_task;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (!local->sum_nr_running)\n\t\tlocal->load_per_task = cpu_avg_load_per_task(env->dst_cpu);\n\telse if (busiest->load_per_task > local->load_per_task)\n\t\timbn = 1;\n\n\tscaled_busy_load_per_task =\n\t\t(busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\tbusiest->group_capacity;\n\n\tif (busiest->avg_load + scaled_busy_load_per_task >=\n\t    local->avg_load + (scaled_busy_load_per_task * imbn)) {\n\t\tenv->imbalance = busiest->load_per_task;\n\t\treturn;\n\t}\n\n\t/*\n\t * OK, we don't have enough imbalance to justify moving tasks,\n\t * however we may be able to increase total CPU capacity used by\n\t * moving them.\n\t */\n\n\tcapa_now += busiest->group_capacity *\n\t\t\tmin(busiest->load_per_task, busiest->avg_load);\n\tcapa_now += local->group_capacity *\n\t\t\tmin(local->load_per_task, local->avg_load);\n\tcapa_now /= SCHED_CAPACITY_SCALE;\n\n\t/* Amount of load we'd subtract */\n\tif (busiest->avg_load > scaled_busy_load_per_task) {\n\t\tcapa_move += busiest->group_capacity *\n\t\t\t    min(busiest->load_per_task,\n\t\t\t\tbusiest->avg_load - scaled_busy_load_per_task);\n\t}\n\n\t/* Amount of load we'd add */\n\tif (busiest->avg_load * busiest->group_capacity <\n\t    busiest->load_per_task * SCHED_CAPACITY_SCALE) {\n\t\ttmp = (busiest->avg_load * busiest->group_capacity) /\n\t\t      local->group_capacity;\n\t} else {\n\t\ttmp = (busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\t      local->group_capacity;\n\t}\n\tcapa_move += local->group_capacity *\n\t\t    min(local->load_per_task, local->avg_load + tmp);\n\tcapa_move /= SCHED_CAPACITY_SCALE;\n\n\t/* Move if we gain throughput */\n\tif (capa_move > capa_now)\n\t\tenv->imbalance = busiest->load_per_task;\n}\n\n/**\n * calculate_imbalance - Calculate the amount of imbalance present within the\n *\t\t\t groups of a given sched_domain during load balance.\n * @env: load balance environment\n * @sds: statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long max_pull, load_above_capacity = ~0UL;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (busiest->group_type == group_imbalanced) {\n\t\t/*\n\t\t * In the group_imb case we cannot rely on group-wide averages\n\t\t * to ensure CPU-load equilibrium, look at wider averages. XXX\n\t\t */\n\t\tbusiest->load_per_task =\n\t\t\tmin(busiest->load_per_task, sds->avg_load);\n\t}\n\n\t/*\n\t * Avg load of busiest sg can be less and avg load of local sg can\n\t * be greater than avg load across all sgs of sd because avg load\n\t * factors in sg capacity and sgs with smaller group_type are\n\t * skipped when updating the busiest sg:\n\t */\n\tif (busiest->group_type != group_misfit_task &&\n\t    (busiest->avg_load <= sds->avg_load ||\n\t     local->avg_load >= sds->avg_load)) {\n\t\tenv->imbalance = 0;\n\t\treturn fix_small_imbalance(env, sds);\n\t}\n\n\t/*\n\t * If there aren't any idle CPUs, avoid creating some.\n\t */\n\tif (busiest->group_type == group_overloaded &&\n\t    local->group_type   == group_overloaded) {\n\t\tload_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;\n\t\tif (load_above_capacity > busiest->group_capacity) {\n\t\t\tload_above_capacity -= busiest->group_capacity;\n\t\t\tload_above_capacity *= scale_load_down(NICE_0_LOAD);\n\t\t\tload_above_capacity /= busiest->group_capacity;\n\t\t} else\n\t\t\tload_above_capacity = ~0UL;\n\t}\n\n\t/*\n\t * We're trying to get all the CPUs to the average_load, so we don't\n\t * want to push ourselves above the average load, nor do we wish to\n\t * reduce the max loaded CPU below the average load. At the same time,\n\t * we also don't want to reduce the group load below the group\n\t * capacity. Thus we look for the minimum possible imbalance.\n\t */\n\tmax_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);\n\n\t/* How much load to actually move to equalise the imbalance */\n\tenv->imbalance = min(\n\t\tmax_pull * busiest->group_capacity,\n\t\t(sds->avg_load - local->avg_load) * local->group_capacity\n\t) / SCHED_CAPACITY_SCALE;\n\n\t/* Boost imbalance to allow misfit task to be balanced. */\n\tif (busiest->group_type == group_misfit_task) {\n\t\tenv->imbalance = max_t(long, env->imbalance,\n\t\t\t\t       busiest->group_misfit_task_load);\n\t}\n\n\t/*\n\t * if *imbalance is less than the average load per runnable task\n\t * there is no guarantee that any tasks will be moved so we'll have\n\t * a think about bumping its value to force at least one task to be\n\t * moved\n\t */\n\tif (env->imbalance < busiest->load_per_task)\n\t\treturn fix_small_imbalance(env, sds);\n}\n\n/******* find_busiest_group() helpers end here *********************/\n\n/**\n * find_busiest_group - Returns the busiest group within the sched_domain\n * if there is an imbalance.\n *\n * Also calculates the amount of weighted load which should be moved\n * to restore balance.\n *\n * @env: The load balancing environment.\n *\n * Return:\t- The busiest group if imbalance exists.\n */\nstatic struct sched_group *find_busiest_group(struct lb_env *env)\n{\n\tstruct sg_lb_stats *local, *busiest;\n\tstruct sd_lb_stats sds;\n\n\tinit_sd_lb_stats(&sds);\n\n\t/*\n\t * Compute the various statistics relavent for load balancing at\n\t * this level.\n\t */\n\tupdate_sd_lb_stats(env, &sds);\n\tlocal = &sds.local_stat;\n\tbusiest = &sds.busiest_stat;\n\n\t/* ASYM feature bypasses nice load balance check */\n\tif (check_asym_packing(env, &sds))\n\t\treturn sds.busiest;\n\n\t/* There is no busy sibling group to pull tasks from */\n\tif (!sds.busiest || busiest->sum_nr_running == 0)\n\t\tgoto out_balanced;\n\n\t/* XXX broken for overlapping NUMA groups */\n\tsds.avg_load = (SCHED_CAPACITY_SCALE * sds.total_load)\n\t\t\t\t\t\t/ sds.total_capacity;\n\n\t/*\n\t * If the busiest group is imbalanced the below checks don't\n\t * work because they assume all things are equal, which typically\n\t * isn't true due to cpus_allowed constraints and the like.\n\t */\n\tif (busiest->group_type == group_imbalanced)\n\t\tgoto force_balance;\n\n\t/*\n\t * When dst_cpu is idle, prevent SMP nice and/or asymmetric group\n\t * capacities from resulting in underutilization due to avg_load.\n\t */\n\tif (env->idle != CPU_NOT_IDLE && group_has_capacity(env, local) &&\n\t    busiest->group_no_capacity)\n\t\tgoto force_balance;\n\n\t/* Misfit tasks should be dealt with regardless of the avg load */\n\tif (busiest->group_type == group_misfit_task)\n\t\tgoto force_balance;\n\n\t/*\n\t * If the local group is busier than the selected busiest group\n\t * don't try and pull any tasks.\n\t */\n\tif (local->avg_load >= busiest->avg_load)\n\t\tgoto out_balanced;\n\n\t/*\n\t * Don't pull any tasks if this group is already above the domain\n\t * average load.\n\t */\n\tif (local->avg_load >= sds.avg_load)\n\t\tgoto out_balanced;\n\n\tif (env->idle == CPU_IDLE) {\n\t\t/*\n\t\t * This CPU is idle. If the busiest group is not overloaded\n\t\t * and there is no imbalance between this and busiest group\n\t\t * wrt idle CPUs, it is balanced. The imbalance becomes\n\t\t * significant if the diff is greater than 1 otherwise we\n\t\t * might end up to just move the imbalance on another group\n\t\t */\n\t\tif ((busiest->group_type != group_overloaded) &&\n\t\t\t\t(local->idle_cpus <= (busiest->idle_cpus + 1)))\n\t\t\tgoto out_balanced;\n\t} else {\n\t\t/*\n\t\t * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use\n\t\t * imbalance_pct to be conservative.\n\t\t */\n\t\tif (100 * busiest->avg_load <=\n\t\t\t\tenv->sd->imbalance_pct * local->avg_load)\n\t\t\tgoto out_balanced;\n\t}\n\nforce_balance:\n\t/* Looks like there is an imbalance. Compute it */\n\tenv->src_grp_type = busiest->group_type;\n\tcalculate_imbalance(env, &sds);\n\treturn env->imbalance ? sds.busiest : NULL;\n\nout_balanced:\n\tenv->imbalance = 0;\n\treturn NULL;\n}\n\n/*\n * find_busiest_queue - find the busiest runqueue among the CPUs in the group.\n */\nstatic struct rq *find_busiest_queue(struct lb_env *env,\n\t\t\t\t     struct sched_group *group)\n{\n\tstruct rq *busiest = NULL, *rq;\n\tunsigned long busiest_load = 0, busiest_capacity = 1;\n\tint i;\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tunsigned long capacity, wl;\n\t\tenum fbq_type rt;\n\n\t\trq = cpu_rq(i);\n\t\trt = fbq_classify_rq(rq);\n\n\t\t/*\n\t\t * We classify groups/runqueues into three groups:\n\t\t *  - regular: there are !numa tasks\n\t\t *  - remote:  there are numa tasks that run on the 'wrong' node\n\t\t *  - all:     there is no distinction\n\t\t *\n\t\t * In order to avoid migrating ideally placed numa tasks,\n\t\t * ignore those when there's better options.\n\t\t *\n\t\t * If we ignore the actual busiest queue to migrate another\n\t\t * task, the next balance pass can still reduce the busiest\n\t\t * queue by moving tasks around inside the node.\n\t\t *\n\t\t * If we cannot move enough load due to this classification\n\t\t * the next pass will adjust the group classification and\n\t\t * allow migration of more tasks.\n\t\t *\n\t\t * Both cases only affect the total convergence complexity.\n\t\t */\n\t\tif (rt > env->fbq_type)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains with misfit tasks we simply\n\t\t * seek the \"biggest\" misfit task.\n\t\t */\n\t\tif (env->src_grp_type == group_misfit_task) {\n\t\t\tif (rq->misfit_task_load > busiest_load) {\n\t\t\t\tbusiest_load = rq->misfit_task_load;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tcapacity = capacity_of(i);\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains, don't pick a CPU that could\n\t\t * eventually lead to active_balancing high->low capacity.\n\t\t * Higher per-CPU capacity is considered better than balancing\n\t\t * average load.\n\t\t */\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    capacity_of(env->dst_cpu) < capacity &&\n\t\t    rq->nr_running == 1)\n\t\t\tcontinue;\n\n\t\twl = weighted_cpuload(rq);\n\n\t\t/*\n\t\t * When comparing with imbalance, use weighted_cpuload()\n\t\t * which is not scaled with the CPU capacity.\n\t\t */\n\n\t\tif (rq->nr_running == 1 && wl > env->imbalance &&\n\t\t    !check_cpu_capacity(rq, env->sd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For the load comparisons with the other CPU's, consider\n\t\t * the weighted_cpuload() scaled with the CPU capacity, so\n\t\t * that the load can be moved away from the CPU that is\n\t\t * potentially running at a lower capacity.\n\t\t *\n\t\t * Thus we're looking for max(wl_i / capacity_i), crosswise\n\t\t * multiplication to rid ourselves of the division works out\n\t\t * to: wl_i * capacity_j > wl_j * capacity_i;  where j is\n\t\t * our previous maximum.\n\t\t */\n\t\tif (wl * busiest_capacity > busiest_load * capacity) {\n\t\t\tbusiest_load = wl;\n\t\t\tbusiest_capacity = capacity;\n\t\t\tbusiest = rq;\n\t\t}\n\t}\n\n\treturn busiest;\n}\n\n/*\n * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but\n * so long as it is large enough.\n */\n#define MAX_PINNED_INTERVAL\t512\n\nstatic int need_active_balance(struct lb_env *env)\n{\n\tstruct sched_domain *sd = env->sd;\n\n\tif (env->idle == CPU_NEWLY_IDLE) {\n\n\t\t/*\n\t\t * ASYM_PACKING needs to force migrate tasks from busy but\n\t\t * lower priority CPUs in order to pack all tasks in the\n\t\t * highest priority CPUs.\n\t\t */\n\t\tif ((sd->flags & SD_ASYM_PACKING) &&\n\t\t    sched_asym_prefer(env->dst_cpu, env->src_cpu))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task.\n\t * It's worth migrating the task if the src_cpu's capacity is reduced\n\t * because of other sched_class or IRQs if more capacity stays\n\t * available on dst_cpu.\n\t */\n\tif ((env->idle != CPU_NOT_IDLE) &&\n\t    (env->src_rq->cfs.h_nr_running == 1)) {\n\t\tif ((check_cpu_capacity(env->src_rq, sd)) &&\n\t\t    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))\n\t\t\treturn 1;\n\t}\n\n\tif (env->src_grp_type == group_misfit_task)\n\t\treturn 1;\n\n\treturn unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);\n}\n\nstatic int active_load_balance_cpu_stop(void *data);\n\nstatic int should_we_balance(struct lb_env *env)\n{\n\tstruct sched_group *sg = env->sd->groups;\n\tint cpu, balance_cpu = -1;\n\n\t/*\n\t * Ensure the balancing environment is consistent; can happen\n\t * when the softirq triggers 'during' hotplug.\n\t */\n\tif (!cpumask_test_cpu(env->dst_cpu, env->cpus))\n\t\treturn 0;\n\n\t/*\n\t * In the newly idle case, we will allow all the CPUs\n\t * to do the newly idle load balance.\n\t */\n\tif (env->idle == CPU_NEWLY_IDLE)\n\t\treturn 1;\n\n\t/* Try to find first idle CPU */\n\tfor_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {\n\t\tif (!idle_cpu(cpu))\n\t\t\tcontinue;\n\n\t\tbalance_cpu = cpu;\n\t\tbreak;\n\t}\n\n\tif (balance_cpu == -1)\n\t\tbalance_cpu = group_balance_cpu(sg);\n\n\t/*\n\t * First idle CPU or the first CPU(busiest) in this sched group\n\t * is eligible for doing load balancing at this and above domains.\n\t */\n\treturn balance_cpu == env->dst_cpu;\n}\n\n/*\n * Check this_cpu to ensure it is balanced within domain. Attempt to move\n * tasks if there is an imbalance.\n */\nstatic int load_balance(int this_cpu, struct rq *this_rq,\n\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n\t\t\tint *continue_balancing)\n{\n\tint ld_moved, cur_ld_moved, active_balance = 0;\n\tstruct sched_domain *sd_parent = sd->parent;\n\tstruct sched_group *group;\n\tstruct rq *busiest;\n\tstruct rq_flags rf;\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);\n\n\tstruct lb_env env = {\n\t\t.sd\t\t= sd,\n\t\t.dst_cpu\t= this_cpu,\n\t\t.dst_rq\t\t= this_rq,\n\t\t.dst_grpmask    = sched_group_span(sd->groups),\n\t\t.idle\t\t= idle,\n\t\t.loop_break\t= sched_nr_migrate_break,\n\t\t.cpus\t\t= cpus,\n\t\t.fbq_type\t= all,\n\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),\n\t};\n\n\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);\n\n\tschedstat_inc(sd->lb_count[idle]);\n\nredo:\n\tif (!should_we_balance(&env)) {\n\t\t*continue_balancing = 0;\n\t\tgoto out_balanced;\n\t}\n\n\tgroup = find_busiest_group(&env);\n\tif (!group) {\n\t\tschedstat_inc(sd->lb_nobusyg[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tbusiest = find_busiest_queue(&env, group);\n\tif (!busiest) {\n\t\tschedstat_inc(sd->lb_nobusyq[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tBUG_ON(busiest == env.dst_rq);\n\n\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);\n\n\tenv.src_cpu = busiest->cpu;\n\tenv.src_rq = busiest;\n\n\tld_moved = 0;\n\tif (busiest->nr_running > 1) {\n\t\t/*\n\t\t * Attempt to move tasks. If find_busiest_group has found\n\t\t * an imbalance but busiest->nr_running <= 1, the group is\n\t\t * still unbalanced. ld_moved simply stays zero, so it is\n\t\t * correctly treated as an imbalance.\n\t\t */\n\t\tenv.flags |= LBF_ALL_PINNED;\n\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);\n\nmore_balance:\n\t\trq_lock_irqsave(busiest, &rf);\n\t\tupdate_rq_clock(busiest);\n\n\t\t/*\n\t\t * cur_ld_moved - load moved in current iteration\n\t\t * ld_moved     - cumulative load moved across iterations\n\t\t */\n\t\tcur_ld_moved = detach_tasks(&env);\n\n\t\t/*\n\t\t * We've detached some tasks from busiest_rq. Every\n\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely\n\t\t * unlock busiest->lock, and we are able to be sure\n\t\t * that nobody can manipulate the tasks in parallel.\n\t\t * See task_rq_lock() family for the details.\n\t\t */\n\n\t\trq_unlock(busiest, &rf);\n\n\t\tif (cur_ld_moved) {\n\t\t\tattach_tasks(&env);\n\t\t\tld_moved += cur_ld_moved;\n\t\t}\n\n\t\tlocal_irq_restore(rf.flags);\n\n\t\tif (env.flags & LBF_NEED_BREAK) {\n\t\t\tenv.flags &= ~LBF_NEED_BREAK;\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to\n\t\t * us and move them to an alternate dst_cpu in our sched_group\n\t\t * where they can run. The upper limit on how many times we\n\t\t * iterate on same src_cpu is dependent on number of CPUs in our\n\t\t * sched_group.\n\t\t *\n\t\t * This changes load balance semantics a bit on who can move\n\t\t * load to a given_cpu. In addition to the given_cpu itself\n\t\t * (or a ilb_cpu acting on its behalf where given_cpu is\n\t\t * nohz-idle), we now have balance_cpu in a position to move\n\t\t * load to given_cpu. In rare situations, this may cause\n\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding\n\t\t * _independently_ and at _same_ time to move some load to\n\t\t * given_cpu) causing exceess load to be moved to given_cpu.\n\t\t * This however should not happen so much in practice and\n\t\t * moreover subsequent load balance cycles should correct the\n\t\t * excess load moved.\n\t\t */\n\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {\n\n\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */\n\t\t\tcpumask_clear_cpu(env.dst_cpu, env.cpus);\n\n\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);\n\t\t\tenv.dst_cpu\t = env.new_dst_cpu;\n\t\t\tenv.flags\t&= ~LBF_DST_PINNED;\n\t\t\tenv.loop\t = 0;\n\t\t\tenv.loop_break\t = sched_nr_migrate_break;\n\n\t\t\t/*\n\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we\n\t\t\t * need to continue with same src_cpu.\n\t\t\t */\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * We failed to reach balance because of affinity.\n\t\t */\n\t\tif (sd_parent) {\n\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)\n\t\t\t\t*group_imbalance = 1;\n\t\t}\n\n\t\t/* All tasks on this runqueue were pinned by CPU affinity */\n\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {\n\t\t\tcpumask_clear_cpu(cpu_of(busiest), cpus);\n\t\t\t/*\n\t\t\t * Attempting to continue load balancing at the current\n\t\t\t * sched_domain level only makes sense if there are\n\t\t\t * active CPUs remaining as possible busiest CPUs to\n\t\t\t * pull load from which are not contained within the\n\t\t\t * destination group that is receiving any migrated\n\t\t\t * load.\n\t\t\t */\n\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {\n\t\t\t\tenv.loop = 0;\n\t\t\t\tenv.loop_break = sched_nr_migrate_break;\n\t\t\t\tgoto redo;\n\t\t\t}\n\t\t\tgoto out_all_pinned;\n\t\t}\n\t}\n\n\tif (!ld_moved) {\n\t\tschedstat_inc(sd->lb_failed[idle]);\n\t\t/*\n\t\t * Increment the failure counter only on periodic balance.\n\t\t * We do not want newidle balance, which can be very\n\t\t * frequent, pollute the failure counter causing\n\t\t * excessive cache_hot migrations and active balances.\n\t\t */\n\t\tif (idle != CPU_NEWLY_IDLE)\n\t\t\tsd->nr_balance_failed++;\n\n\t\tif (need_active_balance(&env)) {\n\t\t\tunsigned long flags;\n\n\t\t\traw_spin_lock_irqsave(&busiest->lock, flags);\n\n\t\t\t/*\n\t\t\t * Don't kick the active_load_balance_cpu_stop,\n\t\t\t * if the curr task on busiest CPU can't be\n\t\t\t * moved to this_cpu:\n\t\t\t */\n\t\t\tif (!cpumask_test_cpu(this_cpu, &busiest->curr->cpus_allowed)) {\n\t\t\t\traw_spin_unlock_irqrestore(&busiest->lock,\n\t\t\t\t\t\t\t    flags);\n\t\t\t\tenv.flags |= LBF_ALL_PINNED;\n\t\t\t\tgoto out_one_pinned;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * ->active_balance synchronizes accesses to\n\t\t\t * ->active_balance_work.  Once set, it's cleared\n\t\t\t * only after active load balance is finished.\n\t\t\t */\n\t\t\tif (!busiest->active_balance) {\n\t\t\t\tbusiest->active_balance = 1;\n\t\t\t\tbusiest->push_cpu = this_cpu;\n\t\t\t\tactive_balance = 1;\n\t\t\t}\n\t\t\traw_spin_unlock_irqrestore(&busiest->lock, flags);\n\n\t\t\tif (active_balance) {\n\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),\n\t\t\t\t\tactive_load_balance_cpu_stop, busiest,\n\t\t\t\t\t&busiest->active_balance_work);\n\t\t\t}\n\n\t\t\t/* We've kicked active balancing, force task migration. */\n\t\t\tsd->nr_balance_failed = sd->cache_nice_tries+1;\n\t\t}\n\t} else\n\t\tsd->nr_balance_failed = 0;\n\n\tif (likely(!active_balance)) {\n\t\t/* We were unbalanced, so reset the balancing interval */\n\t\tsd->balance_interval = sd->min_interval;\n\t} else {\n\t\t/*\n\t\t * If we've begun active balancing, start to back off. This\n\t\t * case may not be covered by the all_pinned logic if there\n\t\t * is only 1 task on the busy runqueue (because we don't call\n\t\t * detach_tasks).\n\t\t */\n\t\tif (sd->balance_interval < sd->max_interval)\n\t\t\tsd->balance_interval *= 2;\n\t}\n\n\tgoto out;\n\nout_balanced:\n\t/*\n\t * We reach balance although we may have faced some affinity\n\t * constraints. Clear the imbalance flag if it was set.\n\t */\n\tif (sd_parent) {\n\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\tif (*group_imbalance)\n\t\t\t*group_imbalance = 0;\n\t}\n\nout_all_pinned:\n\t/*\n\t * We reach balance because all tasks are pinned at this level so\n\t * we can't migrate them. Let the imbalance flag set so parent level\n\t * can try to migrate them.\n\t */\n\tschedstat_inc(sd->lb_balanced[idle]);\n\n\tsd->nr_balance_failed = 0;\n\nout_one_pinned:\n\t/* tune up the balancing interval */\n\tif (((env.flags & LBF_ALL_PINNED) &&\n\t\t\tsd->balance_interval < MAX_PINNED_INTERVAL) ||\n\t\t\t(sd->balance_interval < sd->max_interval))\n\t\tsd->balance_interval *= 2;\n\n\tld_moved = 0;\nout:\n\treturn ld_moved;\n}\n\nstatic inline unsigned long\nget_sd_balance_interval(struct sched_domain *sd, int cpu_busy)\n{\n\tunsigned long interval = sd->balance_interval;\n\n\tif (cpu_busy)\n\t\tinterval *= sd->busy_factor;\n\n\t/* scale ms to jiffies */\n\tinterval = msecs_to_jiffies(interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\n\treturn interval;\n}\n\nstatic inline void\nupdate_next_balance(struct sched_domain *sd, unsigned long *next_balance)\n{\n\tunsigned long interval, next;\n\n\t/* used by idle balance, so cpu_busy = 0 */\n\tinterval = get_sd_balance_interval(sd, 0);\n\tnext = sd->last_balance + interval;\n\n\tif (time_after(*next_balance, next))\n\t\t*next_balance = next;\n}\n\n/*\n * active_load_balance_cpu_stop is run by the CPU stopper. It pushes\n * running tasks off the busiest CPU onto idle CPUs. It requires at\n * least 1 task to be running on each physical CPU where possible, and\n * avoids physical / logical imbalances.\n */\nstatic int active_load_balance_cpu_stop(void *data)\n{\n\tstruct rq *busiest_rq = data;\n\tint busiest_cpu = cpu_of(busiest_rq);\n\tint target_cpu = busiest_rq->push_cpu;\n\tstruct rq *target_rq = cpu_rq(target_cpu);\n\tstruct sched_domain *sd;\n\tstruct task_struct *p = NULL;\n\tstruct rq_flags rf;\n\n\trq_lock_irq(busiest_rq, &rf);\n\t/*\n\t * Between queueing the stop-work and running it is a hole in which\n\t * CPUs can become inactive. We should not move tasks from or to\n\t * inactive CPUs.\n\t */\n\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu))\n\t\tgoto out_unlock;\n\n\t/* Make sure the requested CPU hasn't gone down in the meantime: */\n\tif (unlikely(busiest_cpu != smp_processor_id() ||\n\t\t     !busiest_rq->active_balance))\n\t\tgoto out_unlock;\n\n\t/* Is there any task to move? */\n\tif (busiest_rq->nr_running <= 1)\n\t\tgoto out_unlock;\n\n\t/*\n\t * This condition is \"impossible\", if it occurs\n\t * we need to fix it. Originally reported by\n\t * Bjorn Helgaas on a 128-CPU setup.\n\t */\n\tBUG_ON(busiest_rq == target_rq);\n\n\t/* Search for an sd spanning us and the target CPU. */\n\trcu_read_lock();\n\tfor_each_domain(target_cpu, sd) {\n\t\tif ((sd->flags & SD_LOAD_BALANCE) &&\n\t\t    cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))\n\t\t\t\tbreak;\n\t}\n\n\tif (likely(sd)) {\n\t\tstruct lb_env env = {\n\t\t\t.sd\t\t= sd,\n\t\t\t.dst_cpu\t= target_cpu,\n\t\t\t.dst_rq\t\t= target_rq,\n\t\t\t.src_cpu\t= busiest_rq->cpu,\n\t\t\t.src_rq\t\t= busiest_rq,\n\t\t\t.idle\t\t= CPU_IDLE,\n\t\t\t/*\n\t\t\t * can_migrate_task() doesn't need to compute new_dst_cpu\n\t\t\t * for active balancing. Since we have CPU_IDLE, but no\n\t\t\t * @dst_grpmask we need to make that test go away with lying\n\t\t\t * about DST_PINNED.\n\t\t\t */\n\t\t\t.flags\t\t= LBF_DST_PINNED,\n\t\t};\n\n\t\tschedstat_inc(sd->alb_count);\n\t\tupdate_rq_clock(busiest_rq);\n\n\t\tp = detach_one_task(&env);\n\t\tif (p) {\n\t\t\tschedstat_inc(sd->alb_pushed);\n\t\t\t/* Active balancing done, reset the failure counter. */\n\t\t\tsd->nr_balance_failed = 0;\n\t\t} else {\n\t\t\tschedstat_inc(sd->alb_failed);\n\t\t}\n\t}\n\trcu_read_unlock();\nout_unlock:\n\tbusiest_rq->active_balance = 0;\n\trq_unlock(busiest_rq, &rf);\n\n\tif (p)\n\t\tattach_one_task(target_rq, p);\n\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\nstatic DEFINE_SPINLOCK(balancing);\n\n/*\n * Scale the max load_balance interval with the number of CPUs in the system.\n * This trades load-balance latency on larger machines for less cross talk.\n */\nvoid update_max_interval(void)\n{\n\tmax_load_balance_interval = HZ*num_online_cpus()/10;\n}\n\n/*\n * It checks each scheduling domain to see if it is due to be balanced,\n * and initiates a balancing operation if so.\n *\n * Balancing parameters are set up in init_sched_domains.\n */\nstatic void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)\n{\n\tint continue_balancing = 1;\n\tint cpu = rq->cpu;\n\tunsigned long interval;\n\tstruct sched_domain *sd;\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long next_balance = jiffies + 60*HZ;\n\tint update_next_balance = 0;\n\tint need_serialize, need_decay = 0;\n\tu64 max_cost = 0;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\t/*\n\t\t * Decay the newidle max times here because this is a regular\n\t\t * visit to all the domains. Decay ~1% per second.\n\t\t */\n\t\tif (time_after(jiffies, sd->next_decay_max_lb_cost)) {\n\t\t\tsd->max_newidle_lb_cost =\n\t\t\t\t(sd->max_newidle_lb_cost * 253) / 256;\n\t\t\tsd->next_decay_max_lb_cost = jiffies + HZ;\n\t\t\tneed_decay = 1;\n\t\t}\n\t\tmax_cost += sd->max_newidle_lb_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Stop the load balance at this level. There is another\n\t\t * CPU in our sched group which is doing load balancing more\n\t\t * actively.\n\t\t */\n\t\tif (!continue_balancing) {\n\t\t\tif (need_decay)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\n\t\tneed_serialize = sd->flags & SD_SERIALIZE;\n\t\tif (need_serialize) {\n\t\t\tif (!spin_trylock(&balancing))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (time_after_eq(jiffies, sd->last_balance + interval)) {\n\t\t\tif (load_balance(cpu, rq, sd, idle, &continue_balancing)) {\n\t\t\t\t/*\n\t\t\t\t * The LBF_DST_PINNED logic could have changed\n\t\t\t\t * env->dst_cpu, so we can't know our idle\n\t\t\t\t * state even if we migrated tasks. Update it.\n\t\t\t\t */\n\t\t\t\tidle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;\n\t\t\t}\n\t\t\tsd->last_balance = jiffies;\n\t\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\t\t}\n\t\tif (need_serialize)\n\t\t\tspin_unlock(&balancing);\nout:\n\t\tif (time_after(next_balance, sd->last_balance + interval)) {\n\t\t\tnext_balance = sd->last_balance + interval;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\tif (need_decay) {\n\t\t/*\n\t\t * Ensure the rq-wide value also decays but keep it at a\n\t\t * reasonable floor to avoid funnies with rq->avg_idle.\n\t\t */\n\t\trq->max_idle_balance_cost =\n\t\t\tmax((u64)sysctl_sched_migration_cost, max_cost);\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the cpu is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance)) {\n\t\trq->next_balance = next_balance;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\t\t/*\n\t\t * If this CPU has been elected to perform the nohz idle\n\t\t * balance. Other idle CPUs have already rebalanced with\n\t\t * nohz_idle_balance() and nohz.next_balance has been\n\t\t * updated accordingly. This CPU is now running the idle load\n\t\t * balance for itself and we need to update the\n\t\t * nohz.next_balance accordingly.\n\t\t */\n\t\tif ((idle == CPU_IDLE) && time_after(nohz.next_balance, rq->next_balance))\n\t\t\tnohz.next_balance = rq->next_balance;\n#endif\n\t}\n}\n\nstatic inline int on_null_domain(struct rq *rq)\n{\n\treturn unlikely(!rcu_dereference_sched(rq->sd));\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * idle load balancing details\n * - When one of the busy CPUs notice that there may be an idle rebalancing\n *   needed, they will kick the idle load balancer, which then does idle\n *   load balancing for all the idle CPUs.\n */\n\nstatic inline int find_new_ilb(void)\n{\n\tint ilb = cpumask_first(nohz.idle_cpus_mask);\n\n\tif (ilb < nr_cpu_ids && idle_cpu(ilb))\n\t\treturn ilb;\n\n\treturn nr_cpu_ids;\n}\n\n/*\n * Kick a CPU to do the nohz balancing, if it is time for it. We pick the\n * nohz_load_balancer CPU (if there is one) otherwise fallback to any idle\n * CPU (if there is one).\n */\nstatic void kick_ilb(unsigned int flags)\n{\n\tint ilb_cpu;\n\n\tnohz.next_balance++;\n\n\tilb_cpu = find_new_ilb();\n\n\tif (ilb_cpu >= nr_cpu_ids)\n\t\treturn;\n\n\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));\n\tif (flags & NOHZ_KICK_MASK)\n\t\treturn;\n\n\t/*\n\t * Use smp_send_reschedule() instead of resched_cpu().\n\t * This way we generate a sched IPI on the target CPU which\n\t * is idle. And the softirq performing nohz idle load balance\n\t * will be run before returning from the IPI.\n\t */\n\tsmp_send_reschedule(ilb_cpu);\n}\n\n/*\n * Current heuristic for kicking the idle load balancer in the presence\n * of an idle cpu in the system.\n *   - This rq has more than one task.\n *   - This rq has at least one CFS task and the capacity of the CPU is\n *     significantly reduced because of RT tasks or IRQs.\n *   - At parent of LLC scheduler domain level, this cpu's scheduler group has\n *     multiple busy cpu.\n *   - For SD_ASYM_PACKING, if the lower numbered cpu's in the scheduler\n *     domain span are idle.\n */\nstatic void nohz_balancer_kick(struct rq *rq)\n{\n\tunsigned long now = jiffies;\n\tstruct sched_domain_shared *sds;\n\tstruct sched_domain *sd;\n\tint nr_busy, i, cpu = rq->cpu;\n\tunsigned int flags = 0;\n\n\tif (unlikely(rq->idle_balance))\n\t\treturn;\n\n\t/*\n\t * We may be recently in ticked or tickless idle mode. At the first\n\t * busy tick after returning from idle, we will update the busy stats.\n\t */\n\tnohz_balance_exit_idle(rq);\n\n\t/*\n\t * None are in tickless mode and hence no need for NOHZ idle load\n\t * balancing.\n\t */\n\tif (likely(!atomic_read(&nohz.nr_cpus)))\n\t\treturn;\n\n\tif (READ_ONCE(nohz.has_blocked) &&\n\t    time_after(now, READ_ONCE(nohz.next_blocked)))\n\t\tflags = NOHZ_STATS_KICK;\n\n\tif (time_before(now, nohz.next_balance))\n\t\tgoto out;\n\n\tif (rq->nr_running >= 2 || rq->misfit_task_load) {\n\t\tflags = NOHZ_KICK_MASK;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds) {\n\t\t/*\n\t\t * XXX: write a coherent comment on why we do this.\n\t\t * See also: http://lkml.kernel.org/r/20111202010832.602203411@sbsiddha-desk.sc.intel.com\n\t\t */\n\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);\n\t\tif (nr_busy > 1) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\n\t}\n\n\tsd = rcu_dereference(rq->sd);\n\tif (sd) {\n\t\tif ((rq->cfs.h_nr_running >= 1) &&\n\t\t\t\tcheck_cpu_capacity(rq, sd)) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_asym, cpu));\n\tif (sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\t\tif (i == cpu ||\n\t\t\t    !cpumask_test_cpu(i, nohz.idle_cpus_mask))\n\t\t\t\tcontinue;\n\n\t\t\tif (sched_asym_prefer(i, cpu)) {\n\t\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\nout:\n\tif (flags)\n\t\tkick_ilb(flags);\n}\n\nstatic void set_cpu_sd_state_busy(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || !sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 0;\n\n\tatomic_inc(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\nvoid nohz_balance_exit_idle(struct rq *rq)\n{\n\tSCHED_WARN_ON(rq != this_rq());\n\n\tif (likely(!rq->nohz_tick_stopped))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 0;\n\tcpumask_clear_cpu(rq->cpu, nohz.idle_cpus_mask);\n\tatomic_dec(&nohz.nr_cpus);\n\n\tset_cpu_sd_state_busy(rq->cpu);\n}\n\nstatic void set_cpu_sd_state_idle(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 1;\n\n\tatomic_dec(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\n/*\n * This routine will record that the CPU is going idle with tick stopped.\n * This info will be used in performing idle load balancing in the future.\n */\nvoid nohz_balance_enter_idle(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tSCHED_WARN_ON(cpu != smp_processor_id());\n\n\t/* If this CPU is going down, then nothing needs to be done: */\n\tif (!cpu_active(cpu))\n\t\treturn;\n\n\t/* Spare idle load balancing on CPUs that don't want to be disturbed: */\n\tif (!housekeeping_cpu(cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/*\n\t * Can be set safely without rq->lock held\n\t * If a clear happens, it will have evaluated last additions because\n\t * rq->lock is held during the check and the clear\n\t */\n\trq->has_blocked_load = 1;\n\n\t/*\n\t * The tick is still stopped but load could have been added in the\n\t * meantime. We set the nohz.has_blocked flag to trig a check of the\n\t * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear\n\t * of nohz.has_blocked can only happen after checking the new load\n\t */\n\tif (rq->nohz_tick_stopped)\n\t\tgoto out;\n\n\t/* If we're a completely isolated CPU, we don't play: */\n\tif (on_null_domain(rq))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 1;\n\n\tcpumask_set_cpu(cpu, nohz.idle_cpus_mask);\n\tatomic_inc(&nohz.nr_cpus);\n\n\t/*\n\t * Ensures that if nohz_idle_balance() fails to observe our\n\t * @idle_cpus_mask store, it must observe the @has_blocked\n\t * store.\n\t */\n\tsmp_mb__after_atomic();\n\n\tset_cpu_sd_state_idle(cpu);\n\nout:\n\t/*\n\t * Each time a cpu enter idle, we assume that it has blocked load and\n\t * enable the periodic update of the load of idle cpus\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 1);\n}\n\n/*\n * Internal function that runs load balance for all idle cpus. The load balance\n * can be a simple update of blocked load or a complete load balance with\n * tasks movement depending of flags.\n * The function returns false if the loop has stopped before running\n * through all idle CPUs.\n */\nstatic bool _nohz_idle_balance(struct rq *this_rq, unsigned int flags,\n\t\t\t       enum cpu_idle_type idle)\n{\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long now = jiffies;\n\tunsigned long next_balance = now + 60*HZ;\n\tbool has_blocked_load = false;\n\tint update_next_balance = 0;\n\tint this_cpu = this_rq->cpu;\n\tint balance_cpu;\n\tint ret = false;\n\tstruct rq *rq;\n\n\tSCHED_WARN_ON((flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK);\n\n\t/*\n\t * We assume there will be no idle load after this update and clear\n\t * the has_blocked flag. If a cpu enters idle in the mean time, it will\n\t * set the has_blocked flag and trig another update of idle load.\n\t * Because a cpu that becomes idle, is added to idle_cpus_mask before\n\t * setting the flag, we are sure to not clear the state and not\n\t * check the load of an idle cpu.\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 0);\n\n\t/*\n\t * Ensures that if we miss the CPU, we must see the has_blocked\n\t * store from nohz_balance_enter_idle().\n\t */\n\tsmp_mb();\n\n\tfor_each_cpu(balance_cpu, nohz.idle_cpus_mask) {\n\t\tif (balance_cpu == this_cpu || !idle_cpu(balance_cpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If this CPU gets work to do, stop the load balancing\n\t\t * work being done for other CPUs. Next load\n\t\t * balancing owner will pick it up.\n\t\t */\n\t\tif (need_resched()) {\n\t\t\thas_blocked_load = true;\n\t\t\tgoto abort;\n\t\t}\n\n\t\trq = cpu_rq(balance_cpu);\n\n\t\thas_blocked_load |= update_nohz_stats(rq, true);\n\n\t\t/*\n\t\t * If time for next balance is due,\n\t\t * do the balance.\n\t\t */\n\t\tif (time_after_eq(jiffies, rq->next_balance)) {\n\t\t\tstruct rq_flags rf;\n\n\t\t\trq_lock_irqsave(rq, &rf);\n\t\t\tupdate_rq_clock(rq);\n\t\t\tcpu_load_update_idle(rq);\n\t\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\t\tif (flags & NOHZ_BALANCE_KICK)\n\t\t\t\trebalance_domains(rq, CPU_IDLE);\n\t\t}\n\n\t\tif (time_after(next_balance, rq->next_balance)) {\n\t\t\tnext_balance = rq->next_balance;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\n\t/* Newly idle CPU doesn't need an update */\n\tif (idle != CPU_NEWLY_IDLE) {\n\t\tupdate_blocked_averages(this_cpu);\n\t\thas_blocked_load |= this_rq->has_blocked_load;\n\t}\n\n\tif (flags & NOHZ_BALANCE_KICK)\n\t\trebalance_domains(this_rq, CPU_IDLE);\n\n\tWRITE_ONCE(nohz.next_blocked,\n\t\tnow + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\n\t/* The full idle balance loop has been done */\n\tret = true;\n\nabort:\n\t/* There is still blocked load, enable periodic update */\n\tif (has_blocked_load)\n\t\tWRITE_ONCE(nohz.has_blocked, 1);\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the CPU is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance))\n\t\tnohz.next_balance = next_balance;\n\n\treturn ret;\n}\n\n/*\n * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the\n * rebalancing for all the cpus for whom scheduler ticks are stopped.\n */\nstatic bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\tint this_cpu = this_rq->cpu;\n\tunsigned int flags;\n\n\tif (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\tif (idle != CPU_IDLE) {\n\t\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\t\treturn false;\n\t}\n\n\t/*\n\t * barrier, pairs with nohz_balance_enter_idle(), ensures ...\n\t */\n\tflags = atomic_fetch_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\tif (!(flags & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\t_nohz_idle_balance(this_rq, flags, idle);\n\n\treturn true;\n}\n\nstatic void nohz_newidle_balance(struct rq *this_rq)\n{\n\tint this_cpu = this_rq->cpu;\n\n\t/*\n\t * This CPU doesn't want to be disturbed by scheduler\n\t * housekeeping\n\t */\n\tif (!housekeeping_cpu(this_cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/* Will wake up very soon. No time for doing anything else*/\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost)\n\t\treturn;\n\n\t/* Don't need to update blocked load of idle CPUs*/\n\tif (!READ_ONCE(nohz.has_blocked) ||\n\t    time_before(jiffies, READ_ONCE(nohz.next_blocked)))\n\t\treturn;\n\n\traw_spin_unlock(&this_rq->lock);\n\t/*\n\t * This CPU is going to be idle and blocked load of idle CPUs\n\t * need to be updated. Run the ilb locally as it is a good\n\t * candidate for ilb instead of waking up another idle CPU.\n\t * Kick an normal ilb if we failed to do the update.\n\t */\n\tif (!_nohz_idle_balance(this_rq, NOHZ_STATS_KICK, CPU_NEWLY_IDLE))\n\t\tkick_ilb(NOHZ_STATS_KICK);\n\traw_spin_lock(&this_rq->lock);\n}\n\n#else /* !CONFIG_NO_HZ_COMMON */\nstatic inline void nohz_balancer_kick(struct rq *rq) { }\n\nstatic inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\treturn false;\n}\n\nstatic inline void nohz_newidle_balance(struct rq *this_rq) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\n/*\n * idle_balance is called by schedule() if this_cpu is about to become\n * idle. Attempts to pull tasks from other CPUs.\n */\nstatic int idle_balance(struct rq *this_rq, struct rq_flags *rf)\n{\n\tunsigned long next_balance = jiffies + HZ;\n\tint this_cpu = this_rq->cpu;\n\tstruct sched_domain *sd;\n\tint pulled_task = 0;\n\tu64 curr_cost = 0;\n\n\t/*\n\t * We must set idle_stamp _before_ calling idle_balance(), such that we\n\t * measure the duration of idle_balance() as idle time.\n\t */\n\tthis_rq->idle_stamp = rq_clock(this_rq);\n\n\t/*\n\t * Do not pull tasks towards !active CPUs...\n\t */\n\tif (!cpu_active(this_cpu))\n\t\treturn 0;\n\n\t/*\n\t * This is OK, because current is on_cpu, which avoids it being picked\n\t * for load-balance and preemption/IRQs are still disabled avoiding\n\t * further scheduler activity on it and we're being very careful to\n\t * re-start the picking loop.\n\t */\n\trq_unpin_lock(this_rq, rf);\n\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost ||\n\t    !READ_ONCE(this_rq->rd->overload)) {\n\n\t\trcu_read_lock();\n\t\tsd = rcu_dereference_check_sched_domain(this_rq->sd);\n\t\tif (sd)\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\trcu_read_unlock();\n\n\t\tnohz_newidle_balance(this_rq);\n\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock(&this_rq->lock);\n\n\tupdate_blocked_averages(this_cpu);\n\trcu_read_lock();\n\tfor_each_domain(this_cpu, sd) {\n\t\tint continue_balancing = 1;\n\t\tu64 t0, domain_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\tif (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sd->flags & SD_BALANCE_NEWIDLE) {\n\t\t\tt0 = sched_clock_cpu(this_cpu);\n\n\t\t\tpulled_task = load_balance(this_cpu, this_rq,\n\t\t\t\t\t\t   sd, CPU_NEWLY_IDLE,\n\t\t\t\t\t\t   &continue_balancing);\n\n\t\t\tdomain_cost = sched_clock_cpu(this_cpu) - t0;\n\t\t\tif (domain_cost > sd->max_newidle_lb_cost)\n\t\t\t\tsd->max_newidle_lb_cost = domain_cost;\n\n\t\t\tcurr_cost += domain_cost;\n\t\t}\n\n\t\tupdate_next_balance(sd, &next_balance);\n\n\t\t/*\n\t\t * Stop searching for tasks to pull if there are\n\t\t * now runnable tasks on this rq.\n\t\t */\n\t\tif (pulled_task || this_rq->nr_running > 0)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\traw_spin_lock(&this_rq->lock);\n\n\tif (curr_cost > this_rq->max_idle_balance_cost)\n\t\tthis_rq->max_idle_balance_cost = curr_cost;\n\nout:\n\t/*\n\t * While browsing the domains, we released the rq lock, a task could\n\t * have been enqueued in the meantime. Since we're not going idle,\n\t * pretend we pulled a task.\n\t */\n\tif (this_rq->cfs.h_nr_running && !pulled_task)\n\t\tpulled_task = 1;\n\n\t/* Move the next balance forward */\n\tif (time_after(this_rq->next_balance, next_balance))\n\t\tthis_rq->next_balance = next_balance;\n\n\t/* Is there a task of a high priority class? */\n\tif (this_rq->nr_running != this_rq->cfs.h_nr_running)\n\t\tpulled_task = -1;\n\n\tif (pulled_task)\n\t\tthis_rq->idle_stamp = 0;\n\n\trq_repin_lock(this_rq, rf);\n\n\treturn pulled_task;\n}\n\n/*\n * run_rebalance_domains is triggered when needed from the scheduler tick.\n * Also triggered for nohz idle balancing (with nohz_balancing_kick set).\n */\nstatic __latent_entropy void run_rebalance_domains(struct softirq_action *h)\n{\n\tstruct rq *this_rq = this_rq();\n\tenum cpu_idle_type idle = this_rq->idle_balance ?\n\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;\n\n\t/*\n\t * If this CPU has a pending nohz_balance_kick, then do the\n\t * balancing on behalf of the other idle CPUs whose ticks are\n\t * stopped. Do nohz_idle_balance *before* rebalance_domains to\n\t * give the idle CPUs a chance to load balance. Else we may\n\t * load balance only within the local sched_domain hierarchy\n\t * and abort nohz_idle_balance altogether if we pull some load.\n\t */\n\tif (nohz_idle_balance(this_rq, idle))\n\t\treturn;\n\n\t/* normal load balance */\n\tupdate_blocked_averages(this_rq->cpu);\n\trebalance_domains(this_rq, idle);\n}\n\n/*\n * Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing.\n */\nvoid trigger_load_balance(struct rq *rq)\n{\n\t/* Don't need to rebalance while attached to NULL domain */\n\tif (unlikely(on_null_domain(rq)))\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->next_balance))\n\t\traise_softirq(SCHED_SOFTIRQ);\n\n\tnohz_balancer_kick(rq);\n}\n\nstatic void rq_online_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\tupdate_runtime_enabled(rq);\n}\n\nstatic void rq_offline_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\t/* Ensure any throttled groups are reachable by pick_next_task */\n\tunthrottle_offline_cfs_rqs(rq);\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * scheduler tick hitting a task of our scheduling class.\n *\n * NOTE: This function can be called remotely by the tick offload that\n * goes along full dynticks. Therefore no local assumption can be made\n * and everything must be accessed through the @rq and @curr passed in\n * parameters.\n */\nstatic void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tentity_tick(cfs_rq, se, queued);\n\t}\n\n\tif (static_branch_unlikely(&sched_numa_balancing))\n\t\ttask_tick_numa(rq, curr);\n\n\tupdate_misfit_status(curr, rq);\n}\n\n/*\n * called on fork with the child task as argument from the parent's context\n *  - child not yet on the tasklist\n *  - preemption disabled\n */\nstatic void task_fork_fair(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se, *curr;\n\tstruct rq *rq = this_rq();\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\tcfs_rq = task_cfs_rq(current);\n\tcurr = cfs_rq->curr;\n\tif (curr) {\n\t\tupdate_curr(cfs_rq);\n\t\tse->vruntime = curr->vruntime;\n\t}\n\tplace_entity(cfs_rq, se, 1);\n\n\tif (sysctl_sched_child_runs_first && curr && entity_before(curr, se)) {\n\t\t/*\n\t\t * Upon rescheduling, sched_class::put_prev_task() will place\n\t\t * 'current' within the tree based on its new key value.\n\t\t */\n\t\tswap(curr->vruntime, se->vruntime);\n\t\tresched_curr(rq);\n\t}\n\n\tse->vruntime -= cfs_rq->min_vruntime;\n\trq_unlock(rq, &rf);\n}\n\n/*\n * Priority of the task has changed. Check to see if we preempt\n * the current task.\n */\nstatic void\nprio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tif (!task_on_rq_queued(p))\n\t\treturn;\n\n\t/*\n\t * Reschedule if we are currently running on this runqueue and\n\t * our priority decreased, or if we are not currently running on\n\t * this runqueue and our priority is higher than the current's\n\t */\n\tif (rq->curr == p) {\n\t\tif (p->prio > oldprio)\n\t\t\tresched_curr(rq);\n\t} else\n\t\tcheck_preempt_curr(rq, p, 0);\n}\n\nstatic inline bool vruntime_normalized(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases,\n\t * the dequeue_entity(.flags=0) will already have normalized the\n\t * vruntime.\n\t */\n\tif (p->on_rq)\n\t\treturn true;\n\n\t/*\n\t * When !on_rq, vruntime of the task has usually NOT been normalized.\n\t * But there are some cases where it has already been normalized:\n\t *\n\t * - A forked child which is waiting for being woken up by\n\t *   wake_up_new_task().\n\t * - A task which has been woken up by try_to_wake_up() and\n\t *   waiting for actually being woken up by sched_ttwu_pending().\n\t */\n\tif (!se->sum_exec_runtime ||\n\t    (p->state == TASK_WAKING && p->sched_remote_wakeup))\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n/*\n * Propagate the changes of the sched_entity across the tg tree to make it\n * visible to the root\n */\nstatic void propagate_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq;\n\n\t/* Start to propagate at parent */\n\tse = se->parent;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n}\n#else\nstatic void propagate_entity_cfs_rq(struct sched_entity *se) { }\n#endif\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t/* Catch up with the cfs_rq and remove our load when we leave */\n\tupdate_load_avg(cfs_rq, se, 0);\n\tdetach_entity_load_avg(cfs_rq, se);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void attach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/*\n\t * Since the real-depth could have been changed (only FAIR\n\t * class maintain depth value), reset depth properly.\n\t */\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n#endif\n\n\t/* Synchronize entity with its cfs_rq */\n\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);\n\tattach_entity_load_avg(cfs_rq, se, 0);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void detach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tif (!vruntime_normalized(p)) {\n\t\t/*\n\t\t * Fix up our vruntime so that the current sleep doesn't\n\t\t * cause 'unlimited' sleep bonus.\n\t\t */\n\t\tplace_entity(cfs_rq, se, 0);\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\t}\n\n\tdetach_entity_cfs_rq(se);\n}\n\nstatic void attach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tattach_entity_cfs_rq(se);\n\n\tif (!vruntime_normalized(p))\n\t\tse->vruntime += cfs_rq->min_vruntime;\n}\n\nstatic void switched_from_fair(struct rq *rq, struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n}\n\nstatic void switched_to_fair(struct rq *rq, struct task_struct *p)\n{\n\tattach_task_cfs_rq(p);\n\n\tif (task_on_rq_queued(p)) {\n\t\t/*\n\t\t * We were most likely switched from sched_rt, so\n\t\t * kick off the schedule if running, otherwise just see\n\t\t * if we can still preempt the current task.\n\t\t */\n\t\tif (rq->curr == p)\n\t\t\tresched_curr(rq);\n\t\telse\n\t\t\tcheck_preempt_curr(rq, p, 0);\n\t}\n}\n\n/* Account for a task changing its policy or group.\n *\n * This routine is mostly called to set cfs_rq->curr field when a task\n * migrates between groups/classes.\n */\nstatic void set_curr_task_fair(struct rq *rq)\n{\n\tstruct sched_entity *se = &rq->curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\tset_next_entity(cfs_rq, se);\n\t\t/* ensure bandwidth has been allocated on our new cfs_rq */\n\t\taccount_cfs_rq_runtime(cfs_rq, 0);\n\t}\n}\n\nvoid init_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT_CACHED;\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n#ifndef CONFIG_64BIT\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n#ifdef CONFIG_SMP\n\traw_spin_lock_init(&cfs_rq->removed.lock);\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void task_set_group_fair(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tset_task_rq(p, task_cpu(p));\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n}\n\nstatic void task_move_group_fair(struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n\tset_task_rq(p, task_cpu(p));\n\n#ifdef CONFIG_SMP\n\t/* Tell se's cfs_rq has been changed -- migrated */\n\tp->se.avg.last_update_time = 0;\n#endif\n\tattach_task_cfs_rq(p);\n}\n\nstatic void task_change_group_fair(struct task_struct *p, int type)\n{\n\tswitch (type) {\n\tcase TASK_SET_GROUP:\n\t\ttask_set_group_fair(p);\n\t\tbreak;\n\n\tcase TASK_MOVE_GROUP:\n\t\ttask_move_group_fair(p);\n\t\tbreak;\n\t}\n}\n\nvoid free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct sched_entity *se;\n\tstruct cfs_rq *cfs_rq;\n\tint i;\n\n\ttg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tinit_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_cfs_rq(cfs_rq);\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t\tinit_entity_runnable_average(se);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nvoid online_fair_sched_group(struct task_group *tg)\n{\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\t\tse = tg->se[i];\n\n\t\traw_spin_lock_irq(&rq->lock);\n\t\tupdate_rq_clock(rq);\n\t\tattach_entity_cfs_rq(se);\n\t\tsync_throttle(tg, i);\n\t\traw_spin_unlock_irq(&rq->lock);\n\t}\n}\n\nvoid unregister_fair_sched_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (tg->se[cpu])\n\t\t\tremove_entity_load_avg(tg->se[cpu]);\n\n\t\t/*\n\t\t * Only empty task groups can be destroyed; so we can speculatively\n\t\t * check on_list without danger of it being re-added.\n\t\t */\n\t\tif (!tg->cfs_rq[cpu]->on_list)\n\t\t\tcontinue;\n\n\t\trq = cpu_rq(cpu);\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n}\n\nvoid init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\tstruct sched_entity *se, int cpu,\n\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tcfs_rq->tg = tg;\n\tcfs_rq->rq = rq;\n\tinit_cfs_rq_runtime(cfs_rq);\n\n\ttg->cfs_rq[cpu] = cfs_rq;\n\ttg->se[cpu] = se;\n\n\t/* se could be NULL for root_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent) {\n\t\tse->cfs_rq = &rq->cfs;\n\t\tse->depth = 0;\n\t} else {\n\t\tse->cfs_rq = parent->my_q;\n\t\tse->depth = parent->depth + 1;\n\t}\n\n\tse->my_q = cfs_rq;\n\t/* guarantee group entities always have weight */\n\tupdate_load_set(&se->load, NICE_0_LOAD);\n\tse->parent = parent;\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES));\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg->se[i];\n\t\tstruct rq_flags rf;\n\n\t\t/* Propagate contribution to hierarchy */\n\t\trq_lock_irqsave(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_sched_entity(se) {\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG);\n\t\t\tupdate_cfs_group(se);\n\t\t}\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n#else /* CONFIG_FAIR_GROUP_SCHED */\n\nvoid free_fair_sched_group(struct task_group *tg) { }\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nvoid online_fair_sched_group(struct task_group *tg) { }\n\nvoid unregister_fair_sched_group(struct task_group *tg) { }\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\nstatic unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task)\n{\n\tstruct sched_entity *se = &task->se;\n\tunsigned int rr_interval = 0;\n\n\t/*\n\t * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise\n\t * idle runqueue:\n\t */\n\tif (rq->cfs.load.weight)\n\t\trr_interval = NS_TO_JIFFIES(sched_slice(cfs_rq_of(se), se));\n\n\treturn rr_interval;\n}\n\n/*\n * All the scheduling class methods:\n */\nconst struct sched_class fair_sched_class = {\n\t.next\t\t\t= &idle_sched_class,\n\t.enqueue_task\t\t= enqueue_task_fair,\n\t.dequeue_task\t\t= dequeue_task_fair,\n\t.yield_task\t\t= yield_task_fair,\n\t.yield_to_task\t\t= yield_to_task_fair,\n\n\t.check_preempt_curr\t= check_preempt_wakeup,\n\n\t.pick_next_task\t\t= pick_next_task_fair,\n\t.put_prev_task\t\t= put_prev_task_fair,\n\n#ifdef CONFIG_SMP\n\t.select_task_rq\t\t= select_task_rq_fair,\n\t.migrate_task_rq\t= migrate_task_rq_fair,\n\n\t.rq_online\t\t= rq_online_fair,\n\t.rq_offline\t\t= rq_offline_fair,\n\n\t.task_dead\t\t= task_dead_fair,\n\t.set_cpus_allowed\t= set_cpus_allowed_common,\n#endif\n\n\t.set_curr_task          = set_curr_task_fair,\n\t.task_tick\t\t= task_tick_fair,\n\t.task_fork\t\t= task_fork_fair,\n\n\t.prio_changed\t\t= prio_changed_fair,\n\t.switched_from\t\t= switched_from_fair,\n\t.switched_to\t\t= switched_to_fair,\n\n\t.get_rr_interval\t= get_rr_interval_fair,\n\n\t.update_curr\t\t= update_curr_fair,\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t.task_change_group\t= task_change_group_fair,\n#endif\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nvoid print_cfs_stats(struct seq_file *m, int cpu)\n{\n\tstruct cfs_rq *cfs_rq, *pos;\n\n\trcu_read_lock();\n\tfor_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)\n\t\tprint_cfs_rq(m, cpu, cfs_rq);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nvoid show_numa_stats(struct task_struct *p, struct seq_file *m)\n{\n\tint node;\n\tunsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0;\n\n\tfor_each_online_node(node) {\n\t\tif (p->numa_faults) {\n\t\t\ttsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t\t\ttpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tif (p->numa_group) {\n\t\t\tgsf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 0)],\n\t\t\tgpf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tprint_numa_stats(m, node, tsf, tpf, gsf, gpf);\n\t}\n}\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_SCHED_DEBUG */\n\n__init void init_sched_fair_class(void)\n{\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tnohz.next_balance = jiffies;\n\tnohz.next_blocked = jiffies;\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n#endif\n#endif /* SMP */\n\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "rq_clock_skip_update",
          "args": [
            "rq"
          ],
          "line": 6783
        },
        "resolved": true,
        "details": {
          "function_name": "rq_clock_skip_update",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1021-1025",
          "snippet": "static inline void rq_clock_skip_update(struct rq *rq)\n{\n\tlockdep_assert_held(&rq->lock);\n\trq->clock_update_flags |= RQCF_REQ_SKIP;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [
            "#define RQCF_REQ_SKIP\t\t0x01"
          ],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\n#define RQCF_REQ_SKIP\t\t0x01\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void rq_clock_skip_update(struct rq *rq)\n{\n\tlockdep_assert_held(&rq->lock);\n\trq->clock_update_flags |= RQCF_REQ_SKIP;\n}"
        }
      },
      {
        "call_info": {
          "callee": "clear_buddies",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 6770
        },
        "resolved": true,
        "details": {
          "function_name": "clear_buddies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3944-3954",
          "snippet": "static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "rq->nr_running == 1"
          ],
          "line": 6767
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "put_prev_entity",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 6749
        },
        "resolved": true,
        "details": {
          "function_name": "put_prev_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4145-4167",
          "snippet": "static void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)\n{\n\t/*\n\t * If still on the runqueue then deactivate_task()\n\t * was not called and update_curr() has to be done:\n\t */\n\tif (prev->on_rq)\n\t\tupdate_curr(cfs_rq);\n\n\t/* throttle cfs_rqs exceeding runtime */\n\tcheck_cfs_rq_runtime(cfs_rq);\n\n\tcheck_spread(cfs_rq, prev);\n\n\tif (prev->on_rq) {\n\t\tupdate_stats_wait_start(cfs_rq, prev);\n\t\t/* Put 'current' back into the tree. */\n\t\t__enqueue_entity(cfs_rq, prev);\n\t\t/* in !on_rq case, update occurred at dequeue */\n\t\tupdate_load_avg(cfs_rq, prev, 0);\n\t}\n\tcfs_rq->curr = NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)\n{\n\t/*\n\t * If still on the runqueue then deactivate_task()\n\t * was not called and update_curr() has to be done:\n\t */\n\tif (prev->on_rq)\n\t\tupdate_curr(cfs_rq);\n\n\t/* throttle cfs_rqs exceeding runtime */\n\tcheck_cfs_rq_runtime(cfs_rq);\n\n\tcheck_spread(cfs_rq, prev);\n\n\tif (prev->on_rq) {\n\t\tupdate_stats_wait_start(cfs_rq, prev);\n\t\t/* Put 'current' back into the tree. */\n\t\t__enqueue_entity(cfs_rq, prev);\n\t\t/* in !on_rq case, update occurred at dequeue */\n\t\tupdate_load_avg(cfs_rq, prev, 0);\n\t}\n\tcfs_rq->curr = NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "idle_balance",
          "args": [
            "rq",
            "rf"
          ],
          "line": 6723
        },
        "resolved": true,
        "details": {
          "function_name": "idle_balance",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3745-3748",
          "snippet": "static inline int idle_balance(struct rq *rq, struct rq_flags *rf)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int idle_balance(struct rq *rq, struct rq_flags *rf)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtick_start_fair",
          "args": [
            "rq",
            "p"
          ],
          "line": 6715
        },
        "resolved": true,
        "details": {
          "function_name": "hrtick_start_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5065-5068",
          "snippet": "static inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtick_enabled",
          "args": [
            "rq"
          ],
          "line": 6714
        },
        "resolved": true,
        "details": {
          "function_name": "hrtick_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1844-1847",
          "snippet": "static inline int hrtick_enabled(struct rq *rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_move",
          "args": [
            "&p->se.group_node",
            "&rq->cfs_tasks"
          ],
          "line": 6711
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 6702
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "pick_next_entity",
          "args": [
            "cfs_rq",
            "NULL"
          ],
          "line": 6697
        },
        "resolved": true,
        "details": {
          "function_name": "pick_next_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4092-4141",
          "snippet": "static struct sched_entity *\npick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tstruct sched_entity *left = __pick_first_entity(cfs_rq);\n\tstruct sched_entity *se;\n\n\t/*\n\t * If curr is set we have to see if its left of the leftmost entity\n\t * still in the tree, provided there was anything in the tree at all.\n\t */\n\tif (!left || (curr && entity_before(curr, left)))\n\t\tleft = curr;\n\n\tse = left; /* ideally we run the leftmost entity */\n\n\t/*\n\t * Avoid running the skip buddy, if running something else can\n\t * be done without getting too unfair.\n\t */\n\tif (cfs_rq->skip == se) {\n\t\tstruct sched_entity *second;\n\n\t\tif (se == curr) {\n\t\t\tsecond = __pick_first_entity(cfs_rq);\n\t\t} else {\n\t\t\tsecond = __pick_next_entity(se);\n\t\t\tif (!second || (curr && entity_before(curr, second)))\n\t\t\t\tsecond = curr;\n\t\t}\n\n\t\tif (second && wakeup_preempt_entity(second, left) < 1)\n\t\t\tse = second;\n\t}\n\n\t/*\n\t * Prefer last buddy, try to return the CPU to a preempted task.\n\t */\n\tif (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)\n\t\tse = cfs_rq->last;\n\n\t/*\n\t * Someone really wants this to run. If it's not unfair, run it.\n\t */\n\tif (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)\n\t\tse = cfs_rq->next;\n\n\tclear_buddies(cfs_rq, se);\n\n\treturn se;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic struct sched_entity *\npick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tstruct sched_entity *left = __pick_first_entity(cfs_rq);\n\tstruct sched_entity *se;\n\n\t/*\n\t * If curr is set we have to see if its left of the leftmost entity\n\t * still in the tree, provided there was anything in the tree at all.\n\t */\n\tif (!left || (curr && entity_before(curr, left)))\n\t\tleft = curr;\n\n\tse = left; /* ideally we run the leftmost entity */\n\n\t/*\n\t * Avoid running the skip buddy, if running something else can\n\t * be done without getting too unfair.\n\t */\n\tif (cfs_rq->skip == se) {\n\t\tstruct sched_entity *second;\n\n\t\tif (se == curr) {\n\t\t\tsecond = __pick_first_entity(cfs_rq);\n\t\t} else {\n\t\t\tsecond = __pick_next_entity(se);\n\t\t\tif (!second || (curr && entity_before(curr, second)))\n\t\t\t\tsecond = curr;\n\t\t}\n\n\t\tif (second && wakeup_preempt_entity(second, left) < 1)\n\t\t\tse = second;\n\t}\n\n\t/*\n\t * Prefer last buddy, try to return the CPU to a preempted task.\n\t */\n\tif (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)\n\t\tse = cfs_rq->last;\n\n\t/*\n\t * Someone really wants this to run. If it's not unfair, run it.\n\t */\n\tif (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)\n\t\tse = cfs_rq->next;\n\n\tclear_buddies(cfs_rq, se);\n\n\treturn se;\n}"
        }
      },
      {
        "call_info": {
          "callee": "put_prev_task",
          "args": [
            "rq",
            "prev"
          ],
          "line": 6694
        },
        "resolved": true,
        "details": {
          "function_name": "put_prev_task_idle",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/idle.c",
          "lines": "399-401",
          "snippet": "static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)\n{\n}",
          "includes": [
            "#include <trace/events/power.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/power.h>\n#include \"sched.h\"\n\nstatic void put_prev_task_idle(struct rq *rq, struct task_struct *prev)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "parent_entity",
          "args": [
            "se"
          ],
          "line": 6682
        },
        "resolved": true,
        "details": {
          "function_name": "parent_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "453-456",
          "snippet": "static inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "is_same_group",
          "args": [
            "se",
            "pse"
          ],
          "line": 6672
        },
        "resolved": true,
        "details": {
          "function_name": "is_same_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "361-368",
          "snippet": "static inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "check_cfs_rq_runtime(cfs_rq)"
          ],
          "line": 6648
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "check_cfs_rq_runtime",
          "args": [
            "cfs_rq"
          ],
          "line": 6648
        },
        "resolved": true,
        "details": {
          "function_name": "check_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4986-4986",
          "snippet": "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }"
        }
      },
      {
        "call_info": {
          "callee": "set_last_buddy",
          "args": [
            "se"
          ],
          "line": 6600
        },
        "resolved": true,
        "details": {
          "function_name": "set_last_buddy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6487-6497",
          "snippet": "static void set_last_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->last = se;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void set_last_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->last = se;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 6599
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!se->on_rq || curr == rq->idle"
          ],
          "line": 6596
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wakeup_preempt_entity",
          "args": [
            "se",
            "pse"
          ],
          "line": 6573
        },
        "resolved": true,
        "details": {
          "function_name": "wakeup_preempt_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6472-6485",
          "snippet": "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "!pse"
          ],
          "line": 6572
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_matching_se",
          "args": [
            "&se",
            "&pse"
          ],
          "line": 6570
        },
        "resolved": true,
        "details": {
          "function_name": "find_matching_se",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "458-461",
          "snippet": "static inline void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "p->policy != SCHED_NORMAL"
          ],
          "line": 6567
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "p->policy != SCHED_IDLE"
          ],
          "line": 6560
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "curr->policy == SCHED_IDLE"
          ],
          "line": 6559
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "test_tsk_need_resched",
          "args": [
            "curr"
          ],
          "line": 6555
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "throttled_hierarchy(cfs_rq_of(pse))"
          ],
          "line": 6537
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "se == pse"
          ],
          "line": 6528
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define MAX_PINNED_INTERVAL\t512\n#define LBF_NOHZ_AGAIN\t0x20\n#define LBF_NOHZ_STATS\t0x10\n#define LBF_SOME_PINNED\t0x08\n#define LBF_DST_PINNED  0x04\n#define LBF_NEED_BREAK\t0x02\n#define LBF_ALL_PINNED\t0x01\n#define SKIP_AGE_LOAD\t0x0\n#define UPDATE_TG\t0x0\n#define SKIP_AGE_LOAD\t0x2\n#define UPDATE_TG\t0x1\n\nstatic unsigned int sched_nr_latency = 8;\nunsigned int sysctl_sched_child_runs_first;\nconst_debug unsigned int sysctl_sched_migration_cost\t= 500000UL;\nunsigned int capacity_margin\t\t\t\t= 1280;\nconst struct sched_class fair_sched_class;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void set_skip_buddy(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se)\n\t\tcfs_rq_of(se)->skip = se;\n}\n\n/*\n * Preempt the current task with a newly woken task if needed:\n */\nstatic void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct sched_entity *se = &curr->se, *pse = &p->se;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tint scale = cfs_rq->nr_running >= sched_nr_latency;\n\tint next_buddy_marked = 0;\n\n\tif (unlikely(se == pse))\n\t\treturn;\n\n\t/*\n\t * This is possible from callers such as attach_tasks(), in which we\n\t * unconditionally check_prempt_curr() after an enqueue (which may have\n\t * lead to a throttle).  This both saves work and prevents false\n\t * next-buddy nomination below.\n\t */\n\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))\n\t\treturn;\n\n\tif (sched_feat(NEXT_BUDDY) && scale && !(wake_flags & WF_FORK)) {\n\t\tset_next_buddy(pse);\n\t\tnext_buddy_marked = 1;\n\t}\n\n\t/*\n\t * We can come here with TIF_NEED_RESCHED already set from new task\n\t * wake up path.\n\t *\n\t * Note: this also catches the edge-case of curr being in a throttled\n\t * group (e.g. via set_curr_task), since update_curr() (in the\n\t * enqueue of curr) will have resulted in resched being set.  This\n\t * prevents us from potentially nominating it as a false LAST_BUDDY\n\t * below.\n\t */\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\t/* Idle tasks are by definition preempted by non-idle tasks. */\n\tif (unlikely(curr->policy == SCHED_IDLE) &&\n\t    likely(p->policy != SCHED_IDLE))\n\t\tgoto preempt;\n\n\t/*\n\t * Batch and idle tasks do not preempt non-idle tasks (their preemption\n\t * is driven by the tick):\n\t */\n\tif (unlikely(p->policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION))\n\t\treturn;\n\n\tfind_matching_se(&se, &pse);\n\tupdate_curr(cfs_rq_of(se));\n\tBUG_ON(!pse);\n\tif (wakeup_preempt_entity(se, pse) == 1) {\n\t\t/*\n\t\t * Bias pick_next to pick the sched entity that is\n\t\t * triggering this preemption.\n\t\t */\n\t\tif (!next_buddy_marked)\n\t\t\tset_next_buddy(pse);\n\t\tgoto preempt;\n\t}\n\n\treturn;\n\npreempt:\n\tresched_curr(rq);\n\t/*\n\t * Only set the backward buddy when the current task is still\n\t * on the rq. This can happen when a wakeup gets interleaved\n\t * with schedule on the ->pre_schedule() or idle_balance()\n\t * point, either of which can * drop the rq lock.\n\t *\n\t * Also, during early boot the idle thread is in the fair class,\n\t * for obvious reasons its a bad idea to schedule back to it.\n\t */\n\tif (unlikely(!se->on_rq || curr == rq->idle))\n\t\treturn;\n\n\tif (sched_feat(LAST_BUDDY) && scale && entity_is_task(se))\n\t\tset_last_buddy(se);\n}\n\nstatic struct task_struct *\npick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tstruct sched_entity *se;\n\tstruct task_struct *p;\n\tint new_tasks;\n\nagain:\n\tif (!cfs_rq->nr_running)\n\t\tgoto idle;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (prev->sched_class != &fair_sched_class)\n\t\tgoto simple;\n\n\t/*\n\t * Because of the set_next_buddy() in dequeue_task_fair() it is rather\n\t * likely that a next task is from the same cgroup as the current.\n\t *\n\t * Therefore attempt to avoid putting and setting the entire cgroup\n\t * hierarchy, only change the part that actually changes.\n\t */\n\n\tdo {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\n\t\t/*\n\t\t * Since we got here without doing put_prev_entity() we also\n\t\t * have to consider cfs_rq->curr. If it is still a runnable\n\t\t * entity, update_curr() will update its vruntime, otherwise\n\t\t * forget we've ever seen it.\n\t\t */\n\t\tif (curr) {\n\t\t\tif (curr->on_rq)\n\t\t\t\tupdate_curr(cfs_rq);\n\t\t\telse\n\t\t\t\tcurr = NULL;\n\n\t\t\t/*\n\t\t\t * This call to check_cfs_rq_runtime() will do the\n\t\t\t * throttle and dequeue its entity in the parent(s).\n\t\t\t * Therefore the nr_running test will indeed\n\t\t\t * be correct.\n\t\t\t */\n\t\t\tif (unlikely(check_cfs_rq_runtime(cfs_rq))) {\n\t\t\t\tcfs_rq = &rq->cfs;\n\n\t\t\t\tif (!cfs_rq->nr_running)\n\t\t\t\t\tgoto idle;\n\n\t\t\t\tgoto simple;\n\t\t\t}\n\t\t}\n\n\t\tse = pick_next_entity(cfs_rq, curr);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\n\t/*\n\t * Since we haven't yet done put_prev_entity and if the selected task\n\t * is a different task than we started out with, try and touch the\n\t * least amount of cfs_rqs.\n\t */\n\tif (prev != p) {\n\t\tstruct sched_entity *pse = &prev->se;\n\n\t\twhile (!(cfs_rq = is_same_group(se, pse))) {\n\t\t\tint se_depth = se->depth;\n\t\t\tint pse_depth = pse->depth;\n\n\t\t\tif (se_depth <= pse_depth) {\n\t\t\t\tput_prev_entity(cfs_rq_of(pse), pse);\n\t\t\t\tpse = parent_entity(pse);\n\t\t\t}\n\t\t\tif (se_depth >= pse_depth) {\n\t\t\t\tset_next_entity(cfs_rq_of(se), se);\n\t\t\t\tse = parent_entity(se);\n\t\t\t}\n\t\t}\n\n\t\tput_prev_entity(cfs_rq, pse);\n\t\tset_next_entity(cfs_rq, se);\n\t}\n\n\tgoto done;\nsimple:\n#endif\n\n\tput_prev_task(rq, prev);\n\n\tdo {\n\t\tse = pick_next_entity(cfs_rq, NULL);\n\t\tset_next_entity(cfs_rq, se);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\ndone: __maybe_unused;\n#ifdef CONFIG_SMP\n\t/*\n\t * Move the next running task to the front of\n\t * the list, so our cfs_tasks list becomes MRU\n\t * one.\n\t */\n\tlist_move(&p->se.group_node, &rq->cfs_tasks);\n#endif\n\n\tif (hrtick_enabled(rq))\n\t\thrtick_start_fair(rq, p);\n\n\tupdate_misfit_status(p, rq);\n\n\treturn p;\n\nidle:\n\tupdate_misfit_status(NULL, rq);\n\tnew_tasks = idle_balance(rq, rf);\n\n\t/*\n\t * Because idle_balance() releases (and re-acquires) rq->lock, it is\n\t * possible for any higher priority task to appear. In that case we\n\t * must re-start the pick_next_entity() loop.\n\t */\n\tif (new_tasks < 0)\n\t\treturn RETRY_TASK;\n\n\tif (new_tasks > 0)\n\t\tgoto again;\n\n\treturn NULL;\n}\n\n/*\n * Account for a descheduled task:\n */\nstatic void put_prev_task_fair(struct rq *rq, struct task_struct *prev)\n{\n\tstruct sched_entity *se = &prev->se;\n\tstruct cfs_rq *cfs_rq;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tput_prev_entity(cfs_rq, se);\n\t}\n}\n\n/*\n * sched_yield() is very simple\n *\n * The magic of dealing with the ->skip buddy is in pick_next_entity.\n */\nstatic void yield_task_fair(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tstruct sched_entity *se = &curr->se;\n\n\t/*\n\t * Are we the only task in the tree?\n\t */\n\tif (unlikely(rq->nr_running == 1))\n\t\treturn;\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (curr->policy != SCHED_BATCH) {\n\t\tupdate_rq_clock(rq);\n\t\t/*\n\t\t * Update run-time statistics of the 'current'.\n\t\t */\n\t\tupdate_curr(cfs_rq);\n\t\t/*\n\t\t * Tell update_rq_clock() that we've just updated,\n\t\t * so we don't do microscopic update in schedule()\n\t\t * and double the fastpath cost.\n\t\t */\n\t\trq_clock_skip_update(rq);\n\t}\n\n\tset_skip_buddy(se);\n}\n\nstatic bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/* throttled hierarchies are not runnable */\n\tif (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))\n\t\treturn false;\n\n\t/* Tell the scheduler that we'd really like pse to run next. */\n\tset_next_buddy(se);\n\n\tyield_task_fair(rq);\n\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\n/**************************************************\n * Fair scheduling class load-balancing methods.\n *\n * BASICS\n *\n * The purpose of load-balancing is to achieve the same basic fairness the\n * per-CPU scheduler provides, namely provide a proportional amount of compute\n * time to each task. This is expressed in the following equation:\n *\n *   W_i,n/P_i == W_j,n/P_j for all i,j                               (1)\n *\n * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight\n * W_i,0 is defined as:\n *\n *   W_i,0 = \\Sum_j w_i,j                                             (2)\n *\n * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight\n * is derived from the nice value as per sched_prio_to_weight[].\n *\n * The weight average is an exponential decay average of the instantaneous\n * weight:\n *\n *   W'_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0               (3)\n *\n * C_i is the compute capacity of CPU i, typically it is the\n * fraction of 'recent' time available for SCHED_OTHER task execution. But it\n * can also include other factors [XXX].\n *\n * To achieve this balance we define a measure of imbalance which follows\n * directly from (1):\n *\n *   imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j }    (4)\n *\n * We them move tasks around to minimize the imbalance. In the continuous\n * function space it is obvious this converges, in the discrete case we get\n * a few fun cases generally called infeasible weight scenarios.\n *\n * [XXX expand on:\n *     - infeasible weights;\n *     - local vs global optima in the discrete case. ]\n *\n *\n * SCHED DOMAINS\n *\n * In order to solve the imbalance equation (4), and avoid the obvious O(n^2)\n * for all i,j solution, we create a tree of CPUs that follows the hardware\n * topology where each level pairs two lower groups (or better). This results\n * in O(log n) layers. Furthermore we reduce the number of CPUs going up the\n * tree to only the first of the previous level and we decrease the frequency\n * of load-balance at each level inv. proportional to the number of CPUs in\n * the groups.\n *\n * This yields:\n *\n *     log_2 n     1     n\n *   \\Sum       { --- * --- * 2^i } = O(n)                            (5)\n *     i = 0      2^i   2^i\n *                               `- size of each group\n *         |         |     `- number of CPUs doing load-balance\n *         |         `- freq\n *         `- sum over all levels\n *\n * Coupled with a limit on how many tasks we can migrate every balance pass,\n * this makes (5) the runtime complexity of the balancer.\n *\n * An important property here is that each CPU is still (indirectly) connected\n * to every other CPU in at most O(log n) steps:\n *\n * The adjacency matrix of the resulting graph is given by:\n *\n *             log_2 n\n *   A_i,j = \\Union     (i % 2^k == 0) && i / 2^(k+1) == j / 2^(k+1)  (6)\n *             k = 0\n *\n * And you'll find that:\n *\n *   A^(log_2 n)_i,j != 0  for all i,j                                (7)\n *\n * Showing there's indeed a path between every CPU in at most O(log n) steps.\n * The task movement gives a factor of O(m), giving a convergence complexity\n * of:\n *\n *   O(nm log n),  n := nr_cpus, m := nr_tasks                        (8)\n *\n *\n * WORK CONSERVING\n *\n * In order to avoid CPUs going idle while there's still work to do, new idle\n * balancing is more aggressive and has the newly idle CPU iterate up the domain\n * tree itself instead of relying on other CPUs to bring it work.\n *\n * This adds some complexity to both (5) and (8) but it reduces the total idle\n * time.\n *\n * [XXX more?]\n *\n *\n * CGROUPS\n *\n * Cgroups make a horror show out of (2), instead of a simple sum we get:\n *\n *                                s_k,i\n *   W_i,0 = \\Sum_j \\Prod_k w_k * -----                               (9)\n *                                 S_k\n *\n * Where\n *\n *   s_k,i = \\Sum_j w_i,j,k  and  S_k = \\Sum_i s_k,i                 (10)\n *\n * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i.\n *\n * The big problem is S_k, its a global sum needed to compute a local (W_i)\n * property.\n *\n * [XXX write more on how we solve this.. _after_ merging pjt's patches that\n *      rewrite all of this once again.]\n */\n\nstatic unsigned long __read_mostly max_load_balance_interval = HZ/10;\n\nenum fbq_type { regular, remote, all };\n\nenum group_type {\n\tgroup_other = 0,\n\tgroup_misfit_task,\n\tgroup_imbalanced,\n\tgroup_overloaded,\n};\n\n#define LBF_ALL_PINNED\t0x01\n#define LBF_NEED_BREAK\t0x02\n#define LBF_DST_PINNED  0x04\n#define LBF_SOME_PINNED\t0x08\n#define LBF_NOHZ_STATS\t0x10\n#define LBF_NOHZ_AGAIN\t0x20\n\nstruct lb_env {\n\tstruct sched_domain\t*sd;\n\n\tstruct rq\t\t*src_rq;\n\tint\t\t\tsrc_cpu;\n\n\tint\t\t\tdst_cpu;\n\tstruct rq\t\t*dst_rq;\n\n\tstruct cpumask\t\t*dst_grpmask;\n\tint\t\t\tnew_dst_cpu;\n\tenum cpu_idle_type\tidle;\n\tlong\t\t\timbalance;\n\t/* The set of CPUs under consideration for load-balancing */\n\tstruct cpumask\t\t*cpus;\n\n\tunsigned int\t\tflags;\n\n\tunsigned int\t\tloop;\n\tunsigned int\t\tloop_break;\n\tunsigned int\t\tloop_max;\n\n\tenum fbq_type\t\tfbq_type;\n\tenum group_type\t\tsrc_grp_type;\n\tstruct list_head\ttasks;\n};\n\n/*\n * Is this task likely cache-hot:\n */\nstatic int task_hot(struct task_struct *p, struct lb_env *env)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(p->policy == SCHED_IDLE))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = rq_clock_task(env->src_rq) - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns 1, if task migration degrades locality\n * Returns 0, if task migration improves locality i.e migration preferred.\n * Returns -1, if task migration is not affected by locality.\n */\nstatic int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)\n{\n\tstruct numa_group *numa_group = rcu_dereference(p->numa_group);\n\tunsigned long src_weight, dst_weight;\n\tint src_nid, dst_nid, dist;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn -1;\n\n\tif (!p->numa_faults || !(env->sd->flags & SD_NUMA))\n\t\treturn -1;\n\n\tsrc_nid = cpu_to_node(env->src_cpu);\n\tdst_nid = cpu_to_node(env->dst_cpu);\n\n\tif (src_nid == dst_nid)\n\t\treturn -1;\n\n\t/* Migrating away from the preferred node is always bad. */\n\tif (src_nid == p->numa_preferred_nid) {\n\t\tif (env->src_rq->nr_running > env->src_rq->nr_preferred_running)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\t/* Encourage migration to the preferred node. */\n\tif (dst_nid == p->numa_preferred_nid)\n\t\treturn 0;\n\n\t/* Leaving a core idle is often worse than degrading locality. */\n\tif (env->idle == CPU_IDLE)\n\t\treturn -1;\n\n\tdist = node_distance(src_nid, dst_nid);\n\tif (numa_group) {\n\t\tsrc_weight = group_weight(p, src_nid, dist);\n\t\tdst_weight = group_weight(p, dst_nid, dist);\n\t} else {\n\t\tsrc_weight = task_weight(p, src_nid, dist);\n\t\tdst_weight = task_weight(p, dst_nid, dist);\n\t}\n\n\treturn dst_weight < src_weight;\n}\n\n#else\nstatic inline int migrate_degrades_locality(struct task_struct *p,\n\t\t\t\t\t     struct lb_env *env)\n{\n\treturn -1;\n}\n#endif\n\n/*\n * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?\n */\nstatic\nint can_migrate_task(struct task_struct *p, struct lb_env *env)\n{\n\tint tsk_cache_hot;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\t/*\n\t * We do not migrate tasks that are:\n\t * 1) throttled_lb_pair, or\n\t * 2) cannot be migrated to this CPU due to cpus_allowed, or\n\t * 3) running (obviously), or\n\t * 4) are cache-hot on their current CPU.\n\t */\n\tif (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(env->dst_cpu, &p->cpus_allowed)) {\n\t\tint cpu;\n\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_affine);\n\n\t\tenv->flags |= LBF_SOME_PINNED;\n\n\t\t/*\n\t\t * Remember if this task can be migrated to any other CPU in\n\t\t * our sched_group. We may want to revisit it if we couldn't\n\t\t * meet load balance goals by pulling other tasks on src_cpu.\n\t\t *\n\t\t * Avoid computing new_dst_cpu for NEWLY_IDLE or if we have\n\t\t * already computed one in current iteration.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE || (env->flags & LBF_DST_PINNED))\n\t\t\treturn 0;\n\n\t\t/* Prevent to re-select dst_cpu via env's CPUs: */\n\t\tfor_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {\n\t\t\tif (cpumask_test_cpu(cpu, &p->cpus_allowed)) {\n\t\t\t\tenv->flags |= LBF_DST_PINNED;\n\t\t\t\tenv->new_dst_cpu = cpu;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t/* Record that we found atleast one task that could run on dst_cpu */\n\tenv->flags &= ~LBF_ALL_PINNED;\n\n\tif (task_running(env->src_rq, p)) {\n\t\tschedstat_inc(p->se.statistics.nr_failed_migrations_running);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Aggressive migration if:\n\t * 1) destination numa is preferred\n\t * 2) task is cache cold, or\n\t * 3) too many balance attempts have failed.\n\t */\n\ttsk_cache_hot = migrate_degrades_locality(p, env);\n\tif (tsk_cache_hot == -1)\n\t\ttsk_cache_hot = task_hot(p, env);\n\n\tif (tsk_cache_hot <= 0 ||\n\t    env->sd->nr_balance_failed > env->sd->cache_nice_tries) {\n\t\tif (tsk_cache_hot == 1) {\n\t\t\tschedstat_inc(env->sd->lb_hot_gained[env->idle]);\n\t\t\tschedstat_inc(p->se.statistics.nr_forced_migrations);\n\t\t}\n\t\treturn 1;\n\t}\n\n\tschedstat_inc(p->se.statistics.nr_failed_migrations_hot);\n\treturn 0;\n}\n\n/*\n * detach_task() -- detach the task for the migration specified in env\n */\nstatic void detach_task(struct task_struct *p, struct lb_env *env)\n{\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\tdeactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);\n\tset_task_cpu(p, env->dst_cpu);\n}\n\n/*\n * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as\n * part of active balancing operations within \"domain\".\n *\n * Returns a task if successful and NULL otherwise.\n */\nstatic struct task_struct *detach_one_task(struct lb_env *env)\n{\n\tstruct task_struct *p;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tlist_for_each_entry_reverse(p,\n\t\t\t&env->src_rq->cfs_tasks, se.group_node) {\n\t\tif (!can_migrate_task(p, env))\n\t\t\tcontinue;\n\n\t\tdetach_task(p, env);\n\n\t\t/*\n\t\t * Right now, this is only the second place where\n\t\t * lb_gained[env->idle] is updated (other is detach_tasks)\n\t\t * so we can safely collect stats here rather than\n\t\t * inside detach_tasks().\n\t\t */\n\t\tschedstat_inc(env->sd->lb_gained[env->idle]);\n\t\treturn p;\n\t}\n\treturn NULL;\n}\n\nstatic const unsigned int sched_nr_migrate_break = 32;\n\n/*\n * detach_tasks() -- tries to detach up to imbalance weighted load from\n * busiest_rq, as part of a balancing operation within domain \"sd\".\n *\n * Returns number of detached tasks if successful and 0 otherwise.\n */\nstatic int detach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->src_rq->cfs_tasks;\n\tstruct task_struct *p;\n\tunsigned long load;\n\tint detached = 0;\n\n\tlockdep_assert_held(&env->src_rq->lock);\n\n\tif (env->imbalance <= 0)\n\t\treturn 0;\n\n\twhile (!list_empty(tasks)) {\n\t\t/*\n\t\t * We don't want to steal all, otherwise we may be treated likewise,\n\t\t * which could at worst lead to a livelock crash.\n\t\t */\n\t\tif (env->idle != CPU_NOT_IDLE && env->src_rq->nr_running <= 1)\n\t\t\tbreak;\n\n\t\tp = list_last_entry(tasks, struct task_struct, se.group_node);\n\n\t\tenv->loop++;\n\t\t/* We've more or less seen every task there is, call it quits */\n\t\tif (env->loop > env->loop_max)\n\t\t\tbreak;\n\n\t\t/* take a breather every nr_migrate tasks */\n\t\tif (env->loop > env->loop_break) {\n\t\t\tenv->loop_break += sched_nr_migrate_break;\n\t\t\tenv->flags |= LBF_NEED_BREAK;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!can_migrate_task(p, env))\n\t\t\tgoto next;\n\n\t\tload = task_h_load(p);\n\n\t\tif (sched_feat(LB_MIN) && load < 16 && !env->sd->nr_balance_failed)\n\t\t\tgoto next;\n\n\t\tif ((load / 2) > env->imbalance)\n\t\t\tgoto next;\n\n\t\tdetach_task(p, env);\n\t\tlist_add(&p->se.group_node, &env->tasks);\n\n\t\tdetached++;\n\t\tenv->imbalance -= load;\n\n#ifdef CONFIG_PREEMPT\n\t\t/*\n\t\t * NEWIDLE balancing is a source of latency, so preemptible\n\t\t * kernels will stop after the first task is detached to minimize\n\t\t * the critical section.\n\t\t */\n\t\tif (env->idle == CPU_NEWLY_IDLE)\n\t\t\tbreak;\n#endif\n\n\t\t/*\n\t\t * We only want to steal up to the prescribed amount of\n\t\t * weighted load.\n\t\t */\n\t\tif (env->imbalance <= 0)\n\t\t\tbreak;\n\n\t\tcontinue;\nnext:\n\t\tlist_move(&p->se.group_node, tasks);\n\t}\n\n\t/*\n\t * Right now, this is one of only two places we collect this stat\n\t * so we can safely collect detach_one_task() stats here rather\n\t * than inside detach_one_task().\n\t */\n\tschedstat_add(env->sd->lb_gained[env->idle], detached);\n\n\treturn detached;\n}\n\n/*\n * attach_task() -- attach the task detached by detach_task() to its new rq.\n */\nstatic void attach_task(struct rq *rq, struct task_struct *p)\n{\n\tlockdep_assert_held(&rq->lock);\n\n\tBUG_ON(task_rq(p) != rq);\n\tactivate_task(rq, p, ENQUEUE_NOCLOCK);\n\tp->on_rq = TASK_ON_RQ_QUEUED;\n\tcheck_preempt_curr(rq, p, 0);\n}\n\n/*\n * attach_one_task() -- attaches the task returned from detach_one_task() to\n * its new rq.\n */\nstatic void attach_one_task(struct rq *rq, struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\tattach_task(rq, p);\n\trq_unlock(rq, &rf);\n}\n\n/*\n * attach_tasks() -- attaches all tasks detached by detach_tasks() to their\n * new rq.\n */\nstatic void attach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->tasks;\n\tstruct task_struct *p;\n\tstruct rq_flags rf;\n\n\trq_lock(env->dst_rq, &rf);\n\tupdate_rq_clock(env->dst_rq);\n\n\twhile (!list_empty(tasks)) {\n\t\tp = list_first_entry(tasks, struct task_struct, se.group_node);\n\t\tlist_del_init(&p->se.group_node);\n\n\t\tattach_task(env->dst_rq, p);\n\t}\n\n\trq_unlock(env->dst_rq, &rf);\n}\n\nstatic inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->avg.load_avg)\n\t\treturn true;\n\n\tif (cfs_rq->avg.util_avg)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool others_have_blocked(struct rq *rq)\n{\n\tif (READ_ONCE(rq->avg_rt.util_avg))\n\t\treturn true;\n\n\tif (READ_ONCE(rq->avg_dl.util_avg))\n\t\treturn true;\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tif (READ_ONCE(rq->avg_irq.util_avg))\n\t\treturn true;\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->load.weight)\n\t\treturn false;\n\n\tif (cfs_rq->avg.load_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.util_sum)\n\t\treturn false;\n\n\tif (cfs_rq->avg.runnable_load_sum)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq, *pos;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\tbool done = true;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\t/*\n\t * Iterates the task_group tree in a bottom up fashion, see\n\t * list_add_leaf_cfs_rq() for details.\n\t */\n\tfor_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {\n\t\tstruct sched_entity *se;\n\n\t\t/* throttled entities do not contribute to load */\n\t\tif (throttled_hierarchy(cfs_rq))\n\t\t\tcontinue;\n\n\t\tif (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))\n\t\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t\t/* Propagate pending load changes to the parent, if any: */\n\t\tse = cfs_rq->tg->se[cpu];\n\t\tif (se && !skip_blocked_update(se))\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, 0);\n\n\t\t/*\n\t\t * There can be a lot of idle CPU cgroups.  Don't let fully\n\t\t * decayed cfs_rqs linger on the list.\n\t\t */\n\t\tif (cfs_rq_is_decayed(cfs_rq))\n\t\t\tlist_del_leaf_cfs_rq(cfs_rq);\n\n\t\t/* Don't need periodic decay once load/util_avg are null */\n\t\tif (cfs_rq_has_blocked(cfs_rq))\n\t\t\tdone = false;\n\t}\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n\t/* Don't need periodic decay once load/util_avg are null */\n\tif (others_have_blocked(rq))\n\t\tdone = false;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (done)\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n/*\n * Compute the hierarchical load factor for cfs_rq and all its ascendants.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];\n\tunsigned long now = jiffies;\n\tunsigned long load;\n\n\tif (cfs_rq->last_h_load_update == now)\n\t\treturn;\n\n\tcfs_rq->h_load_next = NULL;\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_load_next = se;\n\t\tif (cfs_rq->last_h_load_update == now)\n\t\t\tbreak;\n\t}\n\n\tif (!se) {\n\t\tcfs_rq->h_load = cfs_rq_load_avg(cfs_rq);\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n\n\twhile ((se = cfs_rq->h_load_next) != NULL) {\n\t\tload = cfs_rq->h_load;\n\t\tload = div64_ul(load * se->avg.load_avg,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n\t\tcfs_rq = group_cfs_rq(se);\n\t\tcfs_rq->h_load = load;\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n\n\tupdate_cfs_rq_h_load(cfs_rq);\n\treturn div64_ul(p->se.avg.load_avg * cfs_rq->h_load,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n}\n#else\nstatic inline void update_blocked_averages(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tconst struct sched_class *curr_class;\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\tupdate_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq);\n\n\tcurr_class = rq->curr->sched_class;\n\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == &rt_sched_class);\n\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == &dl_sched_class);\n\tupdate_irq_load_avg(rq, 0);\n#ifdef CONFIG_NO_HZ_COMMON\n\trq->last_blocked_load_update_tick = jiffies;\n\tif (!cfs_rq_has_blocked(cfs_rq) && !others_have_blocked(rq))\n\t\trq->has_blocked_load = 0;\n#endif\n\trq_unlock_irqrestore(rq, &rf);\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\treturn p->se.avg.load_avg;\n}\n#endif\n\n/********** Helpers for find_busiest_group ************************/\n\n/*\n * sg_lb_stats - stats of a sched_group required for load_balancing\n */\nstruct sg_lb_stats {\n\tunsigned long avg_load; /*Avg load across the CPUs of the group */\n\tunsigned long group_load; /* Total load over the CPUs of the group */\n\tunsigned long sum_weighted_load; /* Weighted load of group's tasks */\n\tunsigned long load_per_task;\n\tunsigned long group_capacity;\n\tunsigned long group_util; /* Total utilization of the group */\n\tunsigned int sum_nr_running; /* Nr tasks running in the group */\n\tunsigned int idle_cpus;\n\tunsigned int group_weight;\n\tenum group_type group_type;\n\tint group_no_capacity;\n\tunsigned long group_misfit_task_load; /* A CPU has a task too big for its capacity */\n#ifdef CONFIG_NUMA_BALANCING\n\tunsigned int nr_numa_running;\n\tunsigned int nr_preferred_running;\n#endif\n};\n\n/*\n * sd_lb_stats - Structure to store the statistics of a sched_domain\n *\t\t during load balancing.\n */\nstruct sd_lb_stats {\n\tstruct sched_group *busiest;\t/* Busiest group in this sd */\n\tstruct sched_group *local;\t/* Local group in this sd */\n\tunsigned long total_running;\n\tunsigned long total_load;\t/* Total load of all groups in sd */\n\tunsigned long total_capacity;\t/* Total capacity of all groups in sd */\n\tunsigned long avg_load;\t/* Average load across all groups in sd */\n\n\tstruct sg_lb_stats busiest_stat;/* Statistics of the busiest group */\n\tstruct sg_lb_stats local_stat;\t/* Statistics of the local group */\n};\n\nstatic inline void init_sd_lb_stats(struct sd_lb_stats *sds)\n{\n\t/*\n\t * Skimp on the clearing to avoid duplicate work. We can avoid clearing\n\t * local_stat because update_sg_lb_stats() does a full clear/assignment.\n\t * We must however clear busiest_stat::avg_load because\n\t * update_sd_pick_busiest() reads this before assignment.\n\t */\n\t*sds = (struct sd_lb_stats){\n\t\t.busiest = NULL,\n\t\t.local = NULL,\n\t\t.total_running = 0UL,\n\t\t.total_load = 0UL,\n\t\t.total_capacity = 0UL,\n\t\t.busiest_stat = {\n\t\t\t.avg_load = 0UL,\n\t\t\t.sum_nr_running = 0,\n\t\t\t.group_type = group_other,\n\t\t},\n\t};\n}\n\n/**\n * get_sd_load_idx - Obtain the load index for a given sched domain.\n * @sd: The sched_domain whose load_idx is to be obtained.\n * @idle: The idle status of the CPU for whose sd load_idx is obtained.\n *\n * Return: The load index.\n */\nstatic inline int get_sd_load_idx(struct sched_domain *sd,\n\t\t\t\t\tenum cpu_idle_type idle)\n{\n\tint load_idx;\n\n\tswitch (idle) {\n\tcase CPU_NOT_IDLE:\n\t\tload_idx = sd->busy_idx;\n\t\tbreak;\n\n\tcase CPU_NEWLY_IDLE:\n\t\tload_idx = sd->newidle_idx;\n\t\tbreak;\n\tdefault:\n\t\tload_idx = sd->idle_idx;\n\t\tbreak;\n\t}\n\n\treturn load_idx;\n}\n\nstatic unsigned long scale_rt_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long max = arch_scale_cpu_capacity(sd, cpu);\n\tunsigned long used, free;\n\tunsigned long irq;\n\n\tirq = cpu_util_irq(rq);\n\n\tif (unlikely(irq >= max))\n\t\treturn 1;\n\n\tused = READ_ONCE(rq->avg_rt.util_avg);\n\tused += READ_ONCE(rq->avg_dl.util_avg);\n\n\tif (unlikely(used >= max))\n\t\treturn 1;\n\n\tfree = max - used;\n\n\treturn scale_irq_capacity(free, irq, max);\n}\n\nstatic void update_cpu_capacity(struct sched_domain *sd, int cpu)\n{\n\tunsigned long capacity = scale_rt_capacity(sd, cpu);\n\tstruct sched_group *sdg = sd->groups;\n\n\tcpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(sd, cpu);\n\n\tif (!capacity)\n\t\tcapacity = 1;\n\n\tcpu_rq(cpu)->cpu_capacity = capacity;\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = capacity;\n\tsdg->sgc->max_capacity = capacity;\n}\n\nvoid update_group_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_domain *child = sd->child;\n\tstruct sched_group *group, *sdg = sd->groups;\n\tunsigned long capacity, min_capacity, max_capacity;\n\tunsigned long interval;\n\n\tinterval = msecs_to_jiffies(sd->balance_interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\tsdg->sgc->next_update = jiffies + interval;\n\n\tif (!child) {\n\t\tupdate_cpu_capacity(sd, cpu);\n\t\treturn;\n\t}\n\n\tcapacity = 0;\n\tmin_capacity = ULONG_MAX;\n\tmax_capacity = 0;\n\n\tif (child->flags & SD_OVERLAP) {\n\t\t/*\n\t\t * SD_OVERLAP domains cannot assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tfor_each_cpu(cpu, sched_group_span(sdg)) {\n\t\t\tstruct sched_group_capacity *sgc;\n\t\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\t\t/*\n\t\t\t * build_sched_domains() -> init_sched_groups_capacity()\n\t\t\t * gets here before we've attached the domains to the\n\t\t\t * runqueues.\n\t\t\t *\n\t\t\t * Use capacity_of(), which is set irrespective of domains\n\t\t\t * in update_cpu_capacity().\n\t\t\t *\n\t\t\t * This avoids capacity from being 0 and\n\t\t\t * causing divide-by-zero issues on boot.\n\t\t\t */\n\t\t\tif (unlikely(!rq->sd)) {\n\t\t\t\tcapacity += capacity_of(cpu);\n\t\t\t} else {\n\t\t\t\tsgc = rq->sd->groups->sgc;\n\t\t\t\tcapacity += sgc->capacity;\n\t\t\t}\n\n\t\t\tmin_capacity = min(capacity, min_capacity);\n\t\t\tmax_capacity = max(capacity, max_capacity);\n\t\t}\n\t} else  {\n\t\t/*\n\t\t * !SD_OVERLAP domains can assume that child groups\n\t\t * span the current group.\n\t\t */\n\n\t\tgroup = child->groups;\n\t\tdo {\n\t\t\tstruct sched_group_capacity *sgc = group->sgc;\n\n\t\t\tcapacity += sgc->capacity;\n\t\t\tmin_capacity = min(sgc->min_capacity, min_capacity);\n\t\t\tmax_capacity = max(sgc->max_capacity, max_capacity);\n\t\t\tgroup = group->next;\n\t\t} while (group != child->groups);\n\t}\n\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = min_capacity;\n\tsdg->sgc->max_capacity = max_capacity;\n}\n\n/*\n * Check whether the capacity of the rq has been noticeably reduced by side\n * activity. The imbalance_pct is used for the threshold.\n * Return true is the capacity is reduced\n */\nstatic inline int\ncheck_cpu_capacity(struct rq *rq, struct sched_domain *sd)\n{\n\treturn ((rq->cpu_capacity * sd->imbalance_pct) <\n\t\t\t\t(rq->cpu_capacity_orig * 100));\n}\n\n/*\n * Group imbalance indicates (and tries to solve) the problem where balancing\n * groups is inadequate due to ->cpus_allowed constraints.\n *\n * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a\n * cpumask covering 1 CPU of the first group and 3 CPUs of the second group.\n * Something like:\n *\n *\t{ 0 1 2 3 } { 4 5 6 7 }\n *\t        *     * * *\n *\n * If we were to balance group-wise we'd place two tasks in the first group and\n * two tasks in the second group. Clearly this is undesired as it will overload\n * cpu 3 and leave one of the CPUs in the second group unused.\n *\n * The current solution to this issue is detecting the skew in the first group\n * by noticing the lower domain failed to reach balance and had difficulty\n * moving tasks due to affinity constraints.\n *\n * When this is so detected; this group becomes a candidate for busiest; see\n * update_sd_pick_busiest(). And calculate_imbalance() and\n * find_busiest_group() avoid some of the usual balance conditions to allow it\n * to create an effective group imbalance.\n *\n * This is a somewhat tricky proposition since the next run might not find the\n * group imbalance and decide the groups need to be balanced again. A most\n * subtle and fragile situation.\n */\n\nstatic inline int sg_imbalanced(struct sched_group *group)\n{\n\treturn group->sgc->imbalance;\n}\n\n/*\n * group_has_capacity returns true if the group has spare capacity that could\n * be used by some tasks.\n * We consider that a group has spare capacity if the  * number of task is\n * smaller than the number of CPUs or if the utilization is lower than the\n * available capacity for CFS tasks.\n * For the latter, we use a threshold to stabilize the state, to take into\n * account the variance of the tasks' load and to return true if the available\n * capacity in meaningful for the load balancer.\n * As an example, an available capacity of 1% can appear but it doesn't make\n * any benefit for the load balance.\n */\nstatic inline bool\ngroup_has_capacity(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running < sgs->group_weight)\n\t\treturn true;\n\n\tif ((sgs->group_capacity * 100) >\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *  group_is_overloaded returns true if the group has more tasks than it can\n *  handle.\n *  group_is_overloaded is not equals to !group_has_capacity because a group\n *  with the exact right number of tasks, has no more spare capacity but is not\n *  overloaded so both group_has_capacity and group_is_overloaded return\n *  false.\n */\nstatic inline bool\ngroup_is_overloaded(struct lb_env *env, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running <= sgs->group_weight)\n\t\treturn false;\n\n\tif ((sgs->group_capacity * 100) <\n\t\t\t(sgs->group_util * env->sd->imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * group_smaller_min_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_min_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->min_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->min_capacity * 1024;\n}\n\n/*\n * group_smaller_max_cpu_capacity: Returns true if sched_group sg has smaller\n * per-CPU capacity_orig than sched_group ref.\n */\nstatic inline bool\ngroup_smaller_max_cpu_capacity(struct sched_group *sg, struct sched_group *ref)\n{\n\treturn sg->sgc->max_capacity * capacity_margin <\n\t\t\t\t\t\tref->sgc->max_capacity * 1024;\n}\n\nstatic inline enum\ngroup_type group_classify(struct sched_group *group,\n\t\t\t  struct sg_lb_stats *sgs)\n{\n\tif (sgs->group_no_capacity)\n\t\treturn group_overloaded;\n\n\tif (sg_imbalanced(group))\n\t\treturn group_imbalanced;\n\n\tif (sgs->group_misfit_task_load)\n\t\treturn group_misfit_task;\n\n\treturn group_other;\n}\n\nstatic bool update_nohz_stats(struct rq *rq, bool force)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned int cpu = rq->cpu;\n\n\tif (!rq->has_blocked_load)\n\t\treturn false;\n\n\tif (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))\n\t\treturn false;\n\n\tif (!force && !time_after(jiffies, rq->last_blocked_load_update_tick))\n\t\treturn true;\n\n\tupdate_blocked_averages(cpu);\n\n\treturn rq->has_blocked_load;\n#else\n\treturn false;\n#endif\n}\n\n/**\n * update_sg_lb_stats - Update sched_group's statistics for load balancing.\n * @env: The load balancing environment.\n * @group: sched_group whose statistics are to be updated.\n * @load_idx: Load index of sched_domain of this_cpu for load calc.\n * @local_group: Does group contain this_cpu.\n * @sgs: variable to hold the statistics for this group.\n * @overload: Indicate pullable load (e.g. >1 runnable task).\n */\nstatic inline void update_sg_lb_stats(struct lb_env *env,\n\t\t\tstruct sched_group *group, int load_idx,\n\t\t\tint local_group, struct sg_lb_stats *sgs,\n\t\t\tbool *overload)\n{\n\tunsigned long load;\n\tint i, nr_running;\n\n\tmemset(sgs, 0, sizeof(*sgs));\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tstruct rq *rq = cpu_rq(i);\n\n\t\tif ((env->flags & LBF_NOHZ_STATS) && update_nohz_stats(rq, false))\n\t\t\tenv->flags |= LBF_NOHZ_AGAIN;\n\n\t\t/* Bias balancing toward CPUs of our domain: */\n\t\tif (local_group)\n\t\t\tload = target_load(i, load_idx);\n\t\telse\n\t\t\tload = source_load(i, load_idx);\n\n\t\tsgs->group_load += load;\n\t\tsgs->group_util += cpu_util(i);\n\t\tsgs->sum_nr_running += rq->cfs.h_nr_running;\n\n\t\tnr_running = rq->nr_running;\n\t\tif (nr_running > 1)\n\t\t\t*overload = true;\n\n#ifdef CONFIG_NUMA_BALANCING\n\t\tsgs->nr_numa_running += rq->nr_numa_running;\n\t\tsgs->nr_preferred_running += rq->nr_preferred_running;\n#endif\n\t\tsgs->sum_weighted_load += weighted_cpuload(rq);\n\t\t/*\n\t\t * No need to call idle_cpu() if nr_running is not 0\n\t\t */\n\t\tif (!nr_running && idle_cpu(i))\n\t\t\tsgs->idle_cpus++;\n\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    sgs->group_misfit_task_load < rq->misfit_task_load) {\n\t\t\tsgs->group_misfit_task_load = rq->misfit_task_load;\n\t\t\t*overload = 1;\n\t\t}\n\t}\n\n\t/* Adjust by relative CPU capacity of the group */\n\tsgs->group_capacity = group->sgc->capacity;\n\tsgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;\n\n\tif (sgs->sum_nr_running)\n\t\tsgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;\n\n\tsgs->group_weight = group->group_weight;\n\n\tsgs->group_no_capacity = group_is_overloaded(env, sgs);\n\tsgs->group_type = group_classify(group, sgs);\n}\n\n/**\n * update_sd_pick_busiest - return 1 on busiest group\n * @env: The load balancing environment.\n * @sds: sched_domain statistics\n * @sg: sched_group candidate to be checked for being the busiest\n * @sgs: sched_group statistics\n *\n * Determine if @sg is a busier group than the previously selected\n * busiest group.\n *\n * Return: %true if @sg is a busier group than the previously selected\n * busiest group. %false otherwise.\n */\nstatic bool update_sd_pick_busiest(struct lb_env *env,\n\t\t\t\t   struct sd_lb_stats *sds,\n\t\t\t\t   struct sched_group *sg,\n\t\t\t\t   struct sg_lb_stats *sgs)\n{\n\tstruct sg_lb_stats *busiest = &sds->busiest_stat;\n\n\t/*\n\t * Don't try to pull misfit tasks we can't help.\n\t * We can use max_capacity here as reduction in capacity on some\n\t * CPUs in the group should either be possible to resolve\n\t * internally or be covered by avg_load imbalance (eventually).\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    (!group_smaller_max_cpu_capacity(sg, sds->local) ||\n\t     !group_has_capacity(env, &sds->local_stat)))\n\t\treturn false;\n\n\tif (sgs->group_type > busiest->group_type)\n\t\treturn true;\n\n\tif (sgs->group_type < busiest->group_type)\n\t\treturn false;\n\n\tif (sgs->avg_load <= busiest->avg_load)\n\t\treturn false;\n\n\tif (!(env->sd->flags & SD_ASYM_CPUCAPACITY))\n\t\tgoto asym_packing;\n\n\t/*\n\t * Candidate sg has no more than one task per CPU and\n\t * has higher per-CPU capacity. Migrating tasks to less\n\t * capable CPUs may harm throughput. Maximize throughput,\n\t * power/energy consequences are not considered.\n\t */\n\tif (sgs->sum_nr_running <= sgs->group_weight &&\n\t    group_smaller_min_cpu_capacity(sds->local, sg))\n\t\treturn false;\n\n\t/*\n\t * If we have more than one misfit sg go with the biggest misfit.\n\t */\n\tif (sgs->group_type == group_misfit_task &&\n\t    sgs->group_misfit_task_load < busiest->group_misfit_task_load)\n\t\treturn false;\n\nasym_packing:\n\t/* This is the busiest node in its class. */\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn true;\n\n\t/* No ASYM_PACKING if target CPU is already busy */\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn true;\n\t/*\n\t * ASYM_PACKING needs to move all the work to the highest\n\t * prority CPUs in the group, therefore mark all groups\n\t * of lower priority than ourself as busy.\n\t */\n\tif (sgs->sum_nr_running &&\n\t    sched_asym_prefer(env->dst_cpu, sg->asym_prefer_cpu)) {\n\t\tif (!sds->busiest)\n\t\t\treturn true;\n\n\t\t/* Prefer to move from lowest priority CPU's work */\n\t\tif (sched_asym_prefer(sds->busiest->asym_prefer_cpu,\n\t\t\t\t      sg->asym_prefer_cpu))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running > sgs->nr_numa_running)\n\t\treturn regular;\n\tif (sgs->sum_nr_running > sgs->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\tif (rq->nr_running > rq->nr_numa_running)\n\t\treturn regular;\n\tif (rq->nr_running > rq->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n#else\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\treturn regular;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n/**\n * update_sd_lb_stats - Update sched_domain's statistics for load balancing.\n * @env: The load balancing environment.\n * @sds: variable to hold the statistics for this sched_domain.\n */\nstatic inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tstruct sched_domain *child = env->sd->child;\n\tstruct sched_group *sg = env->sd->groups;\n\tstruct sg_lb_stats *local = &sds->local_stat;\n\tstruct sg_lb_stats tmp_sgs;\n\tint load_idx;\n\tbool overload = false;\n\tbool prefer_sibling = child && child->flags & SD_PREFER_SIBLING;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif (env->idle == CPU_NEWLY_IDLE && READ_ONCE(nohz.has_blocked))\n\t\tenv->flags |= LBF_NOHZ_STATS;\n#endif\n\n\tload_idx = get_sd_load_idx(env->sd, env->idle);\n\n\tdo {\n\t\tstruct sg_lb_stats *sgs = &tmp_sgs;\n\t\tint local_group;\n\n\t\tlocal_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));\n\t\tif (local_group) {\n\t\t\tsds->local = sg;\n\t\t\tsgs = local;\n\n\t\t\tif (env->idle != CPU_NEWLY_IDLE ||\n\t\t\t    time_after_eq(jiffies, sg->sgc->next_update))\n\t\t\t\tupdate_group_capacity(env->sd, env->dst_cpu);\n\t\t}\n\n\t\tupdate_sg_lb_stats(env, sg, load_idx, local_group, sgs,\n\t\t\t\t\t\t&overload);\n\n\t\tif (local_group)\n\t\t\tgoto next_group;\n\n\t\t/*\n\t\t * In case the child domain prefers tasks go to siblings\n\t\t * first, lower the sg capacity so that we'll try\n\t\t * and move all the excess tasks away. We lower the capacity\n\t\t * of a group only if the local group has the capacity to fit\n\t\t * these excess tasks. The extra check prevents the case where\n\t\t * you always pull from the heaviest group when it is already\n\t\t * under-utilized (possible with a large weight task outweighs\n\t\t * the tasks on the system).\n\t\t */\n\t\tif (prefer_sibling && sds->local &&\n\t\t    group_has_capacity(env, local) &&\n\t\t    (sgs->sum_nr_running > local->sum_nr_running + 1)) {\n\t\t\tsgs->group_no_capacity = 1;\n\t\t\tsgs->group_type = group_classify(sg, sgs);\n\t\t}\n\n\t\tif (update_sd_pick_busiest(env, sds, sg, sgs)) {\n\t\t\tsds->busiest = sg;\n\t\t\tsds->busiest_stat = *sgs;\n\t\t}\n\nnext_group:\n\t\t/* Now, start updating sd_lb_stats */\n\t\tsds->total_running += sgs->sum_nr_running;\n\t\tsds->total_load += sgs->group_load;\n\t\tsds->total_capacity += sgs->group_capacity;\n\n\t\tsg = sg->next;\n\t} while (sg != env->sd->groups);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tif ((env->flags & LBF_NOHZ_AGAIN) &&\n\t    cpumask_subset(nohz.idle_cpus_mask, sched_domain_span(env->sd))) {\n\n\t\tWRITE_ONCE(nohz.next_blocked,\n\t\t\t   jiffies + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\t}\n#endif\n\n\tif (env->sd->flags & SD_NUMA)\n\t\tenv->fbq_type = fbq_classify_group(&sds->busiest_stat);\n\n\tif (!env->sd->parent) {\n\t\t/* update overload indicator if we are at root domain */\n\t\tif (READ_ONCE(env->dst_rq->rd->overload) != overload)\n\t\t\tWRITE_ONCE(env->dst_rq->rd->overload, overload);\n\t}\n}\n\n/**\n * check_asym_packing - Check to see if the group is packed into the\n *\t\t\tsched domain.\n *\n * This is primarily intended to used at the sibling level.  Some\n * cores like POWER7 prefer to use lower numbered SMT threads.  In the\n * case of POWER7, it can move to lower SMT modes only when higher\n * threads are idle.  When in lower SMT modes, the threads will\n * perform better since they share less core resources.  Hence when we\n * have idle threads, we want them to be the higher ones.\n *\n * This packing function is run on idle threads.  It checks to see if\n * the busiest CPU in this domain (core in the P7 case) has a higher\n * CPU number than the packing function is being run on.  Here we are\n * assuming lower CPU number will be equivalent to lower a SMT thread\n * number.\n *\n * Return: 1 when packing is required and a task should be moved to\n * this CPU.  The amount of the imbalance is returned in env->imbalance.\n *\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain which is to be packed\n */\nstatic int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tint busiest_cpu;\n\n\tif (!(env->sd->flags & SD_ASYM_PACKING))\n\t\treturn 0;\n\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn 0;\n\n\tif (!sds->busiest)\n\t\treturn 0;\n\n\tbusiest_cpu = sds->busiest->asym_prefer_cpu;\n\tif (sched_asym_prefer(busiest_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\tenv->imbalance = DIV_ROUND_CLOSEST(\n\t\tsds->busiest_stat.avg_load * sds->busiest_stat.group_capacity,\n\t\tSCHED_CAPACITY_SCALE);\n\n\treturn 1;\n}\n\n/**\n * fix_small_imbalance - Calculate the minor imbalance that exists\n *\t\t\tamongst the groups of a sched_domain, during\n *\t\t\tload balancing.\n * @env: The load balancing environment.\n * @sds: Statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline\nvoid fix_small_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long tmp, capa_now = 0, capa_move = 0;\n\tunsigned int imbn = 2;\n\tunsigned long scaled_busy_load_per_task;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (!local->sum_nr_running)\n\t\tlocal->load_per_task = cpu_avg_load_per_task(env->dst_cpu);\n\telse if (busiest->load_per_task > local->load_per_task)\n\t\timbn = 1;\n\n\tscaled_busy_load_per_task =\n\t\t(busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\tbusiest->group_capacity;\n\n\tif (busiest->avg_load + scaled_busy_load_per_task >=\n\t    local->avg_load + (scaled_busy_load_per_task * imbn)) {\n\t\tenv->imbalance = busiest->load_per_task;\n\t\treturn;\n\t}\n\n\t/*\n\t * OK, we don't have enough imbalance to justify moving tasks,\n\t * however we may be able to increase total CPU capacity used by\n\t * moving them.\n\t */\n\n\tcapa_now += busiest->group_capacity *\n\t\t\tmin(busiest->load_per_task, busiest->avg_load);\n\tcapa_now += local->group_capacity *\n\t\t\tmin(local->load_per_task, local->avg_load);\n\tcapa_now /= SCHED_CAPACITY_SCALE;\n\n\t/* Amount of load we'd subtract */\n\tif (busiest->avg_load > scaled_busy_load_per_task) {\n\t\tcapa_move += busiest->group_capacity *\n\t\t\t    min(busiest->load_per_task,\n\t\t\t\tbusiest->avg_load - scaled_busy_load_per_task);\n\t}\n\n\t/* Amount of load we'd add */\n\tif (busiest->avg_load * busiest->group_capacity <\n\t    busiest->load_per_task * SCHED_CAPACITY_SCALE) {\n\t\ttmp = (busiest->avg_load * busiest->group_capacity) /\n\t\t      local->group_capacity;\n\t} else {\n\t\ttmp = (busiest->load_per_task * SCHED_CAPACITY_SCALE) /\n\t\t      local->group_capacity;\n\t}\n\tcapa_move += local->group_capacity *\n\t\t    min(local->load_per_task, local->avg_load + tmp);\n\tcapa_move /= SCHED_CAPACITY_SCALE;\n\n\t/* Move if we gain throughput */\n\tif (capa_move > capa_now)\n\t\tenv->imbalance = busiest->load_per_task;\n}\n\n/**\n * calculate_imbalance - Calculate the amount of imbalance present within the\n *\t\t\t groups of a given sched_domain during load balance.\n * @env: load balance environment\n * @sds: statistics of the sched_domain whose imbalance is to be calculated.\n */\nstatic inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tunsigned long max_pull, load_above_capacity = ~0UL;\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (busiest->group_type == group_imbalanced) {\n\t\t/*\n\t\t * In the group_imb case we cannot rely on group-wide averages\n\t\t * to ensure CPU-load equilibrium, look at wider averages. XXX\n\t\t */\n\t\tbusiest->load_per_task =\n\t\t\tmin(busiest->load_per_task, sds->avg_load);\n\t}\n\n\t/*\n\t * Avg load of busiest sg can be less and avg load of local sg can\n\t * be greater than avg load across all sgs of sd because avg load\n\t * factors in sg capacity and sgs with smaller group_type are\n\t * skipped when updating the busiest sg:\n\t */\n\tif (busiest->group_type != group_misfit_task &&\n\t    (busiest->avg_load <= sds->avg_load ||\n\t     local->avg_load >= sds->avg_load)) {\n\t\tenv->imbalance = 0;\n\t\treturn fix_small_imbalance(env, sds);\n\t}\n\n\t/*\n\t * If there aren't any idle CPUs, avoid creating some.\n\t */\n\tif (busiest->group_type == group_overloaded &&\n\t    local->group_type   == group_overloaded) {\n\t\tload_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;\n\t\tif (load_above_capacity > busiest->group_capacity) {\n\t\t\tload_above_capacity -= busiest->group_capacity;\n\t\t\tload_above_capacity *= scale_load_down(NICE_0_LOAD);\n\t\t\tload_above_capacity /= busiest->group_capacity;\n\t\t} else\n\t\t\tload_above_capacity = ~0UL;\n\t}\n\n\t/*\n\t * We're trying to get all the CPUs to the average_load, so we don't\n\t * want to push ourselves above the average load, nor do we wish to\n\t * reduce the max loaded CPU below the average load. At the same time,\n\t * we also don't want to reduce the group load below the group\n\t * capacity. Thus we look for the minimum possible imbalance.\n\t */\n\tmax_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);\n\n\t/* How much load to actually move to equalise the imbalance */\n\tenv->imbalance = min(\n\t\tmax_pull * busiest->group_capacity,\n\t\t(sds->avg_load - local->avg_load) * local->group_capacity\n\t) / SCHED_CAPACITY_SCALE;\n\n\t/* Boost imbalance to allow misfit task to be balanced. */\n\tif (busiest->group_type == group_misfit_task) {\n\t\tenv->imbalance = max_t(long, env->imbalance,\n\t\t\t\t       busiest->group_misfit_task_load);\n\t}\n\n\t/*\n\t * if *imbalance is less than the average load per runnable task\n\t * there is no guarantee that any tasks will be moved so we'll have\n\t * a think about bumping its value to force at least one task to be\n\t * moved\n\t */\n\tif (env->imbalance < busiest->load_per_task)\n\t\treturn fix_small_imbalance(env, sds);\n}\n\n/******* find_busiest_group() helpers end here *********************/\n\n/**\n * find_busiest_group - Returns the busiest group within the sched_domain\n * if there is an imbalance.\n *\n * Also calculates the amount of weighted load which should be moved\n * to restore balance.\n *\n * @env: The load balancing environment.\n *\n * Return:\t- The busiest group if imbalance exists.\n */\nstatic struct sched_group *find_busiest_group(struct lb_env *env)\n{\n\tstruct sg_lb_stats *local, *busiest;\n\tstruct sd_lb_stats sds;\n\n\tinit_sd_lb_stats(&sds);\n\n\t/*\n\t * Compute the various statistics relavent for load balancing at\n\t * this level.\n\t */\n\tupdate_sd_lb_stats(env, &sds);\n\tlocal = &sds.local_stat;\n\tbusiest = &sds.busiest_stat;\n\n\t/* ASYM feature bypasses nice load balance check */\n\tif (check_asym_packing(env, &sds))\n\t\treturn sds.busiest;\n\n\t/* There is no busy sibling group to pull tasks from */\n\tif (!sds.busiest || busiest->sum_nr_running == 0)\n\t\tgoto out_balanced;\n\n\t/* XXX broken for overlapping NUMA groups */\n\tsds.avg_load = (SCHED_CAPACITY_SCALE * sds.total_load)\n\t\t\t\t\t\t/ sds.total_capacity;\n\n\t/*\n\t * If the busiest group is imbalanced the below checks don't\n\t * work because they assume all things are equal, which typically\n\t * isn't true due to cpus_allowed constraints and the like.\n\t */\n\tif (busiest->group_type == group_imbalanced)\n\t\tgoto force_balance;\n\n\t/*\n\t * When dst_cpu is idle, prevent SMP nice and/or asymmetric group\n\t * capacities from resulting in underutilization due to avg_load.\n\t */\n\tif (env->idle != CPU_NOT_IDLE && group_has_capacity(env, local) &&\n\t    busiest->group_no_capacity)\n\t\tgoto force_balance;\n\n\t/* Misfit tasks should be dealt with regardless of the avg load */\n\tif (busiest->group_type == group_misfit_task)\n\t\tgoto force_balance;\n\n\t/*\n\t * If the local group is busier than the selected busiest group\n\t * don't try and pull any tasks.\n\t */\n\tif (local->avg_load >= busiest->avg_load)\n\t\tgoto out_balanced;\n\n\t/*\n\t * Don't pull any tasks if this group is already above the domain\n\t * average load.\n\t */\n\tif (local->avg_load >= sds.avg_load)\n\t\tgoto out_balanced;\n\n\tif (env->idle == CPU_IDLE) {\n\t\t/*\n\t\t * This CPU is idle. If the busiest group is not overloaded\n\t\t * and there is no imbalance between this and busiest group\n\t\t * wrt idle CPUs, it is balanced. The imbalance becomes\n\t\t * significant if the diff is greater than 1 otherwise we\n\t\t * might end up to just move the imbalance on another group\n\t\t */\n\t\tif ((busiest->group_type != group_overloaded) &&\n\t\t\t\t(local->idle_cpus <= (busiest->idle_cpus + 1)))\n\t\t\tgoto out_balanced;\n\t} else {\n\t\t/*\n\t\t * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use\n\t\t * imbalance_pct to be conservative.\n\t\t */\n\t\tif (100 * busiest->avg_load <=\n\t\t\t\tenv->sd->imbalance_pct * local->avg_load)\n\t\t\tgoto out_balanced;\n\t}\n\nforce_balance:\n\t/* Looks like there is an imbalance. Compute it */\n\tenv->src_grp_type = busiest->group_type;\n\tcalculate_imbalance(env, &sds);\n\treturn env->imbalance ? sds.busiest : NULL;\n\nout_balanced:\n\tenv->imbalance = 0;\n\treturn NULL;\n}\n\n/*\n * find_busiest_queue - find the busiest runqueue among the CPUs in the group.\n */\nstatic struct rq *find_busiest_queue(struct lb_env *env,\n\t\t\t\t     struct sched_group *group)\n{\n\tstruct rq *busiest = NULL, *rq;\n\tunsigned long busiest_load = 0, busiest_capacity = 1;\n\tint i;\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tunsigned long capacity, wl;\n\t\tenum fbq_type rt;\n\n\t\trq = cpu_rq(i);\n\t\trt = fbq_classify_rq(rq);\n\n\t\t/*\n\t\t * We classify groups/runqueues into three groups:\n\t\t *  - regular: there are !numa tasks\n\t\t *  - remote:  there are numa tasks that run on the 'wrong' node\n\t\t *  - all:     there is no distinction\n\t\t *\n\t\t * In order to avoid migrating ideally placed numa tasks,\n\t\t * ignore those when there's better options.\n\t\t *\n\t\t * If we ignore the actual busiest queue to migrate another\n\t\t * task, the next balance pass can still reduce the busiest\n\t\t * queue by moving tasks around inside the node.\n\t\t *\n\t\t * If we cannot move enough load due to this classification\n\t\t * the next pass will adjust the group classification and\n\t\t * allow migration of more tasks.\n\t\t *\n\t\t * Both cases only affect the total convergence complexity.\n\t\t */\n\t\tif (rt > env->fbq_type)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains with misfit tasks we simply\n\t\t * seek the \"biggest\" misfit task.\n\t\t */\n\t\tif (env->src_grp_type == group_misfit_task) {\n\t\t\tif (rq->misfit_task_load > busiest_load) {\n\t\t\t\tbusiest_load = rq->misfit_task_load;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tcapacity = capacity_of(i);\n\n\t\t/*\n\t\t * For ASYM_CPUCAPACITY domains, don't pick a CPU that could\n\t\t * eventually lead to active_balancing high->low capacity.\n\t\t * Higher per-CPU capacity is considered better than balancing\n\t\t * average load.\n\t\t */\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    capacity_of(env->dst_cpu) < capacity &&\n\t\t    rq->nr_running == 1)\n\t\t\tcontinue;\n\n\t\twl = weighted_cpuload(rq);\n\n\t\t/*\n\t\t * When comparing with imbalance, use weighted_cpuload()\n\t\t * which is not scaled with the CPU capacity.\n\t\t */\n\n\t\tif (rq->nr_running == 1 && wl > env->imbalance &&\n\t\t    !check_cpu_capacity(rq, env->sd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For the load comparisons with the other CPU's, consider\n\t\t * the weighted_cpuload() scaled with the CPU capacity, so\n\t\t * that the load can be moved away from the CPU that is\n\t\t * potentially running at a lower capacity.\n\t\t *\n\t\t * Thus we're looking for max(wl_i / capacity_i), crosswise\n\t\t * multiplication to rid ourselves of the division works out\n\t\t * to: wl_i * capacity_j > wl_j * capacity_i;  where j is\n\t\t * our previous maximum.\n\t\t */\n\t\tif (wl * busiest_capacity > busiest_load * capacity) {\n\t\t\tbusiest_load = wl;\n\t\t\tbusiest_capacity = capacity;\n\t\t\tbusiest = rq;\n\t\t}\n\t}\n\n\treturn busiest;\n}\n\n/*\n * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but\n * so long as it is large enough.\n */\n#define MAX_PINNED_INTERVAL\t512\n\nstatic int need_active_balance(struct lb_env *env)\n{\n\tstruct sched_domain *sd = env->sd;\n\n\tif (env->idle == CPU_NEWLY_IDLE) {\n\n\t\t/*\n\t\t * ASYM_PACKING needs to force migrate tasks from busy but\n\t\t * lower priority CPUs in order to pack all tasks in the\n\t\t * highest priority CPUs.\n\t\t */\n\t\tif ((sd->flags & SD_ASYM_PACKING) &&\n\t\t    sched_asym_prefer(env->dst_cpu, env->src_cpu))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task.\n\t * It's worth migrating the task if the src_cpu's capacity is reduced\n\t * because of other sched_class or IRQs if more capacity stays\n\t * available on dst_cpu.\n\t */\n\tif ((env->idle != CPU_NOT_IDLE) &&\n\t    (env->src_rq->cfs.h_nr_running == 1)) {\n\t\tif ((check_cpu_capacity(env->src_rq, sd)) &&\n\t\t    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))\n\t\t\treturn 1;\n\t}\n\n\tif (env->src_grp_type == group_misfit_task)\n\t\treturn 1;\n\n\treturn unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);\n}\n\nstatic int active_load_balance_cpu_stop(void *data);\n\nstatic int should_we_balance(struct lb_env *env)\n{\n\tstruct sched_group *sg = env->sd->groups;\n\tint cpu, balance_cpu = -1;\n\n\t/*\n\t * Ensure the balancing environment is consistent; can happen\n\t * when the softirq triggers 'during' hotplug.\n\t */\n\tif (!cpumask_test_cpu(env->dst_cpu, env->cpus))\n\t\treturn 0;\n\n\t/*\n\t * In the newly idle case, we will allow all the CPUs\n\t * to do the newly idle load balance.\n\t */\n\tif (env->idle == CPU_NEWLY_IDLE)\n\t\treturn 1;\n\n\t/* Try to find first idle CPU */\n\tfor_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {\n\t\tif (!idle_cpu(cpu))\n\t\t\tcontinue;\n\n\t\tbalance_cpu = cpu;\n\t\tbreak;\n\t}\n\n\tif (balance_cpu == -1)\n\t\tbalance_cpu = group_balance_cpu(sg);\n\n\t/*\n\t * First idle CPU or the first CPU(busiest) in this sched group\n\t * is eligible for doing load balancing at this and above domains.\n\t */\n\treturn balance_cpu == env->dst_cpu;\n}\n\n/*\n * Check this_cpu to ensure it is balanced within domain. Attempt to move\n * tasks if there is an imbalance.\n */\nstatic int load_balance(int this_cpu, struct rq *this_rq,\n\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n\t\t\tint *continue_balancing)\n{\n\tint ld_moved, cur_ld_moved, active_balance = 0;\n\tstruct sched_domain *sd_parent = sd->parent;\n\tstruct sched_group *group;\n\tstruct rq *busiest;\n\tstruct rq_flags rf;\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);\n\n\tstruct lb_env env = {\n\t\t.sd\t\t= sd,\n\t\t.dst_cpu\t= this_cpu,\n\t\t.dst_rq\t\t= this_rq,\n\t\t.dst_grpmask    = sched_group_span(sd->groups),\n\t\t.idle\t\t= idle,\n\t\t.loop_break\t= sched_nr_migrate_break,\n\t\t.cpus\t\t= cpus,\n\t\t.fbq_type\t= all,\n\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),\n\t};\n\n\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);\n\n\tschedstat_inc(sd->lb_count[idle]);\n\nredo:\n\tif (!should_we_balance(&env)) {\n\t\t*continue_balancing = 0;\n\t\tgoto out_balanced;\n\t}\n\n\tgroup = find_busiest_group(&env);\n\tif (!group) {\n\t\tschedstat_inc(sd->lb_nobusyg[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tbusiest = find_busiest_queue(&env, group);\n\tif (!busiest) {\n\t\tschedstat_inc(sd->lb_nobusyq[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tBUG_ON(busiest == env.dst_rq);\n\n\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);\n\n\tenv.src_cpu = busiest->cpu;\n\tenv.src_rq = busiest;\n\n\tld_moved = 0;\n\tif (busiest->nr_running > 1) {\n\t\t/*\n\t\t * Attempt to move tasks. If find_busiest_group has found\n\t\t * an imbalance but busiest->nr_running <= 1, the group is\n\t\t * still unbalanced. ld_moved simply stays zero, so it is\n\t\t * correctly treated as an imbalance.\n\t\t */\n\t\tenv.flags |= LBF_ALL_PINNED;\n\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);\n\nmore_balance:\n\t\trq_lock_irqsave(busiest, &rf);\n\t\tupdate_rq_clock(busiest);\n\n\t\t/*\n\t\t * cur_ld_moved - load moved in current iteration\n\t\t * ld_moved     - cumulative load moved across iterations\n\t\t */\n\t\tcur_ld_moved = detach_tasks(&env);\n\n\t\t/*\n\t\t * We've detached some tasks from busiest_rq. Every\n\t\t * task is masked \"TASK_ON_RQ_MIGRATING\", so we can safely\n\t\t * unlock busiest->lock, and we are able to be sure\n\t\t * that nobody can manipulate the tasks in parallel.\n\t\t * See task_rq_lock() family for the details.\n\t\t */\n\n\t\trq_unlock(busiest, &rf);\n\n\t\tif (cur_ld_moved) {\n\t\t\tattach_tasks(&env);\n\t\t\tld_moved += cur_ld_moved;\n\t\t}\n\n\t\tlocal_irq_restore(rf.flags);\n\n\t\tif (env.flags & LBF_NEED_BREAK) {\n\t\t\tenv.flags &= ~LBF_NEED_BREAK;\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * Revisit (affine) tasks on src_cpu that couldn't be moved to\n\t\t * us and move them to an alternate dst_cpu in our sched_group\n\t\t * where they can run. The upper limit on how many times we\n\t\t * iterate on same src_cpu is dependent on number of CPUs in our\n\t\t * sched_group.\n\t\t *\n\t\t * This changes load balance semantics a bit on who can move\n\t\t * load to a given_cpu. In addition to the given_cpu itself\n\t\t * (or a ilb_cpu acting on its behalf where given_cpu is\n\t\t * nohz-idle), we now have balance_cpu in a position to move\n\t\t * load to given_cpu. In rare situations, this may cause\n\t\t * conflicts (balance_cpu and given_cpu/ilb_cpu deciding\n\t\t * _independently_ and at _same_ time to move some load to\n\t\t * given_cpu) causing exceess load to be moved to given_cpu.\n\t\t * This however should not happen so much in practice and\n\t\t * moreover subsequent load balance cycles should correct the\n\t\t * excess load moved.\n\t\t */\n\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {\n\n\t\t\t/* Prevent to re-select dst_cpu via env's CPUs */\n\t\t\tcpumask_clear_cpu(env.dst_cpu, env.cpus);\n\n\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);\n\t\t\tenv.dst_cpu\t = env.new_dst_cpu;\n\t\t\tenv.flags\t&= ~LBF_DST_PINNED;\n\t\t\tenv.loop\t = 0;\n\t\t\tenv.loop_break\t = sched_nr_migrate_break;\n\n\t\t\t/*\n\t\t\t * Go back to \"more_balance\" rather than \"redo\" since we\n\t\t\t * need to continue with same src_cpu.\n\t\t\t */\n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t/*\n\t\t * We failed to reach balance because of affinity.\n\t\t */\n\t\tif (sd_parent) {\n\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)\n\t\t\t\t*group_imbalance = 1;\n\t\t}\n\n\t\t/* All tasks on this runqueue were pinned by CPU affinity */\n\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {\n\t\t\tcpumask_clear_cpu(cpu_of(busiest), cpus);\n\t\t\t/*\n\t\t\t * Attempting to continue load balancing at the current\n\t\t\t * sched_domain level only makes sense if there are\n\t\t\t * active CPUs remaining as possible busiest CPUs to\n\t\t\t * pull load from which are not contained within the\n\t\t\t * destination group that is receiving any migrated\n\t\t\t * load.\n\t\t\t */\n\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {\n\t\t\t\tenv.loop = 0;\n\t\t\t\tenv.loop_break = sched_nr_migrate_break;\n\t\t\t\tgoto redo;\n\t\t\t}\n\t\t\tgoto out_all_pinned;\n\t\t}\n\t}\n\n\tif (!ld_moved) {\n\t\tschedstat_inc(sd->lb_failed[idle]);\n\t\t/*\n\t\t * Increment the failure counter only on periodic balance.\n\t\t * We do not want newidle balance, which can be very\n\t\t * frequent, pollute the failure counter causing\n\t\t * excessive cache_hot migrations and active balances.\n\t\t */\n\t\tif (idle != CPU_NEWLY_IDLE)\n\t\t\tsd->nr_balance_failed++;\n\n\t\tif (need_active_balance(&env)) {\n\t\t\tunsigned long flags;\n\n\t\t\traw_spin_lock_irqsave(&busiest->lock, flags);\n\n\t\t\t/*\n\t\t\t * Don't kick the active_load_balance_cpu_stop,\n\t\t\t * if the curr task on busiest CPU can't be\n\t\t\t * moved to this_cpu:\n\t\t\t */\n\t\t\tif (!cpumask_test_cpu(this_cpu, &busiest->curr->cpus_allowed)) {\n\t\t\t\traw_spin_unlock_irqrestore(&busiest->lock,\n\t\t\t\t\t\t\t    flags);\n\t\t\t\tenv.flags |= LBF_ALL_PINNED;\n\t\t\t\tgoto out_one_pinned;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * ->active_balance synchronizes accesses to\n\t\t\t * ->active_balance_work.  Once set, it's cleared\n\t\t\t * only after active load balance is finished.\n\t\t\t */\n\t\t\tif (!busiest->active_balance) {\n\t\t\t\tbusiest->active_balance = 1;\n\t\t\t\tbusiest->push_cpu = this_cpu;\n\t\t\t\tactive_balance = 1;\n\t\t\t}\n\t\t\traw_spin_unlock_irqrestore(&busiest->lock, flags);\n\n\t\t\tif (active_balance) {\n\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),\n\t\t\t\t\tactive_load_balance_cpu_stop, busiest,\n\t\t\t\t\t&busiest->active_balance_work);\n\t\t\t}\n\n\t\t\t/* We've kicked active balancing, force task migration. */\n\t\t\tsd->nr_balance_failed = sd->cache_nice_tries+1;\n\t\t}\n\t} else\n\t\tsd->nr_balance_failed = 0;\n\n\tif (likely(!active_balance)) {\n\t\t/* We were unbalanced, so reset the balancing interval */\n\t\tsd->balance_interval = sd->min_interval;\n\t} else {\n\t\t/*\n\t\t * If we've begun active balancing, start to back off. This\n\t\t * case may not be covered by the all_pinned logic if there\n\t\t * is only 1 task on the busy runqueue (because we don't call\n\t\t * detach_tasks).\n\t\t */\n\t\tif (sd->balance_interval < sd->max_interval)\n\t\t\tsd->balance_interval *= 2;\n\t}\n\n\tgoto out;\n\nout_balanced:\n\t/*\n\t * We reach balance although we may have faced some affinity\n\t * constraints. Clear the imbalance flag if it was set.\n\t */\n\tif (sd_parent) {\n\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\tif (*group_imbalance)\n\t\t\t*group_imbalance = 0;\n\t}\n\nout_all_pinned:\n\t/*\n\t * We reach balance because all tasks are pinned at this level so\n\t * we can't migrate them. Let the imbalance flag set so parent level\n\t * can try to migrate them.\n\t */\n\tschedstat_inc(sd->lb_balanced[idle]);\n\n\tsd->nr_balance_failed = 0;\n\nout_one_pinned:\n\t/* tune up the balancing interval */\n\tif (((env.flags & LBF_ALL_PINNED) &&\n\t\t\tsd->balance_interval < MAX_PINNED_INTERVAL) ||\n\t\t\t(sd->balance_interval < sd->max_interval))\n\t\tsd->balance_interval *= 2;\n\n\tld_moved = 0;\nout:\n\treturn ld_moved;\n}\n\nstatic inline unsigned long\nget_sd_balance_interval(struct sched_domain *sd, int cpu_busy)\n{\n\tunsigned long interval = sd->balance_interval;\n\n\tif (cpu_busy)\n\t\tinterval *= sd->busy_factor;\n\n\t/* scale ms to jiffies */\n\tinterval = msecs_to_jiffies(interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\n\treturn interval;\n}\n\nstatic inline void\nupdate_next_balance(struct sched_domain *sd, unsigned long *next_balance)\n{\n\tunsigned long interval, next;\n\n\t/* used by idle balance, so cpu_busy = 0 */\n\tinterval = get_sd_balance_interval(sd, 0);\n\tnext = sd->last_balance + interval;\n\n\tif (time_after(*next_balance, next))\n\t\t*next_balance = next;\n}\n\n/*\n * active_load_balance_cpu_stop is run by the CPU stopper. It pushes\n * running tasks off the busiest CPU onto idle CPUs. It requires at\n * least 1 task to be running on each physical CPU where possible, and\n * avoids physical / logical imbalances.\n */\nstatic int active_load_balance_cpu_stop(void *data)\n{\n\tstruct rq *busiest_rq = data;\n\tint busiest_cpu = cpu_of(busiest_rq);\n\tint target_cpu = busiest_rq->push_cpu;\n\tstruct rq *target_rq = cpu_rq(target_cpu);\n\tstruct sched_domain *sd;\n\tstruct task_struct *p = NULL;\n\tstruct rq_flags rf;\n\n\trq_lock_irq(busiest_rq, &rf);\n\t/*\n\t * Between queueing the stop-work and running it is a hole in which\n\t * CPUs can become inactive. We should not move tasks from or to\n\t * inactive CPUs.\n\t */\n\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu))\n\t\tgoto out_unlock;\n\n\t/* Make sure the requested CPU hasn't gone down in the meantime: */\n\tif (unlikely(busiest_cpu != smp_processor_id() ||\n\t\t     !busiest_rq->active_balance))\n\t\tgoto out_unlock;\n\n\t/* Is there any task to move? */\n\tif (busiest_rq->nr_running <= 1)\n\t\tgoto out_unlock;\n\n\t/*\n\t * This condition is \"impossible\", if it occurs\n\t * we need to fix it. Originally reported by\n\t * Bjorn Helgaas on a 128-CPU setup.\n\t */\n\tBUG_ON(busiest_rq == target_rq);\n\n\t/* Search for an sd spanning us and the target CPU. */\n\trcu_read_lock();\n\tfor_each_domain(target_cpu, sd) {\n\t\tif ((sd->flags & SD_LOAD_BALANCE) &&\n\t\t    cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))\n\t\t\t\tbreak;\n\t}\n\n\tif (likely(sd)) {\n\t\tstruct lb_env env = {\n\t\t\t.sd\t\t= sd,\n\t\t\t.dst_cpu\t= target_cpu,\n\t\t\t.dst_rq\t\t= target_rq,\n\t\t\t.src_cpu\t= busiest_rq->cpu,\n\t\t\t.src_rq\t\t= busiest_rq,\n\t\t\t.idle\t\t= CPU_IDLE,\n\t\t\t/*\n\t\t\t * can_migrate_task() doesn't need to compute new_dst_cpu\n\t\t\t * for active balancing. Since we have CPU_IDLE, but no\n\t\t\t * @dst_grpmask we need to make that test go away with lying\n\t\t\t * about DST_PINNED.\n\t\t\t */\n\t\t\t.flags\t\t= LBF_DST_PINNED,\n\t\t};\n\n\t\tschedstat_inc(sd->alb_count);\n\t\tupdate_rq_clock(busiest_rq);\n\n\t\tp = detach_one_task(&env);\n\t\tif (p) {\n\t\t\tschedstat_inc(sd->alb_pushed);\n\t\t\t/* Active balancing done, reset the failure counter. */\n\t\t\tsd->nr_balance_failed = 0;\n\t\t} else {\n\t\t\tschedstat_inc(sd->alb_failed);\n\t\t}\n\t}\n\trcu_read_unlock();\nout_unlock:\n\tbusiest_rq->active_balance = 0;\n\trq_unlock(busiest_rq, &rf);\n\n\tif (p)\n\t\tattach_one_task(target_rq, p);\n\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\nstatic DEFINE_SPINLOCK(balancing);\n\n/*\n * Scale the max load_balance interval with the number of CPUs in the system.\n * This trades load-balance latency on larger machines for less cross talk.\n */\nvoid update_max_interval(void)\n{\n\tmax_load_balance_interval = HZ*num_online_cpus()/10;\n}\n\n/*\n * It checks each scheduling domain to see if it is due to be balanced,\n * and initiates a balancing operation if so.\n *\n * Balancing parameters are set up in init_sched_domains.\n */\nstatic void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)\n{\n\tint continue_balancing = 1;\n\tint cpu = rq->cpu;\n\tunsigned long interval;\n\tstruct sched_domain *sd;\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long next_balance = jiffies + 60*HZ;\n\tint update_next_balance = 0;\n\tint need_serialize, need_decay = 0;\n\tu64 max_cost = 0;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\t/*\n\t\t * Decay the newidle max times here because this is a regular\n\t\t * visit to all the domains. Decay ~1% per second.\n\t\t */\n\t\tif (time_after(jiffies, sd->next_decay_max_lb_cost)) {\n\t\t\tsd->max_newidle_lb_cost =\n\t\t\t\t(sd->max_newidle_lb_cost * 253) / 256;\n\t\t\tsd->next_decay_max_lb_cost = jiffies + HZ;\n\t\t\tneed_decay = 1;\n\t\t}\n\t\tmax_cost += sd->max_newidle_lb_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Stop the load balance at this level. There is another\n\t\t * CPU in our sched group which is doing load balancing more\n\t\t * actively.\n\t\t */\n\t\tif (!continue_balancing) {\n\t\t\tif (need_decay)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\n\t\tneed_serialize = sd->flags & SD_SERIALIZE;\n\t\tif (need_serialize) {\n\t\t\tif (!spin_trylock(&balancing))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (time_after_eq(jiffies, sd->last_balance + interval)) {\n\t\t\tif (load_balance(cpu, rq, sd, idle, &continue_balancing)) {\n\t\t\t\t/*\n\t\t\t\t * The LBF_DST_PINNED logic could have changed\n\t\t\t\t * env->dst_cpu, so we can't know our idle\n\t\t\t\t * state even if we migrated tasks. Update it.\n\t\t\t\t */\n\t\t\t\tidle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;\n\t\t\t}\n\t\t\tsd->last_balance = jiffies;\n\t\t\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE);\n\t\t}\n\t\tif (need_serialize)\n\t\t\tspin_unlock(&balancing);\nout:\n\t\tif (time_after(next_balance, sd->last_balance + interval)) {\n\t\t\tnext_balance = sd->last_balance + interval;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\tif (need_decay) {\n\t\t/*\n\t\t * Ensure the rq-wide value also decays but keep it at a\n\t\t * reasonable floor to avoid funnies with rq->avg_idle.\n\t\t */\n\t\trq->max_idle_balance_cost =\n\t\t\tmax((u64)sysctl_sched_migration_cost, max_cost);\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the cpu is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance)) {\n\t\trq->next_balance = next_balance;\n\n#ifdef CONFIG_NO_HZ_COMMON\n\t\t/*\n\t\t * If this CPU has been elected to perform the nohz idle\n\t\t * balance. Other idle CPUs have already rebalanced with\n\t\t * nohz_idle_balance() and nohz.next_balance has been\n\t\t * updated accordingly. This CPU is now running the idle load\n\t\t * balance for itself and we need to update the\n\t\t * nohz.next_balance accordingly.\n\t\t */\n\t\tif ((idle == CPU_IDLE) && time_after(nohz.next_balance, rq->next_balance))\n\t\t\tnohz.next_balance = rq->next_balance;\n#endif\n\t}\n}\n\nstatic inline int on_null_domain(struct rq *rq)\n{\n\treturn unlikely(!rcu_dereference_sched(rq->sd));\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n/*\n * idle load balancing details\n * - When one of the busy CPUs notice that there may be an idle rebalancing\n *   needed, they will kick the idle load balancer, which then does idle\n *   load balancing for all the idle CPUs.\n */\n\nstatic inline int find_new_ilb(void)\n{\n\tint ilb = cpumask_first(nohz.idle_cpus_mask);\n\n\tif (ilb < nr_cpu_ids && idle_cpu(ilb))\n\t\treturn ilb;\n\n\treturn nr_cpu_ids;\n}\n\n/*\n * Kick a CPU to do the nohz balancing, if it is time for it. We pick the\n * nohz_load_balancer CPU (if there is one) otherwise fallback to any idle\n * CPU (if there is one).\n */\nstatic void kick_ilb(unsigned int flags)\n{\n\tint ilb_cpu;\n\n\tnohz.next_balance++;\n\n\tilb_cpu = find_new_ilb();\n\n\tif (ilb_cpu >= nr_cpu_ids)\n\t\treturn;\n\n\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));\n\tif (flags & NOHZ_KICK_MASK)\n\t\treturn;\n\n\t/*\n\t * Use smp_send_reschedule() instead of resched_cpu().\n\t * This way we generate a sched IPI on the target CPU which\n\t * is idle. And the softirq performing nohz idle load balance\n\t * will be run before returning from the IPI.\n\t */\n\tsmp_send_reschedule(ilb_cpu);\n}\n\n/*\n * Current heuristic for kicking the idle load balancer in the presence\n * of an idle cpu in the system.\n *   - This rq has more than one task.\n *   - This rq has at least one CFS task and the capacity of the CPU is\n *     significantly reduced because of RT tasks or IRQs.\n *   - At parent of LLC scheduler domain level, this cpu's scheduler group has\n *     multiple busy cpu.\n *   - For SD_ASYM_PACKING, if the lower numbered cpu's in the scheduler\n *     domain span are idle.\n */\nstatic void nohz_balancer_kick(struct rq *rq)\n{\n\tunsigned long now = jiffies;\n\tstruct sched_domain_shared *sds;\n\tstruct sched_domain *sd;\n\tint nr_busy, i, cpu = rq->cpu;\n\tunsigned int flags = 0;\n\n\tif (unlikely(rq->idle_balance))\n\t\treturn;\n\n\t/*\n\t * We may be recently in ticked or tickless idle mode. At the first\n\t * busy tick after returning from idle, we will update the busy stats.\n\t */\n\tnohz_balance_exit_idle(rq);\n\n\t/*\n\t * None are in tickless mode and hence no need for NOHZ idle load\n\t * balancing.\n\t */\n\tif (likely(!atomic_read(&nohz.nr_cpus)))\n\t\treturn;\n\n\tif (READ_ONCE(nohz.has_blocked) &&\n\t    time_after(now, READ_ONCE(nohz.next_blocked)))\n\t\tflags = NOHZ_STATS_KICK;\n\n\tif (time_before(now, nohz.next_balance))\n\t\tgoto out;\n\n\tif (rq->nr_running >= 2 || rq->misfit_task_load) {\n\t\tflags = NOHZ_KICK_MASK;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds) {\n\t\t/*\n\t\t * XXX: write a coherent comment on why we do this.\n\t\t * See also: http://lkml.kernel.org/r/20111202010832.602203411@sbsiddha-desk.sc.intel.com\n\t\t */\n\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);\n\t\tif (nr_busy > 1) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\n\t}\n\n\tsd = rcu_dereference(rq->sd);\n\tif (sd) {\n\t\tif ((rq->cfs.h_nr_running >= 1) &&\n\t\t\t\tcheck_cpu_capacity(rq, sd)) {\n\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_asym, cpu));\n\tif (sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\t\tif (i == cpu ||\n\t\t\t    !cpumask_test_cpu(i, nohz.idle_cpus_mask))\n\t\t\t\tcontinue;\n\n\t\t\tif (sched_asym_prefer(i, cpu)) {\n\t\t\t\tflags = NOHZ_KICK_MASK;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\nout:\n\tif (flags)\n\t\tkick_ilb(flags);\n}\n\nstatic void set_cpu_sd_state_busy(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || !sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 0;\n\n\tatomic_inc(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\nvoid nohz_balance_exit_idle(struct rq *rq)\n{\n\tSCHED_WARN_ON(rq != this_rq());\n\n\tif (likely(!rq->nohz_tick_stopped))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 0;\n\tcpumask_clear_cpu(rq->cpu, nohz.idle_cpus_mask);\n\tatomic_dec(&nohz.nr_cpus);\n\n\tset_cpu_sd_state_busy(rq->cpu);\n}\n\nstatic void set_cpu_sd_state_idle(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 1;\n\n\tatomic_dec(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\n/*\n * This routine will record that the CPU is going idle with tick stopped.\n * This info will be used in performing idle load balancing in the future.\n */\nvoid nohz_balance_enter_idle(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tSCHED_WARN_ON(cpu != smp_processor_id());\n\n\t/* If this CPU is going down, then nothing needs to be done: */\n\tif (!cpu_active(cpu))\n\t\treturn;\n\n\t/* Spare idle load balancing on CPUs that don't want to be disturbed: */\n\tif (!housekeeping_cpu(cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/*\n\t * Can be set safely without rq->lock held\n\t * If a clear happens, it will have evaluated last additions because\n\t * rq->lock is held during the check and the clear\n\t */\n\trq->has_blocked_load = 1;\n\n\t/*\n\t * The tick is still stopped but load could have been added in the\n\t * meantime. We set the nohz.has_blocked flag to trig a check of the\n\t * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear\n\t * of nohz.has_blocked can only happen after checking the new load\n\t */\n\tif (rq->nohz_tick_stopped)\n\t\tgoto out;\n\n\t/* If we're a completely isolated CPU, we don't play: */\n\tif (on_null_domain(rq))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 1;\n\n\tcpumask_set_cpu(cpu, nohz.idle_cpus_mask);\n\tatomic_inc(&nohz.nr_cpus);\n\n\t/*\n\t * Ensures that if nohz_idle_balance() fails to observe our\n\t * @idle_cpus_mask store, it must observe the @has_blocked\n\t * store.\n\t */\n\tsmp_mb__after_atomic();\n\n\tset_cpu_sd_state_idle(cpu);\n\nout:\n\t/*\n\t * Each time a cpu enter idle, we assume that it has blocked load and\n\t * enable the periodic update of the load of idle cpus\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 1);\n}\n\n/*\n * Internal function that runs load balance for all idle cpus. The load balance\n * can be a simple update of blocked load or a complete load balance with\n * tasks movement depending of flags.\n * The function returns false if the loop has stopped before running\n * through all idle CPUs.\n */\nstatic bool _nohz_idle_balance(struct rq *this_rq, unsigned int flags,\n\t\t\t       enum cpu_idle_type idle)\n{\n\t/* Earliest time when we have to do rebalance again */\n\tunsigned long now = jiffies;\n\tunsigned long next_balance = now + 60*HZ;\n\tbool has_blocked_load = false;\n\tint update_next_balance = 0;\n\tint this_cpu = this_rq->cpu;\n\tint balance_cpu;\n\tint ret = false;\n\tstruct rq *rq;\n\n\tSCHED_WARN_ON((flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK);\n\n\t/*\n\t * We assume there will be no idle load after this update and clear\n\t * the has_blocked flag. If a cpu enters idle in the mean time, it will\n\t * set the has_blocked flag and trig another update of idle load.\n\t * Because a cpu that becomes idle, is added to idle_cpus_mask before\n\t * setting the flag, we are sure to not clear the state and not\n\t * check the load of an idle cpu.\n\t */\n\tWRITE_ONCE(nohz.has_blocked, 0);\n\n\t/*\n\t * Ensures that if we miss the CPU, we must see the has_blocked\n\t * store from nohz_balance_enter_idle().\n\t */\n\tsmp_mb();\n\n\tfor_each_cpu(balance_cpu, nohz.idle_cpus_mask) {\n\t\tif (balance_cpu == this_cpu || !idle_cpu(balance_cpu))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If this CPU gets work to do, stop the load balancing\n\t\t * work being done for other CPUs. Next load\n\t\t * balancing owner will pick it up.\n\t\t */\n\t\tif (need_resched()) {\n\t\t\thas_blocked_load = true;\n\t\t\tgoto abort;\n\t\t}\n\n\t\trq = cpu_rq(balance_cpu);\n\n\t\thas_blocked_load |= update_nohz_stats(rq, true);\n\n\t\t/*\n\t\t * If time for next balance is due,\n\t\t * do the balance.\n\t\t */\n\t\tif (time_after_eq(jiffies, rq->next_balance)) {\n\t\t\tstruct rq_flags rf;\n\n\t\t\trq_lock_irqsave(rq, &rf);\n\t\t\tupdate_rq_clock(rq);\n\t\t\tcpu_load_update_idle(rq);\n\t\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\t\tif (flags & NOHZ_BALANCE_KICK)\n\t\t\t\trebalance_domains(rq, CPU_IDLE);\n\t\t}\n\n\t\tif (time_after(next_balance, rq->next_balance)) {\n\t\t\tnext_balance = rq->next_balance;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\n\t/* Newly idle CPU doesn't need an update */\n\tif (idle != CPU_NEWLY_IDLE) {\n\t\tupdate_blocked_averages(this_cpu);\n\t\thas_blocked_load |= this_rq->has_blocked_load;\n\t}\n\n\tif (flags & NOHZ_BALANCE_KICK)\n\t\trebalance_domains(this_rq, CPU_IDLE);\n\n\tWRITE_ONCE(nohz.next_blocked,\n\t\tnow + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\n\t/* The full idle balance loop has been done */\n\tret = true;\n\nabort:\n\t/* There is still blocked load, enable periodic update */\n\tif (has_blocked_load)\n\t\tWRITE_ONCE(nohz.has_blocked, 1);\n\n\t/*\n\t * next_balance will be updated only when there is a need.\n\t * When the CPU is attached to null domain for ex, it will not be\n\t * updated.\n\t */\n\tif (likely(update_next_balance))\n\t\tnohz.next_balance = next_balance;\n\n\treturn ret;\n}\n\n/*\n * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the\n * rebalancing for all the cpus for whom scheduler ticks are stopped.\n */\nstatic bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\tint this_cpu = this_rq->cpu;\n\tunsigned int flags;\n\n\tif (!(atomic_read(nohz_flags(this_cpu)) & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\tif (idle != CPU_IDLE) {\n\t\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\t\treturn false;\n\t}\n\n\t/*\n\t * barrier, pairs with nohz_balance_enter_idle(), ensures ...\n\t */\n\tflags = atomic_fetch_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu));\n\tif (!(flags & NOHZ_KICK_MASK))\n\t\treturn false;\n\n\t_nohz_idle_balance(this_rq, flags, idle);\n\n\treturn true;\n}\n\nstatic void nohz_newidle_balance(struct rq *this_rq)\n{\n\tint this_cpu = this_rq->cpu;\n\n\t/*\n\t * This CPU doesn't want to be disturbed by scheduler\n\t * housekeeping\n\t */\n\tif (!housekeeping_cpu(this_cpu, HK_FLAG_SCHED))\n\t\treturn;\n\n\t/* Will wake up very soon. No time for doing anything else*/\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost)\n\t\treturn;\n\n\t/* Don't need to update blocked load of idle CPUs*/\n\tif (!READ_ONCE(nohz.has_blocked) ||\n\t    time_before(jiffies, READ_ONCE(nohz.next_blocked)))\n\t\treturn;\n\n\traw_spin_unlock(&this_rq->lock);\n\t/*\n\t * This CPU is going to be idle and blocked load of idle CPUs\n\t * need to be updated. Run the ilb locally as it is a good\n\t * candidate for ilb instead of waking up another idle CPU.\n\t * Kick an normal ilb if we failed to do the update.\n\t */\n\tif (!_nohz_idle_balance(this_rq, NOHZ_STATS_KICK, CPU_NEWLY_IDLE))\n\t\tkick_ilb(NOHZ_STATS_KICK);\n\traw_spin_lock(&this_rq->lock);\n}\n\n#else /* !CONFIG_NO_HZ_COMMON */\nstatic inline void nohz_balancer_kick(struct rq *rq) { }\n\nstatic inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\treturn false;\n}\n\nstatic inline void nohz_newidle_balance(struct rq *this_rq) { }\n#endif /* CONFIG_NO_HZ_COMMON */\n\n/*\n * idle_balance is called by schedule() if this_cpu is about to become\n * idle. Attempts to pull tasks from other CPUs.\n */\nstatic int idle_balance(struct rq *this_rq, struct rq_flags *rf)\n{\n\tunsigned long next_balance = jiffies + HZ;\n\tint this_cpu = this_rq->cpu;\n\tstruct sched_domain *sd;\n\tint pulled_task = 0;\n\tu64 curr_cost = 0;\n\n\t/*\n\t * We must set idle_stamp _before_ calling idle_balance(), such that we\n\t * measure the duration of idle_balance() as idle time.\n\t */\n\tthis_rq->idle_stamp = rq_clock(this_rq);\n\n\t/*\n\t * Do not pull tasks towards !active CPUs...\n\t */\n\tif (!cpu_active(this_cpu))\n\t\treturn 0;\n\n\t/*\n\t * This is OK, because current is on_cpu, which avoids it being picked\n\t * for load-balance and preemption/IRQs are still disabled avoiding\n\t * further scheduler activity on it and we're being very careful to\n\t * re-start the picking loop.\n\t */\n\trq_unpin_lock(this_rq, rf);\n\n\tif (this_rq->avg_idle < sysctl_sched_migration_cost ||\n\t    !READ_ONCE(this_rq->rd->overload)) {\n\n\t\trcu_read_lock();\n\t\tsd = rcu_dereference_check_sched_domain(this_rq->sd);\n\t\tif (sd)\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\trcu_read_unlock();\n\n\t\tnohz_newidle_balance(this_rq);\n\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock(&this_rq->lock);\n\n\tupdate_blocked_averages(this_cpu);\n\trcu_read_lock();\n\tfor_each_domain(this_cpu, sd) {\n\t\tint continue_balancing = 1;\n\t\tu64 t0, domain_cost;\n\n\t\tif (!(sd->flags & SD_LOAD_BALANCE))\n\t\t\tcontinue;\n\n\t\tif (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sd->flags & SD_BALANCE_NEWIDLE) {\n\t\t\tt0 = sched_clock_cpu(this_cpu);\n\n\t\t\tpulled_task = load_balance(this_cpu, this_rq,\n\t\t\t\t\t\t   sd, CPU_NEWLY_IDLE,\n\t\t\t\t\t\t   &continue_balancing);\n\n\t\t\tdomain_cost = sched_clock_cpu(this_cpu) - t0;\n\t\t\tif (domain_cost > sd->max_newidle_lb_cost)\n\t\t\t\tsd->max_newidle_lb_cost = domain_cost;\n\n\t\t\tcurr_cost += domain_cost;\n\t\t}\n\n\t\tupdate_next_balance(sd, &next_balance);\n\n\t\t/*\n\t\t * Stop searching for tasks to pull if there are\n\t\t * now runnable tasks on this rq.\n\t\t */\n\t\tif (pulled_task || this_rq->nr_running > 0)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\traw_spin_lock(&this_rq->lock);\n\n\tif (curr_cost > this_rq->max_idle_balance_cost)\n\t\tthis_rq->max_idle_balance_cost = curr_cost;\n\nout:\n\t/*\n\t * While browsing the domains, we released the rq lock, a task could\n\t * have been enqueued in the meantime. Since we're not going idle,\n\t * pretend we pulled a task.\n\t */\n\tif (this_rq->cfs.h_nr_running && !pulled_task)\n\t\tpulled_task = 1;\n\n\t/* Move the next balance forward */\n\tif (time_after(this_rq->next_balance, next_balance))\n\t\tthis_rq->next_balance = next_balance;\n\n\t/* Is there a task of a high priority class? */\n\tif (this_rq->nr_running != this_rq->cfs.h_nr_running)\n\t\tpulled_task = -1;\n\n\tif (pulled_task)\n\t\tthis_rq->idle_stamp = 0;\n\n\trq_repin_lock(this_rq, rf);\n\n\treturn pulled_task;\n}\n\n/*\n * run_rebalance_domains is triggered when needed from the scheduler tick.\n * Also triggered for nohz idle balancing (with nohz_balancing_kick set).\n */\nstatic __latent_entropy void run_rebalance_domains(struct softirq_action *h)\n{\n\tstruct rq *this_rq = this_rq();\n\tenum cpu_idle_type idle = this_rq->idle_balance ?\n\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;\n\n\t/*\n\t * If this CPU has a pending nohz_balance_kick, then do the\n\t * balancing on behalf of the other idle CPUs whose ticks are\n\t * stopped. Do nohz_idle_balance *before* rebalance_domains to\n\t * give the idle CPUs a chance to load balance. Else we may\n\t * load balance only within the local sched_domain hierarchy\n\t * and abort nohz_idle_balance altogether if we pull some load.\n\t */\n\tif (nohz_idle_balance(this_rq, idle))\n\t\treturn;\n\n\t/* normal load balance */\n\tupdate_blocked_averages(this_rq->cpu);\n\trebalance_domains(this_rq, idle);\n}\n\n/*\n * Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing.\n */\nvoid trigger_load_balance(struct rq *rq)\n{\n\t/* Don't need to rebalance while attached to NULL domain */\n\tif (unlikely(on_null_domain(rq)))\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->next_balance))\n\t\traise_softirq(SCHED_SOFTIRQ);\n\n\tnohz_balancer_kick(rq);\n}\n\nstatic void rq_online_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\tupdate_runtime_enabled(rq);\n}\n\nstatic void rq_offline_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\t/* Ensure any throttled groups are reachable by pick_next_task */\n\tunthrottle_offline_cfs_rqs(rq);\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * scheduler tick hitting a task of our scheduling class.\n *\n * NOTE: This function can be called remotely by the tick offload that\n * goes along full dynticks. Therefore no local assumption can be made\n * and everything must be accessed through the @rq and @curr passed in\n * parameters.\n */\nstatic void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tentity_tick(cfs_rq, se, queued);\n\t}\n\n\tif (static_branch_unlikely(&sched_numa_balancing))\n\t\ttask_tick_numa(rq, curr);\n\n\tupdate_misfit_status(curr, rq);\n}\n\n/*\n * called on fork with the child task as argument from the parent's context\n *  - child not yet on the tasklist\n *  - preemption disabled\n */\nstatic void task_fork_fair(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se, *curr;\n\tstruct rq *rq = this_rq();\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\tcfs_rq = task_cfs_rq(current);\n\tcurr = cfs_rq->curr;\n\tif (curr) {\n\t\tupdate_curr(cfs_rq);\n\t\tse->vruntime = curr->vruntime;\n\t}\n\tplace_entity(cfs_rq, se, 1);\n\n\tif (sysctl_sched_child_runs_first && curr && entity_before(curr, se)) {\n\t\t/*\n\t\t * Upon rescheduling, sched_class::put_prev_task() will place\n\t\t * 'current' within the tree based on its new key value.\n\t\t */\n\t\tswap(curr->vruntime, se->vruntime);\n\t\tresched_curr(rq);\n\t}\n\n\tse->vruntime -= cfs_rq->min_vruntime;\n\trq_unlock(rq, &rf);\n}\n\n/*\n * Priority of the task has changed. Check to see if we preempt\n * the current task.\n */\nstatic void\nprio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tif (!task_on_rq_queued(p))\n\t\treturn;\n\n\t/*\n\t * Reschedule if we are currently running on this runqueue and\n\t * our priority decreased, or if we are not currently running on\n\t * this runqueue and our priority is higher than the current's\n\t */\n\tif (rq->curr == p) {\n\t\tif (p->prio > oldprio)\n\t\t\tresched_curr(rq);\n\t} else\n\t\tcheck_preempt_curr(rq, p, 0);\n}\n\nstatic inline bool vruntime_normalized(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases,\n\t * the dequeue_entity(.flags=0) will already have normalized the\n\t * vruntime.\n\t */\n\tif (p->on_rq)\n\t\treturn true;\n\n\t/*\n\t * When !on_rq, vruntime of the task has usually NOT been normalized.\n\t * But there are some cases where it has already been normalized:\n\t *\n\t * - A forked child which is waiting for being woken up by\n\t *   wake_up_new_task().\n\t * - A task which has been woken up by try_to_wake_up() and\n\t *   waiting for actually being woken up by sched_ttwu_pending().\n\t */\n\tif (!se->sum_exec_runtime ||\n\t    (p->state == TASK_WAKING && p->sched_remote_wakeup))\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n/*\n * Propagate the changes of the sched_entity across the tg tree to make it\n * visible to the root\n */\nstatic void propagate_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq;\n\n\t/* Start to propagate at parent */\n\tse = se->parent;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n}\n#else\nstatic void propagate_entity_cfs_rq(struct sched_entity *se) { }\n#endif\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t/* Catch up with the cfs_rq and remove our load when we leave */\n\tupdate_load_avg(cfs_rq, se, 0);\n\tdetach_entity_load_avg(cfs_rq, se);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void attach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/*\n\t * Since the real-depth could have been changed (only FAIR\n\t * class maintain depth value), reset depth properly.\n\t */\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n#endif\n\n\t/* Synchronize entity with its cfs_rq */\n\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);\n\tattach_entity_load_avg(cfs_rq, se, 0);\n\tupdate_tg_load_avg(cfs_rq, false);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void detach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tif (!vruntime_normalized(p)) {\n\t\t/*\n\t\t * Fix up our vruntime so that the current sleep doesn't\n\t\t * cause 'unlimited' sleep bonus.\n\t\t */\n\t\tplace_entity(cfs_rq, se, 0);\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\t}\n\n\tdetach_entity_cfs_rq(se);\n}\n\nstatic void attach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tattach_entity_cfs_rq(se);\n\n\tif (!vruntime_normalized(p))\n\t\tse->vruntime += cfs_rq->min_vruntime;\n}\n\nstatic void switched_from_fair(struct rq *rq, struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n}\n\nstatic void switched_to_fair(struct rq *rq, struct task_struct *p)\n{\n\tattach_task_cfs_rq(p);\n\n\tif (task_on_rq_queued(p)) {\n\t\t/*\n\t\t * We were most likely switched from sched_rt, so\n\t\t * kick off the schedule if running, otherwise just see\n\t\t * if we can still preempt the current task.\n\t\t */\n\t\tif (rq->curr == p)\n\t\t\tresched_curr(rq);\n\t\telse\n\t\t\tcheck_preempt_curr(rq, p, 0);\n\t}\n}\n\n/* Account for a task changing its policy or group.\n *\n * This routine is mostly called to set cfs_rq->curr field when a task\n * migrates between groups/classes.\n */\nstatic void set_curr_task_fair(struct rq *rq)\n{\n\tstruct sched_entity *se = &rq->curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\tset_next_entity(cfs_rq, se);\n\t\t/* ensure bandwidth has been allocated on our new cfs_rq */\n\t\taccount_cfs_rq_runtime(cfs_rq, 0);\n\t}\n}\n\nvoid init_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT_CACHED;\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n#ifndef CONFIG_64BIT\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n#ifdef CONFIG_SMP\n\traw_spin_lock_init(&cfs_rq->removed.lock);\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void task_set_group_fair(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tset_task_rq(p, task_cpu(p));\n\tse->depth = se->parent ? se->parent->depth + 1 : 0;\n}\n\nstatic void task_move_group_fair(struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n\tset_task_rq(p, task_cpu(p));\n\n#ifdef CONFIG_SMP\n\t/* Tell se's cfs_rq has been changed -- migrated */\n\tp->se.avg.last_update_time = 0;\n#endif\n\tattach_task_cfs_rq(p);\n}\n\nstatic void task_change_group_fair(struct task_struct *p, int type)\n{\n\tswitch (type) {\n\tcase TASK_SET_GROUP:\n\t\ttask_set_group_fair(p);\n\t\tbreak;\n\n\tcase TASK_MOVE_GROUP:\n\t\ttask_move_group_fair(p);\n\t\tbreak;\n\t}\n}\n\nvoid free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct sched_entity *se;\n\tstruct cfs_rq *cfs_rq;\n\tint i;\n\n\ttg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tinit_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_cfs_rq(cfs_rq);\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t\tinit_entity_runnable_average(se);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nvoid online_fair_sched_group(struct task_group *tg)\n{\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\t\tse = tg->se[i];\n\n\t\traw_spin_lock_irq(&rq->lock);\n\t\tupdate_rq_clock(rq);\n\t\tattach_entity_cfs_rq(se);\n\t\tsync_throttle(tg, i);\n\t\traw_spin_unlock_irq(&rq->lock);\n\t}\n}\n\nvoid unregister_fair_sched_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (tg->se[cpu])\n\t\t\tremove_entity_load_avg(tg->se[cpu]);\n\n\t\t/*\n\t\t * Only empty task groups can be destroyed; so we can speculatively\n\t\t * check on_list without danger of it being re-added.\n\t\t */\n\t\tif (!tg->cfs_rq[cpu]->on_list)\n\t\t\tcontinue;\n\n\t\trq = cpu_rq(cpu);\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n}\n\nvoid init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\tstruct sched_entity *se, int cpu,\n\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tcfs_rq->tg = tg;\n\tcfs_rq->rq = rq;\n\tinit_cfs_rq_runtime(cfs_rq);\n\n\ttg->cfs_rq[cpu] = cfs_rq;\n\ttg->se[cpu] = se;\n\n\t/* se could be NULL for root_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent) {\n\t\tse->cfs_rq = &rq->cfs;\n\t\tse->depth = 0;\n\t} else {\n\t\tse->cfs_rq = parent->my_q;\n\t\tse->depth = parent->depth + 1;\n\t}\n\n\tse->my_q = cfs_rq;\n\t/* guarantee group entities always have weight */\n\tupdate_load_set(&se->load, NICE_0_LOAD);\n\tse->parent = parent;\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES));\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg->se[i];\n\t\tstruct rq_flags rf;\n\n\t\t/* Propagate contribution to hierarchy */\n\t\trq_lock_irqsave(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_sched_entity(se) {\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG);\n\t\t\tupdate_cfs_group(se);\n\t\t}\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n#else /* CONFIG_FAIR_GROUP_SCHED */\n\nvoid free_fair_sched_group(struct task_group *tg) { }\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nvoid online_fair_sched_group(struct task_group *tg) { }\n\nvoid unregister_fair_sched_group(struct task_group *tg) { }\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\nstatic unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task)\n{\n\tstruct sched_entity *se = &task->se;\n\tunsigned int rr_interval = 0;\n\n\t/*\n\t * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise\n\t * idle runqueue:\n\t */\n\tif (rq->cfs.load.weight)\n\t\trr_interval = NS_TO_JIFFIES(sched_slice(cfs_rq_of(se), se));\n\n\treturn rr_interval;\n}\n\n/*\n * All the scheduling class methods:\n */\nconst struct sched_class fair_sched_class = {\n\t.next\t\t\t= &idle_sched_class,\n\t.enqueue_task\t\t= enqueue_task_fair,\n\t.dequeue_task\t\t= dequeue_task_fair,\n\t.yield_task\t\t= yield_task_fair,\n\t.yield_to_task\t\t= yield_to_task_fair,\n\n\t.check_preempt_curr\t= check_preempt_wakeup,\n\n\t.pick_next_task\t\t= pick_next_task_fair,\n\t.put_prev_task\t\t= put_prev_task_fair,\n\n#ifdef CONFIG_SMP\n\t.select_task_rq\t\t= select_task_rq_fair,\n\t.migrate_task_rq\t= migrate_task_rq_fair,\n\n\t.rq_online\t\t= rq_online_fair,\n\t.rq_offline\t\t= rq_offline_fair,\n\n\t.task_dead\t\t= task_dead_fair,\n\t.set_cpus_allowed\t= set_cpus_allowed_common,\n#endif\n\n\t.set_curr_task          = set_curr_task_fair,\n\t.task_tick\t\t= task_tick_fair,\n\t.task_fork\t\t= task_fork_fair,\n\n\t.prio_changed\t\t= prio_changed_fair,\n\t.switched_from\t\t= switched_from_fair,\n\t.switched_to\t\t= switched_to_fair,\n\n\t.get_rr_interval\t= get_rr_interval_fair,\n\n\t.update_curr\t\t= update_curr_fair,\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t.task_change_group\t= task_change_group_fair,\n#endif\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nvoid print_cfs_stats(struct seq_file *m, int cpu)\n{\n\tstruct cfs_rq *cfs_rq, *pos;\n\n\trcu_read_lock();\n\tfor_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)\n\t\tprint_cfs_rq(m, cpu, cfs_rq);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nvoid show_numa_stats(struct task_struct *p, struct seq_file *m)\n{\n\tint node;\n\tunsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0;\n\n\tfor_each_online_node(node) {\n\t\tif (p->numa_faults) {\n\t\t\ttsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t\t\ttpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tif (p->numa_group) {\n\t\t\tgsf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 0)],\n\t\t\tgpf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tprint_numa_stats(m, node, tsf, tpf, gsf, gpf);\n\t}\n}\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_SCHED_DEBUG */\n\n__init void init_sched_fair_class(void)\n{\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tnohz.next_balance = jiffies;\n\tnohz.next_blocked = jiffies;\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n#endif\n#endif /* SMP */\n\n}"
  },
  {
    "function_name": "set_next_buddy",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6499-6509",
    "snippet": "static void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 6507
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "!se->on_rq"
          ],
          "line": 6505
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "task_of(se)->policy == SCHED_IDLE"
          ],
          "line": 6501
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 6501
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 6501
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}"
  },
  {
    "function_name": "set_last_buddy",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6487-6497",
    "snippet": "static void set_last_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->last = se;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 6495
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "!se->on_rq"
          ],
          "line": 6493
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "task_of(se)->policy == SCHED_IDLE"
          ],
          "line": 6489
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 6489
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 6489
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void set_last_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->last = se;\n\t}\n}"
  },
  {
    "function_name": "wakeup_preempt_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6472-6485",
    "snippet": "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "wakeup_gran",
          "args": [
            "se"
          ],
          "line": 6480
        },
        "resolved": true,
        "details": {
          "function_name": "wakeup_gran",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6438-6456",
          "snippet": "static unsigned long wakeup_gran(struct sched_entity *se)\n{\n\tunsigned long gran = sysctl_sched_wakeup_granularity;\n\n\t/*\n\t * Since its curr running now, convert the gran from real-time\n\t * to virtual-time in his units.\n\t *\n\t * By using 'se' instead of 'curr' we penalize light tasks, so\n\t * they get preempted easier. That is, if 'se' < 'curr' then\n\t * the resulting gran will be larger, therefore penalizing the\n\t * lighter, if otoh 'se' > 'curr' then the resulting gran will\n\t * be smaller, again penalizing the lighter task.\n\t *\n\t * This is especially important for buddies when the leftmost\n\t * task is higher priority than the buddy.\n\t */\n\treturn calc_delta_fair(gran, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int sysctl_sched_wakeup_granularity\t\t= 1000000UL;",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_wakeup_granularity\t\t= 1000000UL;\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic unsigned long wakeup_gran(struct sched_entity *se)\n{\n\tunsigned long gran = sysctl_sched_wakeup_granularity;\n\n\t/*\n\t * Since its curr running now, convert the gran from real-time\n\t * to virtual-time in his units.\n\t *\n\t * By using 'se' instead of 'curr' we penalize light tasks, so\n\t * they get preempted easier. That is, if 'se' < 'curr' then\n\t * the resulting gran will be larger, therefore penalizing the\n\t * lighter, if otoh 'se' > 'curr' then the resulting gran will\n\t * be smaller, again penalizing the lighter task.\n\t *\n\t * This is especially important for buddies when the leftmost\n\t * task is higher priority than the buddy.\n\t */\n\treturn calc_delta_fair(gran, se);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}"
  },
  {
    "function_name": "wakeup_gran",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6438-6456",
    "snippet": "static unsigned long wakeup_gran(struct sched_entity *se)\n{\n\tunsigned long gran = sysctl_sched_wakeup_granularity;\n\n\t/*\n\t * Since its curr running now, convert the gran from real-time\n\t * to virtual-time in his units.\n\t *\n\t * By using 'se' instead of 'curr' we penalize light tasks, so\n\t * they get preempted easier. That is, if 'se' < 'curr' then\n\t * the resulting gran will be larger, therefore penalizing the\n\t * lighter, if otoh 'se' > 'curr' then the resulting gran will\n\t * be smaller, again penalizing the lighter task.\n\t *\n\t * This is especially important for buddies when the leftmost\n\t * task is higher priority than the buddy.\n\t */\n\treturn calc_delta_fair(gran, se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "unsigned int sysctl_sched_wakeup_granularity\t\t= 1000000UL;",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "calc_delta_fair",
          "args": [
            "gran",
            "se"
          ],
          "line": 6455
        },
        "resolved": true,
        "details": {
          "function_name": "calc_delta_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "628-634",
          "snippet": "static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_wakeup_granularity\t\t= 1000000UL;\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic unsigned long wakeup_gran(struct sched_entity *se)\n{\n\tunsigned long gran = sysctl_sched_wakeup_granularity;\n\n\t/*\n\t * Since its curr running now, convert the gran from real-time\n\t * to virtual-time in his units.\n\t *\n\t * By using 'se' instead of 'curr' we penalize light tasks, so\n\t * they get preempted easier. That is, if 'se' < 'curr' then\n\t * the resulting gran will be larger, therefore penalizing the\n\t * lighter, if otoh 'se' > 'curr' then the resulting gran will\n\t * be smaller, again penalizing the lighter task.\n\t *\n\t * This is especially important for buddies when the leftmost\n\t * task is higher priority than the buddy.\n\t */\n\treturn calc_delta_fair(gran, se);\n}"
  },
  {
    "function_name": "task_dead_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6432-6435",
    "snippet": "static void task_dead_fair(struct task_struct *p)\n{\n\tremove_entity_load_avg(&p->se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "remove_entity_load_avg",
          "args": [
            "&p->se"
          ],
          "line": 6434
        },
        "resolved": true,
        "details": {
          "function_name": "remove_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3738-3738",
          "snippet": "static inline void remove_entity_load_avg(struct sched_entity *se) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void remove_entity_load_avg(struct sched_entity *se) {}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void task_dead_fair(struct task_struct *p)\n{\n\tremove_entity_load_avg(&p->se);\n}"
  },
  {
    "function_name": "migrate_task_rq_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6375-6430",
    "snippet": "static void migrate_task_rq_fair(struct task_struct *p, int new_cpu)\n{\n\t/*\n\t * As blocked tasks retain absolute vruntime the migration needs to\n\t * deal with this by subtracting the old and adding the new\n\t * min_vruntime -- the latter is done by enqueue_entity() when placing\n\t * the task on the new runqueue.\n\t */\n\tif (p->state == TASK_WAKING) {\n\t\tstruct sched_entity *se = &p->se;\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tu64 min_vruntime;\n\n#ifndef CONFIG_64BIT\n\t\tu64 min_vruntime_copy;\n\n\t\tdo {\n\t\t\tmin_vruntime_copy = cfs_rq->min_vruntime_copy;\n\t\t\tsmp_rmb();\n\t\t\tmin_vruntime = cfs_rq->min_vruntime;\n\t\t} while (min_vruntime != min_vruntime_copy);\n#else\n\t\tmin_vruntime = cfs_rq->min_vruntime;\n#endif\n\n\t\tse->vruntime -= min_vruntime;\n\t}\n\n\tif (p->on_rq == TASK_ON_RQ_MIGRATING) {\n\t\t/*\n\t\t * In case of TASK_ON_RQ_MIGRATING we in fact hold the 'old'\n\t\t * rq->lock and can modify state directly.\n\t\t */\n\t\tlockdep_assert_held(&task_rq(p)->lock);\n\t\tdetach_entity_cfs_rq(&p->se);\n\n\t} else {\n\t\t/*\n\t\t * We are supposed to update the task to \"current\" time, then\n\t\t * its up to date and ready to go to new CPU/cfs_rq. But we\n\t\t * have difficulty in getting what current time is, so simply\n\t\t * throw away the out-of-date time. This will result in the\n\t\t * wakee task is less decayed, but giving the wakee more load\n\t\t * sounds not bad.\n\t\t */\n\t\tremove_entity_load_avg(&p->se);\n\t}\n\n\t/* Tell new CPU we are migrated */\n\tp->se.avg.last_update_time = 0;\n\n\t/* We have migrated, no longer consider this task hot */\n\tp->se.exec_start = 0;\n\n\tupdate_scan_period(p, new_cpu);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_scan_period",
          "args": [
            "p",
            "new_cpu"
          ],
          "line": 6429
        },
        "resolved": true,
        "details": {
          "function_name": "update_scan_period",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2662-2664",
          "snippet": "static inline void update_scan_period(struct task_struct *p, int new_cpu)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_scan_period(struct task_struct *p, int new_cpu)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "remove_entity_load_avg",
          "args": [
            "&p->se"
          ],
          "line": 6420
        },
        "resolved": true,
        "details": {
          "function_name": "remove_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3738-3738",
          "snippet": "static inline void remove_entity_load_avg(struct sched_entity *se) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void remove_entity_load_avg(struct sched_entity *se) {}"
        }
      },
      {
        "call_info": {
          "callee": "detach_entity_cfs_rq",
          "args": [
            "&p->se"
          ],
          "line": 6409
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&task_rq(p)->lock"
          ],
          "line": 6408
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_rq",
          "args": [
            "p"
          ],
          "line": 6408
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 6393
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 6385
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void migrate_task_rq_fair(struct task_struct *p, int new_cpu)\n{\n\t/*\n\t * As blocked tasks retain absolute vruntime the migration needs to\n\t * deal with this by subtracting the old and adding the new\n\t * min_vruntime -- the latter is done by enqueue_entity() when placing\n\t * the task on the new runqueue.\n\t */\n\tif (p->state == TASK_WAKING) {\n\t\tstruct sched_entity *se = &p->se;\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tu64 min_vruntime;\n\n#ifndef CONFIG_64BIT\n\t\tu64 min_vruntime_copy;\n\n\t\tdo {\n\t\t\tmin_vruntime_copy = cfs_rq->min_vruntime_copy;\n\t\t\tsmp_rmb();\n\t\t\tmin_vruntime = cfs_rq->min_vruntime;\n\t\t} while (min_vruntime != min_vruntime_copy);\n#else\n\t\tmin_vruntime = cfs_rq->min_vruntime;\n#endif\n\n\t\tse->vruntime -= min_vruntime;\n\t}\n\n\tif (p->on_rq == TASK_ON_RQ_MIGRATING) {\n\t\t/*\n\t\t * In case of TASK_ON_RQ_MIGRATING we in fact hold the 'old'\n\t\t * rq->lock and can modify state directly.\n\t\t */\n\t\tlockdep_assert_held(&task_rq(p)->lock);\n\t\tdetach_entity_cfs_rq(&p->se);\n\n\t} else {\n\t\t/*\n\t\t * We are supposed to update the task to \"current\" time, then\n\t\t * its up to date and ready to go to new CPU/cfs_rq. But we\n\t\t * have difficulty in getting what current time is, so simply\n\t\t * throw away the out-of-date time. This will result in the\n\t\t * wakee task is less decayed, but giving the wakee more load\n\t\t * sounds not bad.\n\t\t */\n\t\tremove_entity_load_avg(&p->se);\n\t}\n\n\t/* Tell new CPU we are migrated */\n\tp->se.avg.last_update_time = 0;\n\n\t/* We have migrated, no longer consider this task hot */\n\tp->se.exec_start = 0;\n\n\tupdate_scan_period(p, new_cpu);\n}"
  },
  {
    "function_name": "select_task_rq_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6313-6366",
    "snippet": "static int\nselect_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)\n{\n\tstruct sched_domain *tmp, *sd = NULL;\n\tint cpu = smp_processor_id();\n\tint new_cpu = prev_cpu;\n\tint want_affine = 0;\n\tint sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);\n\n\tif (sd_flag & SD_BALANCE_WAKE) {\n\t\trecord_wakee(p);\n\t\twant_affine = !wake_wide(p) && !wake_cap(p, cpu, prev_cpu)\n\t\t\t      && cpumask_test_cpu(cpu, &p->cpus_allowed);\n\t}\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, tmp) {\n\t\tif (!(tmp->flags & SD_LOAD_BALANCE))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If both 'cpu' and 'prev_cpu' are part of this domain,\n\t\t * cpu is a valid SD_WAKE_AFFINE target.\n\t\t */\n\t\tif (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&\n\t\t    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {\n\t\t\tif (cpu != prev_cpu)\n\t\t\t\tnew_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);\n\n\t\t\tsd = NULL; /* Prefer wake_affine over balance flags */\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tmp->flags & sd_flag)\n\t\t\tsd = tmp;\n\t\telse if (!want_affine)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(sd)) {\n\t\t/* Slow path */\n\t\tnew_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);\n\t} else if (sd_flag & SD_BALANCE_WAKE) { /* XXX always ? */\n\t\t/* Fast path */\n\n\t\tnew_cpu = select_idle_sibling(p, prev_cpu, new_cpu);\n\n\t\tif (want_affine)\n\t\t\tcurrent->recent_used_cpu = cpu;\n\t}\n\trcu_read_unlock();\n\n\treturn new_cpu;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 6363
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "select_idle_sibling",
          "args": [
            "p",
            "prev_cpu",
            "new_cpu"
          ],
          "line": 6358
        },
        "resolved": true,
        "details": {
          "function_name": "select_idle_sibling",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6118-6164",
          "snippet": "static int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}"
        }
      },
      {
        "call_info": {
          "callee": "find_idlest_cpu",
          "args": [
            "sd",
            "p",
            "cpu",
            "prev_cpu",
            "sd_flag"
          ],
          "line": 6354
        },
        "resolved": true,
        "details": {
          "function_name": "find_idlest_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5883-5934",
          "snippet": "static inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,\n\t\t\t\t  int cpu, int prev_cpu, int sd_flag)\n{\n\tint new_cpu = cpu;\n\n\tif (!cpumask_intersects(sched_domain_span(sd), &p->cpus_allowed))\n\t\treturn prev_cpu;\n\n\t/*\n\t * We need task's util for capacity_spare_wake, sync it up to prev_cpu's\n\t * last_update_time.\n\t */\n\tif (!(sd_flag & SD_BALANCE_FORK))\n\t\tsync_entity_load_avg(&p->se);\n\n\twhile (sd) {\n\t\tstruct sched_group *group;\n\t\tstruct sched_domain *tmp;\n\t\tint weight;\n\n\t\tif (!(sd->flags & sd_flag)) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgroup = find_idlest_group(sd, p, cpu, sd_flag);\n\t\tif (!group) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_cpu = find_idlest_group_cpu(group, p, cpu);\n\t\tif (new_cpu == cpu) {\n\t\t\t/* Now try balancing at a lower domain level of 'cpu': */\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Now try balancing at a lower domain level of 'new_cpu': */\n\t\tcpu = new_cpu;\n\t\tweight = sd->span_weight;\n\t\tsd = NULL;\n\t\tfor_each_domain(cpu, tmp) {\n\t\t\tif (weight <= tmp->span_weight)\n\t\t\t\tbreak;\n\t\t\tif (tmp->flags & sd_flag)\n\t\t\t\tsd = tmp;\n\t\t}\n\t}\n\n\treturn new_cpu;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,\n\t\t\t\t  int cpu, int prev_cpu, int sd_flag)\n{\n\tint new_cpu = cpu;\n\n\tif (!cpumask_intersects(sched_domain_span(sd), &p->cpus_allowed))\n\t\treturn prev_cpu;\n\n\t/*\n\t * We need task's util for capacity_spare_wake, sync it up to prev_cpu's\n\t * last_update_time.\n\t */\n\tif (!(sd_flag & SD_BALANCE_FORK))\n\t\tsync_entity_load_avg(&p->se);\n\n\twhile (sd) {\n\t\tstruct sched_group *group;\n\t\tstruct sched_domain *tmp;\n\t\tint weight;\n\n\t\tif (!(sd->flags & sd_flag)) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgroup = find_idlest_group(sd, p, cpu, sd_flag);\n\t\tif (!group) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_cpu = find_idlest_group_cpu(group, p, cpu);\n\t\tif (new_cpu == cpu) {\n\t\t\t/* Now try balancing at a lower domain level of 'cpu': */\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Now try balancing at a lower domain level of 'new_cpu': */\n\t\tcpu = new_cpu;\n\t\tweight = sd->span_weight;\n\t\tsd = NULL;\n\t\tfor_each_domain(cpu, tmp) {\n\t\t\tif (weight <= tmp->span_weight)\n\t\t\t\tbreak;\n\t\t\tif (tmp->flags & sd_flag)\n\t\t\t\tsd = tmp;\n\t\t}\n\t}\n\n\treturn new_cpu;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "sd"
          ],
          "line": 6352
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_affine",
          "args": [
            "tmp",
            "p",
            "cpu",
            "prev_cpu",
            "sync"
          ],
          "line": 6340
        },
        "resolved": true,
        "details": {
          "function_name": "wake_affine",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5657-5675",
          "snippet": "static int wake_affine(struct sched_domain *sd, struct task_struct *p,\n\t\t       int this_cpu, int prev_cpu, int sync)\n{\n\tint target = nr_cpumask_bits;\n\n\tif (sched_feat(WA_IDLE))\n\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);\n\n\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)\n\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);\n\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine_attempts);\n\tif (target == nr_cpumask_bits)\n\t\treturn prev_cpu;\n\n\tschedstat_inc(sd->ttwu_move_affine);\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine);\n\treturn target;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic int wake_affine(struct sched_domain *sd, struct task_struct *p,\n\t\t       int this_cpu, int prev_cpu, int sync)\n{\n\tint target = nr_cpumask_bits;\n\n\tif (sched_feat(WA_IDLE))\n\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);\n\n\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)\n\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);\n\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine_attempts);\n\tif (target == nr_cpumask_bits)\n\t\treturn prev_cpu;\n\n\tschedstat_inc(sd->ttwu_move_affine);\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine);\n\treturn target;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "prev_cpu",
            "sched_domain_span(tmp)"
          ],
          "line": 6338
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "tmp"
          ],
          "line": 6338
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_domain",
          "args": [
            "cpu",
            "tmp"
          ],
          "line": 6329
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 6328
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "&p->cpus_allowed"
          ],
          "line": 6325
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_cap",
          "args": [
            "p",
            "cpu",
            "prev_cpu"
          ],
          "line": 6324
        },
        "resolved": true,
        "details": {
          "function_name": "wake_cap",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6281-6299",
          "snippet": "static int wake_cap(struct task_struct *p, int cpu, int prev_cpu)\n{\n\tlong min_cap, max_cap;\n\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn 0;\n\n\tmin_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu));\n\tmax_cap = cpu_rq(cpu)->rd->max_cpu_capacity;\n\n\t/* Minimum capacity is close to max, no need to abort wake_affine */\n\tif (max_cap - min_cap < max_cap >> 3)\n\t\treturn 0;\n\n\t/* Bring task utilization in sync with prev_cpu */\n\tsync_entity_load_avg(&p->se);\n\n\treturn !task_fits_capacity(p, min_cap);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic int wake_cap(struct task_struct *p, int cpu, int prev_cpu)\n{\n\tlong min_cap, max_cap;\n\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn 0;\n\n\tmin_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu));\n\tmax_cap = cpu_rq(cpu)->rd->max_cpu_capacity;\n\n\t/* Minimum capacity is close to max, no need to abort wake_affine */\n\tif (max_cap - min_cap < max_cap >> 3)\n\t\treturn 0;\n\n\t/* Bring task utilization in sync with prev_cpu */\n\tsync_entity_load_avg(&p->se);\n\n\treturn !task_fits_capacity(p, min_cap);\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_wide",
          "args": [
            "p"
          ],
          "line": 6324
        },
        "resolved": true,
        "details": {
          "function_name": "wake_wide",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5565-5576",
          "snippet": "static int wake_wide(struct task_struct *p)\n{\n\tunsigned int master = current->wakee_flips;\n\tunsigned int slave = p->wakee_flips;\n\tint factor = this_cpu_read(sd_llc_size);\n\n\tif (master < slave)\n\t\tswap(master, slave);\n\tif (slave < factor || master < slave * factor)\n\t\treturn 0;\n\treturn 1;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int wake_wide(struct task_struct *p)\n{\n\tunsigned int master = current->wakee_flips;\n\tunsigned int slave = p->wakee_flips;\n\tint factor = this_cpu_read(sd_llc_size);\n\n\tif (master < slave)\n\t\tswap(master, slave);\n\tif (slave < factor || master < slave * factor)\n\t\treturn 0;\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "record_wakee",
          "args": [
            "p"
          ],
          "line": 6323
        },
        "resolved": true,
        "details": {
          "function_name": "record_wakee",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5531-5546",
          "snippet": "static void record_wakee(struct task_struct *p)\n{\n\t/*\n\t * Only decay a single time; tasks that have less then 1 wakeup per\n\t * jiffy will not have built up many flips.\n\t */\n\tif (time_after(jiffies, current->wakee_flip_decay_ts + HZ)) {\n\t\tcurrent->wakee_flips >>= 1;\n\t\tcurrent->wakee_flip_decay_ts = jiffies;\n\t}\n\n\tif (current->last_wakee != p) {\n\t\tcurrent->last_wakee = p;\n\t\tcurrent->wakee_flips++;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void record_wakee(struct task_struct *p)\n{\n\t/*\n\t * Only decay a single time; tasks that have less then 1 wakeup per\n\t * jiffy will not have built up many flips.\n\t */\n\tif (time_after(jiffies, current->wakee_flip_decay_ts + HZ)) {\n\t\tcurrent->wakee_flips >>= 1;\n\t\tcurrent->wakee_flip_decay_ts = jiffies;\n\t}\n\n\tif (current->last_wakee != p) {\n\t\tcurrent->last_wakee = p;\n\t\tcurrent->wakee_flips++;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 6317
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nselect_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)\n{\n\tstruct sched_domain *tmp, *sd = NULL;\n\tint cpu = smp_processor_id();\n\tint new_cpu = prev_cpu;\n\tint want_affine = 0;\n\tint sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);\n\n\tif (sd_flag & SD_BALANCE_WAKE) {\n\t\trecord_wakee(p);\n\t\twant_affine = !wake_wide(p) && !wake_cap(p, cpu, prev_cpu)\n\t\t\t      && cpumask_test_cpu(cpu, &p->cpus_allowed);\n\t}\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, tmp) {\n\t\tif (!(tmp->flags & SD_LOAD_BALANCE))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If both 'cpu' and 'prev_cpu' are part of this domain,\n\t\t * cpu is a valid SD_WAKE_AFFINE target.\n\t\t */\n\t\tif (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&\n\t\t    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {\n\t\t\tif (cpu != prev_cpu)\n\t\t\t\tnew_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);\n\n\t\t\tsd = NULL; /* Prefer wake_affine over balance flags */\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tmp->flags & sd_flag)\n\t\t\tsd = tmp;\n\t\telse if (!want_affine)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(sd)) {\n\t\t/* Slow path */\n\t\tnew_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);\n\t} else if (sd_flag & SD_BALANCE_WAKE) { /* XXX always ? */\n\t\t/* Fast path */\n\n\t\tnew_cpu = select_idle_sibling(p, prev_cpu, new_cpu);\n\n\t\tif (want_affine)\n\t\t\tcurrent->recent_used_cpu = cpu;\n\t}\n\trcu_read_unlock();\n\n\treturn new_cpu;\n}"
  },
  {
    "function_name": "wake_cap",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6281-6299",
    "snippet": "static int wake_cap(struct task_struct *p, int cpu, int prev_cpu)\n{\n\tlong min_cap, max_cap;\n\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn 0;\n\n\tmin_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu));\n\tmax_cap = cpu_rq(cpu)->rd->max_cpu_capacity;\n\n\t/* Minimum capacity is close to max, no need to abort wake_affine */\n\tif (max_cap - min_cap < max_cap >> 3)\n\t\treturn 0;\n\n\t/* Bring task utilization in sync with prev_cpu */\n\tsync_entity_load_avg(&p->se);\n\n\treturn !task_fits_capacity(p, min_cap);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_fits_capacity",
          "args": [
            "p",
            "min_cap"
          ],
          "line": 6298
        },
        "resolved": true,
        "details": {
          "function_name": "task_fits_capacity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3704-3707",
          "snippet": "static inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int capacity_margin\t\t\t\t= 1280;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int capacity_margin\t\t\t\t= 1280;\n\nstatic inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sync_entity_load_avg",
          "args": [
            "&p->se"
          ],
          "line": 6296
        },
        "resolved": true,
        "details": {
          "function_name": "sync_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3548-3555",
          "snippet": "void sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 6289
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "capacity_orig_of(prev_cpu)",
            "capacity_orig_of(cpu)"
          ],
          "line": 6288
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "capacity_orig_of",
          "args": [
            "cpu"
          ],
          "line": 6288
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_orig_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5514-5517",
          "snippet": "static unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}"
        }
      },
      {
        "call_info": {
          "callee": "static_branch_unlikely",
          "args": [
            "&sched_asym_cpucapacity"
          ],
          "line": 6285
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic int wake_cap(struct task_struct *p, int cpu, int prev_cpu)\n{\n\tlong min_cap, max_cap;\n\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn 0;\n\n\tmin_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu));\n\tmax_cap = cpu_rq(cpu)->rd->max_cpu_capacity;\n\n\t/* Minimum capacity is close to max, no need to abort wake_affine */\n\tif (max_cap - min_cap < max_cap >> 3)\n\t\treturn 0;\n\n\t/* Bring task utilization in sync with prev_cpu */\n\tsync_entity_load_avg(&p->se);\n\n\treturn !task_fits_capacity(p, min_cap);\n}"
  },
  {
    "function_name": "cpu_util_wake",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6222-6272",
    "snippet": "static unsigned long cpu_util_wake(int cpu, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\t/* Task has no contribution or is new */\n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_util(cpu);\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/* Discount task's blocked util from CPU's util */\n\tutil -= min_t(unsigned int, util, task_util(p));\n\n\t/*\n\t * Covered cases:\n\t *\n\t * a) if *p is the only task sleeping on this CPU, then:\n\t *      cpu_util (== task_util) > util_est (== 0)\n\t *    and thus we return:\n\t *      cpu_util_wake = (cpu_util - task_util) = 0\n\t *\n\t * b) if other tasks are SLEEPING on this CPU, which is now exiting\n\t *    IDLE, then:\n\t *      cpu_util >= task_util\n\t *      cpu_util > util_est (== 0)\n\t *    and thus we discount *p's blocked utilization to return:\n\t *      cpu_util_wake = (cpu_util - task_util) >= 0\n\t *\n\t * c) if other tasks are RUNNABLE on that CPU and\n\t *      util_est > cpu_util\n\t *    then we use util_est since it returns a more restrictive\n\t *    estimation of the spare capacity on that CPU, by just\n\t *    considering the expected utilization of tasks already\n\t *    runnable on that CPU.\n\t *\n\t * Cases a) and b) are covered by the above code, while case c) is\n\t * covered by the following code when estimated utilization is\n\t * enabled.\n\t */\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\t/*\n\t * Utilization (estimated) can exceed the CPU capacity, thus let's\n\t * clamp to the maximum CPU capacity to ensure consistency with\n\t * the cpu_util call.\n\t */\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "min_t",
          "args": [
            "unsignedlong",
            "util",
            "capacity_orig_of(cpu)"
          ],
          "line": 6271
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "capacity_orig_of",
          "args": [
            "cpu"
          ],
          "line": 6271
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_orig_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5514-5517",
          "snippet": "static unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}"
        }
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "util",
            "READ_ONCE(cfs_rq->avg.util_est.enqueued)"
          ],
          "line": 6264
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "cfs_rq->avg.util_est.enqueued"
          ],
          "line": 6264
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "UTIL_EST"
          ],
          "line": 6263
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "min_t",
          "args": [
            "unsignedint",
            "util",
            "task_util(p)"
          ],
          "line": 6235
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_util",
          "args": [
            "p"
          ],
          "line": 6235
        },
        "resolved": true,
        "details": {
          "function_name": "task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3610-3613",
          "snippet": "static inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "cfs_rq->avg.util_avg"
          ],
          "line": 6232
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 6231
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_util",
          "args": [
            "cpu"
          ],
          "line": 6229
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_util",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6204-6216",
          "snippet": "static inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "p->se.avg.last_update_time"
          ],
          "line": 6228
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_cpu",
          "args": [
            "p"
          ],
          "line": 6228
        },
        "resolved": true,
        "details": {
          "function_name": "ignore_task_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/ftrace.c",
          "lines": "6542-6556",
          "snippet": "static void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/suspend.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_MUTEX(ftrace_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nstatic DEFINE_MUTEX(ftrace_lock);\n\nstatic void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic unsigned long cpu_util_wake(int cpu, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\t/* Task has no contribution or is new */\n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_util(cpu);\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/* Discount task's blocked util from CPU's util */\n\tutil -= min_t(unsigned int, util, task_util(p));\n\n\t/*\n\t * Covered cases:\n\t *\n\t * a) if *p is the only task sleeping on this CPU, then:\n\t *      cpu_util (== task_util) > util_est (== 0)\n\t *    and thus we return:\n\t *      cpu_util_wake = (cpu_util - task_util) = 0\n\t *\n\t * b) if other tasks are SLEEPING on this CPU, which is now exiting\n\t *    IDLE, then:\n\t *      cpu_util >= task_util\n\t *      cpu_util > util_est (== 0)\n\t *    and thus we discount *p's blocked utilization to return:\n\t *      cpu_util_wake = (cpu_util - task_util) >= 0\n\t *\n\t * c) if other tasks are RUNNABLE on that CPU and\n\t *      util_est > cpu_util\n\t *    then we use util_est since it returns a more restrictive\n\t *    estimation of the spare capacity on that CPU, by just\n\t *    considering the expected utilization of tasks already\n\t *    runnable on that CPU.\n\t *\n\t * Cases a) and b) are covered by the above code, while case c) is\n\t * covered by the following code when estimated utilization is\n\t * enabled.\n\t */\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\t/*\n\t * Utilization (estimated) can exceed the CPU capacity, thus let's\n\t * clamp to the maximum CPU capacity to ensure consistency with\n\t * the cpu_util call.\n\t */\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}"
  },
  {
    "function_name": "cpu_util",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6204-6216",
    "snippet": "static inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "min_t",
          "args": [
            "unsignedlong",
            "util",
            "capacity_orig_of(cpu)"
          ],
          "line": 6215
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "capacity_orig_of",
          "args": [
            "cpu"
          ],
          "line": 6215
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_orig_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5514-5517",
          "snippet": "static unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}"
        }
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "util",
            "READ_ONCE(cfs_rq->avg.util_est.enqueued)"
          ],
          "line": 6213
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "cfs_rq->avg.util_est.enqueued"
          ],
          "line": 6213
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "UTIL_EST"
          ],
          "line": 6212
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "cfs_rq->avg.util_avg"
          ],
          "line": 6210
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 6209
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cpu_util(int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}"
  },
  {
    "function_name": "select_idle_sibling",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6118-6164",
    "snippet": "static int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "select_idle_smt",
          "args": [
            "p",
            "sd",
            "target"
          ],
          "line": 6159
        },
        "resolved": true,
        "details": {
          "function_name": "select_idle_smt",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6054-6057",
          "snippet": "static inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "select_idle_cpu",
          "args": [
            "p",
            "sd",
            "target"
          ],
          "line": 6155
        },
        "resolved": true,
        "details": {
          "function_name": "select_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6066-6113",
          "snippet": "static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct sched_domain *this_sd;\n\tu64 avg_cost, avg_idle;\n\tu64 time, cost;\n\ts64 delta;\n\tint cpu, nr = INT_MAX;\n\n\tthis_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));\n\tif (!this_sd)\n\t\treturn -1;\n\n\t/*\n\t * Due to large variance we need a large fuzz factor; hackbench in\n\t * particularly is sensitive here.\n\t */\n\tavg_idle = this_rq()->avg_idle / 512;\n\tavg_cost = this_sd->avg_scan_cost + 1;\n\n\tif (sched_feat(SIS_AVG_CPU) && avg_idle < avg_cost)\n\t\treturn -1;\n\n\tif (sched_feat(SIS_PROP)) {\n\t\tu64 span_avg = sd->span_weight * avg_idle;\n\t\tif (span_avg > 4*avg_cost)\n\t\t\tnr = div_u64(span_avg, avg_cost);\n\t\telse\n\t\t\tnr = 4;\n\t}\n\n\ttime = local_clock();\n\n\tfor_each_cpu_wrap(cpu, sched_domain_span(sd), target) {\n\t\tif (!--nr)\n\t\t\treturn -1;\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\tbreak;\n\t}\n\n\ttime = local_clock() - time;\n\tcost = this_sd->avg_scan_cost;\n\tdelta = (s64)(time - cost) / 8;\n\tthis_sd->avg_scan_cost += delta;\n\n\treturn cpu;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct sched_domain *this_sd;\n\tu64 avg_cost, avg_idle;\n\tu64 time, cost;\n\ts64 delta;\n\tint cpu, nr = INT_MAX;\n\n\tthis_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));\n\tif (!this_sd)\n\t\treturn -1;\n\n\t/*\n\t * Due to large variance we need a large fuzz factor; hackbench in\n\t * particularly is sensitive here.\n\t */\n\tavg_idle = this_rq()->avg_idle / 512;\n\tavg_cost = this_sd->avg_scan_cost + 1;\n\n\tif (sched_feat(SIS_AVG_CPU) && avg_idle < avg_cost)\n\t\treturn -1;\n\n\tif (sched_feat(SIS_PROP)) {\n\t\tu64 span_avg = sd->span_weight * avg_idle;\n\t\tif (span_avg > 4*avg_cost)\n\t\t\tnr = div_u64(span_avg, avg_cost);\n\t\telse\n\t\t\tnr = 4;\n\t}\n\n\ttime = local_clock();\n\n\tfor_each_cpu_wrap(cpu, sched_domain_span(sd), target) {\n\t\tif (!--nr)\n\t\t\treturn -1;\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\tbreak;\n\t}\n\n\ttime = local_clock() - time;\n\tcost = this_sd->avg_scan_cost;\n\tdelta = (s64)(time - cost) / 8;\n\tthis_sd->avg_scan_cost += delta;\n\n\treturn cpu;\n}"
        }
      },
      {
        "call_info": {
          "callee": "select_idle_core",
          "args": [
            "p",
            "sd",
            "target"
          ],
          "line": 6151
        },
        "resolved": true,
        "details": {
          "function_name": "select_idle_core",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6049-6052",
          "snippet": "static inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "per_cpu(sd_llc, target)"
          ],
          "line": 6147
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "per_cpu",
          "args": [
            "sd_llc",
            "target"
          ],
          "line": 6147
        },
        "resolved": true,
        "details": {
          "function_name": "kdb_per_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/debug/kdb/kdb_main.c",
          "lines": "2575-2640",
          "snippet": "static int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}",
          "includes": [
            "#include \"kdb_private.h\"",
            "#include <linux/slab.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/time.h>",
            "#include <linux/nmi.h>",
            "#include <linux/delay.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kdb.h>",
            "#include <linux/kgdb.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/atomic.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/utsname.h>",
            "#include <linux/smp.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched.h>",
            "#include <linux/reboot.h>",
            "#include <linux/kmsg_dump.h>",
            "#include <linux/kernel.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/ctype.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"kdb_private.h\"\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/proc_fs.h>\n#include <linux/kdebug.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/ptrace.h>\n#include <linux/time.h>\n#include <linux/nmi.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n#include <linux/kdb.h>\n#include <linux/kgdb.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/atomic.h>\n#include <linux/vmalloc.h>\n#include <linux/utsname.h>\n#include <linux/smp.h>\n#include <linux/sysrq.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched.h>\n#include <linux/reboot.h>\n#include <linux/kmsg_dump.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ctype.h>\n\nstatic int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "p->recent_used_cpu",
            "&p->cpus_allowed"
          ],
          "line": 6138
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "available_idle_cpu",
          "args": [
            "recent_used_cpu"
          ],
          "line": 6137
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpus_share_cache",
          "args": [
            "recent_used_cpu",
            "target"
          ],
          "line": 6136
        },
        "resolved": true,
        "details": {
          "function_name": "cpus_share_cache",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "1826-1829",
          "snippet": "bool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\treturn per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nbool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\treturn per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}"
  },
  {
    "function_name": "select_idle_cpu",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6066-6113",
    "snippet": "static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct sched_domain *this_sd;\n\tu64 avg_cost, avg_idle;\n\tu64 time, cost;\n\ts64 delta;\n\tint cpu, nr = INT_MAX;\n\n\tthis_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));\n\tif (!this_sd)\n\t\treturn -1;\n\n\t/*\n\t * Due to large variance we need a large fuzz factor; hackbench in\n\t * particularly is sensitive here.\n\t */\n\tavg_idle = this_rq()->avg_idle / 512;\n\tavg_cost = this_sd->avg_scan_cost + 1;\n\n\tif (sched_feat(SIS_AVG_CPU) && avg_idle < avg_cost)\n\t\treturn -1;\n\n\tif (sched_feat(SIS_PROP)) {\n\t\tu64 span_avg = sd->span_weight * avg_idle;\n\t\tif (span_avg > 4*avg_cost)\n\t\t\tnr = div_u64(span_avg, avg_cost);\n\t\telse\n\t\t\tnr = 4;\n\t}\n\n\ttime = local_clock();\n\n\tfor_each_cpu_wrap(cpu, sched_domain_span(sd), target) {\n\t\tif (!--nr)\n\t\t\treturn -1;\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\tbreak;\n\t}\n\n\ttime = local_clock() - time;\n\tcost = this_sd->avg_scan_cost;\n\tdelta = (s64)(time - cost) / 8;\n\tthis_sd->avg_scan_cost += delta;\n\n\treturn cpu;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "",
          "args": [
            "time - cost"
          ],
          "line": 6109
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "local_clock",
          "args": [],
          "line": 6107
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "available_idle_cpu",
          "args": [
            "cpu"
          ],
          "line": 6103
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "&p->cpus_allowed"
          ],
          "line": 6101
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu_wrap",
          "args": [
            "cpu",
            "sched_domain_span(sd)",
            "target"
          ],
          "line": 6098
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "sd"
          ],
          "line": 6098
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "local_clock",
          "args": [],
          "line": 6096
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "div_u64",
          "args": [
            "span_avg",
            "avg_cost"
          ],
          "line": 6091
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "SIS_PROP"
          ],
          "line": 6088
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_rq",
          "args": [],
          "line": 6082
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "*this_cpu_ptr(&sd_llc)"
          ],
          "line": 6074
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_cpu_ptr",
          "args": [
            "&sd_llc"
          ],
          "line": 6074
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct sched_domain *this_sd;\n\tu64 avg_cost, avg_idle;\n\tu64 time, cost;\n\ts64 delta;\n\tint cpu, nr = INT_MAX;\n\n\tthis_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));\n\tif (!this_sd)\n\t\treturn -1;\n\n\t/*\n\t * Due to large variance we need a large fuzz factor; hackbench in\n\t * particularly is sensitive here.\n\t */\n\tavg_idle = this_rq()->avg_idle / 512;\n\tavg_cost = this_sd->avg_scan_cost + 1;\n\n\tif (sched_feat(SIS_AVG_CPU) && avg_idle < avg_cost)\n\t\treturn -1;\n\n\tif (sched_feat(SIS_PROP)) {\n\t\tu64 span_avg = sd->span_weight * avg_idle;\n\t\tif (span_avg > 4*avg_cost)\n\t\t\tnr = div_u64(span_avg, avg_cost);\n\t\telse\n\t\t\tnr = 4;\n\t}\n\n\ttime = local_clock();\n\n\tfor_each_cpu_wrap(cpu, sched_domain_span(sd), target) {\n\t\tif (!--nr)\n\t\t\treturn -1;\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\tbreak;\n\t}\n\n\ttime = local_clock() - time;\n\tcost = this_sd->avg_scan_cost;\n\tdelta = (s64)(time - cost) / 8;\n\tthis_sd->avg_scan_cost += delta;\n\n\treturn cpu;\n}"
  },
  {
    "function_name": "select_idle_smt",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6054-6057",
    "snippet": "static inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}"
  },
  {
    "function_name": "select_idle_core",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6049-6052",
    "snippet": "static inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\treturn -1;\n}"
  },
  {
    "function_name": "select_idle_smt",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "6030-6045",
    "snippet": "static int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tint cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(target)) {\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\treturn cpu;\n\t}\n\n\treturn -1;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "available_idle_cpu",
          "args": [
            "cpu"
          ],
          "line": 6040
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "&p->cpus_allowed"
          ],
          "line": 6038
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "cpu",
            "cpu_smt_mask(target)"
          ],
          "line": 6037
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_smt_mask",
          "args": [
            "target"
          ],
          "line": 6037
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "static_branch_likely",
          "args": [
            "&sched_smt_present"
          ],
          "line": 6034
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tint cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(target)) {\n\t\tif (!cpumask_test_cpu(cpu, &p->cpus_allowed))\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu))\n\t\t\treturn cpu;\n\t}\n\n\treturn -1;\n}"
  },
  {
    "function_name": "select_idle_core",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5993-6025",
    "snippet": "static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);\n\tint core, cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tif (!test_idle_cores(target, false))\n\t\treturn -1;\n\n\tcpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);\n\n\tfor_each_cpu_wrap(core, cpus, target) {\n\t\tbool idle = true;\n\n\t\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\t\tcpumask_clear_cpu(cpu, cpus);\n\t\t\tif (!available_idle_cpu(cpu))\n\t\t\t\tidle = false;\n\t\t}\n\n\t\tif (idle)\n\t\t\treturn core;\n\t}\n\n\t/*\n\t * Failed to find an idle core; stop looking for one.\n\t */\n\tset_idle_cores(target, 0);\n\n\treturn -1;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "set_idle_cores",
          "args": [
            "target",
            "0"
          ],
          "line": 6022
        },
        "resolved": true,
        "details": {
          "function_name": "set_idle_cores",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5939-5946",
          "snippet": "static inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "available_idle_cpu",
          "args": [
            "cpu"
          ],
          "line": 6011
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_clear_cpu",
          "args": [
            "cpu",
            "cpus"
          ],
          "line": 6010
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "cpu",
            "cpu_smt_mask(core)"
          ],
          "line": 6009
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_smt_mask",
          "args": [
            "core"
          ],
          "line": 6009
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu_wrap",
          "args": [
            "core",
            "cpus",
            "target"
          ],
          "line": 6006
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_and",
          "args": [
            "cpus",
            "sched_domain_span(sd)",
            "&p->cpus_allowed"
          ],
          "line": 6004
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "sd"
          ],
          "line": 6004
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "test_idle_cores",
          "args": [
            "target",
            "false"
          ],
          "line": 6001
        },
        "resolved": true,
        "details": {
          "function_name": "test_idle_cores",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5948-5957",
          "snippet": "static inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}"
        }
      },
      {
        "call_info": {
          "callee": "static_branch_likely",
          "args": [
            "&sched_smt_present"
          ],
          "line": 5998
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "this_cpu_cpumask_var_ptr",
          "args": [
            "select_idle_mask"
          ],
          "line": 5995
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);\n\tint core, cpu;\n\n\tif (!static_branch_likely(&sched_smt_present))\n\t\treturn -1;\n\n\tif (!test_idle_cores(target, false))\n\t\treturn -1;\n\n\tcpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);\n\n\tfor_each_cpu_wrap(core, cpus, target) {\n\t\tbool idle = true;\n\n\t\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\t\tcpumask_clear_cpu(cpu, cpus);\n\t\t\tif (!available_idle_cpu(cpu))\n\t\t\t\tidle = false;\n\t\t}\n\n\t\tif (idle)\n\t\t\treturn core;\n\t}\n\n\t/*\n\t * Failed to find an idle core; stop looking for one.\n\t */\n\tset_idle_cores(target, 0);\n\n\treturn -1;\n}"
  },
  {
    "function_name": "__update_idle_core",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5966-5986",
    "snippet": "void __update_idle_core(struct rq *rq)\n{\n\tint core = cpu_of(rq);\n\tint cpu;\n\n\trcu_read_lock();\n\tif (test_idle_cores(core, true))\n\t\tgoto unlock;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\tif (cpu == core)\n\t\t\tcontinue;\n\n\t\tif (!available_idle_cpu(cpu))\n\t\t\tgoto unlock;\n\t}\n\n\tset_idle_cores(core, 1);\nunlock:\n\trcu_read_unlock();\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 5985
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_idle_cores",
          "args": [
            "core",
            "1"
          ],
          "line": 5983
        },
        "resolved": true,
        "details": {
          "function_name": "set_idle_cores",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5939-5946",
          "snippet": "static inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}"
        }
      },
      {
        "call_info": {
          "callee": "available_idle_cpu",
          "args": [
            "cpu"
          ],
          "line": 5979
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "cpu",
            "cpu_smt_mask(core)"
          ],
          "line": 5975
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_smt_mask",
          "args": [
            "core"
          ],
          "line": 5975
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "test_idle_cores",
          "args": [
            "core",
            "true"
          ],
          "line": 5972
        },
        "resolved": true,
        "details": {
          "function_name": "test_idle_cores",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5948-5957",
          "snippet": "static inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 5971
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 5968
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid __update_idle_core(struct rq *rq)\n{\n\tint core = cpu_of(rq);\n\tint cpu;\n\n\trcu_read_lock();\n\tif (test_idle_cores(core, true))\n\t\tgoto unlock;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\tif (cpu == core)\n\t\t\tcontinue;\n\n\t\tif (!available_idle_cpu(cpu))\n\t\t\tgoto unlock;\n\t}\n\n\tset_idle_cores(core, 1);\nunlock:\n\trcu_read_unlock();\n}"
  },
  {
    "function_name": "test_idle_cores",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5948-5957",
    "snippet": "static inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "sds->has_idle_cores"
          ],
          "line": 5954
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "per_cpu(sd_llc_shared, cpu)"
          ],
          "line": 5952
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "per_cpu",
          "args": [
            "sd_llc_shared",
            "cpu"
          ],
          "line": 5952
        },
        "resolved": true,
        "details": {
          "function_name": "kdb_per_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/debug/kdb/kdb_main.c",
          "lines": "2575-2640",
          "snippet": "static int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}",
          "includes": [
            "#include \"kdb_private.h\"",
            "#include <linux/slab.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/time.h>",
            "#include <linux/nmi.h>",
            "#include <linux/delay.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kdb.h>",
            "#include <linux/kgdb.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/atomic.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/utsname.h>",
            "#include <linux/smp.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched.h>",
            "#include <linux/reboot.h>",
            "#include <linux/kmsg_dump.h>",
            "#include <linux/kernel.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/ctype.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"kdb_private.h\"\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/proc_fs.h>\n#include <linux/kdebug.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/ptrace.h>\n#include <linux/time.h>\n#include <linux/nmi.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n#include <linux/kdb.h>\n#include <linux/kgdb.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/atomic.h>\n#include <linux/vmalloc.h>\n#include <linux/utsname.h>\n#include <linux/smp.h>\n#include <linux/sysrq.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched.h>\n#include <linux/reboot.h>\n#include <linux/kmsg_dump.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ctype.h>\n\nstatic int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline bool test_idle_cores(int cpu, bool def)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn def;\n}"
  },
  {
    "function_name": "set_idle_cores",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5939-5946",
    "snippet": "static inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "sds->has_idle_cores",
            "val"
          ],
          "line": 5945
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "per_cpu(sd_llc_shared, cpu)"
          ],
          "line": 5943
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "per_cpu",
          "args": [
            "sd_llc_shared",
            "cpu"
          ],
          "line": 5943
        },
        "resolved": true,
        "details": {
          "function_name": "kdb_per_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/debug/kdb/kdb_main.c",
          "lines": "2575-2640",
          "snippet": "static int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}",
          "includes": [
            "#include \"kdb_private.h\"",
            "#include <linux/slab.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/time.h>",
            "#include <linux/nmi.h>",
            "#include <linux/delay.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kdb.h>",
            "#include <linux/kgdb.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/atomic.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/utsname.h>",
            "#include <linux/smp.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched.h>",
            "#include <linux/reboot.h>",
            "#include <linux/kmsg_dump.h>",
            "#include <linux/kernel.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/ctype.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"kdb_private.h\"\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/proc_fs.h>\n#include <linux/kdebug.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/ptrace.h>\n#include <linux/time.h>\n#include <linux/nmi.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n#include <linux/kdb.h>\n#include <linux/kgdb.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/atomic.h>\n#include <linux/vmalloc.h>\n#include <linux/utsname.h>\n#include <linux/smp.h>\n#include <linux/sysrq.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched.h>\n#include <linux/reboot.h>\n#include <linux/kmsg_dump.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ctype.h>\n\nstatic int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}"
  },
  {
    "function_name": "find_idlest_cpu",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5883-5934",
    "snippet": "static inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,\n\t\t\t\t  int cpu, int prev_cpu, int sd_flag)\n{\n\tint new_cpu = cpu;\n\n\tif (!cpumask_intersects(sched_domain_span(sd), &p->cpus_allowed))\n\t\treturn prev_cpu;\n\n\t/*\n\t * We need task's util for capacity_spare_wake, sync it up to prev_cpu's\n\t * last_update_time.\n\t */\n\tif (!(sd_flag & SD_BALANCE_FORK))\n\t\tsync_entity_load_avg(&p->se);\n\n\twhile (sd) {\n\t\tstruct sched_group *group;\n\t\tstruct sched_domain *tmp;\n\t\tint weight;\n\n\t\tif (!(sd->flags & sd_flag)) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgroup = find_idlest_group(sd, p, cpu, sd_flag);\n\t\tif (!group) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_cpu = find_idlest_group_cpu(group, p, cpu);\n\t\tif (new_cpu == cpu) {\n\t\t\t/* Now try balancing at a lower domain level of 'cpu': */\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Now try balancing at a lower domain level of 'new_cpu': */\n\t\tcpu = new_cpu;\n\t\tweight = sd->span_weight;\n\t\tsd = NULL;\n\t\tfor_each_domain(cpu, tmp) {\n\t\t\tif (weight <= tmp->span_weight)\n\t\t\t\tbreak;\n\t\t\tif (tmp->flags & sd_flag)\n\t\t\t\tsd = tmp;\n\t\t}\n\t}\n\n\treturn new_cpu;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "for_each_domain",
          "args": [
            "cpu",
            "tmp"
          ],
          "line": 5925
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_idlest_group_cpu",
          "args": [
            "group",
            "p",
            "cpu"
          ],
          "line": 5914
        },
        "resolved": true,
        "details": {
          "function_name": "find_idlest_group_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5833-5881",
          "snippet": "static int\nfind_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)\n{\n\tunsigned long load, min_load = ULONG_MAX;\n\tunsigned int min_exit_latency = UINT_MAX;\n\tu64 latest_idle_timestamp = 0;\n\tint least_loaded_cpu = this_cpu;\n\tint shallowest_idle_cpu = -1;\n\tint i;\n\n\t/* Check if we have any choice: */\n\tif (group->group_weight == 1)\n\t\treturn cpumask_first(sched_group_span(group));\n\n\t/* Traverse only the allowed CPUs */\n\tfor_each_cpu_and(i, sched_group_span(group), &p->cpus_allowed) {\n\t\tif (available_idle_cpu(i)) {\n\t\t\tstruct rq *rq = cpu_rq(i);\n\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);\n\t\t\tif (idle && idle->exit_latency < min_exit_latency) {\n\t\t\t\t/*\n\t\t\t\t * We give priority to a CPU whose idle state\n\t\t\t\t * has the smallest exit latency irrespective\n\t\t\t\t * of any idle timestamp.\n\t\t\t\t */\n\t\t\t\tmin_exit_latency = idle->exit_latency;\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&\n\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {\n\t\t\t\t/*\n\t\t\t\t * If equal or no active idle state, then\n\t\t\t\t * the most recently idled CPU might have\n\t\t\t\t * a warmer cache.\n\t\t\t\t */\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t}\n\t\t} else if (shallowest_idle_cpu == -1) {\n\t\t\tload = weighted_cpuload(cpu_rq(i));\n\t\t\tif (load < min_load) {\n\t\t\t\tmin_load = load;\n\t\t\t\tleast_loaded_cpu = i;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nfind_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)\n{\n\tunsigned long load, min_load = ULONG_MAX;\n\tunsigned int min_exit_latency = UINT_MAX;\n\tu64 latest_idle_timestamp = 0;\n\tint least_loaded_cpu = this_cpu;\n\tint shallowest_idle_cpu = -1;\n\tint i;\n\n\t/* Check if we have any choice: */\n\tif (group->group_weight == 1)\n\t\treturn cpumask_first(sched_group_span(group));\n\n\t/* Traverse only the allowed CPUs */\n\tfor_each_cpu_and(i, sched_group_span(group), &p->cpus_allowed) {\n\t\tif (available_idle_cpu(i)) {\n\t\t\tstruct rq *rq = cpu_rq(i);\n\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);\n\t\t\tif (idle && idle->exit_latency < min_exit_latency) {\n\t\t\t\t/*\n\t\t\t\t * We give priority to a CPU whose idle state\n\t\t\t\t * has the smallest exit latency irrespective\n\t\t\t\t * of any idle timestamp.\n\t\t\t\t */\n\t\t\t\tmin_exit_latency = idle->exit_latency;\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&\n\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {\n\t\t\t\t/*\n\t\t\t\t * If equal or no active idle state, then\n\t\t\t\t * the most recently idled CPU might have\n\t\t\t\t * a warmer cache.\n\t\t\t\t */\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t}\n\t\t} else if (shallowest_idle_cpu == -1) {\n\t\t\tload = weighted_cpuload(cpu_rq(i));\n\t\t\tif (load < min_load) {\n\t\t\t\tmin_load = load;\n\t\t\t\tleast_loaded_cpu = i;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;\n}"
        }
      },
      {
        "call_info": {
          "callee": "find_idlest_group",
          "args": [
            "sd",
            "p",
            "cpu",
            "sd_flag"
          ],
          "line": 5908
        },
        "resolved": true,
        "details": {
          "function_name": "find_idlest_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5690-5828",
          "snippet": "static struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p,\n\t\t  int this_cpu, int sd_flag)\n{\n\tstruct sched_group *idlest = NULL, *group = sd->groups;\n\tstruct sched_group *most_spare_sg = NULL;\n\tunsigned long min_runnable_load = ULONG_MAX;\n\tunsigned long this_runnable_load = ULONG_MAX;\n\tunsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX;\n\tunsigned long most_spare = 0, this_spare = 0;\n\tint load_idx = sd->forkexec_idx;\n\tint imbalance_scale = 100 + (sd->imbalance_pct-100)/2;\n\tunsigned long imbalance = scale_load_down(NICE_0_LOAD) *\n\t\t\t\t(sd->imbalance_pct-100) / 100;\n\n\tif (sd_flag & SD_BALANCE_WAKE)\n\t\tload_idx = sd->wake_idx;\n\n\tdo {\n\t\tunsigned long load, avg_load, runnable_load;\n\t\tunsigned long spare_cap, max_spare_cap;\n\t\tint local_group;\n\t\tint i;\n\n\t\t/* Skip over this group if it has no CPUs allowed */\n\t\tif (!cpumask_intersects(sched_group_span(group),\n\t\t\t\t\t&p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tlocal_group = cpumask_test_cpu(this_cpu,\n\t\t\t\t\t       sched_group_span(group));\n\n\t\t/*\n\t\t * Tally up the load of all CPUs in the group and find\n\t\t * the group containing the CPU with most spare capacity.\n\t\t */\n\t\tavg_load = 0;\n\t\trunnable_load = 0;\n\t\tmax_spare_cap = 0;\n\n\t\tfor_each_cpu(i, sched_group_span(group)) {\n\t\t\t/* Bias balancing toward CPUs of our domain */\n\t\t\tif (local_group)\n\t\t\t\tload = source_load(i, load_idx);\n\t\t\telse\n\t\t\t\tload = target_load(i, load_idx);\n\n\t\t\trunnable_load += load;\n\n\t\t\tavg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);\n\n\t\t\tspare_cap = capacity_spare_wake(i, p);\n\n\t\t\tif (spare_cap > max_spare_cap)\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t}\n\n\t\t/* Adjust by relative CPU capacity of the group */\n\t\tavg_load = (avg_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\t\trunnable_load = (runnable_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\n\t\tif (local_group) {\n\t\t\tthis_runnable_load = runnable_load;\n\t\t\tthis_avg_load = avg_load;\n\t\t\tthis_spare = max_spare_cap;\n\t\t} else {\n\t\t\tif (min_runnable_load > (runnable_load + imbalance)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable load is significantly smaller\n\t\t\t\t * so we can pick this new CPU:\n\t\t\t\t */\n\t\t\t\tmin_runnable_load = runnable_load;\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t} else if ((runnable_load < (min_runnable_load + imbalance)) &&\n\t\t\t\t   (100*min_avg_load > imbalance_scale*avg_load)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable loads are close so take the\n\t\t\t\t * blocked load into account through avg_load:\n\t\t\t\t */\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t}\n\n\t\t\tif (most_spare < max_spare_cap) {\n\t\t\t\tmost_spare = max_spare_cap;\n\t\t\t\tmost_spare_sg = group;\n\t\t\t}\n\t\t}\n\t} while (group = group->next, group != sd->groups);\n\n\t/*\n\t * The cross-over point between using spare capacity or least load\n\t * is too conservative for high utilization tasks on partially\n\t * utilized systems if we require spare_capacity > task_util(p),\n\t * so we allow for some task stuffing by using\n\t * spare_capacity > task_util(p)/2.\n\t *\n\t * Spare capacity can't be used for fork because the utilization has\n\t * not been set yet, we must first select a rq to compute the initial\n\t * utilization.\n\t */\n\tif (sd_flag & SD_BALANCE_FORK)\n\t\tgoto skip_spare;\n\n\tif (this_spare > task_util(p) / 2 &&\n\t    imbalance_scale*this_spare > 100*most_spare)\n\t\treturn NULL;\n\n\tif (most_spare > task_util(p) / 2)\n\t\treturn most_spare_sg;\n\nskip_spare:\n\tif (!idlest)\n\t\treturn NULL;\n\n\t/*\n\t * When comparing groups across NUMA domains, it's possible for the\n\t * local domain to be very lightly loaded relative to the remote\n\t * domains but \"imbalance\" skews the comparison making remote CPUs\n\t * look much more favourable. When considering cross-domain, add\n\t * imbalance to the runnable load on the remote node and consider\n\t * staying local.\n\t */\n\tif ((sd->flags & SD_NUMA) &&\n\t    min_runnable_load + imbalance >= this_runnable_load)\n\t\treturn NULL;\n\n\tif (min_runnable_load > (this_runnable_load + imbalance))\n\t\treturn NULL;\n\n\tif ((this_runnable_load < (min_runnable_load + imbalance)) &&\n\t     (100*this_avg_load < imbalance_scale*min_avg_load))\n\t\treturn NULL;\n\n\treturn idlest;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p,\n\t\t  int this_cpu, int sd_flag)\n{\n\tstruct sched_group *idlest = NULL, *group = sd->groups;\n\tstruct sched_group *most_spare_sg = NULL;\n\tunsigned long min_runnable_load = ULONG_MAX;\n\tunsigned long this_runnable_load = ULONG_MAX;\n\tunsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX;\n\tunsigned long most_spare = 0, this_spare = 0;\n\tint load_idx = sd->forkexec_idx;\n\tint imbalance_scale = 100 + (sd->imbalance_pct-100)/2;\n\tunsigned long imbalance = scale_load_down(NICE_0_LOAD) *\n\t\t\t\t(sd->imbalance_pct-100) / 100;\n\n\tif (sd_flag & SD_BALANCE_WAKE)\n\t\tload_idx = sd->wake_idx;\n\n\tdo {\n\t\tunsigned long load, avg_load, runnable_load;\n\t\tunsigned long spare_cap, max_spare_cap;\n\t\tint local_group;\n\t\tint i;\n\n\t\t/* Skip over this group if it has no CPUs allowed */\n\t\tif (!cpumask_intersects(sched_group_span(group),\n\t\t\t\t\t&p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tlocal_group = cpumask_test_cpu(this_cpu,\n\t\t\t\t\t       sched_group_span(group));\n\n\t\t/*\n\t\t * Tally up the load of all CPUs in the group and find\n\t\t * the group containing the CPU with most spare capacity.\n\t\t */\n\t\tavg_load = 0;\n\t\trunnable_load = 0;\n\t\tmax_spare_cap = 0;\n\n\t\tfor_each_cpu(i, sched_group_span(group)) {\n\t\t\t/* Bias balancing toward CPUs of our domain */\n\t\t\tif (local_group)\n\t\t\t\tload = source_load(i, load_idx);\n\t\t\telse\n\t\t\t\tload = target_load(i, load_idx);\n\n\t\t\trunnable_load += load;\n\n\t\t\tavg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);\n\n\t\t\tspare_cap = capacity_spare_wake(i, p);\n\n\t\t\tif (spare_cap > max_spare_cap)\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t}\n\n\t\t/* Adjust by relative CPU capacity of the group */\n\t\tavg_load = (avg_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\t\trunnable_load = (runnable_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\n\t\tif (local_group) {\n\t\t\tthis_runnable_load = runnable_load;\n\t\t\tthis_avg_load = avg_load;\n\t\t\tthis_spare = max_spare_cap;\n\t\t} else {\n\t\t\tif (min_runnable_load > (runnable_load + imbalance)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable load is significantly smaller\n\t\t\t\t * so we can pick this new CPU:\n\t\t\t\t */\n\t\t\t\tmin_runnable_load = runnable_load;\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t} else if ((runnable_load < (min_runnable_load + imbalance)) &&\n\t\t\t\t   (100*min_avg_load > imbalance_scale*avg_load)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable loads are close so take the\n\t\t\t\t * blocked load into account through avg_load:\n\t\t\t\t */\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t}\n\n\t\t\tif (most_spare < max_spare_cap) {\n\t\t\t\tmost_spare = max_spare_cap;\n\t\t\t\tmost_spare_sg = group;\n\t\t\t}\n\t\t}\n\t} while (group = group->next, group != sd->groups);\n\n\t/*\n\t * The cross-over point between using spare capacity or least load\n\t * is too conservative for high utilization tasks on partially\n\t * utilized systems if we require spare_capacity > task_util(p),\n\t * so we allow for some task stuffing by using\n\t * spare_capacity > task_util(p)/2.\n\t *\n\t * Spare capacity can't be used for fork because the utilization has\n\t * not been set yet, we must first select a rq to compute the initial\n\t * utilization.\n\t */\n\tif (sd_flag & SD_BALANCE_FORK)\n\t\tgoto skip_spare;\n\n\tif (this_spare > task_util(p) / 2 &&\n\t    imbalance_scale*this_spare > 100*most_spare)\n\t\treturn NULL;\n\n\tif (most_spare > task_util(p) / 2)\n\t\treturn most_spare_sg;\n\nskip_spare:\n\tif (!idlest)\n\t\treturn NULL;\n\n\t/*\n\t * When comparing groups across NUMA domains, it's possible for the\n\t * local domain to be very lightly loaded relative to the remote\n\t * domains but \"imbalance\" skews the comparison making remote CPUs\n\t * look much more favourable. When considering cross-domain, add\n\t * imbalance to the runnable load on the remote node and consider\n\t * staying local.\n\t */\n\tif ((sd->flags & SD_NUMA) &&\n\t    min_runnable_load + imbalance >= this_runnable_load)\n\t\treturn NULL;\n\n\tif (min_runnable_load > (this_runnable_load + imbalance))\n\t\treturn NULL;\n\n\tif ((this_runnable_load < (min_runnable_load + imbalance)) &&\n\t     (100*this_avg_load < imbalance_scale*min_avg_load))\n\t\treturn NULL;\n\n\treturn idlest;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sync_entity_load_avg",
          "args": [
            "&p->se"
          ],
          "line": 5896
        },
        "resolved": true,
        "details": {
          "function_name": "sync_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3548-3555",
          "snippet": "void sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_intersects",
          "args": [
            "sched_domain_span(sd)",
            "&p->cpus_allowed"
          ],
          "line": 5888
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_domain_span",
          "args": [
            "sd"
          ],
          "line": 5888
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,\n\t\t\t\t  int cpu, int prev_cpu, int sd_flag)\n{\n\tint new_cpu = cpu;\n\n\tif (!cpumask_intersects(sched_domain_span(sd), &p->cpus_allowed))\n\t\treturn prev_cpu;\n\n\t/*\n\t * We need task's util for capacity_spare_wake, sync it up to prev_cpu's\n\t * last_update_time.\n\t */\n\tif (!(sd_flag & SD_BALANCE_FORK))\n\t\tsync_entity_load_avg(&p->se);\n\n\twhile (sd) {\n\t\tstruct sched_group *group;\n\t\tstruct sched_domain *tmp;\n\t\tint weight;\n\n\t\tif (!(sd->flags & sd_flag)) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgroup = find_idlest_group(sd, p, cpu, sd_flag);\n\t\tif (!group) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_cpu = find_idlest_group_cpu(group, p, cpu);\n\t\tif (new_cpu == cpu) {\n\t\t\t/* Now try balancing at a lower domain level of 'cpu': */\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Now try balancing at a lower domain level of 'new_cpu': */\n\t\tcpu = new_cpu;\n\t\tweight = sd->span_weight;\n\t\tsd = NULL;\n\t\tfor_each_domain(cpu, tmp) {\n\t\t\tif (weight <= tmp->span_weight)\n\t\t\t\tbreak;\n\t\t\tif (tmp->flags & sd_flag)\n\t\t\t\tsd = tmp;\n\t\t}\n\t}\n\n\treturn new_cpu;\n}"
  },
  {
    "function_name": "find_idlest_group_cpu",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5833-5881",
    "snippet": "static int\nfind_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)\n{\n\tunsigned long load, min_load = ULONG_MAX;\n\tunsigned int min_exit_latency = UINT_MAX;\n\tu64 latest_idle_timestamp = 0;\n\tint least_loaded_cpu = this_cpu;\n\tint shallowest_idle_cpu = -1;\n\tint i;\n\n\t/* Check if we have any choice: */\n\tif (group->group_weight == 1)\n\t\treturn cpumask_first(sched_group_span(group));\n\n\t/* Traverse only the allowed CPUs */\n\tfor_each_cpu_and(i, sched_group_span(group), &p->cpus_allowed) {\n\t\tif (available_idle_cpu(i)) {\n\t\t\tstruct rq *rq = cpu_rq(i);\n\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);\n\t\t\tif (idle && idle->exit_latency < min_exit_latency) {\n\t\t\t\t/*\n\t\t\t\t * We give priority to a CPU whose idle state\n\t\t\t\t * has the smallest exit latency irrespective\n\t\t\t\t * of any idle timestamp.\n\t\t\t\t */\n\t\t\t\tmin_exit_latency = idle->exit_latency;\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&\n\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {\n\t\t\t\t/*\n\t\t\t\t * If equal or no active idle state, then\n\t\t\t\t * the most recently idled CPU might have\n\t\t\t\t * a warmer cache.\n\t\t\t\t */\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t}\n\t\t} else if (shallowest_idle_cpu == -1) {\n\t\t\tload = weighted_cpuload(cpu_rq(i));\n\t\t\tif (load < min_load) {\n\t\t\t\tmin_load = load;\n\t\t\t\tleast_loaded_cpu = i;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "cpu_rq(i)"
          ],
          "line": 5872
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "i"
          ],
          "line": 5872
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "idle_get_state",
          "args": [
            "rq"
          ],
          "line": 5851
        },
        "resolved": true,
        "details": {
          "function_name": "idle_get_state",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1724-1727",
          "snippet": "static inline struct cpuidle_state *idle_get_state(struct rq *rq)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline struct cpuidle_state *idle_get_state(struct rq *rq)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "i"
          ],
          "line": 5850
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "available_idle_cpu",
          "args": [
            "i"
          ],
          "line": 5849
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_cpu_and",
          "args": [
            "i",
            "sched_group_span(group)",
            "&p->cpus_allowed"
          ],
          "line": 5848
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_group_span",
          "args": [
            "group"
          ],
          "line": 5848
        },
        "resolved": true,
        "details": {
          "function_name": "sched_group_span",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1330-1333",
          "snippet": "static inline struct cpumask *sched_group_span(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->cpumask);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nstatic inline struct cpumask *sched_group_span(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->cpumask);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_first",
          "args": [
            "sched_group_span(group)"
          ],
          "line": 5845
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nfind_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)\n{\n\tunsigned long load, min_load = ULONG_MAX;\n\tunsigned int min_exit_latency = UINT_MAX;\n\tu64 latest_idle_timestamp = 0;\n\tint least_loaded_cpu = this_cpu;\n\tint shallowest_idle_cpu = -1;\n\tint i;\n\n\t/* Check if we have any choice: */\n\tif (group->group_weight == 1)\n\t\treturn cpumask_first(sched_group_span(group));\n\n\t/* Traverse only the allowed CPUs */\n\tfor_each_cpu_and(i, sched_group_span(group), &p->cpus_allowed) {\n\t\tif (available_idle_cpu(i)) {\n\t\t\tstruct rq *rq = cpu_rq(i);\n\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);\n\t\t\tif (idle && idle->exit_latency < min_exit_latency) {\n\t\t\t\t/*\n\t\t\t\t * We give priority to a CPU whose idle state\n\t\t\t\t * has the smallest exit latency irrespective\n\t\t\t\t * of any idle timestamp.\n\t\t\t\t */\n\t\t\t\tmin_exit_latency = idle->exit_latency;\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&\n\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {\n\t\t\t\t/*\n\t\t\t\t * If equal or no active idle state, then\n\t\t\t\t * the most recently idled CPU might have\n\t\t\t\t * a warmer cache.\n\t\t\t\t */\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t}\n\t\t} else if (shallowest_idle_cpu == -1) {\n\t\t\tload = weighted_cpuload(cpu_rq(i));\n\t\t\tif (load < min_load) {\n\t\t\t\tmin_load = load;\n\t\t\t\tleast_loaded_cpu = i;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;\n}"
  },
  {
    "function_name": "find_idlest_group",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5690-5828",
    "snippet": "static struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p,\n\t\t  int this_cpu, int sd_flag)\n{\n\tstruct sched_group *idlest = NULL, *group = sd->groups;\n\tstruct sched_group *most_spare_sg = NULL;\n\tunsigned long min_runnable_load = ULONG_MAX;\n\tunsigned long this_runnable_load = ULONG_MAX;\n\tunsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX;\n\tunsigned long most_spare = 0, this_spare = 0;\n\tint load_idx = sd->forkexec_idx;\n\tint imbalance_scale = 100 + (sd->imbalance_pct-100)/2;\n\tunsigned long imbalance = scale_load_down(NICE_0_LOAD) *\n\t\t\t\t(sd->imbalance_pct-100) / 100;\n\n\tif (sd_flag & SD_BALANCE_WAKE)\n\t\tload_idx = sd->wake_idx;\n\n\tdo {\n\t\tunsigned long load, avg_load, runnable_load;\n\t\tunsigned long spare_cap, max_spare_cap;\n\t\tint local_group;\n\t\tint i;\n\n\t\t/* Skip over this group if it has no CPUs allowed */\n\t\tif (!cpumask_intersects(sched_group_span(group),\n\t\t\t\t\t&p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tlocal_group = cpumask_test_cpu(this_cpu,\n\t\t\t\t\t       sched_group_span(group));\n\n\t\t/*\n\t\t * Tally up the load of all CPUs in the group and find\n\t\t * the group containing the CPU with most spare capacity.\n\t\t */\n\t\tavg_load = 0;\n\t\trunnable_load = 0;\n\t\tmax_spare_cap = 0;\n\n\t\tfor_each_cpu(i, sched_group_span(group)) {\n\t\t\t/* Bias balancing toward CPUs of our domain */\n\t\t\tif (local_group)\n\t\t\t\tload = source_load(i, load_idx);\n\t\t\telse\n\t\t\t\tload = target_load(i, load_idx);\n\n\t\t\trunnable_load += load;\n\n\t\t\tavg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);\n\n\t\t\tspare_cap = capacity_spare_wake(i, p);\n\n\t\t\tif (spare_cap > max_spare_cap)\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t}\n\n\t\t/* Adjust by relative CPU capacity of the group */\n\t\tavg_load = (avg_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\t\trunnable_load = (runnable_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\n\t\tif (local_group) {\n\t\t\tthis_runnable_load = runnable_load;\n\t\t\tthis_avg_load = avg_load;\n\t\t\tthis_spare = max_spare_cap;\n\t\t} else {\n\t\t\tif (min_runnable_load > (runnable_load + imbalance)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable load is significantly smaller\n\t\t\t\t * so we can pick this new CPU:\n\t\t\t\t */\n\t\t\t\tmin_runnable_load = runnable_load;\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t} else if ((runnable_load < (min_runnable_load + imbalance)) &&\n\t\t\t\t   (100*min_avg_load > imbalance_scale*avg_load)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable loads are close so take the\n\t\t\t\t * blocked load into account through avg_load:\n\t\t\t\t */\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t}\n\n\t\t\tif (most_spare < max_spare_cap) {\n\t\t\t\tmost_spare = max_spare_cap;\n\t\t\t\tmost_spare_sg = group;\n\t\t\t}\n\t\t}\n\t} while (group = group->next, group != sd->groups);\n\n\t/*\n\t * The cross-over point between using spare capacity or least load\n\t * is too conservative for high utilization tasks on partially\n\t * utilized systems if we require spare_capacity > task_util(p),\n\t * so we allow for some task stuffing by using\n\t * spare_capacity > task_util(p)/2.\n\t *\n\t * Spare capacity can't be used for fork because the utilization has\n\t * not been set yet, we must first select a rq to compute the initial\n\t * utilization.\n\t */\n\tif (sd_flag & SD_BALANCE_FORK)\n\t\tgoto skip_spare;\n\n\tif (this_spare > task_util(p) / 2 &&\n\t    imbalance_scale*this_spare > 100*most_spare)\n\t\treturn NULL;\n\n\tif (most_spare > task_util(p) / 2)\n\t\treturn most_spare_sg;\n\nskip_spare:\n\tif (!idlest)\n\t\treturn NULL;\n\n\t/*\n\t * When comparing groups across NUMA domains, it's possible for the\n\t * local domain to be very lightly loaded relative to the remote\n\t * domains but \"imbalance\" skews the comparison making remote CPUs\n\t * look much more favourable. When considering cross-domain, add\n\t * imbalance to the runnable load on the remote node and consider\n\t * staying local.\n\t */\n\tif ((sd->flags & SD_NUMA) &&\n\t    min_runnable_load + imbalance >= this_runnable_load)\n\t\treturn NULL;\n\n\tif (min_runnable_load > (this_runnable_load + imbalance))\n\t\treturn NULL;\n\n\tif ((this_runnable_load < (min_runnable_load + imbalance)) &&\n\t     (100*this_avg_load < imbalance_scale*min_avg_load))\n\t\treturn NULL;\n\n\treturn idlest;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_util",
          "args": [
            "p"
          ],
          "line": 5801
        },
        "resolved": true,
        "details": {
          "function_name": "task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3610-3613",
          "snippet": "static inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}"
        }
      },
      {
        "call_info": {
          "callee": "capacity_spare_wake",
          "args": [
            "i",
            "p"
          ],
          "line": 5741
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_spare_wake",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5679-5682",
          "snippet": "static unsigned long capacity_spare_wake(int cpu, struct task_struct *p)\n{\n\treturn max_t(long, capacity_of(cpu) - cpu_util_wake(cpu, p), 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_spare_wake(int cpu, struct task_struct *p)\n{\n\treturn max_t(long, capacity_of(cpu) - cpu_util_wake(cpu, p), 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_load_avg",
          "args": [
            "&cpu_rq(i)->cfs"
          ],
          "line": 5739
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3591-3594",
          "snippet": "static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "i"
          ],
          "line": 5739
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "target_load",
          "args": [
            "i",
            "load_idx"
          ],
          "line": 5735
        },
        "resolved": true,
        "details": {
          "function_name": "target_load",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5498-5507",
          "snippet": "static unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}"
        }
      },
      {
        "call_info": {
          "callee": "source_load",
          "args": [
            "i",
            "load_idx"
          ],
          "line": 5733
        },
        "resolved": true,
        "details": {
          "function_name": "source_load",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5483-5492",
          "snippet": "static unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "i",
            "sched_group_span(group)"
          ],
          "line": 5730
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_group_span",
          "args": [
            "group"
          ],
          "line": 5730
        },
        "resolved": true,
        "details": {
          "function_name": "sched_group_span",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1330-1333",
          "snippet": "static inline struct cpumask *sched_group_span(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->cpumask);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nstatic inline struct cpumask *sched_group_span(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->cpumask);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "this_cpu",
            "sched_group_span(group)"
          ],
          "line": 5719
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_intersects",
          "args": [
            "sched_group_span(group)",
            "&p->cpus_allowed"
          ],
          "line": 5715
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "NICE_0_LOAD"
          ],
          "line": 5702
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p,\n\t\t  int this_cpu, int sd_flag)\n{\n\tstruct sched_group *idlest = NULL, *group = sd->groups;\n\tstruct sched_group *most_spare_sg = NULL;\n\tunsigned long min_runnable_load = ULONG_MAX;\n\tunsigned long this_runnable_load = ULONG_MAX;\n\tunsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX;\n\tunsigned long most_spare = 0, this_spare = 0;\n\tint load_idx = sd->forkexec_idx;\n\tint imbalance_scale = 100 + (sd->imbalance_pct-100)/2;\n\tunsigned long imbalance = scale_load_down(NICE_0_LOAD) *\n\t\t\t\t(sd->imbalance_pct-100) / 100;\n\n\tif (sd_flag & SD_BALANCE_WAKE)\n\t\tload_idx = sd->wake_idx;\n\n\tdo {\n\t\tunsigned long load, avg_load, runnable_load;\n\t\tunsigned long spare_cap, max_spare_cap;\n\t\tint local_group;\n\t\tint i;\n\n\t\t/* Skip over this group if it has no CPUs allowed */\n\t\tif (!cpumask_intersects(sched_group_span(group),\n\t\t\t\t\t&p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tlocal_group = cpumask_test_cpu(this_cpu,\n\t\t\t\t\t       sched_group_span(group));\n\n\t\t/*\n\t\t * Tally up the load of all CPUs in the group and find\n\t\t * the group containing the CPU with most spare capacity.\n\t\t */\n\t\tavg_load = 0;\n\t\trunnable_load = 0;\n\t\tmax_spare_cap = 0;\n\n\t\tfor_each_cpu(i, sched_group_span(group)) {\n\t\t\t/* Bias balancing toward CPUs of our domain */\n\t\t\tif (local_group)\n\t\t\t\tload = source_load(i, load_idx);\n\t\t\telse\n\t\t\t\tload = target_load(i, load_idx);\n\n\t\t\trunnable_load += load;\n\n\t\t\tavg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);\n\n\t\t\tspare_cap = capacity_spare_wake(i, p);\n\n\t\t\tif (spare_cap > max_spare_cap)\n\t\t\t\tmax_spare_cap = spare_cap;\n\t\t}\n\n\t\t/* Adjust by relative CPU capacity of the group */\n\t\tavg_load = (avg_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\t\trunnable_load = (runnable_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t\tgroup->sgc->capacity;\n\n\t\tif (local_group) {\n\t\t\tthis_runnable_load = runnable_load;\n\t\t\tthis_avg_load = avg_load;\n\t\t\tthis_spare = max_spare_cap;\n\t\t} else {\n\t\t\tif (min_runnable_load > (runnable_load + imbalance)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable load is significantly smaller\n\t\t\t\t * so we can pick this new CPU:\n\t\t\t\t */\n\t\t\t\tmin_runnable_load = runnable_load;\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t} else if ((runnable_load < (min_runnable_load + imbalance)) &&\n\t\t\t\t   (100*min_avg_load > imbalance_scale*avg_load)) {\n\t\t\t\t/*\n\t\t\t\t * The runnable loads are close so take the\n\t\t\t\t * blocked load into account through avg_load:\n\t\t\t\t */\n\t\t\t\tmin_avg_load = avg_load;\n\t\t\t\tidlest = group;\n\t\t\t}\n\n\t\t\tif (most_spare < max_spare_cap) {\n\t\t\t\tmost_spare = max_spare_cap;\n\t\t\t\tmost_spare_sg = group;\n\t\t\t}\n\t\t}\n\t} while (group = group->next, group != sd->groups);\n\n\t/*\n\t * The cross-over point between using spare capacity or least load\n\t * is too conservative for high utilization tasks on partially\n\t * utilized systems if we require spare_capacity > task_util(p),\n\t * so we allow for some task stuffing by using\n\t * spare_capacity > task_util(p)/2.\n\t *\n\t * Spare capacity can't be used for fork because the utilization has\n\t * not been set yet, we must first select a rq to compute the initial\n\t * utilization.\n\t */\n\tif (sd_flag & SD_BALANCE_FORK)\n\t\tgoto skip_spare;\n\n\tif (this_spare > task_util(p) / 2 &&\n\t    imbalance_scale*this_spare > 100*most_spare)\n\t\treturn NULL;\n\n\tif (most_spare > task_util(p) / 2)\n\t\treturn most_spare_sg;\n\nskip_spare:\n\tif (!idlest)\n\t\treturn NULL;\n\n\t/*\n\t * When comparing groups across NUMA domains, it's possible for the\n\t * local domain to be very lightly loaded relative to the remote\n\t * domains but \"imbalance\" skews the comparison making remote CPUs\n\t * look much more favourable. When considering cross-domain, add\n\t * imbalance to the runnable load on the remote node and consider\n\t * staying local.\n\t */\n\tif ((sd->flags & SD_NUMA) &&\n\t    min_runnable_load + imbalance >= this_runnable_load)\n\t\treturn NULL;\n\n\tif (min_runnable_load > (this_runnable_load + imbalance))\n\t\treturn NULL;\n\n\tif ((this_runnable_load < (min_runnable_load + imbalance)) &&\n\t     (100*this_avg_load < imbalance_scale*min_avg_load))\n\t\treturn NULL;\n\n\treturn idlest;\n}"
  },
  {
    "function_name": "capacity_spare_wake",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5679-5682",
    "snippet": "static unsigned long capacity_spare_wake(int cpu, struct task_struct *p)\n{\n\treturn max_t(long, capacity_of(cpu) - cpu_util_wake(cpu, p), 0);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "max_t",
          "args": [
            "long",
            "capacity_of(cpu) - cpu_util_wake(cpu, p)",
            "0"
          ],
          "line": 5681
        },
        "resolved": true,
        "details": {
          "function_name": "update_max_tr_single",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace.c",
          "lines": "1397-1431",
          "snippet": "void\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}",
          "includes": [
            "#include \"trace_selftest.c\"",
            "#include \"trace_output.h\"",
            "#include \"trace.h\"",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/trace.h>",
            "#include <linux/fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/poll.h>",
            "#include <linux/init.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/mount.h>",
            "#include <linux/string.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/splice.h>",
            "#include <linux/percpu.h>",
            "#include <linux/module.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/linkage.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/notifier.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/writeback.h>",
            "#include <linux/stacktrace.h>",
            "#include <generated/utsrelease.h>",
            "#include <linux/ring_buffer.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_selftest.c\"\n#include \"trace_output.h\"\n#include \"trace.h\"\n#include <linux/sched/rt.h>\n#include <linux/sched/clock.h>\n#include <linux/trace.h>\n#include <linux/fs.h>\n#include <linux/nmi.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/rwsem.h>\n#include <linux/mount.h>\n#include <linux/string.h>\n#include <linux/kdebug.h>\n#include <linux/splice.h>\n#include <linux/percpu.h>\n#include <linux/module.h>\n#include <linux/ftrace.h>\n#include <linux/vmalloc.h>\n#include <linux/uaccess.h>\n#include <linux/linkage.h>\n#include <linux/hardirq.h>\n#include <linux/pagemap.h>\n#include <linux/tracefs.h>\n#include <linux/debugfs.h>\n#include <linux/irqflags.h>\n#include <linux/notifier.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/writeback.h>\n#include <linux/stacktrace.h>\n#include <generated/utsrelease.h>\n#include <linux/ring_buffer.h>\n\nstatic __always_inline struct;\n\nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_util_wake",
          "args": [
            "cpu",
            "p"
          ],
          "line": 5681
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_util_wake",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6222-6272",
          "snippet": "static unsigned long cpu_util_wake(int cpu, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\t/* Task has no contribution or is new */\n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_util(cpu);\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/* Discount task's blocked util from CPU's util */\n\tutil -= min_t(unsigned int, util, task_util(p));\n\n\t/*\n\t * Covered cases:\n\t *\n\t * a) if *p is the only task sleeping on this CPU, then:\n\t *      cpu_util (== task_util) > util_est (== 0)\n\t *    and thus we return:\n\t *      cpu_util_wake = (cpu_util - task_util) = 0\n\t *\n\t * b) if other tasks are SLEEPING on this CPU, which is now exiting\n\t *    IDLE, then:\n\t *      cpu_util >= task_util\n\t *      cpu_util > util_est (== 0)\n\t *    and thus we discount *p's blocked utilization to return:\n\t *      cpu_util_wake = (cpu_util - task_util) >= 0\n\t *\n\t * c) if other tasks are RUNNABLE on that CPU and\n\t *      util_est > cpu_util\n\t *    then we use util_est since it returns a more restrictive\n\t *    estimation of the spare capacity on that CPU, by just\n\t *    considering the expected utilization of tasks already\n\t *    runnable on that CPU.\n\t *\n\t * Cases a) and b) are covered by the above code, while case c) is\n\t * covered by the following code when estimated utilization is\n\t * enabled.\n\t */\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\t/*\n\t * Utilization (estimated) can exceed the CPU capacity, thus let's\n\t * clamp to the maximum CPU capacity to ensure consistency with\n\t * the cpu_util call.\n\t */\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic unsigned long cpu_util_wake(int cpu, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int util;\n\n\t/* Task has no contribution or is new */\n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_util(cpu);\n\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n\tutil = READ_ONCE(cfs_rq->avg.util_avg);\n\n\t/* Discount task's blocked util from CPU's util */\n\tutil -= min_t(unsigned int, util, task_util(p));\n\n\t/*\n\t * Covered cases:\n\t *\n\t * a) if *p is the only task sleeping on this CPU, then:\n\t *      cpu_util (== task_util) > util_est (== 0)\n\t *    and thus we return:\n\t *      cpu_util_wake = (cpu_util - task_util) = 0\n\t *\n\t * b) if other tasks are SLEEPING on this CPU, which is now exiting\n\t *    IDLE, then:\n\t *      cpu_util >= task_util\n\t *      cpu_util > util_est (== 0)\n\t *    and thus we discount *p's blocked utilization to return:\n\t *      cpu_util_wake = (cpu_util - task_util) >= 0\n\t *\n\t * c) if other tasks are RUNNABLE on that CPU and\n\t *      util_est > cpu_util\n\t *    then we use util_est since it returns a more restrictive\n\t *    estimation of the spare capacity on that CPU, by just\n\t *    considering the expected utilization of tasks already\n\t *    runnable on that CPU.\n\t *\n\t * Cases a) and b) are covered by the above code, while case c) is\n\t * covered by the following code when estimated utilization is\n\t * enabled.\n\t */\n\tif (sched_feat(UTIL_EST))\n\t\tutil = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));\n\n\t/*\n\t * Utilization (estimated) can exceed the CPU capacity, thus let's\n\t * clamp to the maximum CPU capacity to ensure consistency with\n\t * the cpu_util call.\n\t */\n\treturn min_t(unsigned long, util, capacity_orig_of(cpu));\n}"
        }
      },
      {
        "call_info": {
          "callee": "capacity_of",
          "args": [
            "cpu"
          ],
          "line": 5681
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5509-5512",
          "snippet": "static unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_spare_wake(int cpu, struct task_struct *p)\n{\n\treturn max_t(long, capacity_of(cpu) - cpu_util_wake(cpu, p), 0);\n}"
  },
  {
    "function_name": "wake_affine",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5657-5675",
    "snippet": "static int wake_affine(struct sched_domain *sd, struct task_struct *p,\n\t\t       int this_cpu, int prev_cpu, int sync)\n{\n\tint target = nr_cpumask_bits;\n\n\tif (sched_feat(WA_IDLE))\n\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);\n\n\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)\n\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);\n\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine_attempts);\n\tif (target == nr_cpumask_bits)\n\t\treturn prev_cpu;\n\n\tschedstat_inc(sd->ttwu_move_affine);\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine);\n\treturn target;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "p->se.statistics.nr_wakeups_affine"
          ],
          "line": 5673
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "sd->ttwu_move_affine"
          ],
          "line": 5672
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "p->se.statistics.nr_wakeups_affine_attempts"
          ],
          "line": 5668
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_affine_weight",
          "args": [
            "sd",
            "p",
            "this_cpu",
            "prev_cpu",
            "sync"
          ],
          "line": 5666
        },
        "resolved": true,
        "details": {
          "function_name": "wake_affine_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5614-5655",
          "snippet": "static int\nwake_affine_weight(struct sched_domain *sd, struct task_struct *p,\n\t\t   int this_cpu, int prev_cpu, int sync)\n{\n\ts64 this_eff_load, prev_eff_load;\n\tunsigned long task_load;\n\n\tthis_eff_load = target_load(this_cpu, sd->wake_idx);\n\n\tif (sync) {\n\t\tunsigned long current_load = task_h_load(current);\n\n\t\tif (current_load > this_eff_load)\n\t\t\treturn this_cpu;\n\n\t\tthis_eff_load -= current_load;\n\t}\n\n\ttask_load = task_h_load(p);\n\n\tthis_eff_load += task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tthis_eff_load *= 100;\n\tthis_eff_load *= capacity_of(prev_cpu);\n\n\tprev_eff_load = source_load(prev_cpu, sd->wake_idx);\n\tprev_eff_load -= task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;\n\tprev_eff_load *= capacity_of(this_cpu);\n\n\t/*\n\t * If sync, adjust the weight of prev_eff_load such that if\n\t * prev_eff == this_eff that select_idle_sibling() will consider\n\t * stacking the wakee on top of the waker if no other CPU is\n\t * idle.\n\t */\n\tif (sync)\n\t\tprev_eff_load += 1;\n\n\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwake_affine_weight(struct sched_domain *sd, struct task_struct *p,\n\t\t   int this_cpu, int prev_cpu, int sync)\n{\n\ts64 this_eff_load, prev_eff_load;\n\tunsigned long task_load;\n\n\tthis_eff_load = target_load(this_cpu, sd->wake_idx);\n\n\tif (sync) {\n\t\tunsigned long current_load = task_h_load(current);\n\n\t\tif (current_load > this_eff_load)\n\t\t\treturn this_cpu;\n\n\t\tthis_eff_load -= current_load;\n\t}\n\n\ttask_load = task_h_load(p);\n\n\tthis_eff_load += task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tthis_eff_load *= 100;\n\tthis_eff_load *= capacity_of(prev_cpu);\n\n\tprev_eff_load = source_load(prev_cpu, sd->wake_idx);\n\tprev_eff_load -= task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;\n\tprev_eff_load *= capacity_of(this_cpu);\n\n\t/*\n\t * If sync, adjust the weight of prev_eff_load such that if\n\t * prev_eff == this_eff that select_idle_sibling() will consider\n\t * stacking the wakee on top of the waker if no other CPU is\n\t * idle.\n\t */\n\tif (sync)\n\t\tprev_eff_load += 1;\n\n\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "WA_WEIGHT"
          ],
          "line": 5665
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_affine_idle",
          "args": [
            "this_cpu",
            "prev_cpu",
            "sync"
          ],
          "line": 5663
        },
        "resolved": true,
        "details": {
          "function_name": "wake_affine_idle",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5590-5612",
          "snippet": "static int\nwake_affine_idle(int this_cpu, int prev_cpu, int sync)\n{\n\t/*\n\t * If this_cpu is idle, it implies the wakeup is from interrupt\n\t * context. Only allow the move if cache is shared. Otherwise an\n\t * interrupt intensive workload could force all tasks onto one\n\t * node depending on the IO topology or IRQ affinity settings.\n\t *\n\t * If the prev_cpu is idle and cache affine then avoid a migration.\n\t * There is no guarantee that the cache hot data from an interrupt\n\t * is more important than cache hot data on the prev_cpu and from\n\t * a cpufreq perspective, it's better to have higher utilisation\n\t * on one CPU.\n\t */\n\tif (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))\n\t\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;\n\n\tif (sync && cpu_rq(this_cpu)->nr_running == 1)\n\t\treturn this_cpu;\n\n\treturn nr_cpumask_bits;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwake_affine_idle(int this_cpu, int prev_cpu, int sync)\n{\n\t/*\n\t * If this_cpu is idle, it implies the wakeup is from interrupt\n\t * context. Only allow the move if cache is shared. Otherwise an\n\t * interrupt intensive workload could force all tasks onto one\n\t * node depending on the IO topology or IRQ affinity settings.\n\t *\n\t * If the prev_cpu is idle and cache affine then avoid a migration.\n\t * There is no guarantee that the cache hot data from an interrupt\n\t * is more important than cache hot data on the prev_cpu and from\n\t * a cpufreq perspective, it's better to have higher utilisation\n\t * on one CPU.\n\t */\n\tif (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))\n\t\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;\n\n\tif (sync && cpu_rq(this_cpu)->nr_running == 1)\n\t\treturn this_cpu;\n\n\treturn nr_cpumask_bits;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic int wake_affine(struct sched_domain *sd, struct task_struct *p,\n\t\t       int this_cpu, int prev_cpu, int sync)\n{\n\tint target = nr_cpumask_bits;\n\n\tif (sched_feat(WA_IDLE))\n\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);\n\n\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)\n\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);\n\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine_attempts);\n\tif (target == nr_cpumask_bits)\n\t\treturn prev_cpu;\n\n\tschedstat_inc(sd->ttwu_move_affine);\n\tschedstat_inc(p->se.statistics.nr_wakeups_affine);\n\treturn target;\n}"
  },
  {
    "function_name": "wake_affine_weight",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5614-5655",
    "snippet": "static int\nwake_affine_weight(struct sched_domain *sd, struct task_struct *p,\n\t\t   int this_cpu, int prev_cpu, int sync)\n{\n\ts64 this_eff_load, prev_eff_load;\n\tunsigned long task_load;\n\n\tthis_eff_load = target_load(this_cpu, sd->wake_idx);\n\n\tif (sync) {\n\t\tunsigned long current_load = task_h_load(current);\n\n\t\tif (current_load > this_eff_load)\n\t\t\treturn this_cpu;\n\n\t\tthis_eff_load -= current_load;\n\t}\n\n\ttask_load = task_h_load(p);\n\n\tthis_eff_load += task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tthis_eff_load *= 100;\n\tthis_eff_load *= capacity_of(prev_cpu);\n\n\tprev_eff_load = source_load(prev_cpu, sd->wake_idx);\n\tprev_eff_load -= task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;\n\tprev_eff_load *= capacity_of(this_cpu);\n\n\t/*\n\t * If sync, adjust the weight of prev_eff_load such that if\n\t * prev_eff == this_eff that select_idle_sibling() will consider\n\t * stacking the wakee on top of the waker if no other CPU is\n\t * idle.\n\t */\n\tif (sync)\n\t\tprev_eff_load += 1;\n\n\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "capacity_of",
          "args": [
            "this_cpu"
          ],
          "line": 5643
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5509-5512",
          "snippet": "static unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "WA_BIAS"
          ],
          "line": 5641
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "source_load",
          "args": [
            "prev_cpu",
            "sd->wake_idx"
          ],
          "line": 5639
        },
        "resolved": true,
        "details": {
          "function_name": "source_load",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5483-5492",
          "snippet": "static unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_h_load",
          "args": [
            "p"
          ],
          "line": 5632
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_h_load",
          "args": [
            "current"
          ],
          "line": 5624
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "target_load",
          "args": [
            "this_cpu",
            "sd->wake_idx"
          ],
          "line": 5621
        },
        "resolved": true,
        "details": {
          "function_name": "target_load",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5498-5507",
          "snippet": "static unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwake_affine_weight(struct sched_domain *sd, struct task_struct *p,\n\t\t   int this_cpu, int prev_cpu, int sync)\n{\n\ts64 this_eff_load, prev_eff_load;\n\tunsigned long task_load;\n\n\tthis_eff_load = target_load(this_cpu, sd->wake_idx);\n\n\tif (sync) {\n\t\tunsigned long current_load = task_h_load(current);\n\n\t\tif (current_load > this_eff_load)\n\t\t\treturn this_cpu;\n\n\t\tthis_eff_load -= current_load;\n\t}\n\n\ttask_load = task_h_load(p);\n\n\tthis_eff_load += task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tthis_eff_load *= 100;\n\tthis_eff_load *= capacity_of(prev_cpu);\n\n\tprev_eff_load = source_load(prev_cpu, sd->wake_idx);\n\tprev_eff_load -= task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;\n\tprev_eff_load *= capacity_of(this_cpu);\n\n\t/*\n\t * If sync, adjust the weight of prev_eff_load such that if\n\t * prev_eff == this_eff that select_idle_sibling() will consider\n\t * stacking the wakee on top of the waker if no other CPU is\n\t * idle.\n\t */\n\tif (sync)\n\t\tprev_eff_load += 1;\n\n\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;\n}"
  },
  {
    "function_name": "wake_affine_idle",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5590-5612",
    "snippet": "static int\nwake_affine_idle(int this_cpu, int prev_cpu, int sync)\n{\n\t/*\n\t * If this_cpu is idle, it implies the wakeup is from interrupt\n\t * context. Only allow the move if cache is shared. Otherwise an\n\t * interrupt intensive workload could force all tasks onto one\n\t * node depending on the IO topology or IRQ affinity settings.\n\t *\n\t * If the prev_cpu is idle and cache affine then avoid a migration.\n\t * There is no guarantee that the cache hot data from an interrupt\n\t * is more important than cache hot data on the prev_cpu and from\n\t * a cpufreq perspective, it's better to have higher utilisation\n\t * on one CPU.\n\t */\n\tif (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))\n\t\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;\n\n\tif (sync && cpu_rq(this_cpu)->nr_running == 1)\n\t\treturn this_cpu;\n\n\treturn nr_cpumask_bits;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "this_cpu"
          ],
          "line": 5608
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "available_idle_cpu",
          "args": [
            "prev_cpu"
          ],
          "line": 5606
        },
        "resolved": true,
        "details": {
          "function_name": "available_idle_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4012-4021",
          "snippet": "int available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpus_share_cache",
          "args": [
            "this_cpu",
            "prev_cpu"
          ],
          "line": 5605
        },
        "resolved": true,
        "details": {
          "function_name": "cpus_share_cache",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "1826-1829",
          "snippet": "bool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\treturn per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nbool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\treturn per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwake_affine_idle(int this_cpu, int prev_cpu, int sync)\n{\n\t/*\n\t * If this_cpu is idle, it implies the wakeup is from interrupt\n\t * context. Only allow the move if cache is shared. Otherwise an\n\t * interrupt intensive workload could force all tasks onto one\n\t * node depending on the IO topology or IRQ affinity settings.\n\t *\n\t * If the prev_cpu is idle and cache affine then avoid a migration.\n\t * There is no guarantee that the cache hot data from an interrupt\n\t * is more important than cache hot data on the prev_cpu and from\n\t * a cpufreq perspective, it's better to have higher utilisation\n\t * on one CPU.\n\t */\n\tif (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))\n\t\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;\n\n\tif (sync && cpu_rq(this_cpu)->nr_running == 1)\n\t\treturn this_cpu;\n\n\treturn nr_cpumask_bits;\n}"
  },
  {
    "function_name": "wake_wide",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5565-5576",
    "snippet": "static int wake_wide(struct task_struct *p)\n{\n\tunsigned int master = current->wakee_flips;\n\tunsigned int slave = p->wakee_flips;\n\tint factor = this_cpu_read(sd_llc_size);\n\n\tif (master < slave)\n\t\tswap(master, slave);\n\tif (slave < factor || master < slave * factor)\n\t\treturn 0;\n\treturn 1;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "swap",
          "args": [
            "master",
            "slave"
          ],
          "line": 5572
        },
        "resolved": true,
        "details": {
          "function_name": "__migrate_swap_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "1185-1215",
          "snippet": "static void __migrate_swap_task(struct task_struct *p, int cpu)\n{\n\tif (task_on_rq_queued(p)) {\n\t\tstruct rq *src_rq, *dst_rq;\n\t\tstruct rq_flags srf, drf;\n\n\t\tsrc_rq = task_rq(p);\n\t\tdst_rq = cpu_rq(cpu);\n\n\t\trq_pin_lock(src_rq, &srf);\n\t\trq_pin_lock(dst_rq, &drf);\n\n\t\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\t\tdeactivate_task(src_rq, p, 0);\n\t\tset_task_cpu(p, cpu);\n\t\tactivate_task(dst_rq, p, 0);\n\t\tp->on_rq = TASK_ON_RQ_QUEUED;\n\t\tcheck_preempt_curr(dst_rq, p, 0);\n\n\t\trq_unpin_lock(dst_rq, &drf);\n\t\trq_unpin_lock(src_rq, &srf);\n\n\t} else {\n\t\t/*\n\t\t * Task isn't running anymore; make it appear like we migrated\n\t\t * it before it went to sleep. This means on wakeup we make the\n\t\t * previous CPU our target instead of where it really is.\n\t\t */\n\t\tp->wake_cpu = cpu;\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nstatic void __migrate_swap_task(struct task_struct *p, int cpu)\n{\n\tif (task_on_rq_queued(p)) {\n\t\tstruct rq *src_rq, *dst_rq;\n\t\tstruct rq_flags srf, drf;\n\n\t\tsrc_rq = task_rq(p);\n\t\tdst_rq = cpu_rq(cpu);\n\n\t\trq_pin_lock(src_rq, &srf);\n\t\trq_pin_lock(dst_rq, &drf);\n\n\t\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\t\tdeactivate_task(src_rq, p, 0);\n\t\tset_task_cpu(p, cpu);\n\t\tactivate_task(dst_rq, p, 0);\n\t\tp->on_rq = TASK_ON_RQ_QUEUED;\n\t\tcheck_preempt_curr(dst_rq, p, 0);\n\n\t\trq_unpin_lock(dst_rq, &drf);\n\t\trq_unpin_lock(src_rq, &srf);\n\n\t} else {\n\t\t/*\n\t\t * Task isn't running anymore; make it appear like we migrated\n\t\t * it before it went to sleep. This means on wakeup we make the\n\t\t * previous CPU our target instead of where it really is.\n\t\t */\n\t\tp->wake_cpu = cpu;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_cpu_read",
          "args": [
            "sd_llc_size"
          ],
          "line": 5569
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int wake_wide(struct task_struct *p)\n{\n\tunsigned int master = current->wakee_flips;\n\tunsigned int slave = p->wakee_flips;\n\tint factor = this_cpu_read(sd_llc_size);\n\n\tif (master < slave)\n\t\tswap(master, slave);\n\tif (slave < factor || master < slave * factor)\n\t\treturn 0;\n\treturn 1;\n}"
  },
  {
    "function_name": "record_wakee",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5531-5546",
    "snippet": "static void record_wakee(struct task_struct *p)\n{\n\t/*\n\t * Only decay a single time; tasks that have less then 1 wakeup per\n\t * jiffy will not have built up many flips.\n\t */\n\tif (time_after(jiffies, current->wakee_flip_decay_ts + HZ)) {\n\t\tcurrent->wakee_flips >>= 1;\n\t\tcurrent->wakee_flip_decay_ts = jiffies;\n\t}\n\n\tif (current->last_wakee != p) {\n\t\tcurrent->last_wakee = p;\n\t\tcurrent->wakee_flips++;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "jiffies",
            "current->wakee_flip_decay_ts + HZ"
          ],
          "line": 5537
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void record_wakee(struct task_struct *p)\n{\n\t/*\n\t * Only decay a single time; tasks that have less then 1 wakeup per\n\t * jiffy will not have built up many flips.\n\t */\n\tif (time_after(jiffies, current->wakee_flip_decay_ts + HZ)) {\n\t\tcurrent->wakee_flips >>= 1;\n\t\tcurrent->wakee_flip_decay_ts = jiffies;\n\t}\n\n\tif (current->last_wakee != p) {\n\t\tcurrent->last_wakee = p;\n\t\tcurrent->wakee_flips++;\n\t}\n}"
  },
  {
    "function_name": "cpu_avg_load_per_task",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5519-5529",
    "snippet": "static unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = READ_ONCE(rq->cfs.h_nr_running);\n\tunsigned long load_avg = weighted_cpuload(rq);\n\n\tif (nr_running)\n\t\treturn load_avg / nr_running;\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "rq"
          ],
          "line": 5523
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "rq->cfs.h_nr_running"
          ],
          "line": 5522
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 5521
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = READ_ONCE(rq->cfs.h_nr_running);\n\tunsigned long load_avg = weighted_cpuload(rq);\n\n\tif (nr_running)\n\t\treturn load_avg / nr_running;\n\n\treturn 0;\n}"
  },
  {
    "function_name": "capacity_orig_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5514-5517",
    "snippet": "static unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 5516
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}"
  },
  {
    "function_name": "capacity_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5509-5512",
    "snippet": "static unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 5511
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}"
  },
  {
    "function_name": "target_load",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5498-5507",
    "snippet": "static unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "max",
          "args": [
            "rq->cpu_load[type-1]",
            "total"
          ],
          "line": 5506
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "LB_BIAS"
          ],
          "line": 5503
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "rq"
          ],
          "line": 5501
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 5500
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}"
  },
  {
    "function_name": "source_load",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5483-5492",
    "snippet": "static unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "min",
          "args": [
            "rq->cpu_load[type-1]",
            "total"
          ],
          "line": 5491
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "LB_BIAS"
          ],
          "line": 5488
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "rq"
          ],
          "line": 5486
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 5485
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(rq);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}"
  },
  {
    "function_name": "cpu_load_update_active",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5466-5474",
    "snippet": "void cpu_load_update_active(struct rq *this_rq)\n{\n\tunsigned long load = weighted_cpuload(this_rq);\n\n\tif (tick_nohz_tick_stopped())\n\t\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), load);\n\telse\n\t\tcpu_load_update_periodic(this_rq, load);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_load_update_periodic",
          "args": [
            "this_rq",
            "load"
          ],
          "line": 5473
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_load_update_periodic",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5454-5461",
          "snippet": "static void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\t/* See the mess around cpu_load_update_nohz(). */\n\tthis_rq->last_load_update_tick = READ_ONCE(jiffies);\n#endif\n\tcpu_load_update(this_rq, load, 1);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\t/* See the mess around cpu_load_update_nohz(). */\n\tthis_rq->last_load_update_tick = READ_ONCE(jiffies);\n#endif\n\tcpu_load_update(this_rq, load, 1);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_load_update_nohz",
          "args": [
            "this_rq",
            "READ_ONCE(jiffies)",
            "load"
          ],
          "line": 5471
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_load_update_nohz",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5449-5451",
          "snippet": "static inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "jiffies"
          ],
          "line": 5471
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "tick_nohz_tick_stopped",
          "args": [],
          "line": 5470
        },
        "resolved": true,
        "details": {
          "function_name": "tick_nohz_tick_stopped",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/tick-sched.c",
          "lines": "468-473",
          "snippet": "bool tick_nohz_tick_stopped(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\treturn ts->tick_stopped;\n}",
          "includes": [
            "#include <trace/events/timer.h>",
            "#include \"tick-internal.h\"",
            "#include <asm/irq_regs.h>",
            "#include <linux/mm.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/module.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/profile.h>",
            "#include <linux/nmi.h>",
            "#include <linux/percpu.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/err.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU(struct tick_sched, tick_cpu_sched);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/timer.h>\n#include \"tick-internal.h\"\n#include <asm/irq_regs.h>\n#include <linux/mm.h>\n#include <linux/context_tracking.h>\n#include <linux/posix-timers.h>\n#include <linux/irq_work.h>\n#include <linux/module.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/signal.h>\n#include <linux/profile.h>\n#include <linux/nmi.h>\n#include <linux/percpu.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/hrtimer.h>\n#include <linux/err.h>\n#include <linux/cpu.h>\n\nstatic DEFINE_PER_CPU(struct tick_sched, tick_cpu_sched);\n\nbool tick_nohz_tick_stopped(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\treturn ts->tick_stopped;\n}"
        }
      },
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "this_rq"
          ],
          "line": 5468
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid cpu_load_update_active(struct rq *this_rq)\n{\n\tunsigned long load = weighted_cpuload(this_rq);\n\n\tif (tick_nohz_tick_stopped())\n\t\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), load);\n\telse\n\t\tcpu_load_update_periodic(this_rq, load);\n}"
  },
  {
    "function_name": "cpu_load_update_periodic",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5454-5461",
    "snippet": "static void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\t/* See the mess around cpu_load_update_nohz(). */\n\tthis_rq->last_load_update_tick = READ_ONCE(jiffies);\n#endif\n\tcpu_load_update(this_rq, load, 1);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_load_update",
          "args": [
            "this_rq",
            "load",
            "1"
          ],
          "line": 5460
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_load_update_nohz",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5449-5451",
          "snippet": "static inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "jiffies"
          ],
          "line": 5458
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\t/* See the mess around cpu_load_update_nohz(). */\n\tthis_rq->last_load_update_tick = READ_ONCE(jiffies);\n#endif\n\tcpu_load_update(this_rq, load, 1);\n}"
  },
  {
    "function_name": "cpu_load_update_nohz",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5449-5451",
    "snippet": "static inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }"
  },
  {
    "function_name": "cpu_load_update_nohz_stop",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5432-5447",
    "snippet": "void cpu_load_update_nohz_stop(void)\n{\n\tunsigned long curr_jiffies = READ_ONCE(jiffies);\n\tstruct rq *this_rq = this_rq();\n\tunsigned long load;\n\tstruct rq_flags rf;\n\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tload = weighted_cpuload(this_rq);\n\trq_lock(this_rq, &rf);\n\tupdate_rq_clock(this_rq);\n\tcpu_load_update_nohz(this_rq, curr_jiffies, load);\n\trq_unlock(this_rq, &rf);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rq_unlock",
          "args": [
            "this_rq",
            "&rf"
          ],
          "line": 5446
        },
        "resolved": true,
        "details": {
          "function_name": "double_rq_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2053-2060",
          "snippet": "static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_load_update_nohz",
          "args": [
            "this_rq",
            "curr_jiffies",
            "load"
          ],
          "line": 5445
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_load_update_nohz",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5449-5451",
          "snippet": "static inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }"
        }
      },
      {
        "call_info": {
          "callee": "update_rq_clock",
          "args": [
            "this_rq"
          ],
          "line": 5444
        },
        "resolved": true,
        "details": {
          "function_name": "update_rq_clock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "185-205",
          "snippet": "void update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (rq->clock_update_flags & RQCF_ACT_SKIP)\n\t\treturn;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (sched_feat(WARN_DOUBLE_CLOCK))\n\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);\n\trq->clock_update_flags |= RQCF_UPDATED;\n#endif\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\tif (delta < 0)\n\t\treturn;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (rq->clock_update_flags & RQCF_ACT_SKIP)\n\t\treturn;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (sched_feat(WARN_DOUBLE_CLOCK))\n\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);\n\trq->clock_update_flags |= RQCF_UPDATED;\n#endif\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\tif (delta < 0)\n\t\treturn;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_lock",
          "args": [
            "this_rq",
            "&rf"
          ],
          "line": 5443
        },
        "resolved": true,
        "details": {
          "function_name": "task_rq_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "90-126",
          "snippet": "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, rf->flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\t/*\n\t\t *\tmove_queued_task()\t\ttask_rq_lock()\n\t\t *\n\t\t *\tACQUIRE (rq->lock)\n\t\t *\t[S] ->on_rq = MIGRATING\t\t[L] rq = task_rq()\n\t\t *\tWMB (__set_task_cpu())\t\tACQUIRE (rq->lock);\n\t\t *\t[S] ->cpu = new_cpu\t\t[L] task_rq()\n\t\t *\t\t\t\t\t[L] ->on_rq\n\t\t *\tRELEASE (rq->lock)\n\t\t *\n\t\t * If we observe the old CPU in task_rq_lock, the acquire of\n\t\t * the old rq->lock will fully serialize against the stores.\n\t\t *\n\t\t * If we observe the new CPU in task_rq_lock, the acquire will\n\t\t * pair with the WMB to ensure we must then also see migrating.\n\t\t */\n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, rf->flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\t/*\n\t\t *\tmove_queued_task()\t\ttask_rq_lock()\n\t\t *\n\t\t *\tACQUIRE (rq->lock)\n\t\t *\t[S] ->on_rq = MIGRATING\t\t[L] rq = task_rq()\n\t\t *\tWMB (__set_task_cpu())\t\tACQUIRE (rq->lock);\n\t\t *\t[S] ->cpu = new_cpu\t\t[L] task_rq()\n\t\t *\t\t\t\t\t[L] ->on_rq\n\t\t *\tRELEASE (rq->lock)\n\t\t *\n\t\t * If we observe the old CPU in task_rq_lock, the acquire of\n\t\t * the old rq->lock will fully serialize against the stores.\n\t\t *\n\t\t * If we observe the new CPU in task_rq_lock, the acquire will\n\t\t * pair with the WMB to ensure we must then also see migrating.\n\t\t */\n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "this_rq"
          ],
          "line": 5442
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_rq",
          "args": [],
          "line": 5435
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "jiffies"
          ],
          "line": 5434
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid cpu_load_update_nohz_stop(void)\n{\n\tunsigned long curr_jiffies = READ_ONCE(jiffies);\n\tstruct rq *this_rq = this_rq();\n\tunsigned long load;\n\tstruct rq_flags rf;\n\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tload = weighted_cpuload(this_rq);\n\trq_lock(this_rq, &rf);\n\tupdate_rq_clock(this_rq);\n\tcpu_load_update_nohz(this_rq, curr_jiffies, load);\n\trq_unlock(this_rq, &rf);\n}"
  },
  {
    "function_name": "cpu_load_update_nohz_start",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5417-5427",
    "snippet": "void cpu_load_update_nohz_start(void)\n{\n\tstruct rq *this_rq = this_rq();\n\n\t/*\n\t * This is all lockless but should be fine. If weighted_cpuload changes\n\t * concurrently we'll exit nohz. And cpu_load write can race with\n\t * cpu_load_update_idle() but both updater would be writing the same.\n\t */\n\tthis_rq->cpu_load[0] = weighted_cpuload(this_rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "this_rq"
          ],
          "line": 5426
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "this_rq",
          "args": [],
          "line": 5419
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid cpu_load_update_nohz_start(void)\n{\n\tstruct rq *this_rq = this_rq();\n\n\t/*\n\t * This is all lockless but should be fine. If weighted_cpuload changes\n\t * concurrently we'll exit nohz. And cpu_load write can race with\n\t * cpu_load_update_idle() but both updater would be writing the same.\n\t */\n\tthis_rq->cpu_load[0] = weighted_cpuload(this_rq);\n}"
  },
  {
    "function_name": "cpu_load_update_idle",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5400-5409",
    "snippet": "static void cpu_load_update_idle(struct rq *this_rq)\n{\n\t/*\n\t * bail if there's load or we're actually up-to-date.\n\t */\n\tif (weighted_cpuload(this_rq))\n\t\treturn;\n\n\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_load_update_nohz",
          "args": [
            "this_rq",
            "READ_ONCE(jiffies)",
            "0"
          ],
          "line": 5408
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_load_update_nohz",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5449-5451",
          "snippet": "static inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "jiffies"
          ],
          "line": 5408
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "this_rq"
          ],
          "line": 5405
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void cpu_load_update_idle(struct rq *this_rq)\n{\n\t/*\n\t * bail if there's load or we're actually up-to-date.\n\t */\n\tif (weighted_cpuload(this_rq))\n\t\treturn;\n\n\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);\n}"
  },
  {
    "function_name": "cpu_load_update_nohz",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5378-5394",
    "snippet": "static void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t unsigned long curr_jiffies,\n\t\t\t\t unsigned long load)\n{\n\tunsigned long pending_updates;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tif (pending_updates) {\n\t\tthis_rq->last_load_update_tick = curr_jiffies;\n\t\t/*\n\t\t * In the regular NOHZ case, we were idle, this means load 0.\n\t\t * In the NOHZ_FULL case, we were non-idle, we should consider\n\t\t * its weighted load.\n\t\t */\n\t\tcpu_load_update(this_rq, load, pending_updates);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpu_load_update",
          "args": [
            "this_rq",
            "load",
            "pending_updates"
          ],
          "line": 5392
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_load_update_nohz",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5449-5451",
          "snippet": "static inline void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t\tunsigned long curr_jiffies,\n\t\t\t\t\tunsigned long load) { }",
          "note": "cyclic_reference_detected"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void cpu_load_update_nohz(struct rq *this_rq,\n\t\t\t\t unsigned long curr_jiffies,\n\t\t\t\t unsigned long load)\n{\n\tunsigned long pending_updates;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tif (pending_updates) {\n\t\tthis_rq->last_load_update_tick = curr_jiffies;\n\t\t/*\n\t\t * In the regular NOHZ case, we were idle, this means load 0.\n\t\t * In the NOHZ_FULL case, we were non-idle, we should consider\n\t\t * its weighted load.\n\t\t */\n\t\tcpu_load_update(this_rq, load, pending_updates);\n\t}\n}"
  },
  {
    "function_name": "weighted_cpuload",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5358-5361",
    "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_runnable_load_avg",
          "args": [
            "&rq->cfs"
          ],
          "line": 5360
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_runnable_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3586-3589",
          "snippet": "static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.runnable_load_avg;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.runnable_load_avg;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
  },
  {
    "function_name": "cpu_load_update",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5316-5355",
    "snippet": "static void cpu_load_update(struct rq *this_rq, unsigned long this_load,\n\t\t\t    unsigned long pending_updates)\n{\n\tunsigned long __maybe_unused tickless_load = this_rq->cpu_load[0];\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n#ifdef CONFIG_NO_HZ_COMMON\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tif (tickless_load) {\n\t\t\told_load -= decay_load_missed(tickless_load, pending_updates - 1, i);\n\t\t\t/*\n\t\t\t * old_load can never be a negative value because a\n\t\t\t * decayed tickless_load cannot be greater than the\n\t\t\t * original tickless_load.\n\t\t\t */\n\t\t\told_load += tickless_load;\n\t\t}\n#endif\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "decay_load_missed",
          "args": [
            "tickless_load",
            "pending_updates - 1",
            "i"
          ],
          "line": 5335
        },
        "resolved": true,
        "details": {
          "function_name": "decay_load_missed",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5247-5269",
          "snippet": "static unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define DEGRADE_SHIFT\t\t7"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define DEGRADE_SHIFT\t\t7\n\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void cpu_load_update(struct rq *this_rq, unsigned long this_load,\n\t\t\t    unsigned long pending_updates)\n{\n\tunsigned long __maybe_unused tickless_load = this_rq->cpu_load[0];\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n#ifdef CONFIG_NO_HZ_COMMON\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tif (tickless_load) {\n\t\t\told_load -= decay_load_missed(tickless_load, pending_updates - 1, i);\n\t\t\t/*\n\t\t\t * old_load can never be a negative value because a\n\t\t\t * decayed tickless_load cannot be greater than the\n\t\t\t * original tickless_load.\n\t\t\t */\n\t\t\told_load += tickless_load;\n\t\t}\n#endif\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n}"
  },
  {
    "function_name": "decay_load_missed",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5247-5269",
    "snippet": "static unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define DEGRADE_SHIFT\t\t7"
    ],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define DEGRADE_SHIFT\t\t7\n\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}"
  },
  {
    "function_name": "dequeue_task_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5145-5196",
    "snippet": "static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\tint task_sleep = flags & DEQUEUE_SLEEP;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tdequeue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running decrement below.\n\t\t*/\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running--;\n\n\t\t/* Don't dequeue parent if it has other entities besides us */\n\t\tif (cfs_rq->load.weight) {\n\t\t\t/* Avoid re-evaluating load for this entity: */\n\t\t\tse = parent_entity(se);\n\t\t\t/*\n\t\t\t * Bias pick_next to pick a task from this cfs_rq, as\n\t\t\t * p is sleeping when it is within its sched_slice.\n\t\t\t */\n\t\t\tif (task_sleep && se && !throttled_hierarchy(cfs_rq))\n\t\t\t\tset_next_buddy(se);\n\t\t\tbreak;\n\t\t}\n\t\tflags |= DEQUEUE_SLEEP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running--;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, 1);\n\n\tutil_est_dequeue(&rq->cfs, p, task_sleep);\n\thrtick_update(rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define UPDATE_TG\t0x0",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtick_update",
          "args": [
            "rq"
          ],
          "line": 5195
        },
        "resolved": true,
        "details": {
          "function_name": "hrtick_update",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5070-5072",
          "snippet": "static inline void hrtick_update(struct rq *rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void hrtick_update(struct rq *rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "util_est_dequeue",
          "args": [
            "&rq->cfs",
            "p",
            "task_sleep"
          ],
          "line": 5194
        },
        "resolved": true,
        "details": {
          "function_name": "util_est_dequeue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3753-3755",
          "snippet": "static inline void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p,\n\t\t bool task_sleep) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p,\n\t\t bool task_sleep) {}"
        }
      },
      {
        "call_info": {
          "callee": "sub_nr_running",
          "args": [
            "rq",
            "1"
          ],
          "line": 5192
        },
        "resolved": true,
        "details": {
          "function_name": "sub_nr_running",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1809-1814",
          "snippet": "static inline void sub_nr_running(struct rq *rq, unsigned count)\n{\n\trq->nr_running -= count;\n\t/* Check if we still need preemption */\n\tsched_update_tick_dependency(rq);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void sub_nr_running(struct rq *rq, unsigned count)\n{\n\trq->nr_running -= count;\n\t/* Check if we still need preemption */\n\tsched_update_tick_dependency(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_group",
          "args": [
            "se"
          ],
          "line": 5188
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3009-3011",
          "snippet": "static inline void update_cfs_group(struct sched_entity *se)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq",
            "se",
            "UPDATE_TG"
          ],
          "line": 5187
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 5184
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 5181
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_next_buddy",
          "args": [
            "se"
          ],
          "line": 5174
        },
        "resolved": true,
        "details": {
          "function_name": "set_next_buddy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6499-6509",
          "snippet": "static void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void set_next_buddy(struct sched_entity *se)\n{\n\tif (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))\n\t\treturn;\n\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "throttled_hierarchy",
          "args": [
            "cfs_rq"
          ],
          "line": 5173
        },
        "resolved": true,
        "details": {
          "function_name": "throttled_hierarchy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4996-4999",
          "snippet": "static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "parent_entity",
          "args": [
            "se"
          ],
          "line": 5168
        },
        "resolved": true,
        "details": {
          "function_name": "parent_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "453-456",
          "snippet": "static inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "dequeue_entity",
          "args": [
            "cfs_rq",
            "se",
            "flags"
          ],
          "line": 5153
        },
        "resolved": true,
        "details": {
          "function_name": "dequeue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3958-4008",
          "snippet": "static void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Substract its load from the cfs_rq->runnable_avg.\n\t *   - Substract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define UPDATE_TG\t0x0",
            "#define UPDATE_TG\t0x1"
          ],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Substract its load from the cfs_rq->runnable_avg.\n\t *   - Substract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\tint task_sleep = flags & DEQUEUE_SLEEP;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tdequeue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running decrement below.\n\t\t*/\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running--;\n\n\t\t/* Don't dequeue parent if it has other entities besides us */\n\t\tif (cfs_rq->load.weight) {\n\t\t\t/* Avoid re-evaluating load for this entity: */\n\t\t\tse = parent_entity(se);\n\t\t\t/*\n\t\t\t * Bias pick_next to pick a task from this cfs_rq, as\n\t\t\t * p is sleeping when it is within its sched_slice.\n\t\t\t */\n\t\t\tif (task_sleep && se && !throttled_hierarchy(cfs_rq))\n\t\t\t\tset_next_buddy(se);\n\t\t\tbreak;\n\t\t}\n\t\tflags |= DEQUEUE_SLEEP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running--;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, 1);\n\n\tutil_est_dequeue(&rq->cfs, p, task_sleep);\n\thrtick_update(rq);\n}"
  },
  {
    "function_name": "enqueue_task_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5080-5136",
    "snippet": "static void\nenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * The code below (indirectly) updates schedutil which looks at\n\t * the cfs_rq utilization to select a frequency.\n\t * Let's add the task's estimated utilization to the cfs_rq's\n\t * estimated utilization, before we update schedutil.\n\t */\n\tutil_est_enqueue(&rq->cfs, p);\n\n\t/*\n\t * If in_iowait is set, the code below may not trigger any cpufreq\n\t * utilization updates, so do it here explicitly with the IOWAIT flag\n\t * passed.\n\t */\n\tif (p->in_iowait)\n\t\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT);\n\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tbreak;\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tenqueue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running increment below.\n\t\t */\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running++;\n\n\t\tflags = ENQUEUE_WAKEUP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running++;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, 1);\n\n\thrtick_update(rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define UPDATE_TG\t0x0",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtick_update",
          "args": [
            "rq"
          ],
          "line": 5135
        },
        "resolved": true,
        "details": {
          "function_name": "hrtick_update",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5070-5072",
          "snippet": "static inline void hrtick_update(struct rq *rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void hrtick_update(struct rq *rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_nr_running",
          "args": [
            "rq",
            "1"
          ],
          "line": 5133
        },
        "resolved": true,
        "details": {
          "function_name": "add_nr_running",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1793-1807",
          "snippet": "static inline void add_nr_running(struct rq *rq, unsigned count)\n{\n\tunsigned prev_nr = rq->nr_running;\n\n\trq->nr_running = prev_nr + count;\n\n\tif (prev_nr < 2 && rq->nr_running >= 2) {\n#ifdef CONFIG_SMP\n\t\tif (!READ_ONCE(rq->rd->overload))\n\t\t\tWRITE_ONCE(rq->rd->overload, 1);\n#endif\n\t}\n\n\tsched_update_tick_dependency(rq);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void add_nr_running(struct rq *rq, unsigned count)\n{\n\tunsigned prev_nr = rq->nr_running;\n\n\trq->nr_running = prev_nr + count;\n\n\tif (prev_nr < 2 && rq->nr_running >= 2) {\n#ifdef CONFIG_SMP\n\t\tif (!READ_ONCE(rq->rd->overload))\n\t\t\tWRITE_ONCE(rq->rd->overload, 1);\n#endif\n\t}\n\n\tsched_update_tick_dependency(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_group",
          "args": [
            "se"
          ],
          "line": 5129
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3009-3011",
          "snippet": "static inline void update_cfs_group(struct sched_entity *se)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq",
            "se",
            "UPDATE_TG"
          ],
          "line": 5128
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 5125
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 5122
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "enqueue_entity",
          "args": [
            "cfs_rq",
            "se",
            "flags"
          ],
          "line": 5106
        },
        "resolved": true,
        "details": {
          "function_name": "enqueue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3858-3909",
          "snippet": "static void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define DO_ATTACH\t0x0",
            "#define UPDATE_TG\t0x0",
            "#define DO_ATTACH\t0x4",
            "#define UPDATE_TG\t0x1"
          ],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define DO_ATTACH\t0x0\n#define UPDATE_TG\t0x0\n#define DO_ATTACH\t0x4\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpufreq_update_util",
          "args": [
            "rq",
            "SCHED_CPUFREQ_IOWAIT"
          ],
          "line": 5100
        },
        "resolved": true,
        "details": {
          "function_name": "cpufreq_update_util",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2200-2200",
          "snippet": "static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void activate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void activate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}"
        }
      },
      {
        "call_info": {
          "callee": "util_est_enqueue",
          "args": [
            "&rq->cfs",
            "p"
          ],
          "line": 5092
        },
        "resolved": true,
        "details": {
          "function_name": "util_est_enqueue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3750-3751",
          "snippet": "static inline void\nutil_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void\nutil_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\n\t/*\n\t * The code below (indirectly) updates schedutil which looks at\n\t * the cfs_rq utilization to select a frequency.\n\t * Let's add the task's estimated utilization to the cfs_rq's\n\t * estimated utilization, before we update schedutil.\n\t */\n\tutil_est_enqueue(&rq->cfs, p);\n\n\t/*\n\t * If in_iowait is set, the code below may not trigger any cpufreq\n\t * utilization updates, so do it here explicitly with the IOWAIT flag\n\t * passed.\n\t */\n\tif (p->in_iowait)\n\t\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT);\n\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tbreak;\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tenqueue_entity(cfs_rq, se, flags);\n\n\t\t/*\n\t\t * end evaluation on encountering a throttled cfs_rq\n\t\t *\n\t\t * note: in the case of encountering a throttled cfs_rq we will\n\t\t * post the final h_nr_running increment below.\n\t\t */\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t\tcfs_rq->h_nr_running++;\n\n\t\tflags = ENQUEUE_WAKEUP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tcfs_rq->h_nr_running++;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tupdate_cfs_group(se);\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, 1);\n\n\thrtick_update(rq);\n}"
  },
  {
    "function_name": "hrtick_update",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5070-5072",
    "snippet": "static inline void hrtick_update(struct rq *rq)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void hrtick_update(struct rq *rq)\n{\n}"
  },
  {
    "function_name": "hrtick_start_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5065-5068",
    "snippet": "static inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}"
  },
  {
    "function_name": "hrtick_update",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5054-5063",
    "snippet": "static void hrtick_update(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\n\tif (!hrtick_enabled(rq) || curr->sched_class != &fair_sched_class)\n\t\treturn;\n\n\tif (cfs_rq_of(&curr->se)->nr_running < sched_nr_latency)\n\t\thrtick_start_fair(rq, curr);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static unsigned int sched_nr_latency = 8;",
      "const struct sched_class fair_sched_class;",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtick_start_fair",
          "args": [
            "rq",
            "curr"
          ],
          "line": 5062
        },
        "resolved": true,
        "details": {
          "function_name": "hrtick_start_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5065-5068",
          "snippet": "static inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "&curr->se"
          ],
          "line": 5061
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtick_enabled",
          "args": [
            "rq"
          ],
          "line": 5058
        },
        "resolved": true,
        "details": {
          "function_name": "hrtick_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1844-1847",
          "snippet": "static inline int hrtick_enabled(struct rq *rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int sched_nr_latency = 8;\nconst struct sched_class fair_sched_class;\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void hrtick_update(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\n\tif (!hrtick_enabled(rq) || curr->sched_class != &fair_sched_class)\n\t\treturn;\n\n\tif (cfs_rq_of(&curr->se)->nr_running < sched_nr_latency)\n\t\thrtick_start_fair(rq, curr);\n}"
  },
  {
    "function_name": "hrtick_start_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5028-5047",
    "snippet": "static void hrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tSCHED_WARN_ON(task_rq(p) != rq);\n\n\tif (rq->cfs.h_nr_running > 1) {\n\t\tu64 slice = sched_slice(cfs_rq, se);\n\t\tu64 ran = se->sum_exec_runtime - se->prev_sum_exec_runtime;\n\t\ts64 delta = slice - ran;\n\n\t\tif (delta < 0) {\n\t\t\tif (rq->curr == p)\n\t\t\t\tresched_curr(rq);\n\t\t\treturn;\n\t\t}\n\t\thrtick_start(rq, delta);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtick_start",
          "args": [
            "rq",
            "delta"
          ],
          "line": 5045
        },
        "resolved": true,
        "details": {
          "function_name": "hrtick_start_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5065-5068",
          "snippet": "static inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "resched_curr",
          "args": [
            "rq"
          ],
          "line": 5042
        },
        "resolved": true,
        "details": {
          "function_name": "resched_curr",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "452-474",
          "snippet": "void resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_slice",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 5036
        },
        "resolved": true,
        "details": {
          "function_name": "sched_slice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "658-678",
          "snippet": "static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}"
        }
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "task_rq(p) != rq"
          ],
          "line": 5033
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_rq",
          "args": [
            "p"
          ],
          "line": 5033
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 5031
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void hrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tSCHED_WARN_ON(task_rq(p) != rq);\n\n\tif (rq->cfs.h_nr_running > 1) {\n\t\tu64 slice = sched_slice(cfs_rq, se);\n\t\tu64 ran = se->sum_exec_runtime - se->prev_sum_exec_runtime;\n\t\ts64 delta = slice - ran;\n\n\t\tif (delta < 0) {\n\t\t\tif (rq->curr == p)\n\t\t\t\tresched_curr(rq);\n\t\t\treturn;\n\t\t}\n\t\thrtick_start(rq, delta);\n\t}\n}"
  },
  {
    "function_name": "unthrottle_offline_cfs_rqs",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5019-5019",
    "snippet": "static inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}"
  },
  {
    "function_name": "update_runtime_enabled",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5018-5018",
    "snippet": "static inline void update_runtime_enabled(struct rq *rq) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_runtime_enabled(struct rq *rq) {}"
  },
  {
    "function_name": "destroy_cfs_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5017-5017",
    "snippet": "static inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}"
  },
  {
    "function_name": "tg_cfs_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5013-5016",
    "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}"
  },
  {
    "function_name": "init_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5010-5010",
    "snippet": "static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}"
  },
  {
    "function_name": "init_cfs_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5007-5007",
    "snippet": "void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}"
  },
  {
    "function_name": "throttled_lb_pair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "5001-5005",
    "snippet": "static inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\treturn 0;\n}"
  },
  {
    "function_name": "throttled_hierarchy",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4996-4999",
    "snippet": "static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
  },
  {
    "function_name": "cfs_rq_throttled",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4991-4994",
    "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
  },
  {
    "function_name": "return_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4989-4989",
    "snippet": "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}"
  },
  {
    "function_name": "sync_throttle",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4988-4988",
    "snippet": "static inline void sync_throttle(struct task_group *tg, int cpu) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void sync_throttle(struct task_group *tg, int cpu) {}"
  },
  {
    "function_name": "check_enqueue_throttle",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4987-4987",
    "snippet": "static void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}"
  },
  {
    "function_name": "check_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4986-4986",
    "snippet": "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }"
  },
  {
    "function_name": "account_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4985-4985",
    "snippet": "static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}"
  },
  {
    "function_name": "cfs_rq_clock_task",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4980-4983",
    "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rq_clock_task",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 4982
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4982
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
  },
  {
    "function_name": "unthrottle_offline_cfs_rqs",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4949-4977",
    "snippet": "static void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\tif (!cfs_rq->runtime_enabled)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * clock_task is not advancing so we just need to make sure\n\t\t * there's some valid quota amount\n\t\t */\n\t\tcfs_rq->runtime_remaining = 1;\n\t\t/*\n\t\t * Offline rq is schedulable till CPU is completely disabled\n\t\t * in take_cpu_down(), so we prevent new cfs throttling here.\n\t\t */\n\t\tcfs_rq->runtime_enabled = 0;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\t}\n\trcu_read_unlock();\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 4976
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "unthrottle_cfs_rq",
          "args": [
            "cfs_rq"
          ],
          "line": 4974
        },
        "resolved": true,
        "details": {
          "function_name": "unthrottle_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4499-4544",
          "snippet": "void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 4973
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 4957
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_for_each_entry_rcu",
          "args": [
            "tg",
            "&task_groups",
            "list"
          ],
          "line": 4956
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 4955
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&rq->lock"
          ],
          "line": 4953
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\tif (!cfs_rq->runtime_enabled)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * clock_task is not advancing so we just need to make sure\n\t\t * there's some valid quota amount\n\t\t */\n\t\tcfs_rq->runtime_remaining = 1;\n\t\t/*\n\t\t * Offline rq is schedulable till CPU is completely disabled\n\t\t * in take_cpu_down(), so we prevent new cfs throttling here.\n\t\t */\n\t\tcfs_rq->runtime_enabled = 0;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\t}\n\trcu_read_unlock();\n}"
  },
  {
    "function_name": "update_runtime_enabled",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4930-4946",
    "snippet": "static void __maybe_unused update_runtime_enabled(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\traw_spin_lock(&cfs_b->lock);\n\t\tcfs_rq->runtime_enabled = cfs_b->quota != RUNTIME_INF;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t}\n\trcu_read_unlock();\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 4945
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4943
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4941
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 4939
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_for_each_entry_rcu",
          "args": [
            "tg",
            "&task_groups",
            "list"
          ],
          "line": 4937
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 4936
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&rq->lock"
          ],
          "line": 4934
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void __maybe_unused update_runtime_enabled(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_held(&rq->lock);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\traw_spin_lock(&cfs_b->lock);\n\t\tcfs_rq->runtime_enabled = cfs_b->quota != RUNTIME_INF;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t}\n\trcu_read_unlock();\n}"
  },
  {
    "function_name": "destroy_cfs_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4912-4920",
    "snippet": "static void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\t/* init_cfs_bandwidth() was not called */\n\tif (!cfs_b->throttled_cfs_rq.next)\n\t\treturn;\n\n\thrtimer_cancel(&cfs_b->period_timer);\n\thrtimer_cancel(&cfs_b->slack_timer);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtimer_cancel",
          "args": [
            "&cfs_b->slack_timer"
          ],
          "line": 4919
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_cancel",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/hrtimer.c",
          "lines": "1167-1176",
          "snippet": "int hrtimer_cancel(struct hrtimer *timer)\n{\n\tfor (;;) {\n\t\tint ret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret >= 0)\n\t\t\treturn ret;\n\t\tcpu_relax();\n\t}\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/seq_file.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nint hrtimer_cancel(struct hrtimer *timer)\n{\n\tfor (;;) {\n\t\tint ret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret >= 0)\n\t\t\treturn ret;\n\t\tcpu_relax();\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\t/* init_cfs_bandwidth() was not called */\n\tif (!cfs_b->throttled_cfs_rq.next)\n\t\treturn;\n\n\thrtimer_cancel(&cfs_b->period_timer);\n\thrtimer_cancel(&cfs_b->slack_timer);\n}"
  },
  {
    "function_name": "start_cfs_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4896-4910",
    "snippet": "void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtimer_start_expires",
          "args": [
            "&cfs_b->period_timer",
            "HRTIMER_MODE_ABS_PINNED"
          ],
          "line": 4909
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ktime_to_ns",
          "args": [
            "cfs_b->period"
          ],
          "line": 4907
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "hrtimer_forward_now",
          "args": [
            "&cfs_b->period_timer",
            "cfs_b->period"
          ],
          "line": 4906
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4900
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}"
  },
  {
    "function_name": "init_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4890-4894",
    "snippet": "static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->runtime_enabled = 0;\n\tINIT_LIST_HEAD(&cfs_rq->throttled_list);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "INIT_LIST_HEAD",
          "args": [
            "&cfs_rq->throttled_list"
          ],
          "line": 4893
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->runtime_enabled = 0;\n\tINIT_LIST_HEAD(&cfs_rq->throttled_list);\n}"
  },
  {
    "function_name": "init_cfs_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4875-4888",
    "snippet": "void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\traw_spin_lock_init(&cfs_b->lock);\n\tcfs_b->runtime = 0;\n\tcfs_b->quota = RUNTIME_INF;\n\tcfs_b->period = ns_to_ktime(default_cfs_period());\n\n\tINIT_LIST_HEAD(&cfs_b->throttled_cfs_rq);\n\thrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);\n\tcfs_b->period_timer.function = sched_cfs_period_timer;\n\thrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tcfs_b->slack_timer.function = sched_cfs_slack_timer;\n\tcfs_b->distribute_running = 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtimer_init",
          "args": [
            "&cfs_b->slack_timer",
            "CLOCK_MONOTONIC",
            "HRTIMER_MODE_REL"
          ],
          "line": 4885
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_init",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/hrtimer.c",
          "lines": "1305-1310",
          "snippet": "void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\n\t\t  enum hrtimer_mode mode)\n{\n\tdebug_init(timer, clock_id, mode);\n\t__hrtimer_init(timer, clock_id, mode);\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/seq_file.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nvoid hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\n\t\t  enum hrtimer_mode mode)\n{\n\tdebug_init(timer, clock_id, mode);\n\t__hrtimer_init(timer, clock_id, mode);\n}"
        }
      },
      {
        "call_info": {
          "callee": "INIT_LIST_HEAD",
          "args": [
            "&cfs_b->throttled_cfs_rq"
          ],
          "line": 4882
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ns_to_ktime",
          "args": [
            "default_cfs_period()"
          ],
          "line": 4880
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "default_cfs_period",
          "args": [],
          "line": 4880
        },
        "resolved": true,
        "details": {
          "function_name": "default_cfs_period",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4242-4245",
          "snippet": "static inline u64 default_cfs_period(void)\n{\n\treturn 100000000ULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 default_cfs_period(void)\n{\n\treturn 100000000ULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_init",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4877
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\traw_spin_lock_init(&cfs_b->lock);\n\tcfs_b->runtime = 0;\n\tcfs_b->quota = RUNTIME_INF;\n\tcfs_b->period = ns_to_ktime(default_cfs_period());\n\n\tINIT_LIST_HEAD(&cfs_b->throttled_cfs_rq);\n\thrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);\n\tcfs_b->period_timer.function = sched_cfs_period_timer;\n\thrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tcfs_b->slack_timer.function = sched_cfs_slack_timer;\n\tcfs_b->distribute_running = 0;\n}"
  },
  {
    "function_name": "sched_cfs_period_timer",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4853-4873",
    "snippet": "static enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, period_timer);\n\tint overrun;\n\tint idle = 0;\n\n\traw_spin_lock(&cfs_b->lock);\n\tfor (;;) {\n\t\toverrun = hrtimer_forward_now(timer, cfs_b->period);\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_cfs_period_timer(cfs_b, overrun);\n\t}\n\tif (idle)\n\t\tcfs_b->period_active = 0;\n\traw_spin_unlock(&cfs_b->lock);\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4870
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "do_sched_cfs_period_timer",
          "args": [
            "cfs_b",
            "overrun"
          ],
          "line": 4866
        },
        "resolved": true,
        "details": {
          "function_name": "do_sched_cfs_period_timer",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4592-4658",
          "snippet": "static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)\n{\n\tu64 runtime, runtime_expires;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\truntime_expires = cfs_b->runtime_expires;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n\t\t\t\t\t\t runtime_expires);\n\t\traw_spin_lock(&cfs_b->lock);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)\n{\n\tu64 runtime, runtime_expires;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\truntime_expires = cfs_b->runtime_expires;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n\t\t\t\t\t\t runtime_expires);\n\t\traw_spin_lock(&cfs_b->lock);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_forward_now",
          "args": [
            "timer",
            "cfs_b->period"
          ],
          "line": 4862
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4860
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "timer",
            "structcfs_bandwidth",
            "period_timer"
          ],
          "line": 4856
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, period_timer);\n\tint overrun;\n\tint idle = 0;\n\n\traw_spin_lock(&cfs_b->lock);\n\tfor (;;) {\n\t\toverrun = hrtimer_forward_now(timer, cfs_b->period);\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_cfs_period_timer(cfs_b, overrun);\n\t}\n\tif (idle)\n\t\tcfs_b->period_active = 0;\n\traw_spin_unlock(&cfs_b->lock);\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}"
  },
  {
    "function_name": "sched_cfs_slack_timer",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4843-4851",
    "snippet": "static enum hrtimer_restart sched_cfs_slack_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, slack_timer);\n\n\tdo_sched_cfs_slack_timer(cfs_b);\n\n\treturn HRTIMER_NORESTART;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "do_sched_cfs_slack_timer",
          "args": [
            "cfs_b"
          ],
          "line": 4848
        },
        "resolved": true,
        "details": {
          "function_name": "do_sched_cfs_slack_timer",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4744-4780",
          "snippet": "static void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tu64 expires;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\texpires = cfs_b->runtime_expires;\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock(&cfs_b->lock);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (expires == cfs_b->runtime_expires)\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock(&cfs_b->lock);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tu64 expires;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\texpires = cfs_b->runtime_expires;\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock(&cfs_b->lock);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (expires == cfs_b->runtime_expires)\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock(&cfs_b->lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "timer",
            "structcfs_bandwidth",
            "slack_timer"
          ],
          "line": 4846
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic enum hrtimer_restart sched_cfs_slack_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, slack_timer);\n\n\tdo_sched_cfs_slack_timer(cfs_b);\n\n\treturn HRTIMER_NORESTART;\n}"
  },
  {
    "function_name": "check_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4824-4841",
    "snippet": "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn false;\n\n\tif (likely(!cfs_rq->runtime_enabled || cfs_rq->runtime_remaining > 0))\n\t\treturn false;\n\n\t/*\n\t * it's possible for a throttled entity to be forced into a running\n\t * state (e.g. set_curr_task), in this case we're finished.\n\t */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn true;\n\n\tthrottle_cfs_rq(cfs_rq);\n\treturn true;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "throttle_cfs_rq",
          "args": [
            "cfs_rq"
          ],
          "line": 4839
        },
        "resolved": true,
        "details": {
          "function_name": "unthrottle_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4499-4544",
          "snippet": "void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 4836
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!cfs_rq->runtime_enabled || cfs_rq->runtime_remaining > 0"
          ],
          "line": 4829
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_bandwidth_used",
          "args": [],
          "line": 4826
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_bandwidth_used",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4229-4232",
          "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn false;\n\n\tif (likely(!cfs_rq->runtime_enabled || cfs_rq->runtime_remaining > 0))\n\t\treturn false;\n\n\t/*\n\t * it's possible for a throttled entity to be forced into a running\n\t * state (e.g. set_curr_task), in this case we're finished.\n\t */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn true;\n\n\tthrottle_cfs_rq(cfs_rq);\n\treturn true;\n}"
  },
  {
    "function_name": "sync_throttle",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4806-4821",
    "snippet": "static void sync_throttle(struct task_group *tg, int cpu)\n{\n\tstruct cfs_rq *pcfs_rq, *cfs_rq;\n\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!tg->parent)\n\t\treturn;\n\n\tcfs_rq = tg->cfs_rq[cpu];\n\tpcfs_rq = tg->parent->cfs_rq[cpu];\n\n\tcfs_rq->throttle_count = pcfs_rq->throttle_count;\n\tcfs_rq->throttled_clock_task = rq_clock_task(cpu_rq(cpu));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rq_clock_task",
          "args": [
            "cpu_rq(cpu)"
          ],
          "line": 4820
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 4820
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_bandwidth_used",
          "args": [],
          "line": 4810
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_bandwidth_used",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4229-4232",
          "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void sync_throttle(struct task_group *tg, int cpu)\n{\n\tstruct cfs_rq *pcfs_rq, *cfs_rq;\n\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!tg->parent)\n\t\treturn;\n\n\tcfs_rq = tg->cfs_rq[cpu];\n\tpcfs_rq = tg->parent->cfs_rq[cpu];\n\n\tcfs_rq->throttle_count = pcfs_rq->throttle_count;\n\tcfs_rq->throttled_clock_task = rq_clock_task(cpu_rq(cpu));\n}"
  },
  {
    "function_name": "check_enqueue_throttle",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4787-4804",
    "snippet": "static void check_enqueue_throttle(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\t/* an active group must be handled by the update_curr()->put() path */\n\tif (!cfs_rq->runtime_enabled || cfs_rq->curr)\n\t\treturn;\n\n\t/* ensure the group is not already throttled */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn;\n\n\t/* update runtime allocation */\n\taccount_cfs_rq_runtime(cfs_rq, 0);\n\tif (cfs_rq->runtime_remaining <= 0)\n\t\tthrottle_cfs_rq(cfs_rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "throttle_cfs_rq",
          "args": [
            "cfs_rq"
          ],
          "line": 4803
        },
        "resolved": true,
        "details": {
          "function_name": "unthrottle_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4499-4544",
          "snippet": "void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "account_cfs_rq_runtime",
          "args": [
            "cfs_rq",
            "0"
          ],
          "line": 4801
        },
        "resolved": true,
        "details": {
          "function_name": "account_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4985-4985",
          "snippet": "static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 4797
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_bandwidth_used",
          "args": [],
          "line": 4789
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_bandwidth_used",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4229-4232",
          "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\t/* an active group must be handled by the update_curr()->put() path */\n\tif (!cfs_rq->runtime_enabled || cfs_rq->curr)\n\t\treturn;\n\n\t/* ensure the group is not already throttled */\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn;\n\n\t/* update runtime allocation */\n\taccount_cfs_rq_runtime(cfs_rq, 0);\n\tif (cfs_rq->runtime_remaining <= 0)\n\t\tthrottle_cfs_rq(cfs_rq);\n}"
  },
  {
    "function_name": "do_sched_cfs_slack_timer",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4744-4780",
    "snippet": "static void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tu64 expires;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\texpires = cfs_b->runtime_expires;\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock(&cfs_b->lock);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (expires == cfs_b->runtime_expires)\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock(&cfs_b->lock);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4779
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "runtime",
            "cfs_b->runtime"
          ],
          "line": 4777
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4775
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "distribute_cfs_runtime",
          "args": [
            "cfs_b",
            "runtime",
            "expires"
          ],
          "line": 4773
        },
        "resolved": true,
        "details": {
          "function_name": "distribute_cfs_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4546-4584",
          "snippet": "static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}"
        }
      },
      {
        "call_info": {
          "callee": "runtime_refresh_within",
          "args": [
            "cfs_b",
            "min_bandwidth_expiration"
          ],
          "line": 4756
        },
        "resolved": true,
        "details": {
          "function_name": "runtime_refresh_within",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4674-4689",
          "snippet": "static int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_cfs_bandwidth_slice",
          "args": [],
          "line": 4746
        },
        "resolved": true,
        "details": {
          "function_name": "sched_cfs_bandwidth_slice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4247-4250",
          "snippet": "static inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tu64 expires;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\texpires = cfs_b->runtime_expires;\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock(&cfs_b->lock);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (expires == cfs_b->runtime_expires)\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock(&cfs_b->lock);\n}"
  },
  {
    "function_name": "return_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4729-4738",
    "snippet": "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!cfs_rq->runtime_enabled || cfs_rq->nr_running)\n\t\treturn;\n\n\t__return_cfs_rq_runtime(cfs_rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__return_cfs_rq_runtime",
          "args": [
            "cfs_rq"
          ],
          "line": 4737
        },
        "resolved": true,
        "details": {
          "function_name": "__return_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4705-4727",
          "snippet": "static void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF &&\n\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF &&\n\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_bandwidth_used",
          "args": [],
          "line": 4731
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_bandwidth_used",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4229-4232",
          "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!cfs_rq->runtime_enabled || cfs_rq->nr_running)\n\t\treturn;\n\n\t__return_cfs_rq_runtime(cfs_rq);\n}"
  },
  {
    "function_name": "__return_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4705-4727",
    "snippet": "static void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF &&\n\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4723
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "start_cfs_slack_bandwidth",
          "args": [
            "cfs_b"
          ],
          "line": 4721
        },
        "resolved": true,
        "details": {
          "function_name": "start_cfs_slack_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4691-4702",
          "snippet": "static void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration;\n\n\t/* if there's a quota refresh soon don't bother with slack */\n\tif (runtime_refresh_within(cfs_b, min_left))\n\t\treturn;\n\n\thrtimer_start(&cfs_b->slack_timer,\n\t\t\tns_to_ktime(cfs_bandwidth_slack_period),\n\t\t\tHRTIMER_MODE_REL);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration;\n\n\t/* if there's a quota refresh soon don't bother with slack */\n\tif (runtime_refresh_within(cfs_b, min_left))\n\t\treturn;\n\n\thrtimer_start(&cfs_b->slack_timer,\n\t\t\tns_to_ktime(cfs_bandwidth_slack_period),\n\t\t\tHRTIMER_MODE_REL);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&cfs_b->throttled_cfs_rq"
          ],
          "line": 4720
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_cfs_bandwidth_slice",
          "args": [],
          "line": 4719
        },
        "resolved": true,
        "details": {
          "function_name": "sched_cfs_bandwidth_slice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4247-4250",
          "snippet": "static inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4713
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "tg_cfs_bandwidth",
          "args": [
            "cfs_rq->tg"
          ],
          "line": 4707
        },
        "resolved": true,
        "details": {
          "function_name": "tg_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5013-5016",
          "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF &&\n\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}"
  },
  {
    "function_name": "start_cfs_slack_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4691-4702",
    "snippet": "static void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration;\n\n\t/* if there's a quota refresh soon don't bother with slack */\n\tif (runtime_refresh_within(cfs_b, min_left))\n\t\treturn;\n\n\thrtimer_start(&cfs_b->slack_timer,\n\t\t\tns_to_ktime(cfs_bandwidth_slack_period),\n\t\t\tHRTIMER_MODE_REL);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "hrtimer_start",
          "args": [
            "&cfs_b->slack_timer",
            "ns_to_ktime(cfs_bandwidth_slack_period)",
            "HRTIMER_MODE_REL"
          ],
          "line": 4699
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "ns_to_ktime",
          "args": [
            "cfs_bandwidth_slack_period"
          ],
          "line": 4700
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "runtime_refresh_within",
          "args": [
            "cfs_b",
            "min_left"
          ],
          "line": 4696
        },
        "resolved": true,
        "details": {
          "function_name": "runtime_refresh_within",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4674-4689",
          "snippet": "static int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration;\n\n\t/* if there's a quota refresh soon don't bother with slack */\n\tif (runtime_refresh_within(cfs_b, min_left))\n\t\treturn;\n\n\thrtimer_start(&cfs_b->slack_timer,\n\t\t\tns_to_ktime(cfs_bandwidth_slack_period),\n\t\t\tHRTIMER_MODE_REL);\n}"
  },
  {
    "function_name": "runtime_refresh_within",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4674-4689",
    "snippet": "static int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "ktime_to_ns",
          "args": [
            "hrtimer_expires_remaining(refresh_timer)"
          ],
          "line": 4684
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "hrtimer_expires_remaining",
          "args": [
            "refresh_timer"
          ],
          "line": 4684
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "hrtimer_callback_running",
          "args": [
            "refresh_timer"
          ],
          "line": 4680
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\tu64 remaining;\n\n\t/* if the call-back is running a quota refresh is already occurring */\n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t/* is a quota refresh about to occur? */\n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}"
  },
  {
    "function_name": "do_sched_cfs_period_timer",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4592-4658",
    "snippet": "static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)\n{\n\tu64 runtime, runtime_expires;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\truntime_expires = cfs_b->runtime_expires;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n\t\t\t\t\t\t runtime_expires);\n\t\traw_spin_lock(&cfs_b->lock);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "min",
          "args": [
            "runtime",
            "cfs_b->runtime"
          ],
          "line": 4643
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&cfs_b->throttled_cfs_rq"
          ],
          "line": 4641
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4638
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "distribute_cfs_runtime",
          "args": [
            "cfs_b",
            "runtime",
            "runtime_expires"
          ],
          "line": 4636
        },
        "resolved": true,
        "details": {
          "function_name": "distribute_cfs_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4546-4584",
          "snippet": "static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4634
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__refill_cfs_bandwidth_runtime",
          "args": [
            "cfs_b"
          ],
          "line": 4611
        },
        "resolved": true,
        "details": {
          "function_name": "__refill_cfs_bandwidth_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4259-4270",
          "snippet": "void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)\n{\n\tu64 runtime, runtime_expires;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\truntime_expires = cfs_b->runtime_expires;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n\t\t\t\t\t\t runtime_expires);\n\t\traw_spin_lock(&cfs_b->lock);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tcfs_b->runtime -= min(runtime, cfs_b->runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}"
  },
  {
    "function_name": "distribute_cfs_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4546-4584",
    "snippet": "static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 4581
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_unlock",
          "args": [
            "rq",
            "&rf"
          ],
          "line": 4576
        },
        "resolved": true,
        "details": {
          "function_name": "double_rq_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2053-2060",
          "snippet": "static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);",
            "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tBUG_ON(rq1 != rq2);\n\traw_spin_unlock(&rq1->lock);\n\t__release(rq2->lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unthrottle_cfs_rq",
          "args": [
            "cfs_rq"
          ],
          "line": 4573
        },
        "resolved": true,
        "details": {
          "function_name": "unthrottle_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4499-4544",
          "snippet": "void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 4560
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_lock",
          "args": [
            "rq",
            "&rf"
          ],
          "line": 4559
        },
        "resolved": true,
        "details": {
          "function_name": "task_rq_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "90-126",
          "snippet": "struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, rf->flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\t/*\n\t\t *\tmove_queued_task()\t\ttask_rq_lock()\n\t\t *\n\t\t *\tACQUIRE (rq->lock)\n\t\t *\t[S] ->on_rq = MIGRATING\t\t[L] rq = task_rq()\n\t\t *\tWMB (__set_task_cpu())\t\tACQUIRE (rq->lock);\n\t\t *\t[S] ->cpu = new_cpu\t\t[L] task_rq()\n\t\t *\t\t\t\t\t[L] ->on_rq\n\t\t *\tRELEASE (rq->lock)\n\t\t *\n\t\t * If we observe the old CPU in task_rq_lock, the acquire of\n\t\t * the old rq->lock will fully serialize against the stores.\n\t\t *\n\t\t * If we observe the new CPU in task_rq_lock, the acquire will\n\t\t * pair with the WMB to ensure we must then also see migrating.\n\t\t */\n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, rf->flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\t/*\n\t\t *\tmove_queued_task()\t\ttask_rq_lock()\n\t\t *\n\t\t *\tACQUIRE (rq->lock)\n\t\t *\t[S] ->on_rq = MIGRATING\t\t[L] rq = task_rq()\n\t\t *\tWMB (__set_task_cpu())\t\tACQUIRE (rq->lock);\n\t\t *\t[S] ->cpu = new_cpu\t\t[L] task_rq()\n\t\t *\t\t\t\t\t[L] ->on_rq\n\t\t *\tRELEASE (rq->lock)\n\t\t *\n\t\t * If we observe the old CPU in task_rq_lock, the acquire of\n\t\t * the old rq->lock will fully serialize against the stores.\n\t\t *\n\t\t * If we observe the new CPU in task_rq_lock, the acquire will\n\t\t * pair with the WMB to ensure we must then also see migrating.\n\t\t */\n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_unlock(&rq->lock);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4556
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_for_each_entry_rcu",
          "args": [
            "cfs_rq",
            "&cfs_b->throttled_cfs_rq",
            "throttled_list"
          ],
          "line": 4554
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 4553
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}"
  },
  {
    "function_name": "unthrottle_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4499-4544",
    "snippet": "void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "resched_curr",
          "args": [
            "rq"
          ],
          "line": 4543
        },
        "resolved": true,
        "details": {
          "function_name": "resched_curr",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "452-474",
          "snippet": "void resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_nr_running",
          "args": [
            "rq",
            "task_delta"
          ],
          "line": 4539
        },
        "resolved": true,
        "details": {
          "function_name": "add_nr_running",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1793-1807",
          "snippet": "static inline void add_nr_running(struct rq *rq, unsigned count)\n{\n\tunsigned prev_nr = rq->nr_running;\n\n\trq->nr_running = prev_nr + count;\n\n\tif (prev_nr < 2 && rq->nr_running >= 2) {\n#ifdef CONFIG_SMP\n\t\tif (!READ_ONCE(rq->rd->overload))\n\t\t\tWRITE_ONCE(rq->rd->overload, 1);\n#endif\n\t}\n\n\tsched_update_tick_dependency(rq);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void add_nr_running(struct rq *rq, unsigned count)\n{\n\tunsigned prev_nr = rq->nr_running;\n\n\trq->nr_running = prev_nr + count;\n\n\tif (prev_nr < 2 && rq->nr_running >= 2) {\n#ifdef CONFIG_SMP\n\t\tif (!READ_ONCE(rq->rd->overload))\n\t\t\tWRITE_ONCE(rq->rd->overload, 1);\n#endif\n\t}\n\n\tsched_update_tick_dependency(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_throttled",
          "args": [
            "cfs_rq"
          ],
          "line": 4534
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_throttled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4991-4994",
          "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "enqueue_entity",
          "args": [
            "cfs_rq",
            "se",
            "ENQUEUE_WAKEUP"
          ],
          "line": 4531
        },
        "resolved": true,
        "details": {
          "function_name": "enqueue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3858-3909",
          "snippet": "static void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define DO_ATTACH\t0x0",
            "#define UPDATE_TG\t0x0",
            "#define DO_ATTACH\t0x4",
            "#define UPDATE_TG\t0x1"
          ],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define DO_ATTACH\t0x0\n#define UPDATE_TG\t0x0\n#define DO_ATTACH\t0x4\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 4529
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "walk_tg_tree_from",
          "args": [
            "cfs_rq->tg",
            "tg_nop",
            "tg_unthrottle_up",
            "(void *)rq"
          ],
          "line": 4519
        },
        "resolved": true,
        "details": {
          "function_name": "walk_tg_tree_from",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "655-684",
          "snippet": "int walk_tg_tree_from(struct task_group *from,\n\t\t\t     tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\tparent = from;\n\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret || parent == from)\n\t\tgoto out;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout:\n\treturn ret;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nint walk_tg_tree_from(struct task_group *from,\n\t\t\t     tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\tparent = from;\n\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret || parent == from)\n\t\tgoto out;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout:\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4516
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_del_rcu",
          "args": [
            "&cfs_rq->throttled_list"
          ],
          "line": 4515
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "rq"
          ],
          "line": 4514
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4513
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_rq_clock",
          "args": [
            "rq"
          ],
          "line": 4511
        },
        "resolved": true,
        "details": {
          "function_name": "update_rq_clock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "185-205",
          "snippet": "void update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (rq->clock_update_flags & RQCF_ACT_SKIP)\n\t\treturn;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (sched_feat(WARN_DOUBLE_CLOCK))\n\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);\n\trq->clock_update_flags |= RQCF_UPDATED;\n#endif\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\tif (delta < 0)\n\t\treturn;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (rq->clock_update_flags & RQCF_ACT_SKIP)\n\t\treturn;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (sched_feat(WARN_DOUBLE_CLOCK))\n\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);\n\trq->clock_update_flags |= RQCF_UPDATED;\n#endif\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\tif (delta < 0)\n\t\treturn;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 4507
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "tg_cfs_bandwidth",
          "args": [
            "cfs_rq->tg"
          ],
          "line": 4502
        },
        "resolved": true,
        "details": {
          "function_name": "tg_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5013-5016",
          "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tint enqueue = 1;\n\tlong task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* update hierarchical throttle state */\n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight)\n\t\treturn;\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tenqueue = 0;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tif (enqueue)\n\t\t\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);\n\t\tcfs_rq->h_nr_running += task_delta;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\t}\n\n\tif (!se)\n\t\tadd_nr_running(rq, task_delta);\n\n\t/* Determine whether we need to wake up potentially idle CPU: */\n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}"
  },
  {
    "function_name": "throttle_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4441-4497",
    "snippet": "static void throttle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tlong task_delta, dequeue = 1;\n\tbool empty;\n\n\tse = cfs_rq->tg->se[cpu_of(rq_of(cfs_rq))];\n\n\t/* freeze hierarchy runnable averages while throttled */\n\trcu_read_lock();\n\twalk_tg_tree_from(cfs_rq->tg, tg_throttle_down, tg_nop, (void *)rq);\n\trcu_read_unlock();\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\t\t/* throttled entity or throttle-on-deactivate */\n\t\tif (!se->on_rq)\n\t\t\tbreak;\n\n\t\tif (dequeue)\n\t\t\tdequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);\n\t\tqcfs_rq->h_nr_running -= task_delta;\n\n\t\tif (qcfs_rq->load.weight)\n\t\t\tdequeue = 0;\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, task_delta);\n\n\tcfs_rq->throttled = 1;\n\tcfs_rq->throttled_clock = rq_clock(rq);\n\traw_spin_lock(&cfs_b->lock);\n\tempty = list_empty(&cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * Add to the _head_ of the list, so that an already-started\n\t * distribute_cfs_runtime will not see us. If disribute_cfs_runtime is\n\t * not running add to the tail so that later runqueues don't get starved.\n\t */\n\tif (cfs_b->distribute_running)\n\t\tlist_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\telse\n\t\tlist_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * If we're the first throttled task, make sure the bandwidth\n\t * timer is running.\n\t */\n\tif (empty)\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\traw_spin_unlock(&cfs_b->lock);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4496
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "start_cfs_bandwidth",
          "args": [
            "cfs_b"
          ],
          "line": 4494
        },
        "resolved": true,
        "details": {
          "function_name": "start_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4896-4910",
          "snippet": "void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_add_tail_rcu",
          "args": [
            "&cfs_rq->throttled_list",
            "&cfs_b->throttled_cfs_rq"
          ],
          "line": 4487
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_add_rcu",
          "args": [
            "&cfs_rq->throttled_list",
            "&cfs_b->throttled_cfs_rq"
          ],
          "line": 4485
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&cfs_b->throttled_cfs_rq"
          ],
          "line": 4477
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4476
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "rq"
          ],
          "line": 4475
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "sub_nr_running",
          "args": [
            "rq",
            "task_delta"
          ],
          "line": 4472
        },
        "resolved": true,
        "details": {
          "function_name": "sub_nr_running",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1809-1814",
          "snippet": "static inline void sub_nr_running(struct rq *rq, unsigned count)\n{\n\trq->nr_running -= count;\n\t/* Check if we still need preemption */\n\tsched_update_tick_dependency(rq);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\n\nstatic inline void sub_nr_running(struct rq *rq, unsigned count)\n{\n\trq->nr_running -= count;\n\t/* Check if we still need preemption */\n\tsched_update_tick_dependency(rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "dequeue_entity",
          "args": [
            "qcfs_rq",
            "se",
            "DEQUEUE_SLEEP"
          ],
          "line": 4464
        },
        "resolved": true,
        "details": {
          "function_name": "dequeue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3958-4008",
          "snippet": "static void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Substract its load from the cfs_rq->runnable_avg.\n\t *   - Substract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define UPDATE_TG\t0x0",
            "#define UPDATE_TG\t0x1"
          ],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Substract its load from the cfs_rq->runnable_avg.\n\t *   - Substract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 4458
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 4454
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "walk_tg_tree_from",
          "args": [
            "cfs_rq->tg",
            "tg_throttle_down",
            "tg_nop",
            "(void *)rq"
          ],
          "line": 4453
        },
        "resolved": true,
        "details": {
          "function_name": "walk_tg_tree_from",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "655-684",
          "snippet": "int walk_tg_tree_from(struct task_group *from,\n\t\t\t     tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\tparent = from;\n\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret || parent == from)\n\t\tgoto out;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout:\n\treturn ret;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nint walk_tg_tree_from(struct task_group *from,\n\t\t\t     tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\tparent = from;\n\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret || parent == from)\n\t\tgoto out;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout:\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 4452
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 4449
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "tg_cfs_bandwidth",
          "args": [
            "cfs_rq->tg"
          ],
          "line": 4444
        },
        "resolved": true,
        "details": {
          "function_name": "tg_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5013-5016",
          "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void throttle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tlong task_delta, dequeue = 1;\n\tbool empty;\n\n\tse = cfs_rq->tg->se[cpu_of(rq_of(cfs_rq))];\n\n\t/* freeze hierarchy runnable averages while throttled */\n\trcu_read_lock();\n\twalk_tg_tree_from(cfs_rq->tg, tg_throttle_down, tg_nop, (void *)rq);\n\trcu_read_unlock();\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\t\t/* throttled entity or throttle-on-deactivate */\n\t\tif (!se->on_rq)\n\t\t\tbreak;\n\n\t\tif (dequeue)\n\t\t\tdequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);\n\t\tqcfs_rq->h_nr_running -= task_delta;\n\n\t\tif (qcfs_rq->load.weight)\n\t\t\tdequeue = 0;\n\t}\n\n\tif (!se)\n\t\tsub_nr_running(rq, task_delta);\n\n\tcfs_rq->throttled = 1;\n\tcfs_rq->throttled_clock = rq_clock(rq);\n\traw_spin_lock(&cfs_b->lock);\n\tempty = list_empty(&cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * Add to the _head_ of the list, so that an already-started\n\t * distribute_cfs_runtime will not see us. If disribute_cfs_runtime is\n\t * not running add to the tail so that later runqueues don't get starved.\n\t */\n\tif (cfs_b->distribute_running)\n\t\tlist_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\telse\n\t\tlist_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);\n\n\t/*\n\t * If we're the first throttled task, make sure the bandwidth\n\t * timer is running.\n\t */\n\tif (empty)\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\traw_spin_unlock(&cfs_b->lock);\n}"
  },
  {
    "function_name": "tg_throttle_down",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4428-4439",
    "snippet": "static int tg_throttle_down(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t/* group is entering throttled state, stop time */\n\tif (!cfs_rq->throttle_count)\n\t\tcfs_rq->throttled_clock_task = rq_clock_task(rq);\n\tcfs_rq->throttle_count++;\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rq_clock_task",
          "args": [
            "rq"
          ],
          "line": 4435
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 4431
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic int tg_throttle_down(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t/* group is entering throttled state, stop time */\n\tif (!cfs_rq->throttle_count)\n\t\tcfs_rq->throttled_clock_task = rq_clock_task(rq);\n\tcfs_rq->throttle_count++;\n\n\treturn 0;\n}"
  },
  {
    "function_name": "tg_unthrottle_up",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4413-4426",
    "snippet": "static int tg_unthrottle_up(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\tcfs_rq->throttle_count--;\n\tif (!cfs_rq->throttle_count) {\n\t\t/* adjust cfs_rq_clock_task() */\n\t\tcfs_rq->throttled_clock_task_time += rq_clock_task(rq) -\n\t\t\t\t\t     cfs_rq->throttled_clock_task;\n\t}\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rq_clock_task",
          "args": [
            "rq"
          ],
          "line": 4421
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 4416
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic int tg_unthrottle_up(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\tcfs_rq->throttle_count--;\n\tif (!cfs_rq->throttle_count) {\n\t\t/* adjust cfs_rq_clock_task() */\n\t\tcfs_rq->throttled_clock_task_time += rq_clock_task(rq) -\n\t\t\t\t\t     cfs_rq->throttled_clock_task;\n\t}\n\n\treturn 0;\n}"
  },
  {
    "function_name": "throttled_lb_pair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4401-4411",
    "snippet": "static inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\tstruct cfs_rq *src_cfs_rq, *dest_cfs_rq;\n\n\tsrc_cfs_rq = tg->cfs_rq[src_cpu];\n\tdest_cfs_rq = tg->cfs_rq[dest_cpu];\n\n\treturn throttled_hierarchy(src_cfs_rq) ||\n\t       throttled_hierarchy(dest_cfs_rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "throttled_hierarchy",
          "args": [
            "dest_cfs_rq"
          ],
          "line": 4410
        },
        "resolved": true,
        "details": {
          "function_name": "throttled_hierarchy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4996-4999",
          "snippet": "static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\tstruct cfs_rq *src_cfs_rq, *dest_cfs_rq;\n\n\tsrc_cfs_rq = tg->cfs_rq[src_cpu];\n\tdest_cfs_rq = tg->cfs_rq[dest_cpu];\n\n\treturn throttled_hierarchy(src_cfs_rq) ||\n\t       throttled_hierarchy(dest_cfs_rq);\n}"
  },
  {
    "function_name": "throttled_hierarchy",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4391-4394",
    "snippet": "static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttle_count;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_bandwidth_used",
          "args": [],
          "line": 4393
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_bandwidth_used",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4229-4232",
          "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttle_count;\n}"
  },
  {
    "function_name": "cfs_rq_throttled",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4385-4388",
    "snippet": "static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttled;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_bandwidth_used",
          "args": [],
          "line": 4387
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_bandwidth_used",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4229-4232",
          "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttled;\n}"
  },
  {
    "function_name": "account_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4376-4383",
    "snippet": "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\tif (!cfs_bandwidth_used() || !cfs_rq->runtime_enabled)\n\t\treturn;\n\n\t__account_cfs_rq_runtime(cfs_rq, delta_exec);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__account_cfs_rq_runtime",
          "args": [
            "cfs_rq",
            "delta_exec"
          ],
          "line": 4382
        },
        "resolved": true,
        "details": {
          "function_name": "__account_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4359-4374",
          "snippet": "static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\texpire_cfs_rq_runtime(cfs_rq);\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\texpire_cfs_rq_runtime(cfs_rq);\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_bandwidth_used",
          "args": [],
          "line": 4379
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_bandwidth_used",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4229-4232",
          "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\tif (!cfs_bandwidth_used() || !cfs_rq->runtime_enabled)\n\t\treturn;\n\n\t__account_cfs_rq_runtime(cfs_rq, delta_exec);\n}"
  },
  {
    "function_name": "__account_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4359-4374",
    "snippet": "static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\texpire_cfs_rq_runtime(cfs_rq);\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "resched_curr",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 4373
        },
        "resolved": true,
        "details": {
          "function_name": "resched_curr",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "452-474",
          "snippet": "void resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4373
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "cfs_rq->curr"
          ],
          "line": 4372
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "assign_cfs_rq_runtime",
          "args": [
            "cfs_rq"
          ],
          "line": 4372
        },
        "resolved": true,
        "details": {
          "function_name": "assign_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4287-4325",
          "snippet": "static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount, expires;\n\tint expires_seq;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\texpires_seq = cfs_b->expires_seq;\n\texpires = cfs_b->runtime_expires;\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\t/*\n\t * we may have advanced our local expiration to account for allowed\n\t * spread between our sched_clock and the one on which runtime was\n\t * issued.\n\t */\n\tif (cfs_rq->expires_seq != expires_seq) {\n\t\tcfs_rq->expires_seq = expires_seq;\n\t\tcfs_rq->runtime_expires = expires;\n\t}\n\n\treturn cfs_rq->runtime_remaining > 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount, expires;\n\tint expires_seq;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\texpires_seq = cfs_b->expires_seq;\n\texpires = cfs_b->runtime_expires;\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\t/*\n\t * we may have advanced our local expiration to account for allowed\n\t * spread between our sched_clock and the one on which runtime was\n\t * issued.\n\t */\n\tif (cfs_rq->expires_seq != expires_seq) {\n\t\tcfs_rq->expires_seq = expires_seq;\n\t\tcfs_rq->runtime_expires = expires;\n\t}\n\n\treturn cfs_rq->runtime_remaining > 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "cfs_rq->runtime_remaining > 0"
          ],
          "line": 4365
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "expire_cfs_rq_runtime",
          "args": [
            "cfs_rq"
          ],
          "line": 4363
        },
        "resolved": true,
        "details": {
          "function_name": "expire_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4331-4357",
          "snippet": "static void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\n\t/* if the deadline is ahead of our clock, nothing to do */\n\tif (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))\n\t\treturn;\n\n\tif (cfs_rq->runtime_remaining < 0)\n\t\treturn;\n\n\t/*\n\t * If the local deadline has passed we have to consider the\n\t * possibility that our sched_clock is 'fast' and the global deadline\n\t * has not truly expired.\n\t *\n\t * Fortunately we can check determine whether this the case by checking\n\t * whether the global deadline(cfs_b->expires_seq) has advanced.\n\t */\n\tif (cfs_rq->expires_seq == cfs_b->expires_seq) {\n\t\t/* extend local deadline, drift is bounded above by 2 ticks */\n\t\tcfs_rq->runtime_expires += TICK_NSEC;\n\t} else {\n\t\t/* global deadline is ahead, expiration has passed */\n\t\tcfs_rq->runtime_remaining = 0;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\n\t/* if the deadline is ahead of our clock, nothing to do */\n\tif (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))\n\t\treturn;\n\n\tif (cfs_rq->runtime_remaining < 0)\n\t\treturn;\n\n\t/*\n\t * If the local deadline has passed we have to consider the\n\t * possibility that our sched_clock is 'fast' and the global deadline\n\t * has not truly expired.\n\t *\n\t * Fortunately we can check determine whether this the case by checking\n\t * whether the global deadline(cfs_b->expires_seq) has advanced.\n\t */\n\tif (cfs_rq->expires_seq == cfs_b->expires_seq) {\n\t\t/* extend local deadline, drift is bounded above by 2 ticks */\n\t\tcfs_rq->runtime_expires += TICK_NSEC;\n\t} else {\n\t\t/* global deadline is ahead, expiration has passed */\n\t\tcfs_rq->runtime_remaining = 0;\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\texpire_cfs_rq_runtime(cfs_rq);\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}"
  },
  {
    "function_name": "expire_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4331-4357",
    "snippet": "static void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\n\t/* if the deadline is ahead of our clock, nothing to do */\n\tif (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))\n\t\treturn;\n\n\tif (cfs_rq->runtime_remaining < 0)\n\t\treturn;\n\n\t/*\n\t * If the local deadline has passed we have to consider the\n\t * possibility that our sched_clock is 'fast' and the global deadline\n\t * has not truly expired.\n\t *\n\t * Fortunately we can check determine whether this the case by checking\n\t * whether the global deadline(cfs_b->expires_seq) has advanced.\n\t */\n\tif (cfs_rq->expires_seq == cfs_b->expires_seq) {\n\t\t/* extend local deadline, drift is bounded above by 2 ticks */\n\t\tcfs_rq->runtime_expires += TICK_NSEC;\n\t} else {\n\t\t/* global deadline is ahead, expiration has passed */\n\t\tcfs_rq->runtime_remaining = 0;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "(s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0"
          ],
          "line": 4336
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "",
          "args": [
            "rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires"
          ],
          "line": 4336
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 4336
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4336
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "tg_cfs_bandwidth",
          "args": [
            "cfs_rq->tg"
          ],
          "line": 4333
        },
        "resolved": true,
        "details": {
          "function_name": "tg_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5013-5016",
          "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\n\t/* if the deadline is ahead of our clock, nothing to do */\n\tif (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))\n\t\treturn;\n\n\tif (cfs_rq->runtime_remaining < 0)\n\t\treturn;\n\n\t/*\n\t * If the local deadline has passed we have to consider the\n\t * possibility that our sched_clock is 'fast' and the global deadline\n\t * has not truly expired.\n\t *\n\t * Fortunately we can check determine whether this the case by checking\n\t * whether the global deadline(cfs_b->expires_seq) has advanced.\n\t */\n\tif (cfs_rq->expires_seq == cfs_b->expires_seq) {\n\t\t/* extend local deadline, drift is bounded above by 2 ticks */\n\t\tcfs_rq->runtime_expires += TICK_NSEC;\n\t} else {\n\t\t/* global deadline is ahead, expiration has passed */\n\t\tcfs_rq->runtime_remaining = 0;\n\t}\n}"
  },
  {
    "function_name": "assign_cfs_rq_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4287-4325",
    "snippet": "static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount, expires;\n\tint expires_seq;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\texpires_seq = cfs_b->expires_seq;\n\texpires = cfs_b->runtime_expires;\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\t/*\n\t * we may have advanced our local expiration to account for allowed\n\t * spread between our sched_clock and the one on which runtime was\n\t * issued.\n\t */\n\tif (cfs_rq->expires_seq != expires_seq) {\n\t\tcfs_rq->expires_seq = expires_seq;\n\t\tcfs_rq->runtime_expires = expires;\n\t}\n\n\treturn cfs_rq->runtime_remaining > 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4311
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "cfs_b->runtime",
            "min_amount"
          ],
          "line": 4304
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "start_cfs_bandwidth",
          "args": [
            "cfs_b"
          ],
          "line": 4301
        },
        "resolved": true,
        "details": {
          "function_name": "start_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4896-4910",
          "snippet": "void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_b->lock"
          ],
          "line": 4297
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_cfs_bandwidth_slice",
          "args": [],
          "line": 4295
        },
        "resolved": true,
        "details": {
          "function_name": "sched_cfs_bandwidth_slice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4247-4250",
          "snippet": "static inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}"
        }
      },
      {
        "call_info": {
          "callee": "tg_cfs_bandwidth",
          "args": [
            "tg"
          ],
          "line": 4290
        },
        "resolved": true,
        "details": {
          "function_name": "tg_cfs_bandwidth",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5013-5016",
          "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount, expires;\n\tint expires_seq;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\texpires_seq = cfs_b->expires_seq;\n\texpires = cfs_b->runtime_expires;\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\t/*\n\t * we may have advanced our local expiration to account for allowed\n\t * spread between our sched_clock and the one on which runtime was\n\t * issued.\n\t */\n\tif (cfs_rq->expires_seq != expires_seq) {\n\t\tcfs_rq->expires_seq = expires_seq;\n\t\tcfs_rq->runtime_expires = expires;\n\t}\n\n\treturn cfs_rq->runtime_remaining > 0;\n}"
  },
  {
    "function_name": "cfs_rq_clock_task",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4278-4284",
    "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\tif (unlikely(cfs_rq->throttle_count))\n\t\treturn cfs_rq->throttled_clock_task - cfs_rq->throttled_clock_task_time;\n\n\treturn rq_clock_task(rq_of(cfs_rq)) - cfs_rq->throttled_clock_task_time;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rq_clock_task",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 4283
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4283
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "cfs_rq->throttle_count"
          ],
          "line": 4280
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\tif (unlikely(cfs_rq->throttle_count))\n\t\treturn cfs_rq->throttled_clock_task - cfs_rq->throttled_clock_task_time;\n\n\treturn rq_clock_task(rq_of(cfs_rq)) - cfs_rq->throttled_clock_task_time;\n}"
  },
  {
    "function_name": "tg_cfs_bandwidth",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4272-4275",
    "snippet": "static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn &tg->cfs_bandwidth;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn &tg->cfs_bandwidth;\n}"
  },
  {
    "function_name": "__refill_cfs_bandwidth_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4259-4270",
    "snippet": "void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "ktime_to_ns",
          "args": [
            "cfs_b->period"
          ],
          "line": 4268
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_clock_cpu",
          "args": [
            "smp_processor_id()"
          ],
          "line": 4266
        },
        "resolved": true,
        "details": {
          "function_name": "sched_clock_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/clock.c",
          "lines": "460-466",
          "snippet": "u64 sched_clock_cpu(int cpu)\n{\n\tif (!static_branch_unlikely(&sched_clock_running))\n\t\treturn 0;\n\n\treturn sched_clock();\n}",
          "includes": [
            "#include <linux/sched_clock.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_STATIC_KEY_FALSE(sched_clock_running);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched_clock.h>\n#include \"sched.h\"\n\nstatic DEFINE_STATIC_KEY_FALSE(sched_clock_running);\n\nu64 sched_clock_cpu(int cpu)\n{\n\tif (!static_branch_unlikely(&sched_clock_running))\n\t\treturn 0;\n\n\treturn sched_clock();\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_processor_id",
          "args": [],
          "line": 4266
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n}"
  },
  {
    "function_name": "sched_cfs_bandwidth_slice",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4247-4250",
    "snippet": "static inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}"
  },
  {
    "function_name": "default_cfs_period",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4242-4245",
    "snippet": "static inline u64 default_cfs_period(void)\n{\n\treturn 100000000ULL;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 default_cfs_period(void)\n{\n\treturn 100000000ULL;\n}"
  },
  {
    "function_name": "cfs_bandwidth_usage_dec",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4235-4235",
    "snippet": "void cfs_bandwidth_usage_dec(void) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid cfs_bandwidth_usage_dec(void) {}"
  },
  {
    "function_name": "cfs_bandwidth_usage_inc",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4234-4234",
    "snippet": "void cfs_bandwidth_usage_inc(void) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid cfs_bandwidth_usage_inc(void) {}"
  },
  {
    "function_name": "cfs_bandwidth_used",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4229-4232",
    "snippet": "static bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}"
  },
  {
    "function_name": "cfs_bandwidth_usage_dec",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4224-4227",
    "snippet": "void cfs_bandwidth_usage_dec(void)\n{\n\tstatic_key_slow_dec_cpuslocked(&__cfs_bandwidth_used);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "static_key_slow_dec_cpuslocked",
          "args": [
            "&__cfs_bandwidth_used"
          ],
          "line": 4226
        },
        "resolved": true,
        "details": {
          "function_name": "static_key_slow_dec_cpuslocked",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/jump_label.c",
          "lines": "258-262",
          "snippet": "void static_key_slow_dec_cpuslocked(struct static_key *key)\n{\n\tSTATIC_KEY_CHECK_USE(key);\n\t__static_key_slow_dec_cpuslocked(key, 0, NULL);\n}",
          "includes": [
            "#include <asm/sections.h>",
            "#include <linux/cpu.h>",
            "#include <linux/bug.h>",
            "#include <linux/jump_label_ratelimit.h>",
            "#include <linux/static_key.h>",
            "#include <linux/err.h>",
            "#include <linux/sort.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/module.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/memory.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/sections.h>\n#include <linux/cpu.h>\n#include <linux/bug.h>\n#include <linux/jump_label_ratelimit.h>\n#include <linux/static_key.h>\n#include <linux/err.h>\n#include <linux/sort.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/uaccess.h>\n#include <linux/memory.h>\n\nvoid static_key_slow_dec_cpuslocked(struct static_key *key)\n{\n\tSTATIC_KEY_CHECK_USE(key);\n\t__static_key_slow_dec_cpuslocked(key, 0, NULL);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid cfs_bandwidth_usage_dec(void)\n{\n\tstatic_key_slow_dec_cpuslocked(&__cfs_bandwidth_used);\n}"
  },
  {
    "function_name": "cfs_bandwidth_usage_inc",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4219-4222",
    "snippet": "void cfs_bandwidth_usage_inc(void)\n{\n\tstatic_key_slow_inc_cpuslocked(&__cfs_bandwidth_used);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "static_key_slow_inc_cpuslocked",
          "args": [
            "&__cfs_bandwidth_used"
          ],
          "line": 4221
        },
        "resolved": true,
        "details": {
          "function_name": "static_key_slow_inc_cpuslocked",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/jump_label.c",
          "lines": "103-141",
          "snippet": "void static_key_slow_inc_cpuslocked(struct static_key *key)\n{\n\tint v, v1;\n\n\tSTATIC_KEY_CHECK_USE(key);\n\tlockdep_assert_cpus_held();\n\n\t/*\n\t * Careful if we get concurrent static_key_slow_inc() calls;\n\t * later calls must wait for the first one to _finish_ the\n\t * jump_label_update() process.  At the same time, however,\n\t * the jump_label_update() call below wants to see\n\t * static_key_enabled(&key) for jumps to be updated properly.\n\t *\n\t * So give a special meaning to negative key->enabled: it sends\n\t * static_key_slow_inc() down the slow path, and it is non-zero\n\t * so it counts as \"enabled\" in jump_label_update().  Note that\n\t * atomic_inc_unless_negative() checks >= 0, so roll our own.\n\t */\n\tfor (v = atomic_read(&key->enabled); v > 0; v = v1) {\n\t\tv1 = atomic_cmpxchg(&key->enabled, v, v + 1);\n\t\tif (likely(v1 == v))\n\t\t\treturn;\n\t}\n\n\tjump_label_lock();\n\tif (atomic_read(&key->enabled) == 0) {\n\t\tatomic_set(&key->enabled, -1);\n\t\tjump_label_update(key);\n\t\t/*\n\t\t * Ensure that if the above cmpxchg loop observes our positive\n\t\t * value, it must also observe all the text changes.\n\t\t */\n\t\tatomic_set_release(&key->enabled, 1);\n\t} else {\n\t\tatomic_inc(&key->enabled);\n\t}\n\tjump_label_unlock();\n}",
          "includes": [
            "#include <asm/sections.h>",
            "#include <linux/cpu.h>",
            "#include <linux/bug.h>",
            "#include <linux/jump_label_ratelimit.h>",
            "#include <linux/static_key.h>",
            "#include <linux/err.h>",
            "#include <linux/sort.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/module.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/memory.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/sections.h>\n#include <linux/cpu.h>\n#include <linux/bug.h>\n#include <linux/jump_label_ratelimit.h>\n#include <linux/static_key.h>\n#include <linux/err.h>\n#include <linux/sort.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/uaccess.h>\n#include <linux/memory.h>\n\nvoid static_key_slow_inc_cpuslocked(struct static_key *key)\n{\n\tint v, v1;\n\n\tSTATIC_KEY_CHECK_USE(key);\n\tlockdep_assert_cpus_held();\n\n\t/*\n\t * Careful if we get concurrent static_key_slow_inc() calls;\n\t * later calls must wait for the first one to _finish_ the\n\t * jump_label_update() process.  At the same time, however,\n\t * the jump_label_update() call below wants to see\n\t * static_key_enabled(&key) for jumps to be updated properly.\n\t *\n\t * So give a special meaning to negative key->enabled: it sends\n\t * static_key_slow_inc() down the slow path, and it is non-zero\n\t * so it counts as \"enabled\" in jump_label_update().  Note that\n\t * atomic_inc_unless_negative() checks >= 0, so roll our own.\n\t */\n\tfor (v = atomic_read(&key->enabled); v > 0; v = v1) {\n\t\tv1 = atomic_cmpxchg(&key->enabled, v, v + 1);\n\t\tif (likely(v1 == v))\n\t\t\treturn;\n\t}\n\n\tjump_label_lock();\n\tif (atomic_read(&key->enabled) == 0) {\n\t\tatomic_set(&key->enabled, -1);\n\t\tjump_label_update(key);\n\t\t/*\n\t\t * Ensure that if the above cmpxchg loop observes our positive\n\t\t * value, it must also observe all the text changes.\n\t\t */\n\t\tatomic_set_release(&key->enabled, 1);\n\t} else {\n\t\tatomic_inc(&key->enabled);\n\t}\n\tjump_label_unlock();\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid cfs_bandwidth_usage_inc(void)\n{\n\tstatic_key_slow_inc_cpuslocked(&__cfs_bandwidth_used);\n}"
  },
  {
    "function_name": "cfs_bandwidth_used",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4214-4217",
    "snippet": "static inline bool cfs_bandwidth_used(void)\n{\n\treturn static_key_false(&__cfs_bandwidth_used);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "static_key_false",
          "args": [
            "&__cfs_bandwidth_used"
          ],
          "line": 4216
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline bool cfs_bandwidth_used(void)\n{\n\treturn static_key_false(&__cfs_bandwidth_used);\n}"
  },
  {
    "function_name": "entity_tick",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4169-4202",
    "snippet": "static void\nentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Ensure that runnable average is periodically updated.\n\t */\n\tupdate_load_avg(cfs_rq, curr, UPDATE_TG);\n\tupdate_cfs_group(curr);\n\n#ifdef CONFIG_SCHED_HRTICK\n\t/*\n\t * queued ticks are scheduled to match the slice, so don't bother\n\t * validating it and just reschedule.\n\t */\n\tif (queued) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\treturn;\n\t}\n\t/*\n\t * don't let the period tick interfere with the hrtick preemption\n\t */\n\tif (!sched_feat(DOUBLE_TICK) &&\n\t\t\thrtimer_active(&rq_of(cfs_rq)->hrtick_timer))\n\t\treturn;\n#endif\n\n\tif (cfs_rq->nr_running > 1)\n\t\tcheck_preempt_tick(cfs_rq, curr);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define UPDATE_TG\t0x0",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "check_preempt_tick",
          "args": [
            "cfs_rq",
            "curr"
          ],
          "line": 4201
        },
        "resolved": true,
        "details": {
          "function_name": "check_preempt_tick",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4013-4048",
          "snippet": "static void\ncheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tunsigned long ideal_runtime, delta_exec;\n\tstruct sched_entity *se;\n\ts64 delta;\n\n\tideal_runtime = sched_slice(cfs_rq, curr);\n\tdelta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;\n\tif (delta_exec > ideal_runtime) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\t/*\n\t\t * The current task ran long enough, ensure it doesn't get\n\t\t * re-elected due to buddy favours.\n\t\t */\n\t\tclear_buddies(cfs_rq, curr);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ensure that a task that missed wakeup preemption by a\n\t * narrow margin doesn't have to wait for a full slice.\n\t * This also mitigates buddy induced latencies under load.\n\t */\n\tif (delta_exec < sysctl_sched_min_granularity)\n\t\treturn;\n\n\tse = __pick_first_entity(cfs_rq);\n\tdelta = curr->vruntime - se->vruntime;\n\n\tif (delta < 0)\n\t\treturn;\n\n\tif (delta > ideal_runtime)\n\t\tresched_curr(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int sysctl_sched_min_granularity\t\t= 750000ULL;",
            "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_min_granularity\t\t= 750000ULL;\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\ncheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tunsigned long ideal_runtime, delta_exec;\n\tstruct sched_entity *se;\n\ts64 delta;\n\n\tideal_runtime = sched_slice(cfs_rq, curr);\n\tdelta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;\n\tif (delta_exec > ideal_runtime) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\t/*\n\t\t * The current task ran long enough, ensure it doesn't get\n\t\t * re-elected due to buddy favours.\n\t\t */\n\t\tclear_buddies(cfs_rq, curr);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ensure that a task that missed wakeup preemption by a\n\t * narrow margin doesn't have to wait for a full slice.\n\t * This also mitigates buddy induced latencies under load.\n\t */\n\tif (delta_exec < sysctl_sched_min_granularity)\n\t\treturn;\n\n\tse = __pick_first_entity(cfs_rq);\n\tdelta = curr->vruntime - se->vruntime;\n\n\tif (delta < 0)\n\t\treturn;\n\n\tif (delta > ideal_runtime)\n\t\tresched_curr(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_active",
          "args": [
            "&rq_of(cfs_rq)->hrtick_timer"
          ],
          "line": 4196
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_active",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/hrtimer.c",
          "lines": "1320-1337",
          "snippet": "bool hrtimer_active(const struct hrtimer *timer)\n{\n\tstruct hrtimer_clock_base *base;\n\tunsigned int seq;\n\n\tdo {\n\t\tbase = READ_ONCE(timer->base);\n\t\tseq = raw_read_seqcount_begin(&base->seq);\n\n\t\tif (timer->state != HRTIMER_STATE_INACTIVE ||\n\t\t    base->running == timer)\n\t\t\treturn true;\n\n\t} while (read_seqcount_retry(&base->seq, seq) ||\n\t\t base != READ_ONCE(timer->base));\n\n\treturn false;\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/seq_file.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nbool hrtimer_active(const struct hrtimer *timer)\n{\n\tstruct hrtimer_clock_base *base;\n\tunsigned int seq;\n\n\tdo {\n\t\tbase = READ_ONCE(timer->base);\n\t\tseq = raw_read_seqcount_begin(&base->seq);\n\n\t\tif (timer->state != HRTIMER_STATE_INACTIVE ||\n\t\t    base->running == timer)\n\t\t\treturn true;\n\n\t} while (read_seqcount_retry(&base->seq, seq) ||\n\t\t base != READ_ONCE(timer->base));\n\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4196
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "DOUBLE_TICK"
          ],
          "line": 4195
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "resched_curr",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 4189
        },
        "resolved": true,
        "details": {
          "function_name": "resched_curr",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "452-474",
          "snippet": "void resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_group",
          "args": [
            "curr"
          ],
          "line": 4181
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3009-3011",
          "snippet": "static inline void update_cfs_group(struct sched_entity *se)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq",
            "curr",
            "UPDATE_TG"
          ],
          "line": 4180
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_curr",
          "args": [
            "cfs_rq"
          ],
          "line": 4175
        },
        "resolved": true,
        "details": {
          "function_name": "update_curr_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "838-841",
          "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void\nentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Ensure that runnable average is periodically updated.\n\t */\n\tupdate_load_avg(cfs_rq, curr, UPDATE_TG);\n\tupdate_cfs_group(curr);\n\n#ifdef CONFIG_SCHED_HRTICK\n\t/*\n\t * queued ticks are scheduled to match the slice, so don't bother\n\t * validating it and just reschedule.\n\t */\n\tif (queued) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\treturn;\n\t}\n\t/*\n\t * don't let the period tick interfere with the hrtick preemption\n\t */\n\tif (!sched_feat(DOUBLE_TICK) &&\n\t\t\thrtimer_active(&rq_of(cfs_rq)->hrtick_timer))\n\t\treturn;\n#endif\n\n\tif (cfs_rq->nr_running > 1)\n\t\tcheck_preempt_tick(cfs_rq, curr);\n}"
  },
  {
    "function_name": "put_prev_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4145-4167",
    "snippet": "static void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)\n{\n\t/*\n\t * If still on the runqueue then deactivate_task()\n\t * was not called and update_curr() has to be done:\n\t */\n\tif (prev->on_rq)\n\t\tupdate_curr(cfs_rq);\n\n\t/* throttle cfs_rqs exceeding runtime */\n\tcheck_cfs_rq_runtime(cfs_rq);\n\n\tcheck_spread(cfs_rq, prev);\n\n\tif (prev->on_rq) {\n\t\tupdate_stats_wait_start(cfs_rq, prev);\n\t\t/* Put 'current' back into the tree. */\n\t\t__enqueue_entity(cfs_rq, prev);\n\t\t/* in !on_rq case, update occurred at dequeue */\n\t\tupdate_load_avg(cfs_rq, prev, 0);\n\t}\n\tcfs_rq->curr = NULL;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq",
            "prev",
            "0"
          ],
          "line": 4164
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__enqueue_entity",
          "args": [
            "cfs_rq",
            "prev"
          ],
          "line": 4162
        },
        "resolved": true,
        "details": {
          "function_name": "__enqueue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "531-559",
          "snippet": "static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_stats_wait_start",
          "args": [
            "cfs_rq",
            "prev"
          ],
          "line": 4160
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_wait_start",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "843-859",
          "snippet": "static inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}"
        }
      },
      {
        "call_info": {
          "callee": "check_spread",
          "args": [
            "cfs_rq",
            "prev"
          ],
          "line": 4157
        },
        "resolved": true,
        "details": {
          "function_name": "check_spread",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3760-3771",
          "snippet": "static void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "check_cfs_rq_runtime",
          "args": [
            "cfs_rq"
          ],
          "line": 4155
        },
        "resolved": true,
        "details": {
          "function_name": "check_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4986-4986",
          "snippet": "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }"
        }
      },
      {
        "call_info": {
          "callee": "update_curr",
          "args": [
            "cfs_rq"
          ],
          "line": 4152
        },
        "resolved": true,
        "details": {
          "function_name": "update_curr_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "838-841",
          "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)\n{\n\t/*\n\t * If still on the runqueue then deactivate_task()\n\t * was not called and update_curr() has to be done:\n\t */\n\tif (prev->on_rq)\n\t\tupdate_curr(cfs_rq);\n\n\t/* throttle cfs_rqs exceeding runtime */\n\tcheck_cfs_rq_runtime(cfs_rq);\n\n\tcheck_spread(cfs_rq, prev);\n\n\tif (prev->on_rq) {\n\t\tupdate_stats_wait_start(cfs_rq, prev);\n\t\t/* Put 'current' back into the tree. */\n\t\t__enqueue_entity(cfs_rq, prev);\n\t\t/* in !on_rq case, update occurred at dequeue */\n\t\tupdate_load_avg(cfs_rq, prev, 0);\n\t}\n\tcfs_rq->curr = NULL;\n}"
  },
  {
    "function_name": "pick_next_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4092-4141",
    "snippet": "static struct sched_entity *\npick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tstruct sched_entity *left = __pick_first_entity(cfs_rq);\n\tstruct sched_entity *se;\n\n\t/*\n\t * If curr is set we have to see if its left of the leftmost entity\n\t * still in the tree, provided there was anything in the tree at all.\n\t */\n\tif (!left || (curr && entity_before(curr, left)))\n\t\tleft = curr;\n\n\tse = left; /* ideally we run the leftmost entity */\n\n\t/*\n\t * Avoid running the skip buddy, if running something else can\n\t * be done without getting too unfair.\n\t */\n\tif (cfs_rq->skip == se) {\n\t\tstruct sched_entity *second;\n\n\t\tif (se == curr) {\n\t\t\tsecond = __pick_first_entity(cfs_rq);\n\t\t} else {\n\t\t\tsecond = __pick_next_entity(se);\n\t\t\tif (!second || (curr && entity_before(curr, second)))\n\t\t\t\tsecond = curr;\n\t\t}\n\n\t\tif (second && wakeup_preempt_entity(second, left) < 1)\n\t\t\tse = second;\n\t}\n\n\t/*\n\t * Prefer last buddy, try to return the CPU to a preempted task.\n\t */\n\tif (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)\n\t\tse = cfs_rq->last;\n\n\t/*\n\t * Someone really wants this to run. If it's not unfair, run it.\n\t */\n\tif (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)\n\t\tse = cfs_rq->next;\n\n\tclear_buddies(cfs_rq, se);\n\n\treturn se;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "clear_buddies",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 4138
        },
        "resolved": true,
        "details": {
          "function_name": "clear_buddies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3944-3954",
          "snippet": "static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "wakeup_preempt_entity",
          "args": [
            "cfs_rq->next",
            "left"
          ],
          "line": 4135
        },
        "resolved": true,
        "details": {
          "function_name": "wakeup_preempt_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6472-6485",
          "snippet": "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)\n{\n\ts64 gran, vdiff = curr->vruntime - se->vruntime;\n\n\tif (vdiff <= 0)\n\t\treturn -1;\n\n\tgran = wakeup_gran(se);\n\tif (vdiff > gran)\n\t\treturn 1;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_before",
          "args": [
            "curr",
            "second"
          ],
          "line": 4118
        },
        "resolved": true,
        "details": {
          "function_name": "entity_before",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "490-494",
          "snippet": "static inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__pick_next_entity",
          "args": [
            "se"
          ],
          "line": 4117
        },
        "resolved": true,
        "details": {
          "function_name": "__pick_next_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "576-584",
          "snippet": "static struct sched_entity *__pick_next_entity(struct sched_entity *se)\n{\n\tstruct rb_node *next = rb_next(&se->run_node);\n\n\tif (!next)\n\t\treturn NULL;\n\n\treturn rb_entry(next, struct sched_entity, run_node);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic struct sched_entity *__pick_next_entity(struct sched_entity *se)\n{\n\tstruct rb_node *next = rb_next(&se->run_node);\n\n\tif (!next)\n\t\treturn NULL;\n\n\treturn rb_entry(next, struct sched_entity, run_node);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__pick_first_entity",
          "args": [
            "cfs_rq"
          ],
          "line": 4115
        },
        "resolved": true,
        "details": {
          "function_name": "__pick_first_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "566-574",
          "snippet": "struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstruct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic struct sched_entity *\npick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tstruct sched_entity *left = __pick_first_entity(cfs_rq);\n\tstruct sched_entity *se;\n\n\t/*\n\t * If curr is set we have to see if its left of the leftmost entity\n\t * still in the tree, provided there was anything in the tree at all.\n\t */\n\tif (!left || (curr && entity_before(curr, left)))\n\t\tleft = curr;\n\n\tse = left; /* ideally we run the leftmost entity */\n\n\t/*\n\t * Avoid running the skip buddy, if running something else can\n\t * be done without getting too unfair.\n\t */\n\tif (cfs_rq->skip == se) {\n\t\tstruct sched_entity *second;\n\n\t\tif (se == curr) {\n\t\t\tsecond = __pick_first_entity(cfs_rq);\n\t\t} else {\n\t\t\tsecond = __pick_next_entity(se);\n\t\t\tif (!second || (curr && entity_before(curr, second)))\n\t\t\t\tsecond = curr;\n\t\t}\n\n\t\tif (second && wakeup_preempt_entity(second, left) < 1)\n\t\t\tse = second;\n\t}\n\n\t/*\n\t * Prefer last buddy, try to return the CPU to a preempted task.\n\t */\n\tif (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)\n\t\tse = cfs_rq->last;\n\n\t/*\n\t * Someone really wants this to run. If it's not unfair, run it.\n\t */\n\tif (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)\n\t\tse = cfs_rq->next;\n\n\tclear_buddies(cfs_rq, se);\n\n\treturn se;\n}"
  },
  {
    "function_name": "set_next_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4050-4080",
    "snippet": "static void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/* 'current' is not kept within the tree. */\n\tif (se->on_rq) {\n\t\t/*\n\t\t * Any task has to be enqueued before it get to execute on\n\t\t * a CPU. So account for the time it spent waiting on the\n\t\t * runqueue.\n\t\t */\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n\n\tupdate_stats_curr_start(cfs_rq, se);\n\tcfs_rq->curr = se;\n\n\t/*\n\t * Track our maximum slice length, if the CPU's load is at\n\t * least twice that of our own weight (i.e. dont track it\n\t * when there are only lesser-weight tasks around):\n\t */\n\tif (schedstat_enabled() && rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {\n\t\tschedstat_set(se->statistics.slice_max,\n\t\t\tmax((u64)schedstat_val(se->statistics.slice_max),\n\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime));\n\t}\n\n\tse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define UPDATE_TG\t0x0",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "schedstat_set",
          "args": [
            "se->statistics.slice_max",
            "max((u64)schedstat_val(se->statistics.slice_max),\n\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime)"
          ],
          "line": 4074
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "(u64)schedstat_val(se->statistics.slice_max)",
            "se->sum_exec_runtime - se->prev_sum_exec_runtime"
          ],
          "line": 4075
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.slice_max"
          ],
          "line": 4075
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4073
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_enabled",
          "args": [],
          "line": 4073
        },
        "resolved": true,
        "details": {
          "function_name": "force_schedstat_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2231-2237",
          "snippet": "void force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_stats_curr_start",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 4065
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_curr_start",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1010-1017",
          "snippet": "static inline void\nupdate_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/*\n\t * We are starting a new run period:\n\t */\n\tse->exec_start = rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/*\n\t * We are starting a new run period:\n\t */\n\tse->exec_start = rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq",
            "se",
            "UPDATE_TG"
          ],
          "line": 4062
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__dequeue_entity",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 4061
        },
        "resolved": true,
        "details": {
          "function_name": "__dequeue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "561-564",
          "snippet": "static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_stats_wait_end",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 4060
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_wait_end",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "861-891",
          "snippet": "static inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/* 'current' is not kept within the tree. */\n\tif (se->on_rq) {\n\t\t/*\n\t\t * Any task has to be enqueued before it get to execute on\n\t\t * a CPU. So account for the time it spent waiting on the\n\t\t * runqueue.\n\t\t */\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t}\n\n\tupdate_stats_curr_start(cfs_rq, se);\n\tcfs_rq->curr = se;\n\n\t/*\n\t * Track our maximum slice length, if the CPU's load is at\n\t * least twice that of our own weight (i.e. dont track it\n\t * when there are only lesser-weight tasks around):\n\t */\n\tif (schedstat_enabled() && rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {\n\t\tschedstat_set(se->statistics.slice_max,\n\t\t\tmax((u64)schedstat_val(se->statistics.slice_max),\n\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime));\n\t}\n\n\tse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}"
  },
  {
    "function_name": "check_preempt_tick",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "4013-4048",
    "snippet": "static void\ncheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tunsigned long ideal_runtime, delta_exec;\n\tstruct sched_entity *se;\n\ts64 delta;\n\n\tideal_runtime = sched_slice(cfs_rq, curr);\n\tdelta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;\n\tif (delta_exec > ideal_runtime) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\t/*\n\t\t * The current task ran long enough, ensure it doesn't get\n\t\t * re-elected due to buddy favours.\n\t\t */\n\t\tclear_buddies(cfs_rq, curr);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ensure that a task that missed wakeup preemption by a\n\t * narrow margin doesn't have to wait for a full slice.\n\t * This also mitigates buddy induced latencies under load.\n\t */\n\tif (delta_exec < sysctl_sched_min_granularity)\n\t\treturn;\n\n\tse = __pick_first_entity(cfs_rq);\n\tdelta = curr->vruntime - se->vruntime;\n\n\tif (delta < 0)\n\t\treturn;\n\n\tif (delta > ideal_runtime)\n\t\tresched_curr(rq_of(cfs_rq));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "unsigned int sysctl_sched_min_granularity\t\t= 750000ULL;",
      "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "resched_curr",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 4047
        },
        "resolved": true,
        "details": {
          "function_name": "resched_curr",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "452-474",
          "snippet": "void resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_held(&rq->lock);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 4047
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__pick_first_entity",
          "args": [
            "cfs_rq"
          ],
          "line": 4040
        },
        "resolved": true,
        "details": {
          "function_name": "__pick_first_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "566-574",
          "snippet": "struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstruct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}"
        }
      },
      {
        "call_info": {
          "callee": "clear_buddies",
          "args": [
            "cfs_rq",
            "curr"
          ],
          "line": 4028
        },
        "resolved": true,
        "details": {
          "function_name": "clear_buddies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3944-3954",
          "snippet": "static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_slice",
          "args": [
            "cfs_rq",
            "curr"
          ],
          "line": 4020
        },
        "resolved": true,
        "details": {
          "function_name": "sched_slice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "658-678",
          "snippet": "static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_min_granularity\t\t= 750000ULL;\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\ncheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\tunsigned long ideal_runtime, delta_exec;\n\tstruct sched_entity *se;\n\ts64 delta;\n\n\tideal_runtime = sched_slice(cfs_rq, curr);\n\tdelta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;\n\tif (delta_exec > ideal_runtime) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\t/*\n\t\t * The current task ran long enough, ensure it doesn't get\n\t\t * re-elected due to buddy favours.\n\t\t */\n\t\tclear_buddies(cfs_rq, curr);\n\t\treturn;\n\t}\n\n\t/*\n\t * Ensure that a task that missed wakeup preemption by a\n\t * narrow margin doesn't have to wait for a full slice.\n\t * This also mitigates buddy induced latencies under load.\n\t */\n\tif (delta_exec < sysctl_sched_min_granularity)\n\t\treturn;\n\n\tse = __pick_first_entity(cfs_rq);\n\tdelta = curr->vruntime - se->vruntime;\n\n\tif (delta < 0)\n\t\treturn;\n\n\tif (delta > ideal_runtime)\n\t\tresched_curr(rq_of(cfs_rq));\n}"
  },
  {
    "function_name": "dequeue_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3958-4008",
    "snippet": "static void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Substract its load from the cfs_rq->runnable_avg.\n\t *   - Substract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define UPDATE_TG\t0x0",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_min_vruntime",
          "args": [
            "cfs_rq"
          ],
          "line": 4007
        },
        "resolved": true,
        "details": {
          "function_name": "update_min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "496-526",
          "snippet": "static void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_group",
          "args": [
            "se"
          ],
          "line": 3998
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3009-3011",
          "snippet": "static inline void update_cfs_group(struct sched_entity *se)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "return_cfs_rq_runtime",
          "args": [
            "cfs_rq"
          ],
          "line": 3996
        },
        "resolved": true,
        "details": {
          "function_name": "return_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4989-4989",
          "snippet": "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}"
        }
      },
      {
        "call_info": {
          "callee": "account_entity_dequeue",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3984
        },
        "resolved": true,
        "details": {
          "function_name": "account_entity_dequeue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2685-2698",
          "snippet": "static void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__dequeue_entity",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3982
        },
        "resolved": true,
        "details": {
          "function_name": "__dequeue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "561-564",
          "snippet": "static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}"
        }
      },
      {
        "call_info": {
          "callee": "clear_buddies",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3979
        },
        "resolved": true,
        "details": {
          "function_name": "clear_buddies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3944-3954",
          "snippet": "static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_stats_dequeue",
          "args": [
            "cfs_rq",
            "se",
            "flags"
          ],
          "line": 3977
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_dequeue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "981-1005",
          "snippet": "static inline void\nupdate_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Mark the end of the wait period if dequeueing a\n\t * waiting task:\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\n\tif ((flags & DEQUEUE_SLEEP) && entity_is_task(se)) {\n\t\tstruct task_struct *tsk = task_of(se);\n\n\t\tif (tsk->state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.sleep_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t\tif (tsk->state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.block_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Mark the end of the wait period if dequeueing a\n\t * waiting task:\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\n\tif ((flags & DEQUEUE_SLEEP) && entity_is_task(se)) {\n\t\tstruct task_struct *tsk = task_of(se);\n\n\t\tif (tsk->state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.sleep_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t\tif (tsk->state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.block_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "dequeue_runnable_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3975
        },
        "resolved": true,
        "details": {
          "function_name": "dequeue_runnable_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2773-2774",
          "snippet": "static inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      },
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq",
            "se",
            "UPDATE_TG"
          ],
          "line": 3974
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_curr",
          "args": [
            "cfs_rq"
          ],
          "line": 3964
        },
        "resolved": true,
        "details": {
          "function_name": "update_curr_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "838-841",
          "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define UPDATE_TG\t0x0\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\t/*\n\t * Update run-time statistics of the 'current'.\n\t */\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * When dequeuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Substract its load from the cfs_rq->runnable_avg.\n\t *   - Substract its previous weight from cfs_rq->load.weight.\n\t *   - For group entity, update its weight to reflect the new share\n\t *     of its group cfs_rq.\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\tdequeue_runnable_load_avg(cfs_rq, se);\n\n\tupdate_stats_dequeue(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t/*\n\t * Normalize after update_curr(); which will also have moved\n\t * min_vruntime if @se is the one holding it back. But before doing\n\t * update_min_vruntime() again, which will discount @se's position and\n\t * can move min_vruntime forward still more.\n\t */\n\tif (!(flags & DEQUEUE_SLEEP))\n\t\tse->vruntime -= cfs_rq->min_vruntime;\n\n\t/* return excess runtime on last dequeue */\n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t/*\n\t * Now advance min_vruntime if @se was the entity holding it back,\n\t * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be\n\t * put back on, and if we advance min_vruntime, we'll be placed back\n\t * further than we started -- ie. we'll be penalized.\n\t */\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n}"
  },
  {
    "function_name": "clear_buddies",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3944-3954",
    "snippet": "static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__clear_buddies_skip",
          "args": [
            "se"
          ],
          "line": 3953
        },
        "resolved": true,
        "details": {
          "function_name": "__clear_buddies_skip",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3933-3942",
          "snippet": "static void __clear_buddies_skip(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->skip != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->skip = NULL;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __clear_buddies_skip(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->skip != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->skip = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "__clear_buddies_next",
          "args": [
            "se"
          ],
          "line": 3950
        },
        "resolved": true,
        "details": {
          "function_name": "__clear_buddies_next",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3922-3931",
          "snippet": "static void __clear_buddies_next(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->next != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->next = NULL;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __clear_buddies_next(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->next != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->next = NULL;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "__clear_buddies_last",
          "args": [
            "se"
          ],
          "line": 3947
        },
        "resolved": true,
        "details": {
          "function_name": "__clear_buddies_last",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3911-3920",
          "snippet": "static void __clear_buddies_last(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->last != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->last = NULL;\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __clear_buddies_last(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->last != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->last = NULL;\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->last == se)\n\t\t__clear_buddies_last(se);\n\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n\n\tif (cfs_rq->skip == se)\n\t\t__clear_buddies_skip(se);\n}"
  },
  {
    "function_name": "__clear_buddies_skip",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3933-3942",
    "snippet": "static void __clear_buddies_skip(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->skip != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->skip = NULL;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 3936
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __clear_buddies_skip(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->skip != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->skip = NULL;\n\t}\n}"
  },
  {
    "function_name": "__clear_buddies_next",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3922-3931",
    "snippet": "static void __clear_buddies_next(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->next != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->next = NULL;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 3925
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __clear_buddies_next(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->next != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->next = NULL;\n\t}\n}"
  },
  {
    "function_name": "__clear_buddies_last",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3911-3920",
    "snippet": "static void __clear_buddies_last(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->last != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->last = NULL;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 3914
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __clear_buddies_last(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->last != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->last = NULL;\n\t}\n}"
  },
  {
    "function_name": "enqueue_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3858-3909",
    "snippet": "static void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define DO_ATTACH\t0x0",
      "#define UPDATE_TG\t0x0",
      "#define DO_ATTACH\t0x4",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "check_enqueue_throttle",
          "args": [
            "cfs_rq"
          ],
          "line": 3907
        },
        "resolved": true,
        "details": {
          "function_name": "check_enqueue_throttle",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4987-4987",
          "snippet": "static void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}"
        }
      },
      {
        "call_info": {
          "callee": "list_add_leaf_cfs_rq",
          "args": [
            "cfs_rq"
          ],
          "line": 3906
        },
        "resolved": true,
        "details": {
          "function_name": "list_add_leaf_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "442-444",
          "snippet": "static inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "__enqueue_entity",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3902
        },
        "resolved": true,
        "details": {
          "function_name": "__enqueue_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "531-559",
          "snippet": "static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}"
        }
      },
      {
        "call_info": {
          "callee": "check_spread",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3900
        },
        "resolved": true,
        "details": {
          "function_name": "check_spread",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3760-3771",
          "snippet": "static void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_stats_enqueue",
          "args": [
            "cfs_rq",
            "se",
            "flags"
          ],
          "line": 3899
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_enqueue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "964-979",
          "snippet": "static inline void\nupdate_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Are we enqueueing a waiting task? (for current tasks\n\t * a dequeue/enqueue event is a NOP)\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_start(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper(cfs_rq, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Are we enqueueing a waiting task? (for current tasks\n\t * a dequeue/enqueue event is a NOP)\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_start(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper(cfs_rq, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "check_schedstat_required",
          "args": [],
          "line": 3898
        },
        "resolved": true,
        "details": {
          "function_name": "check_schedstat_required",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3807-3825",
          "snippet": "static inline void check_schedstat_required(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tif (schedstat_enabled())\n\t\treturn;\n\n\t/* Force schedstat enabled if a dependent tracepoint is active */\n\tif (trace_sched_stat_wait_enabled()    ||\n\t\t\ttrace_sched_stat_sleep_enabled()   ||\n\t\t\ttrace_sched_stat_iowait_enabled()  ||\n\t\t\ttrace_sched_stat_blocked_enabled() ||\n\t\t\ttrace_sched_stat_runtime_enabled())  {\n\t\tprintk_deferred_once(\"Scheduler tracepoints stat_sleep, stat_iowait, \"\n\t\t\t     \"stat_blocked and stat_runtime require the \"\n\t\t\t     \"kernel parameter schedstats=enable or \"\n\t\t\t     \"kernel.sched_schedstats=1\\n\");\n\t}\n#endif\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void check_schedstat_required(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tif (schedstat_enabled())\n\t\treturn;\n\n\t/* Force schedstat enabled if a dependent tracepoint is active */\n\tif (trace_sched_stat_wait_enabled()    ||\n\t\t\ttrace_sched_stat_sleep_enabled()   ||\n\t\t\ttrace_sched_stat_iowait_enabled()  ||\n\t\t\ttrace_sched_stat_blocked_enabled() ||\n\t\t\ttrace_sched_stat_runtime_enabled())  {\n\t\tprintk_deferred_once(\"Scheduler tracepoints stat_sleep, stat_iowait, \"\n\t\t\t     \"stat_blocked and stat_runtime require the \"\n\t\t\t     \"kernel parameter schedstats=enable or \"\n\t\t\t     \"kernel.sched_schedstats=1\\n\");\n\t}\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "place_entity",
          "args": [
            "cfs_rq",
            "se",
            "0"
          ],
          "line": 3896
        },
        "resolved": true,
        "details": {
          "function_name": "place_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3773-3803",
          "snippet": "static void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}"
        }
      },
      {
        "call_info": {
          "callee": "account_entity_enqueue",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3893
        },
        "resolved": true,
        "details": {
          "function_name": "account_entity_enqueue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2668-2683",
          "snippet": "static void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}"
        }
      },
      {
        "call_info": {
          "callee": "enqueue_runnable_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3892
        },
        "resolved": true,
        "details": {
          "function_name": "enqueue_runnable_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2771-2772",
          "snippet": "static inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_group",
          "args": [
            "se"
          ],
          "line": 3891
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3009-3011",
          "snippet": "static inline void update_cfs_group(struct sched_entity *se)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_avg",
          "args": [
            "cfs_rq",
            "se",
            "UPDATE_TG | DO_ATTACH"
          ],
          "line": 3890
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3733-3736",
          "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_curr",
          "args": [
            "cfs_rq"
          ],
          "line": 3871
        },
        "resolved": true,
        "details": {
          "function_name": "update_curr_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "838-841",
          "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define DO_ATTACH\t0x0\n#define UPDATE_TG\t0x0\n#define DO_ATTACH\t0x4\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);\n\tbool curr = cfs_rq->curr == se;\n\n\t/*\n\t * If we're the current task, we must renormalise before calling\n\t * update_curr().\n\t */\n\tif (renorm && curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\tupdate_curr(cfs_rq);\n\n\t/*\n\t * Otherwise, renormalise after, such that we're placed at the current\n\t * moment in time, instead of some random moment in the past. Being\n\t * placed in the past could significantly boost this task to the\n\t * fairness detriment of existing tasks.\n\t */\n\tif (renorm && !curr)\n\t\tse->vruntime += cfs_rq->min_vruntime;\n\n\t/*\n\t * When enqueuing a sched_entity, we must:\n\t *   - Update loads to have both entity and cfs_rq synced with now.\n\t *   - Add its load to cfs_rq->runnable_avg\n\t *   - For group_entity, update its weight to reflect the new share of\n\t *     its group cfs_rq\n\t *   - Add its new weight to cfs_rq->load.weight\n\t */\n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tupdate_cfs_group(se);\n\tenqueue_runnable_load_avg(cfs_rq, se);\n\taccount_entity_enqueue(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tplace_entity(cfs_rq, se, 0);\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue(cfs_rq, se, flags);\n\tcheck_spread(cfs_rq, se);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t}\n}"
  },
  {
    "function_name": "check_schedstat_required",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3807-3825",
    "snippet": "static inline void check_schedstat_required(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tif (schedstat_enabled())\n\t\treturn;\n\n\t/* Force schedstat enabled if a dependent tracepoint is active */\n\tif (trace_sched_stat_wait_enabled()    ||\n\t\t\ttrace_sched_stat_sleep_enabled()   ||\n\t\t\ttrace_sched_stat_iowait_enabled()  ||\n\t\t\ttrace_sched_stat_blocked_enabled() ||\n\t\t\ttrace_sched_stat_runtime_enabled())  {\n\t\tprintk_deferred_once(\"Scheduler tracepoints stat_sleep, stat_iowait, \"\n\t\t\t     \"stat_blocked and stat_runtime require the \"\n\t\t\t     \"kernel parameter schedstats=enable or \"\n\t\t\t     \"kernel.sched_schedstats=1\\n\");\n\t}\n#endif\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "printk_deferred_once",
          "args": [
            "\"Scheduler tracepoints stat_sleep, stat_iowait, \"\n\t\t\t     \"stat_blocked and stat_runtime require the \"\n\t\t\t     \"kernel parameter schedstats=enable or \"\n\t\t\t     \"kernel.sched_schedstats=1\\n\""
          ],
          "line": 3819
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_runtime_enabled",
          "args": [],
          "line": 3818
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_blocked_enabled",
          "args": [],
          "line": 3817
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_iowait_enabled",
          "args": [],
          "line": 3816
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_sleep_enabled",
          "args": [],
          "line": 3815
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_wait_enabled",
          "args": [],
          "line": 3814
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_enabled",
          "args": [],
          "line": 3810
        },
        "resolved": true,
        "details": {
          "function_name": "force_schedstat_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2231-2237",
          "snippet": "void force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void check_schedstat_required(void)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tif (schedstat_enabled())\n\t\treturn;\n\n\t/* Force schedstat enabled if a dependent tracepoint is active */\n\tif (trace_sched_stat_wait_enabled()    ||\n\t\t\ttrace_sched_stat_sleep_enabled()   ||\n\t\t\ttrace_sched_stat_iowait_enabled()  ||\n\t\t\ttrace_sched_stat_blocked_enabled() ||\n\t\t\ttrace_sched_stat_runtime_enabled())  {\n\t\tprintk_deferred_once(\"Scheduler tracepoints stat_sleep, stat_iowait, \"\n\t\t\t     \"stat_blocked and stat_runtime require the \"\n\t\t\t     \"kernel parameter schedstats=enable or \"\n\t\t\t     \"kernel.sched_schedstats=1\\n\");\n\t}\n#endif\n}"
  },
  {
    "function_name": "place_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3773-3803",
    "snippet": "static void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "max_vruntime",
          "args": [
            "se->vruntime",
            "vruntime"
          ],
          "line": 3802
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "GENTLE_FAIR_SLEEPERS"
          ],
          "line": 3795
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_vslice",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3785
        },
        "resolved": true,
        "details": {
          "function_name": "sched_vslice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "685-688",
          "snippet": "static u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\treturn calc_delta_fair(sched_slice(cfs_rq, se), se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\treturn calc_delta_fair(sched_slice(cfs_rq, se), se);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n{\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\t/*\n\t * The 'current' period is already promised to the current tasks,\n\t * however the extra weight of the new task will slow them down a\n\t * little, place the new task so that it fits in the slot that\n\t * stays open at the end.\n\t */\n\tif (initial && sched_feat(START_DEBIT))\n\t\tvruntime += sched_vslice(cfs_rq, se);\n\n\t/* sleeps up to a single latency don't count. */\n\tif (!initial) {\n\t\tunsigned long thresh = sysctl_sched_latency;\n\n\t\t/*\n\t\t * Halve their sleep time's effect, to allow\n\t\t * for a gentler effect of sleepers:\n\t\t */\n\t\tif (sched_feat(GENTLE_FAIR_SLEEPERS))\n\t\t\tthresh >>= 1;\n\n\t\tvruntime -= thresh;\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tse->vruntime = max_vruntime(se->vruntime, vruntime);\n}"
  },
  {
    "function_name": "check_spread",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3760-3771",
    "snippet": "static void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "schedstat_inc",
          "args": [
            "cfs_rq->nr_spread_over"
          ],
          "line": 3769
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n\n\tif (d < 0)\n\t\td = -d;\n\n\tif (d > 3*sysctl_sched_latency)\n\t\tschedstat_inc(cfs_rq->nr_spread_over);\n#endif\n}"
  },
  {
    "function_name": "update_misfit_status",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3756-3756",
    "snippet": "static inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}"
  },
  {
    "function_name": "util_est_dequeue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3753-3755",
    "snippet": "static inline void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p,\n\t\t bool task_sleep) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p,\n\t\t bool task_sleep) {}"
  },
  {
    "function_name": "util_est_enqueue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3750-3751",
    "snippet": "static inline void\nutil_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void\nutil_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}"
  },
  {
    "function_name": "idle_balance",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3745-3748",
    "snippet": "static inline int idle_balance(struct rq *rq, struct rq_flags *rf)\n{\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int idle_balance(struct rq *rq, struct rq_flags *rf)\n{\n\treturn 0;\n}"
  },
  {
    "function_name": "detach_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3742-3743",
    "snippet": "static inline void\ndetach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndetach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}"
  },
  {
    "function_name": "attach_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3740-3741",
    "snippet": "static inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}"
  },
  {
    "function_name": "remove_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3738-3738",
    "snippet": "static inline void remove_entity_load_avg(struct sched_entity *se) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void remove_entity_load_avg(struct sched_entity *se) {}"
  },
  {
    "function_name": "update_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3733-3736",
    "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_util_change",
          "args": [
            "cfs_rq",
            "0"
          ],
          "line": 3735
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_util_change",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3014-3035",
          "snippet": "static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
  },
  {
    "function_name": "update_misfit_status",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3709-3725",
    "snippet": "static inline void update_misfit_status(struct task_struct *p, struct rq *rq)\n{\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn;\n\n\tif (!p) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\tif (task_fits_capacity(p, capacity_of(cpu_of(rq)))) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\trq->misfit_task_load = task_h_load(p);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_h_load",
          "args": [
            "p"
          ],
          "line": 3724
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_fits_capacity",
          "args": [
            "p",
            "capacity_of(cpu_of(rq))"
          ],
          "line": 3719
        },
        "resolved": true,
        "details": {
          "function_name": "task_fits_capacity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3704-3707",
          "snippet": "static inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int capacity_margin\t\t\t\t= 1280;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int capacity_margin\t\t\t\t= 1280;\n\nstatic inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}"
        }
      },
      {
        "call_info": {
          "callee": "capacity_of",
          "args": [
            "cpu_of(rq)"
          ],
          "line": 3719
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5509-5512",
          "snippet": "static unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 3719
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "static_branch_unlikely",
          "args": [
            "&sched_asym_cpucapacity"
          ],
          "line": 3711
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq)\n{\n\tif (!static_branch_unlikely(&sched_asym_cpucapacity))\n\t\treturn;\n\n\tif (!p) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\tif (task_fits_capacity(p, capacity_of(cpu_of(rq)))) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\trq->misfit_task_load = task_h_load(p);\n}"
  },
  {
    "function_name": "task_fits_capacity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3704-3707",
    "snippet": "static inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "unsigned int capacity_margin\t\t\t\t= 1280;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_util_est",
          "args": [
            "p"
          ],
          "line": 3706
        },
        "resolved": true,
        "details": {
          "function_name": "task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3610-3613",
          "snippet": "static inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int capacity_margin\t\t\t\t= 1280;\n\nstatic inline int task_fits_capacity(struct task_struct *p, long capacity)\n{\n\treturn capacity * 1024 > task_util_est(p) * capacity_margin;\n}"
  },
  {
    "function_name": "util_est_dequeue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3642-3702",
    "snippet": "static void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p, bool task_sleep)\n{\n\tlong last_ewma_diff;\n\tstruct util_est ue;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tue.enqueued  = cfs_rq->avg.util_est.enqueued;\n\tue.enqueued -= min_t(unsigned int, ue.enqueued,\n\t\t\t     (_task_util_est(p) | UTIL_AVG_UNCHANGED));\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, ue.enqueued);\n\n\t/*\n\t * Skip update of task's estimated utilization when the task has not\n\t * yet completed an activation, e.g. being migrated.\n\t */\n\tif (!task_sleep)\n\t\treturn;\n\n\t/*\n\t * If the PELT values haven't changed since enqueue time,\n\t * skip the util_est update.\n\t */\n\tue = p->se.avg.util_est;\n\tif (ue.enqueued & UTIL_AVG_UNCHANGED)\n\t\treturn;\n\n\t/*\n\t * Skip update of task's estimated utilization when its EWMA is\n\t * already ~1% close to its last activation value.\n\t */\n\tue.enqueued = (task_util(p) | UTIL_AVG_UNCHANGED);\n\tlast_ewma_diff = ue.enqueued - ue.ewma;\n\tif (within_margin(last_ewma_diff, (SCHED_CAPACITY_SCALE / 100)))\n\t\treturn;\n\n\t/*\n\t * Update Task's estimated utilization\n\t *\n\t * When *p completes an activation we can consolidate another sample\n\t * of the task size. This is done by storing the current PELT value\n\t * as ue.enqueued and by using this value to update the Exponential\n\t * Weighted Moving Average (EWMA):\n\t *\n\t *  ewma(t) = w *  task_util(p) + (1-w) * ewma(t-1)\n\t *          = w *  task_util(p) +         ewma(t-1)  - w * ewma(t-1)\n\t *          = w * (task_util(p) -         ewma(t-1)) +     ewma(t-1)\n\t *          = w * (      last_ewma_diff            ) +     ewma(t-1)\n\t *          = w * (last_ewma_diff  +  ewma(t-1) / w)\n\t *\n\t * Where 'w' is the weight of new samples, which is configured to be\n\t * 0.25, thus making w=1/4 ( >>= UTIL_EST_WEIGHT_SHIFT)\n\t */\n\tue.ewma <<= UTIL_EST_WEIGHT_SHIFT;\n\tue.ewma  += last_ewma_diff;\n\tue.ewma >>= UTIL_EST_WEIGHT_SHIFT;\n\tWRITE_ONCE(p->se.avg.util_est, ue);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "p->se.avg.util_est",
            "ue"
          ],
          "line": 3701
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "within_margin",
          "args": [
            "last_ewma_diff",
            "(SCHED_CAPACITY_SCALE / 100)"
          ],
          "line": 3678
        },
        "resolved": true,
        "details": {
          "function_name": "within_margin",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3637-3640",
          "snippet": "static inline bool within_margin(int value, int margin)\n{\n\treturn ((unsigned int)(value + margin - 1) < (2 * margin - 1));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline bool within_margin(int value, int margin)\n{\n\treturn ((unsigned int)(value + margin - 1) < (2 * margin - 1));\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_util",
          "args": [
            "p"
          ],
          "line": 3676
        },
        "resolved": true,
        "details": {
          "function_name": "task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3610-3613",
          "snippet": "static inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "cfs_rq->avg.util_est.enqueued",
            "ue.enqueued"
          ],
          "line": 3655
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "min_t",
          "args": [
            "unsignedint",
            "ue.enqueued",
            "(_task_util_est(p) | UTIL_AVG_UNCHANGED)"
          ],
          "line": 3653
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "_task_util_est",
          "args": [
            "p"
          ],
          "line": 3654
        },
        "resolved": true,
        "details": {
          "function_name": "_task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3603-3608",
          "snippet": "static inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "UTIL_EST"
          ],
          "line": 3648
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p, bool task_sleep)\n{\n\tlong last_ewma_diff;\n\tstruct util_est ue;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tue.enqueued  = cfs_rq->avg.util_est.enqueued;\n\tue.enqueued -= min_t(unsigned int, ue.enqueued,\n\t\t\t     (_task_util_est(p) | UTIL_AVG_UNCHANGED));\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, ue.enqueued);\n\n\t/*\n\t * Skip update of task's estimated utilization when the task has not\n\t * yet completed an activation, e.g. being migrated.\n\t */\n\tif (!task_sleep)\n\t\treturn;\n\n\t/*\n\t * If the PELT values haven't changed since enqueue time,\n\t * skip the util_est update.\n\t */\n\tue = p->se.avg.util_est;\n\tif (ue.enqueued & UTIL_AVG_UNCHANGED)\n\t\treturn;\n\n\t/*\n\t * Skip update of task's estimated utilization when its EWMA is\n\t * already ~1% close to its last activation value.\n\t */\n\tue.enqueued = (task_util(p) | UTIL_AVG_UNCHANGED);\n\tlast_ewma_diff = ue.enqueued - ue.ewma;\n\tif (within_margin(last_ewma_diff, (SCHED_CAPACITY_SCALE / 100)))\n\t\treturn;\n\n\t/*\n\t * Update Task's estimated utilization\n\t *\n\t * When *p completes an activation we can consolidate another sample\n\t * of the task size. This is done by storing the current PELT value\n\t * as ue.enqueued and by using this value to update the Exponential\n\t * Weighted Moving Average (EWMA):\n\t *\n\t *  ewma(t) = w *  task_util(p) + (1-w) * ewma(t-1)\n\t *          = w *  task_util(p) +         ewma(t-1)  - w * ewma(t-1)\n\t *          = w * (task_util(p) -         ewma(t-1)) +     ewma(t-1)\n\t *          = w * (      last_ewma_diff            ) +     ewma(t-1)\n\t *          = w * (last_ewma_diff  +  ewma(t-1) / w)\n\t *\n\t * Where 'w' is the weight of new samples, which is configured to be\n\t * 0.25, thus making w=1/4 ( >>= UTIL_EST_WEIGHT_SHIFT)\n\t */\n\tue.ewma <<= UTIL_EST_WEIGHT_SHIFT;\n\tue.ewma  += last_ewma_diff;\n\tue.ewma >>= UTIL_EST_WEIGHT_SHIFT;\n\tWRITE_ONCE(p->se.avg.util_est, ue);\n}"
  },
  {
    "function_name": "within_margin",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3637-3640",
    "snippet": "static inline bool within_margin(int value, int margin)\n{\n\treturn ((unsigned int)(value + margin - 1) < (2 * margin - 1));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline bool within_margin(int value, int margin)\n{\n\treturn ((unsigned int)(value + margin - 1) < (2 * margin - 1));\n}"
  },
  {
    "function_name": "util_est_enqueue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3615-3627",
    "snippet": "static inline void util_est_enqueue(struct cfs_rq *cfs_rq,\n\t\t\t\t    struct task_struct *p)\n{\n\tunsigned int enqueued;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tenqueued  = cfs_rq->avg.util_est.enqueued;\n\tenqueued += (_task_util_est(p) | UTIL_AVG_UNCHANGED);\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "cfs_rq->avg.util_est.enqueued",
            "enqueued"
          ],
          "line": 3626
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "_task_util_est",
          "args": [
            "p"
          ],
          "line": 3625
        },
        "resolved": true,
        "details": {
          "function_name": "_task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3603-3608",
          "snippet": "static inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "UTIL_EST"
          ],
          "line": 3620
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void util_est_enqueue(struct cfs_rq *cfs_rq,\n\t\t\t\t    struct task_struct *p)\n{\n\tunsigned int enqueued;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t/* Update root cfs_rq's estimated utilization */\n\tenqueued  = cfs_rq->avg.util_est.enqueued;\n\tenqueued += (_task_util_est(p) | UTIL_AVG_UNCHANGED);\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);\n}"
  },
  {
    "function_name": "task_util_est",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3610-3613",
    "snippet": "static inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "max",
          "args": [
            "task_util(p)",
            "_task_util_est(p)"
          ],
          "line": 3612
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "_task_util_est",
          "args": [
            "p"
          ],
          "line": 3612
        },
        "resolved": true,
        "details": {
          "function_name": "_task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3603-3608",
          "snippet": "static inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_util",
          "args": [
            "p"
          ],
          "line": 3612
        },
        "resolved": true,
        "details": {
          "function_name": "task_util_est",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3610-3613",
          "snippet": "static inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}",
          "note": "cyclic_reference_detected"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}"
  },
  {
    "function_name": "_task_util_est",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3603-3608",
    "snippet": "static inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "max",
          "args": [
            "ue.ewma",
            "ue.enqueued"
          ],
          "line": 3607
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "p->se.avg.util_est"
          ],
          "line": 3605
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, ue.enqueued);\n}"
  },
  {
    "function_name": "task_util",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3598-3601",
    "snippet": "static inline unsigned long task_util(struct task_struct *p)\n{\n\treturn READ_ONCE(p->se.avg.util_avg);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "p->se.avg.util_avg"
          ],
          "line": 3600
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline unsigned long task_util(struct task_struct *p)\n{\n\treturn READ_ONCE(p->se.avg.util_avg);\n}"
  },
  {
    "function_name": "cfs_rq_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3591-3594",
    "snippet": "static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}"
  },
  {
    "function_name": "cfs_rq_runnable_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3586-3589",
    "snippet": "static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.runnable_load_avg;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.runnable_load_avg;\n}"
  },
  {
    "function_name": "remove_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3561-3584",
    "snippet": "void remove_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tunsigned long flags;\n\n\t/*\n\t * tasks cannot exit without having gone through wake_up_new_task() ->\n\t * post_init_entity_util_avg() which will have added things to the\n\t * cfs_rq, so we can remove unconditionally.\n\t *\n\t * Similarly for groups, they will have passed through\n\t * post_init_entity_util_avg() before unregister_sched_fair_group()\n\t * calls this.\n\t */\n\n\tsync_entity_load_avg(se);\n\n\traw_spin_lock_irqsave(&cfs_rq->removed.lock, flags);\n\t++cfs_rq->removed.nr;\n\tcfs_rq->removed.util_avg\t+= se->avg.util_avg;\n\tcfs_rq->removed.load_avg\t+= se->avg.load_avg;\n\tcfs_rq->removed.runnable_sum\t+= se->avg.load_sum; /* == runnable_sum */\n\traw_spin_unlock_irqrestore(&cfs_rq->removed.lock, flags);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&cfs_rq->removed.lock",
            "flags"
          ],
          "line": 3583
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&cfs_rq->removed.lock",
            "flags"
          ],
          "line": 3578
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sync_entity_load_avg",
          "args": [
            "se"
          ],
          "line": 3576
        },
        "resolved": true,
        "details": {
          "function_name": "sync_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3548-3555",
          "snippet": "void sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 3563
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid remove_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tunsigned long flags;\n\n\t/*\n\t * tasks cannot exit without having gone through wake_up_new_task() ->\n\t * post_init_entity_util_avg() which will have added things to the\n\t * cfs_rq, so we can remove unconditionally.\n\t *\n\t * Similarly for groups, they will have passed through\n\t * post_init_entity_util_avg() before unregister_sched_fair_group()\n\t * calls this.\n\t */\n\n\tsync_entity_load_avg(se);\n\n\traw_spin_lock_irqsave(&cfs_rq->removed.lock, flags);\n\t++cfs_rq->removed.nr;\n\tcfs_rq->removed.util_avg\t+= se->avg.util_avg;\n\tcfs_rq->removed.load_avg\t+= se->avg.load_avg;\n\tcfs_rq->removed.runnable_sum\t+= se->avg.load_sum; /* == runnable_sum */\n\traw_spin_unlock_irqrestore(&cfs_rq->removed.lock, flags);\n}"
  },
  {
    "function_name": "sync_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3548-3555",
    "snippet": "void sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__update_load_avg_blocked_se",
          "args": [
            "last_update_time",
            "cpu_of(rq_of(cfs_rq))",
            "se"
          ],
          "line": 3554
        },
        "resolved": true,
        "details": {
          "function_name": "__update_load_avg_blocked_se",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/pelt.c",
          "lines": "270-278",
          "snippet": "int __update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se)\n{\n\tif (___update_load_sum(now, cpu, &se->avg, 0, 0, 0)) {\n\t\t___update_load_avg(&se->avg, se_weight(se), se_runnable(se));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"pelt.h\"",
            "#include \"sched-pelt.h\"",
            "#include \"sched.h\"",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"pelt.h\"\n#include \"sched-pelt.h\"\n#include \"sched.h\"\n#include <linux/sched.h>\n\nint __update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se)\n{\n\tif (___update_load_sum(now, cpu, &se->avg, 0, 0, 0)) {\n\t\t___update_load_avg(&se->avg, se_weight(se), se_runnable(se));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 3554
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 3554
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_last_update_time",
          "args": [
            "cfs_rq"
          ],
          "line": 3553
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_last_update_time",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3538-3541",
          "snippet": "static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.last_update_time;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.last_update_time;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);\n}"
  },
  {
    "function_name": "cfs_rq_last_update_time",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3538-3541",
    "snippet": "static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.last_update_time;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.last_update_time;\n}"
  },
  {
    "function_name": "cfs_rq_last_update_time",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3524-3536",
    "snippet": "static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\tu64 last_update_time_copy;\n\tu64 last_update_time;\n\n\tdo {\n\t\tlast_update_time_copy = cfs_rq->load_last_update_time_copy;\n\t\tsmp_rmb();\n\t\tlast_update_time = cfs_rq->avg.last_update_time;\n\t} while (last_update_time != last_update_time_copy);\n\n\treturn last_update_time;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 3531
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\tu64 last_update_time_copy;\n\tu64 last_update_time;\n\n\tdo {\n\t\tlast_update_time_copy = cfs_rq->load_last_update_time_copy;\n\t\tsmp_rmb();\n\t\tlast_update_time = cfs_rq->avg.last_update_time;\n\t} while (last_update_time != last_update_time_copy);\n\n\treturn last_update_time;\n}"
  },
  {
    "function_name": "update_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3490-3521",
    "snippet": "static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu64 now = cfs_rq_clock_task(cfs_rq);\n\tstruct rq *rq = rq_of(cfs_rq);\n\tint cpu = cpu_of(rq);\n\tint decayed;\n\n\t/*\n\t * Track task load average for carrying it to new CPU after migrated, and\n\t * track group sched_entity load average for task_h_load calc in migration\n\t */\n\tif (se->avg.last_update_time && !(flags & SKIP_AGE_LOAD))\n\t\t__update_load_avg_se(now, cpu, cfs_rq, se);\n\n\tdecayed  = update_cfs_rq_load_avg(now, cfs_rq);\n\tdecayed |= propagate_entity_load_avg(se);\n\n\tif (!se->avg.last_update_time && (flags & DO_ATTACH)) {\n\n\t\t/*\n\t\t * DO_ATTACH means we're here from enqueue_entity().\n\t\t * !last_update_time means we've passed through\n\t\t * migrate_task_rq_fair() indicating we migrated.\n\t\t *\n\t\t * IOW we're enqueueing a task on a new CPU.\n\t\t */\n\t\tattach_entity_load_avg(cfs_rq, se, SCHED_CPUFREQ_MIGRATION);\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t} else if (decayed && (flags & UPDATE_TG))\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define DO_ATTACH\t0x0",
      "#define SKIP_AGE_LOAD\t0x0",
      "#define UPDATE_TG\t0x0",
      "#define DO_ATTACH\t0x4",
      "#define SKIP_AGE_LOAD\t0x2",
      "#define UPDATE_TG\t0x1"
    ],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_tg_load_avg",
          "args": [
            "cfs_rq",
            "0"
          ],
          "line": 3520
        },
        "resolved": true,
        "details": {
          "function_name": "update_tg_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3344-3344",
          "snippet": "static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}"
        }
      },
      {
        "call_info": {
          "callee": "attach_entity_load_avg",
          "args": [
            "cfs_rq",
            "se",
            "SCHED_CPUFREQ_MIGRATION"
          ],
          "line": 3516
        },
        "resolved": true,
        "details": {
          "function_name": "attach_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3740-3741",
          "snippet": "static inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {}"
        }
      },
      {
        "call_info": {
          "callee": "propagate_entity_load_avg",
          "args": [
            "se"
          ],
          "line": 3505
        },
        "resolved": true,
        "details": {
          "function_name": "propagate_entity_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3346-3349",
          "snippet": "static inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_cfs_rq_load_avg",
          "args": [
            "now",
            "cfs_rq"
          ],
          "line": 3504
        },
        "resolved": true,
        "details": {
          "function_name": "update_cfs_rq_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3371-3413",
          "snippet": "static inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__update_load_avg_se",
          "args": [
            "now",
            "cpu",
            "cfs_rq",
            "se"
          ],
          "line": 3502
        },
        "resolved": true,
        "details": {
          "function_name": "__update_load_avg_se",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/pelt.c",
          "lines": "280-291",
          "snippet": "int __update_load_avg_se(u64 now, int cpu, struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (___update_load_sum(now, cpu, &se->avg, !!se->on_rq, !!se->on_rq,\n\t\t\t\tcfs_rq->curr == se)) {\n\n\t\t___update_load_avg(&se->avg, se_weight(se), se_runnable(se));\n\t\tcfs_se_util_change(&se->avg);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"pelt.h\"",
            "#include \"sched-pelt.h\"",
            "#include \"sched.h\"",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"pelt.h\"\n#include \"sched-pelt.h\"\n#include \"sched.h\"\n#include <linux/sched.h>\n\nint __update_load_avg_se(u64 now, int cpu, struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (___update_load_sum(now, cpu, &se->avg, !!se->on_rq, !!se->on_rq,\n\t\t\t\tcfs_rq->curr == se)) {\n\n\t\t___update_load_avg(&se->avg, se_weight(se), se_runnable(se));\n\t\tcfs_se_util_change(&se->avg);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 3494
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 3493
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_clock_task",
          "args": [
            "cfs_rq"
          ],
          "line": 3492
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define DO_ATTACH\t0x0\n#define SKIP_AGE_LOAD\t0x0\n#define UPDATE_TG\t0x0\n#define DO_ATTACH\t0x4\n#define SKIP_AGE_LOAD\t0x2\n#define UPDATE_TG\t0x1\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu64 now = cfs_rq_clock_task(cfs_rq);\n\tstruct rq *rq = rq_of(cfs_rq);\n\tint cpu = cpu_of(rq);\n\tint decayed;\n\n\t/*\n\t * Track task load average for carrying it to new CPU after migrated, and\n\t * track group sched_entity load average for task_h_load calc in migration\n\t */\n\tif (se->avg.last_update_time && !(flags & SKIP_AGE_LOAD))\n\t\t__update_load_avg_se(now, cpu, cfs_rq, se);\n\n\tdecayed  = update_cfs_rq_load_avg(now, cfs_rq);\n\tdecayed |= propagate_entity_load_avg(se);\n\n\tif (!se->avg.last_update_time && (flags & DO_ATTACH)) {\n\n\t\t/*\n\t\t * DO_ATTACH means we're here from enqueue_entity().\n\t\t * !last_update_time means we've passed through\n\t\t * migrate_task_rq_fair() indicating we migrated.\n\t\t *\n\t\t * IOW we're enqueueing a task on a new CPU.\n\t\t */\n\t\tattach_entity_load_avg(cfs_rq, se, SCHED_CPUFREQ_MIGRATION);\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n\n\t} else if (decayed && (flags & UPDATE_TG))\n\t\tupdate_tg_load_avg(cfs_rq, 0);\n}"
  },
  {
    "function_name": "detach_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3471-3480",
    "snippet": "static void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tdequeue_load_avg(cfs_rq, se);\n\tsub_positive(&cfs_rq->avg.util_avg, se->avg.util_avg);\n\tsub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);\n\n\tadd_tg_cfs_propagate(cfs_rq, -se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, 0);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_util_change",
          "args": [
            "cfs_rq",
            "0"
          ],
          "line": 3479
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_util_change",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3014-3035",
          "snippet": "static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_tg_cfs_propagate",
          "args": [
            "cfs_rq",
            "-se->avg.load_sum"
          ],
          "line": 3477
        },
        "resolved": true,
        "details": {
          "function_name": "add_tg_cfs_propagate",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3351-3351",
          "snippet": "static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}"
        }
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&cfs_rq->avg.util_sum",
            "se->avg.util_sum"
          ],
          "line": 3475
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&cfs_rq->avg.util_avg",
            "se->avg.util_avg"
          ],
          "line": 3474
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "dequeue_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3473
        },
        "resolved": true,
        "details": {
          "function_name": "dequeue_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2777-2778",
          "snippet": "static inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tdequeue_load_avg(cfs_rq, se);\n\tsub_positive(&cfs_rq->avg.util_avg, se->avg.util_avg);\n\tsub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);\n\n\tadd_tg_cfs_propagate(cfs_rq, -se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, 0);\n}"
  },
  {
    "function_name": "attach_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3424-3461",
    "snippet": "static void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu32 divider = LOAD_AVG_MAX - 1024 + cfs_rq->avg.period_contrib;\n\n\t/*\n\t * When we attach the @se to the @cfs_rq, we must align the decay\n\t * window because without that, really weird and wonderful things can\n\t * happen.\n\t *\n\t * XXX illustrate\n\t */\n\tse->avg.last_update_time = cfs_rq->avg.last_update_time;\n\tse->avg.period_contrib = cfs_rq->avg.period_contrib;\n\n\t/*\n\t * Hell(o) Nasty stuff.. we need to recompute _sum based on the new\n\t * period_contrib. This isn't strictly correct, but since we're\n\t * entirely outside of the PELT hierarchy, nobody cares if we truncate\n\t * _sum a little.\n\t */\n\tse->avg.util_sum = se->avg.util_avg * divider;\n\n\tse->avg.load_sum = divider;\n\tif (se_weight(se)) {\n\t\tse->avg.load_sum =\n\t\t\tdiv_u64(se->avg.load_avg * se->avg.load_sum, se_weight(se));\n\t}\n\n\tse->avg.runnable_load_sum = se->avg.load_sum;\n\n\tenqueue_load_avg(cfs_rq, se);\n\tcfs_rq->avg.util_avg += se->avg.util_avg;\n\tcfs_rq->avg.util_sum += se->avg.util_sum;\n\n\tadd_tg_cfs_propagate(cfs_rq, se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, flags);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_util_change",
          "args": [
            "cfs_rq",
            "flags"
          ],
          "line": 3460
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_util_change",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3014-3035",
          "snippet": "static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_tg_cfs_propagate",
          "args": [
            "cfs_rq",
            "se->avg.load_sum"
          ],
          "line": 3458
        },
        "resolved": true,
        "details": {
          "function_name": "add_tg_cfs_propagate",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3351-3351",
          "snippet": "static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}"
        }
      },
      {
        "call_info": {
          "callee": "enqueue_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 3454
        },
        "resolved": true,
        "details": {
          "function_name": "enqueue_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2775-2776",
          "snippet": "static inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      },
      {
        "call_info": {
          "callee": "div_u64",
          "args": [
            "se->avg.load_avg * se->avg.load_sum",
            "se_weight(se)"
          ],
          "line": 3449
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "se_weight",
          "args": [
            "se"
          ],
          "line": 3449
        },
        "resolved": true,
        "details": {
          "function_name": "se_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "691-694",
          "snippet": "static inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu32 divider = LOAD_AVG_MAX - 1024 + cfs_rq->avg.period_contrib;\n\n\t/*\n\t * When we attach the @se to the @cfs_rq, we must align the decay\n\t * window because without that, really weird and wonderful things can\n\t * happen.\n\t *\n\t * XXX illustrate\n\t */\n\tse->avg.last_update_time = cfs_rq->avg.last_update_time;\n\tse->avg.period_contrib = cfs_rq->avg.period_contrib;\n\n\t/*\n\t * Hell(o) Nasty stuff.. we need to recompute _sum based on the new\n\t * period_contrib. This isn't strictly correct, but since we're\n\t * entirely outside of the PELT hierarchy, nobody cares if we truncate\n\t * _sum a little.\n\t */\n\tse->avg.util_sum = se->avg.util_avg * divider;\n\n\tse->avg.load_sum = divider;\n\tif (se_weight(se)) {\n\t\tse->avg.load_sum =\n\t\t\tdiv_u64(se->avg.load_avg * se->avg.load_sum, se_weight(se));\n\t}\n\n\tse->avg.runnable_load_sum = se->avg.load_sum;\n\n\tenqueue_load_avg(cfs_rq, se);\n\tcfs_rq->avg.util_avg += se->avg.util_avg;\n\tcfs_rq->avg.util_sum += se->avg.util_sum;\n\n\tadd_tg_cfs_propagate(cfs_rq, se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, flags);\n}"
  },
  {
    "function_name": "update_cfs_rq_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3371-3413",
    "snippet": "static inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cfs_rq_util_change",
          "args": [
            "cfs_rq",
            "0"
          ],
          "line": 3410
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_util_change",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3014-3035",
          "snippet": "static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_wmb",
          "args": [],
          "line": 3405
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__update_load_avg_cfs_rq",
          "args": [
            "now",
            "cpu_of(rq_of(cfs_rq))",
            "cfs_rq"
          ],
          "line": 3402
        },
        "resolved": true,
        "details": {
          "function_name": "__update_load_avg_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/pelt.c",
          "lines": "293-305",
          "snippet": "int __update_load_avg_cfs_rq(u64 now, int cpu, struct cfs_rq *cfs_rq)\n{\n\tif (___update_load_sum(now, cpu, &cfs_rq->avg,\n\t\t\t\tscale_load_down(cfs_rq->load.weight),\n\t\t\t\tscale_load_down(cfs_rq->runnable_weight),\n\t\t\t\tcfs_rq->curr != NULL)) {\n\n\t\t___update_load_avg(&cfs_rq->avg, 1, 1);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"pelt.h\"",
            "#include \"sched-pelt.h\"",
            "#include \"sched.h\"",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"pelt.h\"\n#include \"sched-pelt.h\"\n#include \"sched.h\"\n#include <linux/sched.h>\n\nint __update_load_avg_cfs_rq(u64 now, int cpu, struct cfs_rq *cfs_rq)\n{\n\tif (___update_load_sum(now, cpu, &cfs_rq->avg,\n\t\t\t\tscale_load_down(cfs_rq->load.weight),\n\t\t\t\tscale_load_down(cfs_rq->runnable_weight),\n\t\t\t\tcfs_rq->curr != NULL)) {\n\n\t\t___update_load_avg(&cfs_rq->avg, 1, 1);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 3402
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 3402
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_tg_cfs_propagate",
          "args": [
            "cfs_rq",
            "-(long)removed_runnable_sum"
          ],
          "line": 3397
        },
        "resolved": true,
        "details": {
          "function_name": "add_tg_cfs_propagate",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3351-3351",
          "snippet": "static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}"
        }
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&sa->util_sum",
            "r * divider"
          ],
          "line": 3395
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&sa->util_avg",
            "r"
          ],
          "line": 3394
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&sa->load_sum",
            "r * divider"
          ],
          "line": 3391
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&sa->load_avg",
            "r"
          ],
          "line": 3390
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&cfs_rq->removed.lock"
          ],
          "line": 3387
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "swap",
          "args": [
            "cfs_rq->removed.runnable_sum",
            "removed_runnable_sum"
          ],
          "line": 3385
        },
        "resolved": true,
        "details": {
          "function_name": "__migrate_swap_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "1185-1215",
          "snippet": "static void __migrate_swap_task(struct task_struct *p, int cpu)\n{\n\tif (task_on_rq_queued(p)) {\n\t\tstruct rq *src_rq, *dst_rq;\n\t\tstruct rq_flags srf, drf;\n\n\t\tsrc_rq = task_rq(p);\n\t\tdst_rq = cpu_rq(cpu);\n\n\t\trq_pin_lock(src_rq, &srf);\n\t\trq_pin_lock(dst_rq, &drf);\n\n\t\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\t\tdeactivate_task(src_rq, p, 0);\n\t\tset_task_cpu(p, cpu);\n\t\tactivate_task(dst_rq, p, 0);\n\t\tp->on_rq = TASK_ON_RQ_QUEUED;\n\t\tcheck_preempt_curr(dst_rq, p, 0);\n\n\t\trq_unpin_lock(dst_rq, &drf);\n\t\trq_unpin_lock(src_rq, &srf);\n\n\t} else {\n\t\t/*\n\t\t * Task isn't running anymore; make it appear like we migrated\n\t\t * it before it went to sleep. This means on wakeup we make the\n\t\t * previous CPU our target instead of where it really is.\n\t\t */\n\t\tp->wake_cpu = cpu;\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nstatic void __migrate_swap_task(struct task_struct *p, int cpu)\n{\n\tif (task_on_rq_queued(p)) {\n\t\tstruct rq *src_rq, *dst_rq;\n\t\tstruct rq_flags srf, drf;\n\n\t\tsrc_rq = task_rq(p);\n\t\tdst_rq = cpu_rq(cpu);\n\n\t\trq_pin_lock(src_rq, &srf);\n\t\trq_pin_lock(dst_rq, &drf);\n\n\t\tp->on_rq = TASK_ON_RQ_MIGRATING;\n\t\tdeactivate_task(src_rq, p, 0);\n\t\tset_task_cpu(p, cpu);\n\t\tactivate_task(dst_rq, p, 0);\n\t\tp->on_rq = TASK_ON_RQ_QUEUED;\n\t\tcheck_preempt_curr(dst_rq, p, 0);\n\n\t\trq_unpin_lock(dst_rq, &drf);\n\t\trq_unpin_lock(src_rq, &srf);\n\n\t} else {\n\t\t/*\n\t\t * Task isn't running anymore; make it appear like we migrated\n\t\t * it before it went to sleep. This means on wakeup we make the\n\t\t * previous CPU our target instead of where it really is.\n\t\t */\n\t\tp->wake_cpu = cpu;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&cfs_rq->removed.lock"
          ],
          "line": 3382
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_sum, removed_runnable_sum);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\n\t\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);\n\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->load_last_update_time_copy = sa->last_update_time;\n#endif\n\n\tif (decayed)\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\treturn decayed;\n}"
  },
  {
    "function_name": "add_tg_cfs_propagate",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3351-3351",
    "snippet": "static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}"
  },
  {
    "function_name": "propagate_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3346-3349",
    "snippet": "static inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\treturn 0;\n}"
  },
  {
    "function_name": "update_tg_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3344-3344",
    "snippet": "static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}"
  },
  {
    "function_name": "skip_blocked_update",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3316-3340",
    "snippet": "static inline bool skip_blocked_update(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\n\t/*\n\t * If sched_entity still have not zero load or utilization, we have to\n\t * decay it:\n\t */\n\tif (se->avg.load_avg || se->avg.util_avg)\n\t\treturn false;\n\n\t/*\n\t * If there is a pending propagation, we have to update the load and\n\t * the utilization of the sched_entity:\n\t */\n\tif (gcfs_rq->propagate)\n\t\treturn false;\n\n\t/*\n\t * Otherwise, the load and the utilization of the sched_entity is\n\t * already zero and there is no pending propagation, so it will be a\n\t * waste of time to try to decay it:\n\t */\n\treturn true;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "group_cfs_rq",
          "args": [
            "se"
          ],
          "line": 3318
        },
        "resolved": true,
        "details": {
          "function_name": "group_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "437-440",
          "snippet": "static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline bool skip_blocked_update(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\n\t/*\n\t * If sched_entity still have not zero load or utilization, we have to\n\t * decay it:\n\t */\n\tif (se->avg.load_avg || se->avg.util_avg)\n\t\treturn false;\n\n\t/*\n\t * If there is a pending propagation, we have to update the load and\n\t * the utilization of the sched_entity:\n\t */\n\tif (gcfs_rq->propagate)\n\t\treturn false;\n\n\t/*\n\t * Otherwise, the load and the utilization of the sched_entity is\n\t * already zero and there is no pending propagation, so it will be a\n\t * waste of time to try to decay it:\n\t */\n\treturn true;\n}"
  },
  {
    "function_name": "propagate_entity_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3289-3310",
    "snippet": "static inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq, *gcfs_rq;\n\n\tif (entity_is_task(se))\n\t\treturn 0;\n\n\tgcfs_rq = group_cfs_rq(se);\n\tif (!gcfs_rq->propagate)\n\t\treturn 0;\n\n\tgcfs_rq->propagate = 0;\n\n\tcfs_rq = cfs_rq_of(se);\n\n\tadd_tg_cfs_propagate(cfs_rq, gcfs_rq->prop_runnable_sum);\n\n\tupdate_tg_cfs_util(cfs_rq, se, gcfs_rq);\n\tupdate_tg_cfs_runnable(cfs_rq, se, gcfs_rq);\n\n\treturn 1;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_tg_cfs_runnable",
          "args": [
            "cfs_rq",
            "se",
            "gcfs_rq"
          ],
          "line": 3307
        },
        "resolved": true,
        "details": {
          "function_name": "update_tg_cfs_runnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3214-3280",
          "snippet": "static inline void\nupdate_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;\n\tunsigned long runnable_load_avg, load_avg;\n\tu64 runnable_load_sum, load_sum = 0;\n\ts64 delta_sum;\n\n\tif (!runnable_sum)\n\t\treturn;\n\n\tgcfs_rq->prop_runnable_sum = 0;\n\n\tif (runnable_sum >= 0) {\n\t\t/*\n\t\t * Add runnable; clip at LOAD_AVG_MAX. Reflects that until\n\t\t * the CPU is saturated running == runnable.\n\t\t */\n\t\trunnable_sum += se->avg.load_sum;\n\t\trunnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX);\n\t} else {\n\t\t/*\n\t\t * Estimate the new unweighted runnable_sum of the gcfs_rq by\n\t\t * assuming all tasks are equally runnable.\n\t\t */\n\t\tif (scale_load_down(gcfs_rq->load.weight)) {\n\t\t\tload_sum = div_s64(gcfs_rq->avg.load_sum,\n\t\t\t\tscale_load_down(gcfs_rq->load.weight));\n\t\t}\n\n\t\t/* But make sure to not inflate se's runnable */\n\t\trunnable_sum = min(se->avg.load_sum, load_sum);\n\t}\n\n\t/*\n\t * runnable_sum can't be lower than running_sum\n\t * As running sum is scale with CPU capacity wehreas the runnable sum\n\t * is not we rescale running_sum 1st\n\t */\n\trunning_sum = se->avg.util_sum /\n\t\tarch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\trunnable_sum = max(runnable_sum, running_sum);\n\n\tload_sum = (s64)se_weight(se) * runnable_sum;\n\tload_avg = div_s64(load_sum, LOAD_AVG_MAX);\n\n\tdelta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;\n\tdelta_avg = load_avg - se->avg.load_avg;\n\n\tse->avg.load_sum = runnable_sum;\n\tse->avg.load_avg = load_avg;\n\tadd_positive(&cfs_rq->avg.load_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.load_sum, delta_sum);\n\n\trunnable_load_sum = (s64)se_runnable(se) * runnable_sum;\n\trunnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX);\n\tdelta_sum = runnable_load_sum - se_weight(se) * se->avg.runnable_load_sum;\n\tdelta_avg = runnable_load_avg - se->avg.runnable_load_avg;\n\n\tse->avg.runnable_load_sum = runnable_sum;\n\tse->avg.runnable_load_avg = runnable_load_avg;\n\n\tif (se->on_rq) {\n\t\tadd_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);\n\t\tadd_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;\n\tunsigned long runnable_load_avg, load_avg;\n\tu64 runnable_load_sum, load_sum = 0;\n\ts64 delta_sum;\n\n\tif (!runnable_sum)\n\t\treturn;\n\n\tgcfs_rq->prop_runnable_sum = 0;\n\n\tif (runnable_sum >= 0) {\n\t\t/*\n\t\t * Add runnable; clip at LOAD_AVG_MAX. Reflects that until\n\t\t * the CPU is saturated running == runnable.\n\t\t */\n\t\trunnable_sum += se->avg.load_sum;\n\t\trunnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX);\n\t} else {\n\t\t/*\n\t\t * Estimate the new unweighted runnable_sum of the gcfs_rq by\n\t\t * assuming all tasks are equally runnable.\n\t\t */\n\t\tif (scale_load_down(gcfs_rq->load.weight)) {\n\t\t\tload_sum = div_s64(gcfs_rq->avg.load_sum,\n\t\t\t\tscale_load_down(gcfs_rq->load.weight));\n\t\t}\n\n\t\t/* But make sure to not inflate se's runnable */\n\t\trunnable_sum = min(se->avg.load_sum, load_sum);\n\t}\n\n\t/*\n\t * runnable_sum can't be lower than running_sum\n\t * As running sum is scale with CPU capacity wehreas the runnable sum\n\t * is not we rescale running_sum 1st\n\t */\n\trunning_sum = se->avg.util_sum /\n\t\tarch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\trunnable_sum = max(runnable_sum, running_sum);\n\n\tload_sum = (s64)se_weight(se) * runnable_sum;\n\tload_avg = div_s64(load_sum, LOAD_AVG_MAX);\n\n\tdelta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;\n\tdelta_avg = load_avg - se->avg.load_avg;\n\n\tse->avg.load_sum = runnable_sum;\n\tse->avg.load_avg = load_avg;\n\tadd_positive(&cfs_rq->avg.load_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.load_sum, delta_sum);\n\n\trunnable_load_sum = (s64)se_runnable(se) * runnable_sum;\n\trunnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX);\n\tdelta_sum = runnable_load_sum - se_weight(se) * se->avg.runnable_load_sum;\n\tdelta_avg = runnable_load_avg - se->avg.runnable_load_avg;\n\n\tse->avg.runnable_load_sum = runnable_sum;\n\tse->avg.runnable_load_avg = runnable_load_avg;\n\n\tif (se->on_rq) {\n\t\tadd_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);\n\t\tadd_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_tg_cfs_util",
          "args": [
            "cfs_rq",
            "se",
            "gcfs_rq"
          ],
          "line": 3306
        },
        "resolved": true,
        "details": {
          "function_name": "update_tg_cfs_util",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3188-3212",
          "snippet": "static inline void\nupdate_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta = gcfs_rq->avg.util_avg - se->avg.util_avg;\n\n\t/* Nothing to update */\n\tif (!delta)\n\t\treturn;\n\n\t/*\n\t * The relation between sum and avg is:\n\t *\n\t *   LOAD_AVG_MAX - 1024 + sa->period_contrib\n\t *\n\t * however, the PELT windows are not aligned between grq and gse.\n\t */\n\n\t/* Set new sched_entity's utilization */\n\tse->avg.util_avg = gcfs_rq->avg.util_avg;\n\tse->avg.util_sum = se->avg.util_avg * LOAD_AVG_MAX;\n\n\t/* Update parent cfs_rq utilization */\n\tadd_positive(&cfs_rq->avg.util_avg, delta);\n\tcfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta = gcfs_rq->avg.util_avg - se->avg.util_avg;\n\n\t/* Nothing to update */\n\tif (!delta)\n\t\treturn;\n\n\t/*\n\t * The relation between sum and avg is:\n\t *\n\t *   LOAD_AVG_MAX - 1024 + sa->period_contrib\n\t *\n\t * however, the PELT windows are not aligned between grq and gse.\n\t */\n\n\t/* Set new sched_entity's utilization */\n\tse->avg.util_avg = gcfs_rq->avg.util_avg;\n\tse->avg.util_sum = se->avg.util_avg * LOAD_AVG_MAX;\n\n\t/* Update parent cfs_rq utilization */\n\tadd_positive(&cfs_rq->avg.util_avg, delta);\n\tcfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_tg_cfs_propagate",
          "args": [
            "cfs_rq",
            "gcfs_rq->prop_runnable_sum"
          ],
          "line": 3304
        },
        "resolved": true,
        "details": {
          "function_name": "add_tg_cfs_propagate",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "3351-3351",
          "snippet": "static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 3302
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_cfs_rq",
          "args": [
            "se"
          ],
          "line": 3296
        },
        "resolved": true,
        "details": {
          "function_name": "group_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "437-440",
          "snippet": "static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 3293
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq, *gcfs_rq;\n\n\tif (entity_is_task(se))\n\t\treturn 0;\n\n\tgcfs_rq = group_cfs_rq(se);\n\tif (!gcfs_rq->propagate)\n\t\treturn 0;\n\n\tgcfs_rq->propagate = 0;\n\n\tcfs_rq = cfs_rq_of(se);\n\n\tadd_tg_cfs_propagate(cfs_rq, gcfs_rq->prop_runnable_sum);\n\n\tupdate_tg_cfs_util(cfs_rq, se, gcfs_rq);\n\tupdate_tg_cfs_runnable(cfs_rq, se, gcfs_rq);\n\n\treturn 1;\n}"
  },
  {
    "function_name": "add_tg_cfs_propagate",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3282-3286",
    "snippet": "static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum)\n{\n\tcfs_rq->propagate = 1;\n\tcfs_rq->prop_runnable_sum += runnable_sum;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum)\n{\n\tcfs_rq->propagate = 1;\n\tcfs_rq->prop_runnable_sum += runnable_sum;\n}"
  },
  {
    "function_name": "update_tg_cfs_runnable",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3214-3280",
    "snippet": "static inline void\nupdate_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;\n\tunsigned long runnable_load_avg, load_avg;\n\tu64 runnable_load_sum, load_sum = 0;\n\ts64 delta_sum;\n\n\tif (!runnable_sum)\n\t\treturn;\n\n\tgcfs_rq->prop_runnable_sum = 0;\n\n\tif (runnable_sum >= 0) {\n\t\t/*\n\t\t * Add runnable; clip at LOAD_AVG_MAX. Reflects that until\n\t\t * the CPU is saturated running == runnable.\n\t\t */\n\t\trunnable_sum += se->avg.load_sum;\n\t\trunnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX);\n\t} else {\n\t\t/*\n\t\t * Estimate the new unweighted runnable_sum of the gcfs_rq by\n\t\t * assuming all tasks are equally runnable.\n\t\t */\n\t\tif (scale_load_down(gcfs_rq->load.weight)) {\n\t\t\tload_sum = div_s64(gcfs_rq->avg.load_sum,\n\t\t\t\tscale_load_down(gcfs_rq->load.weight));\n\t\t}\n\n\t\t/* But make sure to not inflate se's runnable */\n\t\trunnable_sum = min(se->avg.load_sum, load_sum);\n\t}\n\n\t/*\n\t * runnable_sum can't be lower than running_sum\n\t * As running sum is scale with CPU capacity wehreas the runnable sum\n\t * is not we rescale running_sum 1st\n\t */\n\trunning_sum = se->avg.util_sum /\n\t\tarch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\trunnable_sum = max(runnable_sum, running_sum);\n\n\tload_sum = (s64)se_weight(se) * runnable_sum;\n\tload_avg = div_s64(load_sum, LOAD_AVG_MAX);\n\n\tdelta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;\n\tdelta_avg = load_avg - se->avg.load_avg;\n\n\tse->avg.load_sum = runnable_sum;\n\tse->avg.load_avg = load_avg;\n\tadd_positive(&cfs_rq->avg.load_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.load_sum, delta_sum);\n\n\trunnable_load_sum = (s64)se_runnable(se) * runnable_sum;\n\trunnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX);\n\tdelta_sum = runnable_load_sum - se_weight(se) * se->avg.runnable_load_sum;\n\tdelta_avg = runnable_load_avg - se->avg.runnable_load_avg;\n\n\tse->avg.runnable_load_sum = runnable_sum;\n\tse->avg.runnable_load_avg = runnable_load_avg;\n\n\tif (se->on_rq) {\n\t\tadd_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);\n\t\tadd_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "add_positive",
          "args": [
            "&cfs_rq->avg.runnable_load_sum",
            "delta_sum"
          ],
          "line": 3278
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "add_positive",
          "args": [
            "&cfs_rq->avg.runnable_load_avg",
            "delta_avg"
          ],
          "line": 3277
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "se_weight",
          "args": [
            "se"
          ],
          "line": 3270
        },
        "resolved": true,
        "details": {
          "function_name": "se_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "691-694",
          "snippet": "static inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}"
        }
      },
      {
        "call_info": {
          "callee": "div_s64",
          "args": [
            "runnable_load_sum",
            "LOAD_AVG_MAX"
          ],
          "line": 3269
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "se_runnable",
          "args": [
            "se"
          ],
          "line": 3268
        },
        "resolved": true,
        "details": {
          "function_name": "se_runnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "696-699",
          "snippet": "static inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}"
        }
      },
      {
        "call_info": {
          "callee": "add_positive",
          "args": [
            "&cfs_rq->avg.load_sum",
            "delta_sum"
          ],
          "line": 3266
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "add_positive",
          "args": [
            "&cfs_rq->avg.load_avg",
            "delta_avg"
          ],
          "line": 3265
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "div_s64",
          "args": [
            "load_sum",
            "LOAD_AVG_MAX"
          ],
          "line": 3258
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "runnable_sum",
            "running_sum"
          ],
          "line": 3255
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "arch_scale_cpu_capacity",
          "args": [
            "NULL",
            "cpu_of(rq_of(cfs_rq))"
          ],
          "line": 3254
        },
        "resolved": true,
        "details": {
          "function_name": "arch_scale_cpu_capacity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1872-1876",
          "snippet": "static __always_inline\nunsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void resched_cpu(int cpu);\n\nstatic __always_inline\nunsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 3254
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 3254
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "se->avg.load_sum",
            "load_sum"
          ],
          "line": 3245
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "div_s64",
          "args": [
            "gcfs_rq->avg.load_sum",
            "scale_load_down(gcfs_rq->load.weight)"
          ],
          "line": 3240
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "gcfs_rq->load.weight"
          ],
          "line": 3241
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "gcfs_rq->load.weight"
          ],
          "line": 3239
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;\n\tunsigned long runnable_load_avg, load_avg;\n\tu64 runnable_load_sum, load_sum = 0;\n\ts64 delta_sum;\n\n\tif (!runnable_sum)\n\t\treturn;\n\n\tgcfs_rq->prop_runnable_sum = 0;\n\n\tif (runnable_sum >= 0) {\n\t\t/*\n\t\t * Add runnable; clip at LOAD_AVG_MAX. Reflects that until\n\t\t * the CPU is saturated running == runnable.\n\t\t */\n\t\trunnable_sum += se->avg.load_sum;\n\t\trunnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX);\n\t} else {\n\t\t/*\n\t\t * Estimate the new unweighted runnable_sum of the gcfs_rq by\n\t\t * assuming all tasks are equally runnable.\n\t\t */\n\t\tif (scale_load_down(gcfs_rq->load.weight)) {\n\t\t\tload_sum = div_s64(gcfs_rq->avg.load_sum,\n\t\t\t\tscale_load_down(gcfs_rq->load.weight));\n\t\t}\n\n\t\t/* But make sure to not inflate se's runnable */\n\t\trunnable_sum = min(se->avg.load_sum, load_sum);\n\t}\n\n\t/*\n\t * runnable_sum can't be lower than running_sum\n\t * As running sum is scale with CPU capacity wehreas the runnable sum\n\t * is not we rescale running_sum 1st\n\t */\n\trunning_sum = se->avg.util_sum /\n\t\tarch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\trunnable_sum = max(runnable_sum, running_sum);\n\n\tload_sum = (s64)se_weight(se) * runnable_sum;\n\tload_avg = div_s64(load_sum, LOAD_AVG_MAX);\n\n\tdelta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;\n\tdelta_avg = load_avg - se->avg.load_avg;\n\n\tse->avg.load_sum = runnable_sum;\n\tse->avg.load_avg = load_avg;\n\tadd_positive(&cfs_rq->avg.load_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.load_sum, delta_sum);\n\n\trunnable_load_sum = (s64)se_runnable(se) * runnable_sum;\n\trunnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX);\n\tdelta_sum = runnable_load_sum - se_weight(se) * se->avg.runnable_load_sum;\n\tdelta_avg = runnable_load_avg - se->avg.runnable_load_avg;\n\n\tse->avg.runnable_load_sum = runnable_sum;\n\tse->avg.runnable_load_avg = runnable_load_avg;\n\n\tif (se->on_rq) {\n\t\tadd_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);\n\t\tadd_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);\n\t}\n}"
  },
  {
    "function_name": "update_tg_cfs_util",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3188-3212",
    "snippet": "static inline void\nupdate_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta = gcfs_rq->avg.util_avg - se->avg.util_avg;\n\n\t/* Nothing to update */\n\tif (!delta)\n\t\treturn;\n\n\t/*\n\t * The relation between sum and avg is:\n\t *\n\t *   LOAD_AVG_MAX - 1024 + sa->period_contrib\n\t *\n\t * however, the PELT windows are not aligned between grq and gse.\n\t */\n\n\t/* Set new sched_entity's utilization */\n\tse->avg.util_avg = gcfs_rq->avg.util_avg;\n\tse->avg.util_sum = se->avg.util_avg * LOAD_AVG_MAX;\n\n\t/* Update parent cfs_rq utilization */\n\tadd_positive(&cfs_rq->avg.util_avg, delta);\n\tcfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "add_positive",
          "args": [
            "&cfs_rq->avg.util_avg",
            "delta"
          ],
          "line": 3210
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta = gcfs_rq->avg.util_avg - se->avg.util_avg;\n\n\t/* Nothing to update */\n\tif (!delta)\n\t\treturn;\n\n\t/*\n\t * The relation between sum and avg is:\n\t *\n\t *   LOAD_AVG_MAX - 1024 + sa->period_contrib\n\t *\n\t * however, the PELT windows are not aligned between grq and gse.\n\t */\n\n\t/* Set new sched_entity's utilization */\n\tse->avg.util_avg = gcfs_rq->avg.util_avg;\n\tse->avg.util_sum = se->avg.util_avg * LOAD_AVG_MAX;\n\n\t/* Update parent cfs_rq utilization */\n\tadd_positive(&cfs_rq->avg.util_avg, delta);\n\tcfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;\n}"
  },
  {
    "function_name": "set_task_rq_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3075-3117",
    "snippet": "void set_task_rq_fair(struct sched_entity *se,\n\t\t      struct cfs_rq *prev, struct cfs_rq *next)\n{\n\tu64 p_last_update_time;\n\tu64 n_last_update_time;\n\n\tif (!sched_feat(ATTACH_AGE_LOAD))\n\t\treturn;\n\n\t/*\n\t * We are supposed to update the task to \"current\" time, then its up to\n\t * date and ready to go to new CPU/cfs_rq. But we have difficulty in\n\t * getting what current time is, so simply throw away the out-of-date\n\t * time. This will result in the wakee task is less decayed, but giving\n\t * the wakee more load sounds not bad.\n\t */\n\tif (!(se->avg.last_update_time && prev))\n\t\treturn;\n\n#ifndef CONFIG_64BIT\n\t{\n\t\tu64 p_last_update_time_copy;\n\t\tu64 n_last_update_time_copy;\n\n\t\tdo {\n\t\t\tp_last_update_time_copy = prev->load_last_update_time_copy;\n\t\t\tn_last_update_time_copy = next->load_last_update_time_copy;\n\n\t\t\tsmp_rmb();\n\n\t\t\tp_last_update_time = prev->avg.last_update_time;\n\t\t\tn_last_update_time = next->avg.last_update_time;\n\n\t\t} while (p_last_update_time != p_last_update_time_copy ||\n\t\t\t n_last_update_time != n_last_update_time_copy);\n\t}\n#else\n\tp_last_update_time = prev->avg.last_update_time;\n\tn_last_update_time = next->avg.last_update_time;\n#endif\n\t__update_load_avg_blocked_se(p_last_update_time, cpu_of(rq_of(prev)), se);\n\tse->avg.last_update_time = n_last_update_time;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__update_load_avg_blocked_se",
          "args": [
            "p_last_update_time",
            "cpu_of(rq_of(prev))",
            "se"
          ],
          "line": 3115
        },
        "resolved": true,
        "details": {
          "function_name": "__update_load_avg_blocked_se",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/pelt.c",
          "lines": "270-278",
          "snippet": "int __update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se)\n{\n\tif (___update_load_sum(now, cpu, &se->avg, 0, 0, 0)) {\n\t\t___update_load_avg(&se->avg, se_weight(se), se_runnable(se));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"pelt.h\"",
            "#include \"sched-pelt.h\"",
            "#include \"sched.h\"",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"pelt.h\"\n#include \"sched-pelt.h\"\n#include \"sched.h\"\n#include <linux/sched.h>\n\nint __update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se)\n{\n\tif (___update_load_sum(now, cpu, &se->avg, 0, 0, 0)) {\n\t\t___update_load_avg(&se->avg, se_weight(se), se_runnable(se));\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq_of(prev)"
          ],
          "line": 3115
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "prev"
          ],
          "line": 3115
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_rmb",
          "args": [],
          "line": 3103
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_feat",
          "args": [
            "ATTACH_AGE_LOAD"
          ],
          "line": 3081
        },
        "resolved": true,
        "details": {
          "function_name": "sched_feat_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/debug.c",
          "lines": "104-127",
          "snippet": "static int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static const char * const sched_feat_names[] = {\n#include \"features.h\"\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid set_task_rq_fair(struct sched_entity *se,\n\t\t      struct cfs_rq *prev, struct cfs_rq *next)\n{\n\tu64 p_last_update_time;\n\tu64 n_last_update_time;\n\n\tif (!sched_feat(ATTACH_AGE_LOAD))\n\t\treturn;\n\n\t/*\n\t * We are supposed to update the task to \"current\" time, then its up to\n\t * date and ready to go to new CPU/cfs_rq. But we have difficulty in\n\t * getting what current time is, so simply throw away the out-of-date\n\t * time. This will result in the wakee task is less decayed, but giving\n\t * the wakee more load sounds not bad.\n\t */\n\tif (!(se->avg.last_update_time && prev))\n\t\treturn;\n\n#ifndef CONFIG_64BIT\n\t{\n\t\tu64 p_last_update_time_copy;\n\t\tu64 n_last_update_time_copy;\n\n\t\tdo {\n\t\t\tp_last_update_time_copy = prev->load_last_update_time_copy;\n\t\t\tn_last_update_time_copy = next->load_last_update_time_copy;\n\n\t\t\tsmp_rmb();\n\n\t\t\tp_last_update_time = prev->avg.last_update_time;\n\t\t\tn_last_update_time = next->avg.last_update_time;\n\n\t\t} while (p_last_update_time != p_last_update_time_copy ||\n\t\t\t n_last_update_time != n_last_update_time_copy);\n\t}\n#else\n\tp_last_update_time = prev->avg.last_update_time;\n\tn_last_update_time = next->avg.last_update_time;\n#endif\n\t__update_load_avg_blocked_se(p_last_update_time, cpu_of(rq_of(prev)), se);\n\tse->avg.last_update_time = n_last_update_time;\n}"
  },
  {
    "function_name": "update_tg_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3054-3068",
    "snippet": "static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n\tlong delta = cfs_rq->avg.load_avg - cfs_rq->tg_load_avg_contrib;\n\n\t/*\n\t * No need to update load_avg for root_task_group as it is not used.\n\t */\n\tif (cfs_rq->tg == &root_task_group)\n\t\treturn;\n\n\tif (force || abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {\n\t\tatomic_long_add(delta, &cfs_rq->tg->load_avg);\n\t\tcfs_rq->tg_load_avg_contrib = cfs_rq->avg.load_avg;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_long_add",
          "args": [
            "delta",
            "&cfs_rq->tg->load_avg"
          ],
          "line": 3065
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "abs",
          "args": [
            "delta"
          ],
          "line": 3064
        },
        "resolved": true,
        "details": {
          "function_name": "torture_shutdown_absorb",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/torture.c",
          "lines": "462-469",
          "snippet": "void torture_shutdown_absorb(const char *title)\n{\n\twhile (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {\n\t\tpr_notice(\"torture thread %s parking due to system shutdown\\n\",\n\t\t\t  title);\n\t\tschedule_timeout_uninterruptible(MAX_SCHEDULE_TIMEOUT);\n\t}\n}",
          "includes": [
            "#include \"rcu/rcu.h\"",
            "#include <linux/torture.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/ktime.h>",
            "#include <linux/trace_clock.h>",
            "#include <linux/slab.h>",
            "#include <linux/stat.h>",
            "#include <linux/delay.h>",
            "#include <linux/cpu.h>",
            "#include <linux/freezer.h>",
            "#include <linux/reboot.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/completion.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/err.h>",
            "#include <linux/kthread.h>",
            "#include <linux/module.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [
            "#define FULLSTOP_SHUTDOWN 1\t/* System shutdown with torture running. */"
          ],
          "globals_used": [
            "static int fullstop = FULLSTOP_RMMOD;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu/rcu.h\"\n#include <linux/torture.h>\n#include <asm/byteorder.h>\n#include <linux/ktime.h>\n#include <linux/trace_clock.h>\n#include <linux/slab.h>\n#include <linux/stat.h>\n#include <linux/delay.h>\n#include <linux/cpu.h>\n#include <linux/freezer.h>\n#include <linux/reboot.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/moduleparam.h>\n#include <linux/completion.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/err.h>\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\n#define FULLSTOP_SHUTDOWN 1\t/* System shutdown with torture running. */\n\nstatic int fullstop = FULLSTOP_RMMOD;\n\nvoid torture_shutdown_absorb(const char *title)\n{\n\twhile (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {\n\t\tpr_notice(\"torture thread %s parking due to system shutdown\\n\",\n\t\t\t  title);\n\t\tschedule_timeout_uninterruptible(MAX_SCHEDULE_TIMEOUT);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n\tlong delta = cfs_rq->avg.load_avg - cfs_rq->tg_load_avg_contrib;\n\n\t/*\n\t * No need to update load_avg for root_task_group as it is not used.\n\t */\n\tif (cfs_rq->tg == &root_task_group)\n\t\treturn;\n\n\tif (force || abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {\n\t\tatomic_long_add(delta, &cfs_rq->tg->load_avg);\n\t\tcfs_rq->tg_load_avg_contrib = cfs_rq->avg.load_avg;\n\t}\n}"
  },
  {
    "function_name": "cfs_rq_util_change",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3014-3035",
    "snippet": "static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cpufreq_update_util",
          "args": [
            "rq",
            "flags"
          ],
          "line": 3033
        },
        "resolved": true,
        "details": {
          "function_name": "cpufreq_update_util",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "2200-2200",
          "snippet": "static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void activate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);",
            "extern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void activate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 3016
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq || (flags & SCHED_CPUFREQ_MIGRATION)) {\n\t\t/*\n\t\t * There are a few boundary cases this might miss but it should\n\t\t * get called often enough that that should (hopefully) not be\n\t\t * a real problem.\n\t\t *\n\t\t * It will not get called when we go idle, because the idle\n\t\t * thread is a different class (!fair), nor will the utilization\n\t\t * number include things like RT tasks.\n\t\t *\n\t\t * As is, the util number is not freq-invariant (we'd have to\n\t\t * implement arch_scale_freq_capacity() for that).\n\t\t *\n\t\t * See cpu_util().\n\t\t */\n\t\tcpufreq_update_util(rq, flags);\n\t}\n}"
  },
  {
    "function_name": "update_cfs_group",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "3009-3011",
    "snippet": "static inline void update_cfs_group(struct sched_entity *se)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}"
  },
  {
    "function_name": "update_cfs_group",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2984-3006",
    "snippet": "static void update_cfs_group(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\tlong shares, runnable;\n\n\tif (!gcfs_rq)\n\t\treturn;\n\n\tif (throttled_hierarchy(gcfs_rq))\n\t\treturn;\n\n#ifndef CONFIG_SMP\n\trunnable = shares = READ_ONCE(gcfs_rq->tg->shares);\n\n\tif (likely(se->load.weight == shares))\n\t\treturn;\n#else\n\tshares   = calc_group_shares(gcfs_rq);\n\trunnable = calc_group_runnable(gcfs_rq, shares);\n#endif\n\n\treweight_entity(cfs_rq_of(se), se, shares, runnable);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "reweight_entity",
          "args": [
            "cfs_rq_of(se)",
            "se",
            "shares",
            "runnable"
          ],
          "line": 3005
        },
        "resolved": true,
        "details": {
          "function_name": "reweight_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2781-2811",
          "snippet": "static void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 3005
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "calc_group_runnable",
          "args": [
            "gcfs_rq",
            "shares"
          ],
          "line": 3002
        },
        "resolved": true,
        "details": {
          "function_name": "calc_group_runnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2960-2975",
          "snippet": "static long calc_group_runnable(struct cfs_rq *cfs_rq, long shares)\n{\n\tlong runnable, load_avg;\n\n\tload_avg = max(cfs_rq->avg.load_avg,\n\t\t       scale_load_down(cfs_rq->load.weight));\n\n\trunnable = max(cfs_rq->avg.runnable_load_avg,\n\t\t       scale_load_down(cfs_rq->runnable_weight));\n\n\trunnable *= shares;\n\tif (load_avg)\n\t\trunnable /= load_avg;\n\n\treturn clamp_t(long, runnable, MIN_SHARES, shares);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic long calc_group_runnable(struct cfs_rq *cfs_rq, long shares)\n{\n\tlong runnable, load_avg;\n\n\tload_avg = max(cfs_rq->avg.load_avg,\n\t\t       scale_load_down(cfs_rq->load.weight));\n\n\trunnable = max(cfs_rq->avg.runnable_load_avg,\n\t\t       scale_load_down(cfs_rq->runnable_weight));\n\n\trunnable *= shares;\n\tif (load_avg)\n\t\trunnable /= load_avg;\n\n\treturn clamp_t(long, runnable, MIN_SHARES, shares);\n}"
        }
      },
      {
        "call_info": {
          "callee": "calc_group_shares",
          "args": [
            "gcfs_rq"
          ],
          "line": 3001
        },
        "resolved": true,
        "details": {
          "function_name": "calc_group_shares",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2899-2931",
          "snippet": "static long calc_group_shares(struct cfs_rq *cfs_rq)\n{\n\tlong tg_weight, tg_shares, load, shares;\n\tstruct task_group *tg = cfs_rq->tg;\n\n\ttg_shares = READ_ONCE(tg->shares);\n\n\tload = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);\n\n\ttg_weight = atomic_long_read(&tg->load_avg);\n\n\t/* Ensure tg_weight >= load */\n\ttg_weight -= cfs_rq->tg_load_avg_contrib;\n\ttg_weight += load;\n\n\tshares = (tg_shares * load);\n\tif (tg_weight)\n\t\tshares /= tg_weight;\n\n\t/*\n\t * MIN_SHARES has to be unscaled here to support per-CPU partitioning\n\t * of a group with small tg->shares value. It is a floor value which is\n\t * assigned as a minimum load.weight to the sched_entity representing\n\t * the group on a CPU.\n\t *\n\t * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024\n\t * on an 8-core system with 8 tasks each runnable on one CPU shares has\n\t * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In\n\t * case no task is runnable on a CPU MIN_SHARES=2 should be returned\n\t * instead of 0.\n\t */\n\treturn clamp_t(long, shares, MIN_SHARES, tg_shares);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic long calc_group_shares(struct cfs_rq *cfs_rq)\n{\n\tlong tg_weight, tg_shares, load, shares;\n\tstruct task_group *tg = cfs_rq->tg;\n\n\ttg_shares = READ_ONCE(tg->shares);\n\n\tload = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);\n\n\ttg_weight = atomic_long_read(&tg->load_avg);\n\n\t/* Ensure tg_weight >= load */\n\ttg_weight -= cfs_rq->tg_load_avg_contrib;\n\ttg_weight += load;\n\n\tshares = (tg_shares * load);\n\tif (tg_weight)\n\t\tshares /= tg_weight;\n\n\t/*\n\t * MIN_SHARES has to be unscaled here to support per-CPU partitioning\n\t * of a group with small tg->shares value. It is a floor value which is\n\t * assigned as a minimum load.weight to the sched_entity representing\n\t * the group on a CPU.\n\t *\n\t * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024\n\t * on an 8-core system with 8 tasks each runnable on one CPU shares has\n\t * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In\n\t * case no task is runnable on a CPU MIN_SHARES=2 should be returned\n\t * instead of 0.\n\t */\n\treturn clamp_t(long, shares, MIN_SHARES, tg_shares);\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "se->load.weight == shares"
          ],
          "line": 2998
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "gcfs_rq->tg->shares"
          ],
          "line": 2996
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "throttled_hierarchy",
          "args": [
            "gcfs_rq"
          ],
          "line": 2992
        },
        "resolved": true,
        "details": {
          "function_name": "throttled_hierarchy",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4996-4999",
          "snippet": "static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_cfs_rq",
          "args": [
            "se"
          ],
          "line": 2986
        },
        "resolved": true,
        "details": {
          "function_name": "group_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "437-440",
          "snippet": "static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_cfs_group(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\tlong shares, runnable;\n\n\tif (!gcfs_rq)\n\t\treturn;\n\n\tif (throttled_hierarchy(gcfs_rq))\n\t\treturn;\n\n#ifndef CONFIG_SMP\n\trunnable = shares = READ_ONCE(gcfs_rq->tg->shares);\n\n\tif (likely(se->load.weight == shares))\n\t\treturn;\n#else\n\tshares   = calc_group_shares(gcfs_rq);\n\trunnable = calc_group_runnable(gcfs_rq, shares);\n#endif\n\n\treweight_entity(cfs_rq_of(se), se, shares, runnable);\n}"
  },
  {
    "function_name": "calc_group_runnable",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2960-2975",
    "snippet": "static long calc_group_runnable(struct cfs_rq *cfs_rq, long shares)\n{\n\tlong runnable, load_avg;\n\n\tload_avg = max(cfs_rq->avg.load_avg,\n\t\t       scale_load_down(cfs_rq->load.weight));\n\n\trunnable = max(cfs_rq->avg.runnable_load_avg,\n\t\t       scale_load_down(cfs_rq->runnable_weight));\n\n\trunnable *= shares;\n\tif (load_avg)\n\t\trunnable /= load_avg;\n\n\treturn clamp_t(long, runnable, MIN_SHARES, shares);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "clamp_t",
          "args": [
            "long",
            "runnable",
            "MIN_SHARES",
            "shares"
          ],
          "line": 2974
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "cfs_rq->avg.runnable_load_avg",
            "scale_load_down(cfs_rq->runnable_weight)"
          ],
          "line": 2967
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "cfs_rq->runnable_weight"
          ],
          "line": 2968
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "cfs_rq->load.weight"
          ],
          "line": 2965
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic long calc_group_runnable(struct cfs_rq *cfs_rq, long shares)\n{\n\tlong runnable, load_avg;\n\n\tload_avg = max(cfs_rq->avg.load_avg,\n\t\t       scale_load_down(cfs_rq->load.weight));\n\n\trunnable = max(cfs_rq->avg.runnable_load_avg,\n\t\t       scale_load_down(cfs_rq->runnable_weight));\n\n\trunnable *= shares;\n\tif (load_avg)\n\t\trunnable /= load_avg;\n\n\treturn clamp_t(long, runnable, MIN_SHARES, shares);\n}"
  },
  {
    "function_name": "calc_group_shares",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2899-2931",
    "snippet": "static long calc_group_shares(struct cfs_rq *cfs_rq)\n{\n\tlong tg_weight, tg_shares, load, shares;\n\tstruct task_group *tg = cfs_rq->tg;\n\n\ttg_shares = READ_ONCE(tg->shares);\n\n\tload = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);\n\n\ttg_weight = atomic_long_read(&tg->load_avg);\n\n\t/* Ensure tg_weight >= load */\n\ttg_weight -= cfs_rq->tg_load_avg_contrib;\n\ttg_weight += load;\n\n\tshares = (tg_shares * load);\n\tif (tg_weight)\n\t\tshares /= tg_weight;\n\n\t/*\n\t * MIN_SHARES has to be unscaled here to support per-CPU partitioning\n\t * of a group with small tg->shares value. It is a floor value which is\n\t * assigned as a minimum load.weight to the sched_entity representing\n\t * the group on a CPU.\n\t *\n\t * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024\n\t * on an 8-core system with 8 tasks each runnable on one CPU shares has\n\t * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In\n\t * case no task is runnable on a CPU MIN_SHARES=2 should be returned\n\t * instead of 0.\n\t */\n\treturn clamp_t(long, shares, MIN_SHARES, tg_shares);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "clamp_t",
          "args": [
            "long",
            "shares",
            "MIN_SHARES",
            "tg_shares"
          ],
          "line": 2930
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_long_read",
          "args": [
            "&tg->load_avg"
          ],
          "line": 2908
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "scale_load_down(cfs_rq->load.weight)",
            "cfs_rq->avg.load_avg"
          ],
          "line": 2906
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "cfs_rq->load.weight"
          ],
          "line": 2906
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "tg->shares"
          ],
          "line": 2904
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic long calc_group_shares(struct cfs_rq *cfs_rq)\n{\n\tlong tg_weight, tg_shares, load, shares;\n\tstruct task_group *tg = cfs_rq->tg;\n\n\ttg_shares = READ_ONCE(tg->shares);\n\n\tload = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);\n\n\ttg_weight = atomic_long_read(&tg->load_avg);\n\n\t/* Ensure tg_weight >= load */\n\ttg_weight -= cfs_rq->tg_load_avg_contrib;\n\ttg_weight += load;\n\n\tshares = (tg_shares * load);\n\tif (tg_weight)\n\t\tshares /= tg_weight;\n\n\t/*\n\t * MIN_SHARES has to be unscaled here to support per-CPU partitioning\n\t * of a group with small tg->shares value. It is a floor value which is\n\t * assigned as a minimum load.weight to the sched_entity representing\n\t * the group on a CPU.\n\t *\n\t * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024\n\t * on an 8-core system with 8 tasks each runnable on one CPU shares has\n\t * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In\n\t * case no task is runnable on a CPU MIN_SHARES=2 should be returned\n\t * instead of 0.\n\t */\n\treturn clamp_t(long, shares, MIN_SHARES, tg_shares);\n}"
  },
  {
    "function_name": "reweight_task",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2813-2822",
    "snippet": "void reweight_task(struct task_struct *p, int prio)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct load_weight *load = &se->load;\n\tunsigned long weight = scale_load(sched_prio_to_weight[prio]);\n\n\treweight_entity(cfs_rq, se, weight, weight);\n\tload->inv_weight = sched_prio_to_wmult[prio];\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "reweight_entity",
          "args": [
            "cfs_rq",
            "se",
            "weight",
            "weight"
          ],
          "line": 2820
        },
        "resolved": true,
        "details": {
          "function_name": "reweight_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2781-2811",
          "snippet": "static void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "scale_load",
          "args": [
            "sched_prio_to_weight[prio]"
          ],
          "line": 2818
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 2816
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid reweight_task(struct task_struct *p, int prio)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct load_weight *load = &se->load;\n\tunsigned long weight = scale_load(sched_prio_to_weight[prio]);\n\n\treweight_entity(cfs_rq, se, weight, weight);\n\tload->inv_weight = sched_prio_to_wmult[prio];\n}"
  },
  {
    "function_name": "reweight_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2781-2811",
    "snippet": "static void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "enqueue_runnable_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 2809
        },
        "resolved": true,
        "details": {
          "function_name": "enqueue_runnable_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2771-2772",
          "snippet": "static inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      },
      {
        "call_info": {
          "callee": "account_entity_enqueue",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 2808
        },
        "resolved": true,
        "details": {
          "function_name": "account_entity_enqueue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2668-2683",
          "snippet": "static void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}"
        }
      },
      {
        "call_info": {
          "callee": "enqueue_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 2806
        },
        "resolved": true,
        "details": {
          "function_name": "enqueue_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2775-2776",
          "snippet": "static inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      },
      {
        "call_info": {
          "callee": "div_u64",
          "args": [
            "se_runnable(se) * se->avg.runnable_load_sum",
            "divider"
          ],
          "line": 2802
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "se_runnable",
          "args": [
            "se"
          ],
          "line": 2802
        },
        "resolved": true,
        "details": {
          "function_name": "se_runnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "696-699",
          "snippet": "static inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}"
        }
      },
      {
        "call_info": {
          "callee": "div_u64",
          "args": [
            "se_weight(se) * se->avg.load_sum",
            "divider"
          ],
          "line": 2800
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "se_weight",
          "args": [
            "se"
          ],
          "line": 2800
        },
        "resolved": true,
        "details": {
          "function_name": "se_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "691-694",
          "snippet": "static inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_set",
          "args": [
            "&se->load",
            "weight"
          ],
          "line": 2794
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_set",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "133-137",
          "snippet": "static inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "dequeue_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 2791
        },
        "resolved": true,
        "details": {
          "function_name": "dequeue_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2777-2778",
          "snippet": "static inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      },
      {
        "call_info": {
          "callee": "dequeue_runnable_load_avg",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 2789
        },
        "resolved": true,
        "details": {
          "function_name": "dequeue_runnable_load_avg",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2773-2774",
          "snippet": "static inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
        }
      },
      {
        "call_info": {
          "callee": "account_entity_dequeue",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 2788
        },
        "resolved": true,
        "details": {
          "function_name": "account_entity_dequeue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2685-2698",
          "snippet": "static void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_curr",
          "args": [
            "cfs_rq"
          ],
          "line": 2787
        },
        "resolved": true,
        "details": {
          "function_name": "update_curr_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "838-841",
          "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight, unsigned long runnable)\n{\n\tif (se->on_rq) {\n\t\t/* commit outstanding execution time */\n\t\tif (cfs_rq->curr == se)\n\t\t\tupdate_curr(cfs_rq);\n\t\taccount_entity_dequeue(cfs_rq, se);\n\t\tdequeue_runnable_load_avg(cfs_rq, se);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tse->runnable_weight = runnable;\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t\tse->avg.runnable_load_avg =\n\t\t\tdiv_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\taccount_entity_enqueue(cfs_rq, se);\n\t\tenqueue_runnable_load_avg(cfs_rq, se);\n\t}\n}"
  },
  {
    "function_name": "dequeue_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2777-2778",
    "snippet": "static inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
  },
  {
    "function_name": "enqueue_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2775-2776",
    "snippet": "static inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
  },
  {
    "function_name": "dequeue_runnable_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2773-2774",
    "snippet": "static inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
  },
  {
    "function_name": "enqueue_runnable_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2771-2772",
    "snippet": "static inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }"
  },
  {
    "function_name": "dequeue_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2764-2769",
    "snippet": "static inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tsub_positive(&cfs_rq->avg.load_avg, se->avg.load_avg);\n\tsub_positive(&cfs_rq->avg.load_sum, se_weight(se) * se->avg.load_sum);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&cfs_rq->avg.load_sum",
            "se_weight(se) * se->avg.load_sum"
          ],
          "line": 2768
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "se_weight",
          "args": [
            "se"
          ],
          "line": 2768
        },
        "resolved": true,
        "details": {
          "function_name": "se_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "691-694",
          "snippet": "static inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&cfs_rq->avg.load_avg",
            "se->avg.load_avg"
          ],
          "line": 2767
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tsub_positive(&cfs_rq->avg.load_avg, se->avg.load_avg);\n\tsub_positive(&cfs_rq->avg.load_sum, se_weight(se) * se->avg.load_sum);\n}"
  },
  {
    "function_name": "enqueue_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2757-2762",
    "snippet": "static inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->avg.load_avg += se->avg.load_avg;\n\tcfs_rq->avg.load_sum += se_weight(se) * se->avg.load_sum;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "se_weight",
          "args": [
            "se"
          ],
          "line": 2761
        },
        "resolved": true,
        "details": {
          "function_name": "se_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "691-694",
          "snippet": "static inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->avg.load_avg += se->avg.load_avg;\n\tcfs_rq->avg.load_sum += se_weight(se) * se->avg.load_sum;\n}"
  },
  {
    "function_name": "dequeue_runnable_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2747-2755",
    "snippet": "static inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight -= se->runnable_weight;\n\n\tsub_positive(&cfs_rq->avg.runnable_load_avg, se->avg.runnable_load_avg);\n\tsub_positive(&cfs_rq->avg.runnable_load_sum,\n\t\t     se_runnable(se) * se->avg.runnable_load_sum);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&cfs_rq->avg.runnable_load_sum",
            "se_runnable(se) * se->avg.runnable_load_sum"
          ],
          "line": 2753
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "se_runnable",
          "args": [
            "se"
          ],
          "line": 2754
        },
        "resolved": true,
        "details": {
          "function_name": "se_runnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "696-699",
          "snippet": "static inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}"
        }
      },
      {
        "call_info": {
          "callee": "sub_positive",
          "args": [
            "&cfs_rq->avg.runnable_load_avg",
            "se->avg.runnable_load_avg"
          ],
          "line": 2752
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\ndequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight -= se->runnable_weight;\n\n\tsub_positive(&cfs_rq->avg.runnable_load_avg, se->avg.runnable_load_avg);\n\tsub_positive(&cfs_rq->avg.runnable_load_sum,\n\t\t     se_runnable(se) * se->avg.runnable_load_sum);\n}"
  },
  {
    "function_name": "enqueue_runnable_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2738-2745",
    "snippet": "static inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight += se->runnable_weight;\n\n\tcfs_rq->avg.runnable_load_avg += se->avg.runnable_load_avg;\n\tcfs_rq->avg.runnable_load_sum += se_runnable(se) * se->avg.runnable_load_sum;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "se_runnable",
          "args": [
            "se"
          ],
          "line": 2744
        },
        "resolved": true,
        "details": {
          "function_name": "se_runnable",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "696-699",
          "snippet": "static inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern void init_entity_runnable_average(struct sched_entity *se);",
            "extern void post_init_entity_util_avg(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct sched_entity *se);\n\nstatic inline long se_runnable(struct sched_entity *se)\n{\n\treturn scale_load_down(se->runnable_weight);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nenqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->runnable_weight += se->runnable_weight;\n\n\tcfs_rq->avg.runnable_load_avg += se->avg.runnable_load_avg;\n\tcfs_rq->avg.runnable_load_sum += se_runnable(se) * se->avg.runnable_load_sum;\n}"
  },
  {
    "function_name": "account_entity_dequeue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2685-2698",
    "snippet": "static void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "list_del_init",
          "args": [
            "&se->group_node"
          ],
          "line": 2694
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "account_numa_dequeue",
          "args": [
            "rq_of(cfs_rq)",
            "task_of(se)"
          ],
          "line": 2693
        },
        "resolved": true,
        "details": {
          "function_name": "account_numa_dequeue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2658-2660",
          "snippet": "static inline void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 2693
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 2693
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 2692
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_load_sub",
          "args": [
            "&rq_of(cfs_rq)->load",
            "se->load.weight"
          ],
          "line": 2690
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_sub",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "127-131",
          "snippet": "static inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "parent_entity",
          "args": [
            "se"
          ],
          "line": 2689
        },
        "resolved": true,
        "details": {
          "function_name": "parent_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "453-456",
          "snippet": "static inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_sub(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n}"
  },
  {
    "function_name": "account_entity_enqueue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2668-2683",
    "snippet": "static void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "list_add",
          "args": [
            "&se->group_node",
            "&rq->cfs_tasks"
          ],
          "line": 2679
        },
        "resolved": true,
        "details": {
          "function_name": "list_add_event",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/events/core.c",
          "lines": "1662-1690",
          "snippet": "static void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\tevent->tstamp = perf_event_time(event);\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tevent->group_caps = event->event_caps;\n\t\tadd_event_to_groups(event, ctx);\n\t}\n\n\tlist_update_cgroup_event(event, ctx, true);\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n\n\tctx->generation++;\n}",
          "includes": [
            "#include <asm/irq_regs.h>",
            "#include \"internal.h\"",
            "#include <linux/mount.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/parser.h>",
            "#include <linux/namei.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf.h>",
            "#include <linux/compat.h>",
            "#include <linux/mman.h>",
            "#include <linux/module.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <linux/trace_events.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/anon_inodes.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/rculist.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/export.h>",
            "#include <linux/device.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/reboot.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/percpu.h>",
            "#include <linux/dcache.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/tick.h>",
            "#include <linux/hash.h>",
            "#include <linux/slab.h>",
            "#include <linux/poll.h>",
            "#include <linux/file.h>",
            "#include <linux/idr.h>",
            "#include <linux/smp.h>",
            "#include <linux/cpu.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void update_context_time(struct perf_event_context *ctx);",
            "static u64 perf_event_time(struct perf_event *event);",
            "static __must_check struct",
            "static void perf_log_itrace_start(struct perf_event *event);",
            "static void perf_event_free_filter(struct perf_event *event);",
            "static void perf_event_free_bpf_prog(struct perf_event *event);",
            "static int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr);",
            "static void perf_pmu_output_stop(struct perf_event *event);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <asm/irq_regs.h>\n#include \"internal.h\"\n#include <linux/mount.h>\n#include <linux/proc_ns.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/clock.h>\n#include <linux/parser.h>\n#include <linux/namei.h>\n#include <linux/filter.h>\n#include <linux/bpf.h>\n#include <linux/compat.h>\n#include <linux/mman.h>\n#include <linux/module.h>\n#include <linux/mm_types.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/trace_events.h>\n#include <linux/perf_event.h>\n#include <linux/cgroup.h>\n#include <linux/kernel_stat.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/rculist.h>\n#include <linux/hardirq.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <linux/device.h>\n#include <linux/vmstat.h>\n#include <linux/reboot.h>\n#include <linux/ptrace.h>\n#include <linux/percpu.h>\n#include <linux/dcache.h>\n#include <linux/sysfs.h>\n#include <linux/tick.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/idr.h>\n#include <linux/smp.h>\n#include <linux/cpu.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n\nstatic void update_context_time(struct perf_event_context *ctx);\nstatic u64 perf_event_time(struct perf_event *event);\nstatic __must_check struct;\nstatic void perf_log_itrace_start(struct perf_event *event);\nstatic void perf_event_free_filter(struct perf_event *event);\nstatic void perf_event_free_bpf_prog(struct perf_event *event);\nstatic int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr);\nstatic void perf_pmu_output_stop(struct perf_event *event);\n\nstatic void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\tevent->tstamp = perf_event_time(event);\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tevent->group_caps = event->event_caps;\n\t\tadd_event_to_groups(event, ctx);\n\t}\n\n\tlist_update_cgroup_event(event, ctx, true);\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n\n\tctx->generation++;\n}"
        }
      },
      {
        "call_info": {
          "callee": "account_numa_enqueue",
          "args": [
            "rq",
            "task_of(se)"
          ],
          "line": 2678
        },
        "resolved": true,
        "details": {
          "function_name": "account_numa_enqueue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2654-2656",
          "snippet": "static inline void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 2678
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 2676
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 2675
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_load_add",
          "args": [
            "&rq_of(cfs_rq)->load",
            "se->load.weight"
          ],
          "line": 2673
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_add",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "121-125",
          "snippet": "static inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "parent_entity",
          "args": [
            "se"
          ],
          "line": 2672
        },
        "resolved": true,
        "details": {
          "function_name": "parent_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "453-456",
          "snippet": "static inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\tif (!parent_entity(se))\n\t\tupdate_load_add(&rq_of(cfs_rq)->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n}"
  },
  {
    "function_name": "update_scan_period",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2662-2664",
    "snippet": "static inline void update_scan_period(struct task_struct *p, int new_cpu)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_scan_period(struct task_struct *p, int new_cpu)\n{\n}"
  },
  {
    "function_name": "account_numa_dequeue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2658-2660",
    "snippet": "static inline void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n}"
  },
  {
    "function_name": "account_numa_enqueue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2654-2656",
    "snippet": "static inline void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n}"
  },
  {
    "function_name": "task_tick_numa",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2650-2652",
    "snippet": "static void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n}"
  },
  {
    "function_name": "update_scan_period",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2616-2647",
    "snippet": "static void update_scan_period(struct task_struct *p, int new_cpu)\n{\n\tint src_nid = cpu_to_node(task_cpu(p));\n\tint dst_nid = cpu_to_node(new_cpu);\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\tif (!p->mm || !p->numa_faults || (p->flags & PF_EXITING))\n\t\treturn;\n\n\tif (src_nid == dst_nid)\n\t\treturn;\n\n\t/*\n\t * Allow resets if faults have been trapped before one scan\n\t * has completed. This is most likely due to a new task that\n\t * is pulled cross-node due to wakeups or load balancing.\n\t */\n\tif (p->numa_scan_seq) {\n\t\t/*\n\t\t * Avoid scan adjustments if moving to the preferred\n\t\t * node or if the task was not previously running on\n\t\t * the preferred node.\n\t\t */\n\t\tif (dst_nid == p->numa_preferred_nid ||\n\t\t    (p->numa_preferred_nid != -1 && src_nid != p->numa_preferred_nid))\n\t\t\treturn;\n\t}\n\n\tp->numa_scan_period = task_scan_start(p);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_scan_start",
          "args": [
            "p"
          ],
          "line": 2646
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_start",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1097-1114",
          "snippet": "static unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}"
        }
      },
      {
        "call_info": {
          "callee": "static_branch_likely",
          "args": [
            "&sched_numa_balancing"
          ],
          "line": 2621
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "new_cpu"
          ],
          "line": 2619
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "task_cpu(p)"
          ],
          "line": 2618
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_cpu",
          "args": [
            "p"
          ],
          "line": 2618
        },
        "resolved": true,
        "details": {
          "function_name": "ignore_task_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/ftrace.c",
          "lines": "6542-6556",
          "snippet": "static void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/suspend.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_MUTEX(ftrace_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nstatic DEFINE_MUTEX(ftrace_lock);\n\nstatic void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void update_scan_period(struct task_struct *p, int new_cpu)\n{\n\tint src_nid = cpu_to_node(task_cpu(p));\n\tint dst_nid = cpu_to_node(new_cpu);\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\tif (!p->mm || !p->numa_faults || (p->flags & PF_EXITING))\n\t\treturn;\n\n\tif (src_nid == dst_nid)\n\t\treturn;\n\n\t/*\n\t * Allow resets if faults have been trapped before one scan\n\t * has completed. This is most likely due to a new task that\n\t * is pulled cross-node due to wakeups or load balancing.\n\t */\n\tif (p->numa_scan_seq) {\n\t\t/*\n\t\t * Avoid scan adjustments if moving to the preferred\n\t\t * node or if the task was not previously running on\n\t\t * the preferred node.\n\t\t */\n\t\tif (dst_nid == p->numa_preferred_nid ||\n\t\t    (p->numa_preferred_nid != -1 && src_nid != p->numa_preferred_nid))\n\t\t\treturn;\n\t}\n\n\tp->numa_scan_period = task_scan_start(p);\n}"
  },
  {
    "function_name": "task_tick_numa",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2584-2614",
    "snippet": "void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n\tstruct callback_head *work = &curr->numa_work;\n\tu64 period, now;\n\n\t/*\n\t * We don't care about NUMA placement if we don't have memory.\n\t */\n\tif (!curr->mm || (curr->flags & PF_EXITING) || work->next != work)\n\t\treturn;\n\n\t/*\n\t * Using runtime rather than walltime has the dual advantage that\n\t * we (mostly) drive the selection from busy threads and that the\n\t * task needs to have done some actual work before we bother with\n\t * NUMA placement.\n\t */\n\tnow = curr->se.sum_exec_runtime;\n\tperiod = (u64)curr->numa_scan_period * NSEC_PER_MSEC;\n\n\tif (now > curr->node_stamp + period) {\n\t\tif (!curr->node_stamp)\n\t\t\tcurr->numa_scan_period = task_scan_start(curr);\n\t\tcurr->node_stamp += period;\n\n\t\tif (!time_before(jiffies, curr->mm->numa_next_scan)) {\n\t\t\tinit_task_work(work, task_numa_work); /* TODO: move this into sched_fork() */\n\t\t\ttask_work_add(curr, work, true);\n\t\t}\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_work_add",
          "args": [
            "curr",
            "work",
            "true"
          ],
          "line": 2611
        },
        "resolved": true,
        "details": {
          "function_name": "task_work_add",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/task_work.c",
          "lines": "27-42",
          "snippet": "int\ntask_work_add(struct task_struct *task, struct callback_head *work, bool notify)\n{\n\tstruct callback_head *head;\n\n\tdo {\n\t\thead = READ_ONCE(task->task_works);\n\t\tif (unlikely(head == &work_exited))\n\t\t\treturn -ESRCH;\n\t\twork->next = head;\n\t} while (cmpxchg(&task->task_works, head, work) != head);\n\n\tif (notify)\n\t\tset_notify_resume(task);\n\treturn 0;\n}",
          "includes": [
            "#include <linux/tracehook.h>",
            "#include <linux/task_work.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static struct callback_head work_exited;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/tracehook.h>\n#include <linux/task_work.h>\n#include <linux/spinlock.h>\n\nstatic struct callback_head work_exited;\n\nint\ntask_work_add(struct task_struct *task, struct callback_head *work, bool notify)\n{\n\tstruct callback_head *head;\n\n\tdo {\n\t\thead = READ_ONCE(task->task_works);\n\t\tif (unlikely(head == &work_exited))\n\t\t\treturn -ESRCH;\n\t\twork->next = head;\n\t} while (cmpxchg(&task->task_works, head, work) != head);\n\n\tif (notify)\n\t\tset_notify_resume(task);\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "init_task_work",
          "args": [
            "work",
            "task_numa_work"
          ],
          "line": 2610
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "time_before",
          "args": [
            "jiffies",
            "curr->mm->numa_next_scan"
          ],
          "line": 2609
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_scan_start",
          "args": [
            "curr"
          ],
          "line": 2606
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_start",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1097-1114",
          "snippet": "static unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n\tstruct callback_head *work = &curr->numa_work;\n\tu64 period, now;\n\n\t/*\n\t * We don't care about NUMA placement if we don't have memory.\n\t */\n\tif (!curr->mm || (curr->flags & PF_EXITING) || work->next != work)\n\t\treturn;\n\n\t/*\n\t * Using runtime rather than walltime has the dual advantage that\n\t * we (mostly) drive the selection from busy threads and that the\n\t * task needs to have done some actual work before we bother with\n\t * NUMA placement.\n\t */\n\tnow = curr->se.sum_exec_runtime;\n\tperiod = (u64)curr->numa_scan_period * NSEC_PER_MSEC;\n\n\tif (now > curr->node_stamp + period) {\n\t\tif (!curr->node_stamp)\n\t\t\tcurr->numa_scan_period = task_scan_start(curr);\n\t\tcurr->node_stamp += period;\n\n\t\tif (!time_before(jiffies, curr->mm->numa_next_scan)) {\n\t\t\tinit_task_work(work, task_numa_work); /* TODO: move this into sched_fork() */\n\t\t\ttask_work_add(curr, work, true);\n\t\t}\n\t}\n}"
  },
  {
    "function_name": "task_numa_work",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2439-2579",
    "snippet": "void task_numa_work(struct callback_head *work)\n{\n\tunsigned long migrate, next_scan, now = jiffies;\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tu64 runtime = p->se.sum_exec_runtime;\n\tstruct vm_area_struct *vma;\n\tunsigned long start, end;\n\tunsigned long nr_pte_updates = 0;\n\tlong pages, virtpages;\n\n\tSCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));\n\n\twork->next = work; /* protect against double add */\n\t/*\n\t * Who cares about NUMA placement when they're dying.\n\t *\n\t * NOTE: make sure not to dereference p->mm before this check,\n\t * exit_task_work() happens _after_ exit_mm() so we could be called\n\t * without p->mm even though we still had it when we enqueued this\n\t * work.\n\t */\n\tif (p->flags & PF_EXITING)\n\t\treturn;\n\n\tif (!mm->numa_next_scan) {\n\t\tmm->numa_next_scan = now +\n\t\t\tmsecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t}\n\n\t/*\n\t * Enforce maximal scan/migration frequency..\n\t */\n\tmigrate = mm->numa_next_scan;\n\tif (time_before(now, migrate))\n\t\treturn;\n\n\tif (p->numa_scan_period == 0) {\n\t\tp->numa_scan_period_max = task_scan_max(p);\n\t\tp->numa_scan_period = task_scan_start(p);\n\t}\n\n\tnext_scan = now + msecs_to_jiffies(p->numa_scan_period);\n\tif (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)\n\t\treturn;\n\n\t/*\n\t * Delay this task enough that another task of this mm will likely win\n\t * the next time around.\n\t */\n\tp->node_stamp += 2 * TICK_NSEC;\n\n\tstart = mm->numa_scan_offset;\n\tpages = sysctl_numa_balancing_scan_size;\n\tpages <<= 20 - PAGE_SHIFT; /* MB in pages */\n\tvirtpages = pages * 8;\t   /* Scan up to this much virtual space */\n\tif (!pages)\n\t\treturn;\n\n\n\tif (!down_read_trylock(&mm->mmap_sem))\n\t\treturn;\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\treset_ptenuma_scan(p);\n\t\tstart = 0;\n\t\tvma = mm->mmap;\n\t}\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!vma_migratable(vma) || !vma_policy_mof(vma) ||\n\t\t\tis_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Shared library pages mapped by multiple processes are not\n\t\t * migrated as it is expected they are cache replicated. Avoid\n\t\t * hinting faults in read-only file-backed mappings or the vdso\n\t\t * as migrating the pages will be of marginal benefit.\n\t\t */\n\t\tif (!vma->vm_mm ||\n\t\t    (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Skip inaccessible VMAs to avoid any confusion between\n\t\t * PROT_NONE and NUMA hinting ptes\n\t\t */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tcontinue;\n\n\t\tdo {\n\t\t\tstart = max(start, vma->vm_start);\n\t\t\tend = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);\n\t\t\tend = min(end, vma->vm_end);\n\t\t\tnr_pte_updates = change_prot_numa(vma, start, end);\n\n\t\t\t/*\n\t\t\t * Try to scan sysctl_numa_balancing_size worth of\n\t\t\t * hpages that have at least one present PTE that\n\t\t\t * is not already pte-numa. If the VMA contains\n\t\t\t * areas that are unused or already full of prot_numa\n\t\t\t * PTEs, scan up to virtpages, to skip through those\n\t\t\t * areas faster.\n\t\t\t */\n\t\t\tif (nr_pte_updates)\n\t\t\t\tpages -= (end - start) >> PAGE_SHIFT;\n\t\t\tvirtpages -= (end - start) >> PAGE_SHIFT;\n\n\t\t\tstart = end;\n\t\t\tif (pages <= 0 || virtpages <= 0)\n\t\t\t\tgoto out;\n\n\t\t\tcond_resched();\n\t\t} while (end != vma->vm_end);\n\t}\n\nout:\n\t/*\n\t * It is possible to reach the end of the VMA list but the last few\n\t * VMAs are not guaranteed to the vma_migratable. If they are not, we\n\t * would find the !migratable VMA on the next scan but not reset the\n\t * scanner to the start so check it now.\n\t */\n\tif (vma)\n\t\tmm->numa_scan_offset = start;\n\telse\n\t\treset_ptenuma_scan(p);\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Make sure tasks use at least 32x as much time to run other code\n\t * than they used here, to limit NUMA PTE scanning overhead to 3% max.\n\t * Usually update_task_scan_period slows down scanning enough; on an\n\t * overloaded system we need to limit overhead on a per task basis.\n\t */\n\tif (unlikely(p->se.sum_exec_runtime != runtime)) {\n\t\tu64 diff = p->se.sum_exec_runtime - runtime;\n\t\tp->node_stamp += 32 * diff;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "p->se.sum_exec_runtime != runtime"
          ],
          "line": 2575
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "up_read",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 2567
        },
        "resolved": true,
        "details": {
          "function_name": "wakeup_readers",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/relay.c",
          "lines": "336-342",
          "snippet": "static void wakeup_readers(struct irq_work *work)\n{\n\tstruct rchan_buf *buf;\n\n\tbuf = container_of(work, struct rchan_buf, wakeup_work);\n\twake_up_interruptible(&buf->read_wait);\n}",
          "includes": [
            "#include <linux/splice.h>",
            "#include <linux/cpu.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/relay.h>",
            "#include <linux/string.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>",
            "#include <linux/stddef.h>",
            "#include <linux/errno.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/splice.h>\n#include <linux/cpu.h>\n#include <linux/mm.h>\n#include <linux/vmalloc.h>\n#include <linux/relay.h>\n#include <linux/string.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/stddef.h>\n#include <linux/errno.h>\n\nstatic void wakeup_readers(struct irq_work *work)\n{\n\tstruct rchan_buf *buf;\n\n\tbuf = container_of(work, struct rchan_buf, wakeup_work);\n\twake_up_interruptible(&buf->read_wait);\n}"
        }
      },
      {
        "call_info": {
          "callee": "reset_ptenuma_scan",
          "args": [
            "p"
          ],
          "line": 2566
        },
        "resolved": true,
        "details": {
          "function_name": "reset_ptenuma_scan",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2421-2433",
          "snippet": "static void reset_ptenuma_scan(struct task_struct *p)\n{\n\t/*\n\t * We only did a read acquisition of the mmap sem, so\n\t * p->mm->numa_scan_seq is written to without exclusive access\n\t * and the update is not guaranteed to be atomic. That's not\n\t * much of an issue though, since this is just used for\n\t * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not\n\t * expensive, to avoid any form of compiler optimizations:\n\t */\n\tWRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);\n\tp->mm->numa_scan_offset = 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void reset_ptenuma_scan(struct task_struct *p)\n{\n\t/*\n\t * We only did a read acquisition of the mmap sem, so\n\t * p->mm->numa_scan_seq is written to without exclusive access\n\t * and the update is not guaranteed to be atomic. That's not\n\t * much of an issue though, since this is just used for\n\t * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not\n\t * expensive, to avoid any form of compiler optimizations:\n\t */\n\tWRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);\n\tp->mm->numa_scan_offset = 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cond_resched",
          "args": [],
          "line": 2552
        },
        "resolved": true,
        "details": {
          "function_name": "_cond_resched",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "4957-4965",
          "snippet": "int __sched _cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\trcu_all_qs();\n\treturn 0;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic void __sched;\nstatic void __sched;\n\nint __sched _cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\trcu_all_qs();\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "change_prot_numa",
          "args": [
            "vma",
            "start",
            "end"
          ],
          "line": 2534
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "end",
            "vma->vm_end"
          ],
          "line": 2533
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "ALIGN",
          "args": [
            "start + (pages << PAGE_SHIFT)",
            "HPAGE_SIZE"
          ],
          "line": 2532
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "start",
            "vma->vm_start"
          ],
          "line": 2531
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "is_vm_hugetlb_page",
          "args": [
            "vma"
          ],
          "line": 2509
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vma_policy_mof",
          "args": [
            "vma"
          ],
          "line": 2508
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vma_migratable",
          "args": [
            "vma"
          ],
          "line": 2508
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_vma",
          "args": [
            "mm",
            "start"
          ],
          "line": 2501
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "down_read_trylock",
          "args": [
            "&mm->mmap_sem"
          ],
          "line": 2499
        },
        "resolved": true,
        "details": {
          "function_name": "down_read_trylock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rwsem.c",
          "lines": "51-60",
          "snippet": "int down_read_trylock(struct rw_semaphore *sem)\n{\n\tint ret = __down_read_trylock(sem);\n\n\tif (ret == 1) {\n\t\trwsem_acquire_read(&sem->dep_map, 0, 1, _RET_IP_);\n\t\trwsem_set_reader_owned(sem);\n\t}\n\treturn ret;\n}",
          "includes": [
            "#include \"rwsem.h\"",
            "#include <linux/atomic.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/export.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rwsem.h\"\n#include <linux/atomic.h>\n#include <linux/rwsem.h>\n#include <linux/export.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint down_read_trylock(struct rw_semaphore *sem)\n{\n\tint ret = __down_read_trylock(sem);\n\n\tif (ret == 1) {\n\t\trwsem_acquire_read(&sem->dep_map, 0, 1, _RET_IP_);\n\t\trwsem_set_reader_owned(sem);\n\t}\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cmpxchg",
          "args": [
            "&mm->numa_next_scan",
            "migrate",
            "next_scan"
          ],
          "line": 2482
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "msecs_to_jiffies",
          "args": [
            "p->numa_scan_period"
          ],
          "line": 2481
        },
        "resolved": true,
        "details": {
          "function_name": "__msecs_to_jiffies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/time.c",
          "lines": "565-573",
          "snippet": "unsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}",
          "includes": [
            "#include \"timekeeping.h\"",
            "#include <generated/timeconst.h>",
            "#include <asm/unistd.h>",
            "#include <linux/compat.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/math64.h>",
            "#include <linux/fs.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/errno.h>",
            "#include <linux/timekeeper_internal.h>",
            "#include <linux/capability.h>",
            "#include <linux/timex.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"timekeeping.h\"\n#include <generated/timeconst.h>\n#include <asm/unistd.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/math64.h>\n#include <linux/fs.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/errno.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/capability.h>\n#include <linux/timex.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nunsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_scan_start",
          "args": [
            "p"
          ],
          "line": 2478
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_start",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1097-1114",
          "snippet": "static unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_scan_max",
          "args": [
            "p"
          ],
          "line": 2477
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_max",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1116-1139",
          "snippet": "static unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}"
        }
      },
      {
        "call_info": {
          "callee": "time_before",
          "args": [
            "now",
            "migrate"
          ],
          "line": 2473
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "p != container_of(work, struct task_struct, numa_work)"
          ],
          "line": 2450
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "work",
            "structtask_struct",
            "numa_work"
          ],
          "line": 2450
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid task_numa_work(struct callback_head *work)\n{\n\tunsigned long migrate, next_scan, now = jiffies;\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tu64 runtime = p->se.sum_exec_runtime;\n\tstruct vm_area_struct *vma;\n\tunsigned long start, end;\n\tunsigned long nr_pte_updates = 0;\n\tlong pages, virtpages;\n\n\tSCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));\n\n\twork->next = work; /* protect against double add */\n\t/*\n\t * Who cares about NUMA placement when they're dying.\n\t *\n\t * NOTE: make sure not to dereference p->mm before this check,\n\t * exit_task_work() happens _after_ exit_mm() so we could be called\n\t * without p->mm even though we still had it when we enqueued this\n\t * work.\n\t */\n\tif (p->flags & PF_EXITING)\n\t\treturn;\n\n\tif (!mm->numa_next_scan) {\n\t\tmm->numa_next_scan = now +\n\t\t\tmsecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t}\n\n\t/*\n\t * Enforce maximal scan/migration frequency..\n\t */\n\tmigrate = mm->numa_next_scan;\n\tif (time_before(now, migrate))\n\t\treturn;\n\n\tif (p->numa_scan_period == 0) {\n\t\tp->numa_scan_period_max = task_scan_max(p);\n\t\tp->numa_scan_period = task_scan_start(p);\n\t}\n\n\tnext_scan = now + msecs_to_jiffies(p->numa_scan_period);\n\tif (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)\n\t\treturn;\n\n\t/*\n\t * Delay this task enough that another task of this mm will likely win\n\t * the next time around.\n\t */\n\tp->node_stamp += 2 * TICK_NSEC;\n\n\tstart = mm->numa_scan_offset;\n\tpages = sysctl_numa_balancing_scan_size;\n\tpages <<= 20 - PAGE_SHIFT; /* MB in pages */\n\tvirtpages = pages * 8;\t   /* Scan up to this much virtual space */\n\tif (!pages)\n\t\treturn;\n\n\n\tif (!down_read_trylock(&mm->mmap_sem))\n\t\treturn;\n\tvma = find_vma(mm, start);\n\tif (!vma) {\n\t\treset_ptenuma_scan(p);\n\t\tstart = 0;\n\t\tvma = mm->mmap;\n\t}\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!vma_migratable(vma) || !vma_policy_mof(vma) ||\n\t\t\tis_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Shared library pages mapped by multiple processes are not\n\t\t * migrated as it is expected they are cache replicated. Avoid\n\t\t * hinting faults in read-only file-backed mappings or the vdso\n\t\t * as migrating the pages will be of marginal benefit.\n\t\t */\n\t\tif (!vma->vm_mm ||\n\t\t    (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Skip inaccessible VMAs to avoid any confusion between\n\t\t * PROT_NONE and NUMA hinting ptes\n\t\t */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tcontinue;\n\n\t\tdo {\n\t\t\tstart = max(start, vma->vm_start);\n\t\t\tend = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);\n\t\t\tend = min(end, vma->vm_end);\n\t\t\tnr_pte_updates = change_prot_numa(vma, start, end);\n\n\t\t\t/*\n\t\t\t * Try to scan sysctl_numa_balancing_size worth of\n\t\t\t * hpages that have at least one present PTE that\n\t\t\t * is not already pte-numa. If the VMA contains\n\t\t\t * areas that are unused or already full of prot_numa\n\t\t\t * PTEs, scan up to virtpages, to skip through those\n\t\t\t * areas faster.\n\t\t\t */\n\t\t\tif (nr_pte_updates)\n\t\t\t\tpages -= (end - start) >> PAGE_SHIFT;\n\t\t\tvirtpages -= (end - start) >> PAGE_SHIFT;\n\n\t\t\tstart = end;\n\t\t\tif (pages <= 0 || virtpages <= 0)\n\t\t\t\tgoto out;\n\n\t\t\tcond_resched();\n\t\t} while (end != vma->vm_end);\n\t}\n\nout:\n\t/*\n\t * It is possible to reach the end of the VMA list but the last few\n\t * VMAs are not guaranteed to the vma_migratable. If they are not, we\n\t * would find the !migratable VMA on the next scan but not reset the\n\t * scanner to the start so check it now.\n\t */\n\tif (vma)\n\t\tmm->numa_scan_offset = start;\n\telse\n\t\treset_ptenuma_scan(p);\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Make sure tasks use at least 32x as much time to run other code\n\t * than they used here, to limit NUMA PTE scanning overhead to 3% max.\n\t * Usually update_task_scan_period slows down scanning enough; on an\n\t * overloaded system we need to limit overhead on a per task basis.\n\t */\n\tif (unlikely(p->se.sum_exec_runtime != runtime)) {\n\t\tu64 diff = p->se.sum_exec_runtime - runtime;\n\t\tp->node_stamp += 32 * diff;\n\t}\n}"
  },
  {
    "function_name": "reset_ptenuma_scan",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2421-2433",
    "snippet": "static void reset_ptenuma_scan(struct task_struct *p)\n{\n\t/*\n\t * We only did a read acquisition of the mmap sem, so\n\t * p->mm->numa_scan_seq is written to without exclusive access\n\t * and the update is not guaranteed to be atomic. That's not\n\t * much of an issue though, since this is just used for\n\t * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not\n\t * expensive, to avoid any form of compiler optimizations:\n\t */\n\tWRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);\n\tp->mm->numa_scan_offset = 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "p->mm->numa_scan_seq",
            "READ_ONCE(p->mm->numa_scan_seq) + 1"
          ],
          "line": 2431
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "p->mm->numa_scan_seq"
          ],
          "line": 2431
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void reset_ptenuma_scan(struct task_struct *p)\n{\n\t/*\n\t * We only did a read acquisition of the mmap sem, so\n\t * p->mm->numa_scan_seq is written to without exclusive access\n\t * and the update is not guaranteed to be atomic. That's not\n\t * much of an issue though, since this is just used for\n\t * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not\n\t * expensive, to avoid any form of compiler optimizations:\n\t */\n\tWRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);\n\tp->mm->numa_scan_offset = 0;\n}"
  },
  {
    "function_name": "task_numa_fault",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2349-2419",
    "snippet": "void task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)\n{\n\tstruct task_struct *p = current;\n\tbool migrated = flags & TNF_MIGRATED;\n\tint cpu_node = task_node(current);\n\tint local = !!(flags & TNF_FAULT_LOCAL);\n\tstruct numa_group *ng;\n\tint priv;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\t/* for example, ksmd faulting in a user's mm */\n\tif (!p->mm)\n\t\treturn;\n\n\t/* Allocate buffer to track faults on a per-node basis */\n\tif (unlikely(!p->numa_faults)) {\n\t\tint size = sizeof(*p->numa_faults) *\n\t\t\t   NR_NUMA_HINT_FAULT_BUCKETS * nr_node_ids;\n\n\t\tp->numa_faults = kzalloc(size, GFP_KERNEL|__GFP_NOWARN);\n\t\tif (!p->numa_faults)\n\t\t\treturn;\n\n\t\tp->total_numa_faults = 0;\n\t\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n\t}\n\n\t/*\n\t * First accesses are treated as private, otherwise consider accesses\n\t * to be private if the accessing pid has not changed\n\t */\n\tif (unlikely(last_cpupid == (-1 & LAST_CPUPID_MASK))) {\n\t\tpriv = 1;\n\t} else {\n\t\tpriv = cpupid_match_pid(p, last_cpupid);\n\t\tif (!priv && !(flags & TNF_NO_GROUP))\n\t\t\ttask_numa_group(p, last_cpupid, flags, &priv);\n\t}\n\n\t/*\n\t * If a workload spans multiple NUMA nodes, a shared fault that\n\t * occurs wholly within the set of nodes that the workload is\n\t * actively using should be counted as local. This allows the\n\t * scan rate to slow down when a workload has settled down.\n\t */\n\tng = p->numa_group;\n\tif (!priv && !local && ng && ng->active_nodes > 1 &&\n\t\t\t\tnuma_is_active_node(cpu_node, ng) &&\n\t\t\t\tnuma_is_active_node(mem_node, ng))\n\t\tlocal = 1;\n\n\t/*\n\t * Retry task to preferred node migration periodically, in case it\n\t * case it previously failed, or the scheduler moved us.\n\t */\n\tif (time_after(jiffies, p->numa_migrate_retry)) {\n\t\ttask_numa_placement(p);\n\t\tnuma_migrate_preferred(p);\n\t}\n\n\tif (migrated)\n\t\tp->numa_pages_migrated += pages;\n\tif (flags & TNF_MIGRATE_FAIL)\n\t\tp->numa_faults_locality[2] += pages;\n\n\tp->numa_faults[task_faults_idx(NUMA_MEMBUF, mem_node, priv)] += pages;\n\tp->numa_faults[task_faults_idx(NUMA_CPUBUF, cpu_node, priv)] += pages;\n\tp->numa_faults_locality[local] += pages;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define NR_NUMA_HINT_FAULT_BUCKETS (NR_NUMA_HINT_FAULT_STATS * 2)"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_CPUBUF",
            "cpu_node",
            "priv"
          ],
          "line": 2417
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      },
      {
        "call_info": {
          "callee": "numa_migrate_preferred",
          "args": [
            "p"
          ],
          "line": 2408
        },
        "resolved": true,
        "details": {
          "function_name": "numa_migrate_preferred",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1847-1865",
          "snippet": "static void numa_migrate_preferred(struct task_struct *p)\n{\n\tunsigned long interval = HZ;\n\n\t/* This task has no NUMA fault statistics yet */\n\tif (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))\n\t\treturn;\n\n\t/* Periodically retry migrating the task to the preferred node */\n\tinterval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);\n\tp->numa_migrate_retry = jiffies + interval;\n\n\t/* Success if task is already running on preferred CPU */\n\tif (task_node(p) == p->numa_preferred_nid)\n\t\treturn;\n\n\t/* Otherwise, try migrate to a CPU on the preferred node */\n\ttask_numa_migrate(p);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void numa_migrate_preferred(struct task_struct *p)\n{\n\tunsigned long interval = HZ;\n\n\t/* This task has no NUMA fault statistics yet */\n\tif (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))\n\t\treturn;\n\n\t/* Periodically retry migrating the task to the preferred node */\n\tinterval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);\n\tp->numa_migrate_retry = jiffies + interval;\n\n\t/* Success if task is already running on preferred CPU */\n\tif (task_node(p) == p->numa_preferred_nid)\n\t\treturn;\n\n\t/* Otherwise, try migrate to a CPU on the preferred node */\n\ttask_numa_migrate(p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_numa_placement",
          "args": [
            "p"
          ],
          "line": 2407
        },
        "resolved": true,
        "details": {
          "function_name": "task_numa_placement",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2097-2203",
          "snippet": "static void task_numa_placement(struct task_struct *p)\n{\n\tint seq, nid, max_nid = -1;\n\tunsigned long max_faults = 0;\n\tunsigned long fault_types[2] = { 0, 0 };\n\tunsigned long total_faults;\n\tu64 runtime, period;\n\tspinlock_t *group_lock = NULL;\n\n\t/*\n\t * The p->mm->numa_scan_seq field gets updated without\n\t * exclusive access. Use READ_ONCE() here to ensure\n\t * that the field is read in a single access:\n\t */\n\tseq = READ_ONCE(p->mm->numa_scan_seq);\n\tif (p->numa_scan_seq == seq)\n\t\treturn;\n\tp->numa_scan_seq = seq;\n\tp->numa_scan_period_max = task_scan_max(p);\n\n\ttotal_faults = p->numa_faults_locality[0] +\n\t\t       p->numa_faults_locality[1];\n\truntime = numa_get_avg_runtime(p, &period);\n\n\t/* If the task is part of a group prevent parallel updates to group stats */\n\tif (p->numa_group) {\n\t\tgroup_lock = &p->numa_group->lock;\n\t\tspin_lock_irq(group_lock);\n\t}\n\n\t/* Find the node with the highest number of faults */\n\tfor_each_online_node(nid) {\n\t\t/* Keep track of the offsets in numa_faults array */\n\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;\n\t\tunsigned long faults = 0, group_faults = 0;\n\t\tint priv;\n\n\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {\n\t\t\tlong diff, f_diff, f_weight;\n\n\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);\n\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);\n\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);\n\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);\n\n\t\t\t/* Decay existing window, copy faults since last scan */\n\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;\n\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];\n\t\t\tp->numa_faults[membuf_idx] = 0;\n\n\t\t\t/*\n\t\t\t * Normalize the faults_from, so all tasks in a group\n\t\t\t * count according to CPU use, instead of by the raw\n\t\t\t * number of faults. Tasks with little runtime have\n\t\t\t * little over-all impact on throughput, and thus their\n\t\t\t * faults are less important.\n\t\t\t */\n\t\t\tf_weight = div64_u64(runtime << 16, period + 1);\n\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /\n\t\t\t\t   (total_faults + 1);\n\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;\n\t\t\tp->numa_faults[cpubuf_idx] = 0;\n\n\t\t\tp->numa_faults[mem_idx] += diff;\n\t\t\tp->numa_faults[cpu_idx] += f_diff;\n\t\t\tfaults += p->numa_faults[mem_idx];\n\t\t\tp->total_numa_faults += diff;\n\t\t\tif (p->numa_group) {\n\t\t\t\t/*\n\t\t\t\t * safe because we can only change our own group\n\t\t\t\t *\n\t\t\t\t * mem_idx represents the offset for a given\n\t\t\t\t * nid and priv in a specific region because it\n\t\t\t\t * is at the beginning of the numa_faults array.\n\t\t\t\t */\n\t\t\t\tp->numa_group->faults[mem_idx] += diff;\n\t\t\t\tp->numa_group->faults_cpu[mem_idx] += f_diff;\n\t\t\t\tp->numa_group->total_faults += diff;\n\t\t\t\tgroup_faults += p->numa_group->faults[mem_idx];\n\t\t\t}\n\t\t}\n\n\t\tif (!p->numa_group) {\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_nid = nid;\n\t\t\t}\n\t\t} else if (group_faults > max_faults) {\n\t\t\tmax_faults = group_faults;\n\t\t\tmax_nid = nid;\n\t\t}\n\t}\n\n\tif (p->numa_group) {\n\t\tnuma_group_count_active_nodes(p->numa_group);\n\t\tspin_unlock_irq(group_lock);\n\t\tmax_nid = preferred_group_nid(p, max_nid);\n\t}\n\n\tif (max_faults) {\n\t\t/* Set the new preferred node */\n\t\tif (max_nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, max_nid);\n\t}\n\n\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic void task_numa_placement(struct task_struct *p)\n{\n\tint seq, nid, max_nid = -1;\n\tunsigned long max_faults = 0;\n\tunsigned long fault_types[2] = { 0, 0 };\n\tunsigned long total_faults;\n\tu64 runtime, period;\n\tspinlock_t *group_lock = NULL;\n\n\t/*\n\t * The p->mm->numa_scan_seq field gets updated without\n\t * exclusive access. Use READ_ONCE() here to ensure\n\t * that the field is read in a single access:\n\t */\n\tseq = READ_ONCE(p->mm->numa_scan_seq);\n\tif (p->numa_scan_seq == seq)\n\t\treturn;\n\tp->numa_scan_seq = seq;\n\tp->numa_scan_period_max = task_scan_max(p);\n\n\ttotal_faults = p->numa_faults_locality[0] +\n\t\t       p->numa_faults_locality[1];\n\truntime = numa_get_avg_runtime(p, &period);\n\n\t/* If the task is part of a group prevent parallel updates to group stats */\n\tif (p->numa_group) {\n\t\tgroup_lock = &p->numa_group->lock;\n\t\tspin_lock_irq(group_lock);\n\t}\n\n\t/* Find the node with the highest number of faults */\n\tfor_each_online_node(nid) {\n\t\t/* Keep track of the offsets in numa_faults array */\n\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;\n\t\tunsigned long faults = 0, group_faults = 0;\n\t\tint priv;\n\n\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {\n\t\t\tlong diff, f_diff, f_weight;\n\n\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);\n\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);\n\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);\n\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);\n\n\t\t\t/* Decay existing window, copy faults since last scan */\n\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;\n\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];\n\t\t\tp->numa_faults[membuf_idx] = 0;\n\n\t\t\t/*\n\t\t\t * Normalize the faults_from, so all tasks in a group\n\t\t\t * count according to CPU use, instead of by the raw\n\t\t\t * number of faults. Tasks with little runtime have\n\t\t\t * little over-all impact on throughput, and thus their\n\t\t\t * faults are less important.\n\t\t\t */\n\t\t\tf_weight = div64_u64(runtime << 16, period + 1);\n\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /\n\t\t\t\t   (total_faults + 1);\n\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;\n\t\t\tp->numa_faults[cpubuf_idx] = 0;\n\n\t\t\tp->numa_faults[mem_idx] += diff;\n\t\t\tp->numa_faults[cpu_idx] += f_diff;\n\t\t\tfaults += p->numa_faults[mem_idx];\n\t\t\tp->total_numa_faults += diff;\n\t\t\tif (p->numa_group) {\n\t\t\t\t/*\n\t\t\t\t * safe because we can only change our own group\n\t\t\t\t *\n\t\t\t\t * mem_idx represents the offset for a given\n\t\t\t\t * nid and priv in a specific region because it\n\t\t\t\t * is at the beginning of the numa_faults array.\n\t\t\t\t */\n\t\t\t\tp->numa_group->faults[mem_idx] += diff;\n\t\t\t\tp->numa_group->faults_cpu[mem_idx] += f_diff;\n\t\t\t\tp->numa_group->total_faults += diff;\n\t\t\t\tgroup_faults += p->numa_group->faults[mem_idx];\n\t\t\t}\n\t\t}\n\n\t\tif (!p->numa_group) {\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_nid = nid;\n\t\t\t}\n\t\t} else if (group_faults > max_faults) {\n\t\t\tmax_faults = group_faults;\n\t\t\tmax_nid = nid;\n\t\t}\n\t}\n\n\tif (p->numa_group) {\n\t\tnuma_group_count_active_nodes(p->numa_group);\n\t\tspin_unlock_irq(group_lock);\n\t\tmax_nid = preferred_group_nid(p, max_nid);\n\t}\n\n\tif (max_faults) {\n\t\t/* Set the new preferred node */\n\t\tif (max_nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, max_nid);\n\t}\n\n\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);\n}"
        }
      },
      {
        "call_info": {
          "callee": "time_after",
          "args": [
            "jiffies",
            "p->numa_migrate_retry"
          ],
          "line": 2406
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "numa_is_active_node",
          "args": [
            "mem_node",
            "ng"
          ],
          "line": 2399
        },
        "resolved": true,
        "details": {
          "function_name": "numa_is_active_node",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1274-1277",
          "snippet": "static bool numa_is_active_node(int nid, struct numa_group *ng)\n{\n\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION > ng->max_faults_cpu;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define ACTIVE_NODE_FRACTION 3"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define ACTIVE_NODE_FRACTION 3\n\nstatic bool numa_is_active_node(int nid, struct numa_group *ng)\n{\n\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION > ng->max_faults_cpu;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_numa_group",
          "args": [
            "p",
            "last_cpupid",
            "flags",
            "&priv"
          ],
          "line": 2387
        },
        "resolved": true,
        "details": {
          "function_name": "task_numa_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2216-2321",
          "snippet": "static void task_numa_group(struct task_struct *p, int cpupid, int flags,\n\t\t\tint *priv)\n{\n\tstruct numa_group *grp, *my_grp;\n\tstruct task_struct *tsk;\n\tbool join = false;\n\tint cpu = cpupid_to_cpu(cpupid);\n\tint i;\n\n\tif (unlikely(!p->numa_group)) {\n\t\tunsigned int size = sizeof(struct numa_group) +\n\t\t\t\t    4*nr_node_ids*sizeof(unsigned long);\n\n\t\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!grp)\n\t\t\treturn;\n\n\t\tatomic_set(&grp->refcount, 1);\n\t\tgrp->active_nodes = 1;\n\t\tgrp->max_faults_cpu = 0;\n\t\tspin_lock_init(&grp->lock);\n\t\tgrp->gid = p->pid;\n\t\t/* Second half of the array tracks nids where faults happen */\n\t\tgrp->faults_cpu = grp->faults + NR_NUMA_HINT_FAULT_TYPES *\n\t\t\t\t\t\tnr_node_ids;\n\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] = p->numa_faults[i];\n\n\t\tgrp->total_faults = p->total_numa_faults;\n\n\t\tgrp->nr_tasks++;\n\t\trcu_assign_pointer(p->numa_group, grp);\n\t}\n\n\trcu_read_lock();\n\ttsk = READ_ONCE(cpu_rq(cpu)->curr);\n\n\tif (!cpupid_match_pid(tsk, cpupid))\n\t\tgoto no_join;\n\n\tgrp = rcu_dereference(tsk->numa_group);\n\tif (!grp)\n\t\tgoto no_join;\n\n\tmy_grp = p->numa_group;\n\tif (grp == my_grp)\n\t\tgoto no_join;\n\n\t/*\n\t * Only join the other group if its bigger; if we're the bigger group,\n\t * the other task will join us.\n\t */\n\tif (my_grp->nr_tasks > grp->nr_tasks)\n\t\tgoto no_join;\n\n\t/*\n\t * Tie-break on the grp address.\n\t */\n\tif (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)\n\t\tgoto no_join;\n\n\t/* Always join threads in the same process. */\n\tif (tsk->mm == current->mm)\n\t\tjoin = true;\n\n\t/* Simple filter to avoid false positives due to PID collisions */\n\tif (flags & TNF_SHARED)\n\t\tjoin = true;\n\n\t/* Update priv based on whether false sharing was detected */\n\t*priv = !join;\n\n\tif (join && !get_numa_group(grp))\n\t\tgoto no_join;\n\n\trcu_read_unlock();\n\n\tif (!join)\n\t\treturn;\n\n\tBUG_ON(irqs_disabled());\n\tdouble_lock_irq(&my_grp->lock, &grp->lock);\n\n\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {\n\t\tmy_grp->faults[i] -= p->numa_faults[i];\n\t\tgrp->faults[i] += p->numa_faults[i];\n\t}\n\tmy_grp->total_faults -= p->total_numa_faults;\n\tgrp->total_faults += p->total_numa_faults;\n\n\tmy_grp->nr_tasks--;\n\tgrp->nr_tasks++;\n\n\tspin_unlock(&my_grp->lock);\n\tspin_unlock_irq(&grp->lock);\n\n\trcu_assign_pointer(p->numa_group, grp);\n\n\tput_numa_group(my_grp);\n\treturn;\n\nno_join:\n\trcu_read_unlock();\n\treturn;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)",
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic void task_numa_group(struct task_struct *p, int cpupid, int flags,\n\t\t\tint *priv)\n{\n\tstruct numa_group *grp, *my_grp;\n\tstruct task_struct *tsk;\n\tbool join = false;\n\tint cpu = cpupid_to_cpu(cpupid);\n\tint i;\n\n\tif (unlikely(!p->numa_group)) {\n\t\tunsigned int size = sizeof(struct numa_group) +\n\t\t\t\t    4*nr_node_ids*sizeof(unsigned long);\n\n\t\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!grp)\n\t\t\treturn;\n\n\t\tatomic_set(&grp->refcount, 1);\n\t\tgrp->active_nodes = 1;\n\t\tgrp->max_faults_cpu = 0;\n\t\tspin_lock_init(&grp->lock);\n\t\tgrp->gid = p->pid;\n\t\t/* Second half of the array tracks nids where faults happen */\n\t\tgrp->faults_cpu = grp->faults + NR_NUMA_HINT_FAULT_TYPES *\n\t\t\t\t\t\tnr_node_ids;\n\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] = p->numa_faults[i];\n\n\t\tgrp->total_faults = p->total_numa_faults;\n\n\t\tgrp->nr_tasks++;\n\t\trcu_assign_pointer(p->numa_group, grp);\n\t}\n\n\trcu_read_lock();\n\ttsk = READ_ONCE(cpu_rq(cpu)->curr);\n\n\tif (!cpupid_match_pid(tsk, cpupid))\n\t\tgoto no_join;\n\n\tgrp = rcu_dereference(tsk->numa_group);\n\tif (!grp)\n\t\tgoto no_join;\n\n\tmy_grp = p->numa_group;\n\tif (grp == my_grp)\n\t\tgoto no_join;\n\n\t/*\n\t * Only join the other group if its bigger; if we're the bigger group,\n\t * the other task will join us.\n\t */\n\tif (my_grp->nr_tasks > grp->nr_tasks)\n\t\tgoto no_join;\n\n\t/*\n\t * Tie-break on the grp address.\n\t */\n\tif (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)\n\t\tgoto no_join;\n\n\t/* Always join threads in the same process. */\n\tif (tsk->mm == current->mm)\n\t\tjoin = true;\n\n\t/* Simple filter to avoid false positives due to PID collisions */\n\tif (flags & TNF_SHARED)\n\t\tjoin = true;\n\n\t/* Update priv based on whether false sharing was detected */\n\t*priv = !join;\n\n\tif (join && !get_numa_group(grp))\n\t\tgoto no_join;\n\n\trcu_read_unlock();\n\n\tif (!join)\n\t\treturn;\n\n\tBUG_ON(irqs_disabled());\n\tdouble_lock_irq(&my_grp->lock, &grp->lock);\n\n\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {\n\t\tmy_grp->faults[i] -= p->numa_faults[i];\n\t\tgrp->faults[i] += p->numa_faults[i];\n\t}\n\tmy_grp->total_faults -= p->total_numa_faults;\n\tgrp->total_faults += p->total_numa_faults;\n\n\tmy_grp->nr_tasks--;\n\tgrp->nr_tasks++;\n\n\tspin_unlock(&my_grp->lock);\n\tspin_unlock_irq(&grp->lock);\n\n\trcu_assign_pointer(p->numa_group, grp);\n\n\tput_numa_group(my_grp);\n\treturn;\n\nno_join:\n\trcu_read_unlock();\n\treturn;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpupid_match_pid",
          "args": [
            "p",
            "last_cpupid"
          ],
          "line": 2385
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "last_cpupid == (-1 & LAST_CPUPID_MASK)"
          ],
          "line": 2382
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "p->numa_faults_locality",
            "0",
            "sizeof(p->numa_faults_locality)"
          ],
          "line": 2375
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kzalloc",
          "args": [
            "size",
            "GFP_KERNEL|__GFP_NOWARN"
          ],
          "line": 2370
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!p->numa_faults"
          ],
          "line": 2366
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "static_branch_likely",
          "args": [
            "&sched_numa_balancing"
          ],
          "line": 2358
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_node",
          "args": [
            "current"
          ],
          "line": 2353
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_BUCKETS (NR_NUMA_HINT_FAULT_STATS * 2)\n\nvoid task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)\n{\n\tstruct task_struct *p = current;\n\tbool migrated = flags & TNF_MIGRATED;\n\tint cpu_node = task_node(current);\n\tint local = !!(flags & TNF_FAULT_LOCAL);\n\tstruct numa_group *ng;\n\tint priv;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\t/* for example, ksmd faulting in a user's mm */\n\tif (!p->mm)\n\t\treturn;\n\n\t/* Allocate buffer to track faults on a per-node basis */\n\tif (unlikely(!p->numa_faults)) {\n\t\tint size = sizeof(*p->numa_faults) *\n\t\t\t   NR_NUMA_HINT_FAULT_BUCKETS * nr_node_ids;\n\n\t\tp->numa_faults = kzalloc(size, GFP_KERNEL|__GFP_NOWARN);\n\t\tif (!p->numa_faults)\n\t\t\treturn;\n\n\t\tp->total_numa_faults = 0;\n\t\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n\t}\n\n\t/*\n\t * First accesses are treated as private, otherwise consider accesses\n\t * to be private if the accessing pid has not changed\n\t */\n\tif (unlikely(last_cpupid == (-1 & LAST_CPUPID_MASK))) {\n\t\tpriv = 1;\n\t} else {\n\t\tpriv = cpupid_match_pid(p, last_cpupid);\n\t\tif (!priv && !(flags & TNF_NO_GROUP))\n\t\t\ttask_numa_group(p, last_cpupid, flags, &priv);\n\t}\n\n\t/*\n\t * If a workload spans multiple NUMA nodes, a shared fault that\n\t * occurs wholly within the set of nodes that the workload is\n\t * actively using should be counted as local. This allows the\n\t * scan rate to slow down when a workload has settled down.\n\t */\n\tng = p->numa_group;\n\tif (!priv && !local && ng && ng->active_nodes > 1 &&\n\t\t\t\tnuma_is_active_node(cpu_node, ng) &&\n\t\t\t\tnuma_is_active_node(mem_node, ng))\n\t\tlocal = 1;\n\n\t/*\n\t * Retry task to preferred node migration periodically, in case it\n\t * case it previously failed, or the scheduler moved us.\n\t */\n\tif (time_after(jiffies, p->numa_migrate_retry)) {\n\t\ttask_numa_placement(p);\n\t\tnuma_migrate_preferred(p);\n\t}\n\n\tif (migrated)\n\t\tp->numa_pages_migrated += pages;\n\tif (flags & TNF_MIGRATE_FAIL)\n\t\tp->numa_faults_locality[2] += pages;\n\n\tp->numa_faults[task_faults_idx(NUMA_MEMBUF, mem_node, priv)] += pages;\n\tp->numa_faults[task_faults_idx(NUMA_CPUBUF, cpu_node, priv)] += pages;\n\tp->numa_faults_locality[local] += pages;\n}"
  },
  {
    "function_name": "task_numa_free",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2323-2344",
    "snippet": "void task_numa_free(struct task_struct *p)\n{\n\tstruct numa_group *grp = p->numa_group;\n\tvoid *numa_faults = p->numa_faults;\n\tunsigned long flags;\n\tint i;\n\n\tif (grp) {\n\t\tspin_lock_irqsave(&grp->lock, flags);\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] -= p->numa_faults[i];\n\t\tgrp->total_faults -= p->total_numa_faults;\n\n\t\tgrp->nr_tasks--;\n\t\tspin_unlock_irqrestore(&grp->lock, flags);\n\t\tRCU_INIT_POINTER(p->numa_group, NULL);\n\t\tput_numa_group(grp);\n\t}\n\n\tp->numa_faults = NULL;\n\tkfree(numa_faults);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "numa_faults"
          ],
          "line": 2343
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/params.c",
          "lines": "73-86",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "put_numa_group",
          "args": [
            "grp"
          ],
          "line": 2339
        },
        "resolved": true,
        "details": {
          "function_name": "put_numa_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2210-2214",
          "snippet": "static inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "RCU_INIT_POINTER",
          "args": [
            "p->numa_group",
            "NULL"
          ],
          "line": 2338
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_unlock_irqrestore",
          "args": [
            "&grp->lock",
            "flags"
          ],
          "line": 2337
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock_irqsave",
          "args": [
            "&grp->lock",
            "flags"
          ],
          "line": 2331
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)\n\nvoid task_numa_free(struct task_struct *p)\n{\n\tstruct numa_group *grp = p->numa_group;\n\tvoid *numa_faults = p->numa_faults;\n\tunsigned long flags;\n\tint i;\n\n\tif (grp) {\n\t\tspin_lock_irqsave(&grp->lock, flags);\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] -= p->numa_faults[i];\n\t\tgrp->total_faults -= p->total_numa_faults;\n\n\t\tgrp->nr_tasks--;\n\t\tspin_unlock_irqrestore(&grp->lock, flags);\n\t\tRCU_INIT_POINTER(p->numa_group, NULL);\n\t\tput_numa_group(grp);\n\t}\n\n\tp->numa_faults = NULL;\n\tkfree(numa_faults);\n}"
  },
  {
    "function_name": "task_numa_group",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2216-2321",
    "snippet": "static void task_numa_group(struct task_struct *p, int cpupid, int flags,\n\t\t\tint *priv)\n{\n\tstruct numa_group *grp, *my_grp;\n\tstruct task_struct *tsk;\n\tbool join = false;\n\tint cpu = cpupid_to_cpu(cpupid);\n\tint i;\n\n\tif (unlikely(!p->numa_group)) {\n\t\tunsigned int size = sizeof(struct numa_group) +\n\t\t\t\t    4*nr_node_ids*sizeof(unsigned long);\n\n\t\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!grp)\n\t\t\treturn;\n\n\t\tatomic_set(&grp->refcount, 1);\n\t\tgrp->active_nodes = 1;\n\t\tgrp->max_faults_cpu = 0;\n\t\tspin_lock_init(&grp->lock);\n\t\tgrp->gid = p->pid;\n\t\t/* Second half of the array tracks nids where faults happen */\n\t\tgrp->faults_cpu = grp->faults + NR_NUMA_HINT_FAULT_TYPES *\n\t\t\t\t\t\tnr_node_ids;\n\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] = p->numa_faults[i];\n\n\t\tgrp->total_faults = p->total_numa_faults;\n\n\t\tgrp->nr_tasks++;\n\t\trcu_assign_pointer(p->numa_group, grp);\n\t}\n\n\trcu_read_lock();\n\ttsk = READ_ONCE(cpu_rq(cpu)->curr);\n\n\tif (!cpupid_match_pid(tsk, cpupid))\n\t\tgoto no_join;\n\n\tgrp = rcu_dereference(tsk->numa_group);\n\tif (!grp)\n\t\tgoto no_join;\n\n\tmy_grp = p->numa_group;\n\tif (grp == my_grp)\n\t\tgoto no_join;\n\n\t/*\n\t * Only join the other group if its bigger; if we're the bigger group,\n\t * the other task will join us.\n\t */\n\tif (my_grp->nr_tasks > grp->nr_tasks)\n\t\tgoto no_join;\n\n\t/*\n\t * Tie-break on the grp address.\n\t */\n\tif (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)\n\t\tgoto no_join;\n\n\t/* Always join threads in the same process. */\n\tif (tsk->mm == current->mm)\n\t\tjoin = true;\n\n\t/* Simple filter to avoid false positives due to PID collisions */\n\tif (flags & TNF_SHARED)\n\t\tjoin = true;\n\n\t/* Update priv based on whether false sharing was detected */\n\t*priv = !join;\n\n\tif (join && !get_numa_group(grp))\n\t\tgoto no_join;\n\n\trcu_read_unlock();\n\n\tif (!join)\n\t\treturn;\n\n\tBUG_ON(irqs_disabled());\n\tdouble_lock_irq(&my_grp->lock, &grp->lock);\n\n\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {\n\t\tmy_grp->faults[i] -= p->numa_faults[i];\n\t\tgrp->faults[i] += p->numa_faults[i];\n\t}\n\tmy_grp->total_faults -= p->total_numa_faults;\n\tgrp->total_faults += p->total_numa_faults;\n\n\tmy_grp->nr_tasks--;\n\tgrp->nr_tasks++;\n\n\tspin_unlock(&my_grp->lock);\n\tspin_unlock_irq(&grp->lock);\n\n\trcu_assign_pointer(p->numa_group, grp);\n\n\tput_numa_group(my_grp);\n\treturn;\n\nno_join:\n\trcu_read_unlock();\n\treturn;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)",
      "#define NR_NUMA_HINT_FAULT_TYPES 2"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 2319
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "put_numa_group",
          "args": [
            "my_grp"
          ],
          "line": 2315
        },
        "resolved": true,
        "details": {
          "function_name": "put_numa_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2210-2214",
          "snippet": "static inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_assign_pointer",
          "args": [
            "p->numa_group",
            "grp"
          ],
          "line": 2313
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_unlock_irq",
          "args": [
            "&grp->lock"
          ],
          "line": 2311
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&my_grp->lock"
          ],
          "line": 2310
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_queued_spin_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "546-560",
          "snippet": "__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}"
        }
      },
      {
        "call_info": {
          "callee": "double_lock_irq",
          "args": [
            "&my_grp->lock",
            "&grp->lock"
          ],
          "line": 2298
        },
        "resolved": true,
        "details": {
          "function_name": "double_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1965-1972",
          "snippet": "static inline void double_lock_irq(spinlock_t *l1, spinlock_t *l2)\n{\n\tif (l1 > l2)\n\t\tswap(l1, l2);\n\n\tspin_lock_irq(l1);\n\tspin_lock_nested(l2, SINGLE_DEPTH_NESTING);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nstatic inline void double_lock_irq(spinlock_t *l1, spinlock_t *l2)\n{\n\tif (l1 > l2)\n\t\tswap(l1, l2);\n\n\tspin_lock_irq(l1);\n\tspin_lock_nested(l2, SINGLE_DEPTH_NESTING);\n}"
        }
      },
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "irqs_disabled()"
          ],
          "line": 2297
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "irqs_disabled",
          "args": [],
          "line": 2297
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_numa_group",
          "args": [
            "grp"
          ],
          "line": 2289
        },
        "resolved": true,
        "details": {
          "function_name": "get_numa_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2205-2208",
          "snippet": "static inline int get_numa_group(struct numa_group *grp)\n{\n\treturn atomic_inc_not_zero(&grp->refcount);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int get_numa_group(struct numa_group *grp)\n{\n\treturn atomic_inc_not_zero(&grp->refcount);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "tsk->numa_group"
          ],
          "line": 2257
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpupid_match_pid",
          "args": [
            "tsk",
            "cpupid"
          ],
          "line": 2254
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "cpu_rq(cpu)->curr"
          ],
          "line": 2252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 2252
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 2251
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_assign_pointer",
          "args": [
            "p->numa_group",
            "grp"
          ],
          "line": 2248
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_lock_init",
          "args": [
            "&grp->lock"
          ],
          "line": 2236
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_set",
          "args": [
            "&grp->refcount",
            "1"
          ],
          "line": 2233
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kzalloc",
          "args": [
            "size",
            "GFP_KERNEL | __GFP_NOWARN"
          ],
          "line": 2229
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!p->numa_group"
          ],
          "line": 2225
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpupid_to_cpu",
          "args": [
            "cpupid"
          ],
          "line": 2222
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic void task_numa_group(struct task_struct *p, int cpupid, int flags,\n\t\t\tint *priv)\n{\n\tstruct numa_group *grp, *my_grp;\n\tstruct task_struct *tsk;\n\tbool join = false;\n\tint cpu = cpupid_to_cpu(cpupid);\n\tint i;\n\n\tif (unlikely(!p->numa_group)) {\n\t\tunsigned int size = sizeof(struct numa_group) +\n\t\t\t\t    4*nr_node_ids*sizeof(unsigned long);\n\n\t\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!grp)\n\t\t\treturn;\n\n\t\tatomic_set(&grp->refcount, 1);\n\t\tgrp->active_nodes = 1;\n\t\tgrp->max_faults_cpu = 0;\n\t\tspin_lock_init(&grp->lock);\n\t\tgrp->gid = p->pid;\n\t\t/* Second half of the array tracks nids where faults happen */\n\t\tgrp->faults_cpu = grp->faults + NR_NUMA_HINT_FAULT_TYPES *\n\t\t\t\t\t\tnr_node_ids;\n\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] = p->numa_faults[i];\n\n\t\tgrp->total_faults = p->total_numa_faults;\n\n\t\tgrp->nr_tasks++;\n\t\trcu_assign_pointer(p->numa_group, grp);\n\t}\n\n\trcu_read_lock();\n\ttsk = READ_ONCE(cpu_rq(cpu)->curr);\n\n\tif (!cpupid_match_pid(tsk, cpupid))\n\t\tgoto no_join;\n\n\tgrp = rcu_dereference(tsk->numa_group);\n\tif (!grp)\n\t\tgoto no_join;\n\n\tmy_grp = p->numa_group;\n\tif (grp == my_grp)\n\t\tgoto no_join;\n\n\t/*\n\t * Only join the other group if its bigger; if we're the bigger group,\n\t * the other task will join us.\n\t */\n\tif (my_grp->nr_tasks > grp->nr_tasks)\n\t\tgoto no_join;\n\n\t/*\n\t * Tie-break on the grp address.\n\t */\n\tif (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)\n\t\tgoto no_join;\n\n\t/* Always join threads in the same process. */\n\tif (tsk->mm == current->mm)\n\t\tjoin = true;\n\n\t/* Simple filter to avoid false positives due to PID collisions */\n\tif (flags & TNF_SHARED)\n\t\tjoin = true;\n\n\t/* Update priv based on whether false sharing was detected */\n\t*priv = !join;\n\n\tif (join && !get_numa_group(grp))\n\t\tgoto no_join;\n\n\trcu_read_unlock();\n\n\tif (!join)\n\t\treturn;\n\n\tBUG_ON(irqs_disabled());\n\tdouble_lock_irq(&my_grp->lock, &grp->lock);\n\n\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {\n\t\tmy_grp->faults[i] -= p->numa_faults[i];\n\t\tgrp->faults[i] += p->numa_faults[i];\n\t}\n\tmy_grp->total_faults -= p->total_numa_faults;\n\tgrp->total_faults += p->total_numa_faults;\n\n\tmy_grp->nr_tasks--;\n\tgrp->nr_tasks++;\n\n\tspin_unlock(&my_grp->lock);\n\tspin_unlock_irq(&grp->lock);\n\n\trcu_assign_pointer(p->numa_group, grp);\n\n\tput_numa_group(my_grp);\n\treturn;\n\nno_join:\n\trcu_read_unlock();\n\treturn;\n}"
  },
  {
    "function_name": "put_numa_group",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2210-2214",
    "snippet": "static inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree_rcu",
          "args": [
            "grp",
            "rcu"
          ],
          "line": 2213
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_dec_and_test",
          "args": [
            "&grp->refcount"
          ],
          "line": 2212
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void put_numa_group(struct numa_group *grp)\n{\n\tif (atomic_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}"
  },
  {
    "function_name": "get_numa_group",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2205-2208",
    "snippet": "static inline int get_numa_group(struct numa_group *grp)\n{\n\treturn atomic_inc_not_zero(&grp->refcount);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_inc_not_zero",
          "args": [
            "&grp->refcount"
          ],
          "line": 2207
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int get_numa_group(struct numa_group *grp)\n{\n\treturn atomic_inc_not_zero(&grp->refcount);\n}"
  },
  {
    "function_name": "task_numa_placement",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2097-2203",
    "snippet": "static void task_numa_placement(struct task_struct *p)\n{\n\tint seq, nid, max_nid = -1;\n\tunsigned long max_faults = 0;\n\tunsigned long fault_types[2] = { 0, 0 };\n\tunsigned long total_faults;\n\tu64 runtime, period;\n\tspinlock_t *group_lock = NULL;\n\n\t/*\n\t * The p->mm->numa_scan_seq field gets updated without\n\t * exclusive access. Use READ_ONCE() here to ensure\n\t * that the field is read in a single access:\n\t */\n\tseq = READ_ONCE(p->mm->numa_scan_seq);\n\tif (p->numa_scan_seq == seq)\n\t\treturn;\n\tp->numa_scan_seq = seq;\n\tp->numa_scan_period_max = task_scan_max(p);\n\n\ttotal_faults = p->numa_faults_locality[0] +\n\t\t       p->numa_faults_locality[1];\n\truntime = numa_get_avg_runtime(p, &period);\n\n\t/* If the task is part of a group prevent parallel updates to group stats */\n\tif (p->numa_group) {\n\t\tgroup_lock = &p->numa_group->lock;\n\t\tspin_lock_irq(group_lock);\n\t}\n\n\t/* Find the node with the highest number of faults */\n\tfor_each_online_node(nid) {\n\t\t/* Keep track of the offsets in numa_faults array */\n\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;\n\t\tunsigned long faults = 0, group_faults = 0;\n\t\tint priv;\n\n\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {\n\t\t\tlong diff, f_diff, f_weight;\n\n\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);\n\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);\n\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);\n\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);\n\n\t\t\t/* Decay existing window, copy faults since last scan */\n\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;\n\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];\n\t\t\tp->numa_faults[membuf_idx] = 0;\n\n\t\t\t/*\n\t\t\t * Normalize the faults_from, so all tasks in a group\n\t\t\t * count according to CPU use, instead of by the raw\n\t\t\t * number of faults. Tasks with little runtime have\n\t\t\t * little over-all impact on throughput, and thus their\n\t\t\t * faults are less important.\n\t\t\t */\n\t\t\tf_weight = div64_u64(runtime << 16, period + 1);\n\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /\n\t\t\t\t   (total_faults + 1);\n\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;\n\t\t\tp->numa_faults[cpubuf_idx] = 0;\n\n\t\t\tp->numa_faults[mem_idx] += diff;\n\t\t\tp->numa_faults[cpu_idx] += f_diff;\n\t\t\tfaults += p->numa_faults[mem_idx];\n\t\t\tp->total_numa_faults += diff;\n\t\t\tif (p->numa_group) {\n\t\t\t\t/*\n\t\t\t\t * safe because we can only change our own group\n\t\t\t\t *\n\t\t\t\t * mem_idx represents the offset for a given\n\t\t\t\t * nid and priv in a specific region because it\n\t\t\t\t * is at the beginning of the numa_faults array.\n\t\t\t\t */\n\t\t\t\tp->numa_group->faults[mem_idx] += diff;\n\t\t\t\tp->numa_group->faults_cpu[mem_idx] += f_diff;\n\t\t\t\tp->numa_group->total_faults += diff;\n\t\t\t\tgroup_faults += p->numa_group->faults[mem_idx];\n\t\t\t}\n\t\t}\n\n\t\tif (!p->numa_group) {\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_nid = nid;\n\t\t\t}\n\t\t} else if (group_faults > max_faults) {\n\t\t\tmax_faults = group_faults;\n\t\t\tmax_nid = nid;\n\t\t}\n\t}\n\n\tif (p->numa_group) {\n\t\tnuma_group_count_active_nodes(p->numa_group);\n\t\tspin_unlock_irq(group_lock);\n\t\tmax_nid = preferred_group_nid(p, max_nid);\n\t}\n\n\tif (max_faults) {\n\t\t/* Set the new preferred node */\n\t\tif (max_nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, max_nid);\n\t}\n\n\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define NR_NUMA_HINT_FAULT_TYPES 2"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_task_scan_period",
          "args": [
            "p",
            "fault_types[0]",
            "fault_types[1]"
          ],
          "line": 2202
        },
        "resolved": true,
        "details": {
          "function_name": "update_task_scan_period",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1910-1979",
          "snippet": "static void update_task_scan_period(struct task_struct *p,\n\t\t\tunsigned long shared, unsigned long private)\n{\n\tunsigned int period_slot;\n\tint lr_ratio, ps_ratio;\n\tint diff;\n\n\tunsigned long remote = p->numa_faults_locality[0];\n\tunsigned long local = p->numa_faults_locality[1];\n\n\t/*\n\t * If there were no record hinting faults then either the task is\n\t * completely idle or all activity is areas that are not of interest\n\t * to automatic numa balancing. Related to that, if there were failed\n\t * migration then it implies we are migrating too quickly or the local\n\t * node is overloaded. In either case, scan slower\n\t */\n\tif (local + shared == 0 || p->numa_faults_locality[2]) {\n\t\tp->numa_scan_period = min(p->numa_scan_period_max,\n\t\t\tp->numa_scan_period << 1);\n\n\t\tp->mm->numa_next_scan = jiffies +\n\t\t\tmsecs_to_jiffies(p->numa_scan_period);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Prepare to scale scan period relative to the current period.\n\t *\t == NUMA_PERIOD_THRESHOLD scan period stays the same\n\t *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)\n\t *\t >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)\n\t */\n\tperiod_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);\n\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);\n\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared);\n\n\tif (ps_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are local. There is no need to\n\t\t * do fast NUMA scanning, since memory is already local.\n\t\t */\n\t\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else if (lr_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are shared with other tasks.\n\t\t * There is no point in continuing fast NUMA scanning,\n\t\t * since other tasks may just move the memory elsewhere.\n\t\t */\n\t\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else {\n\t\t/*\n\t\t * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS,\n\t\t * yet they are not on the local NUMA node. Speed up\n\t\t * NUMA scanning to get the memory moved over.\n\t\t */\n\t\tint ratio = max(lr_ratio, ps_ratio);\n\t\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;\n\t}\n\n\tp->numa_scan_period = clamp(p->numa_scan_period + diff,\n\t\t\ttask_scan_min(p), task_scan_max(p));\n\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NUMA_PERIOD_THRESHOLD 7",
            "#define NUMA_PERIOD_SLOTS 10"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NUMA_PERIOD_THRESHOLD 7\n#define NUMA_PERIOD_SLOTS 10\n\nstatic void update_task_scan_period(struct task_struct *p,\n\t\t\tunsigned long shared, unsigned long private)\n{\n\tunsigned int period_slot;\n\tint lr_ratio, ps_ratio;\n\tint diff;\n\n\tunsigned long remote = p->numa_faults_locality[0];\n\tunsigned long local = p->numa_faults_locality[1];\n\n\t/*\n\t * If there were no record hinting faults then either the task is\n\t * completely idle or all activity is areas that are not of interest\n\t * to automatic numa balancing. Related to that, if there were failed\n\t * migration then it implies we are migrating too quickly or the local\n\t * node is overloaded. In either case, scan slower\n\t */\n\tif (local + shared == 0 || p->numa_faults_locality[2]) {\n\t\tp->numa_scan_period = min(p->numa_scan_period_max,\n\t\t\tp->numa_scan_period << 1);\n\n\t\tp->mm->numa_next_scan = jiffies +\n\t\t\tmsecs_to_jiffies(p->numa_scan_period);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Prepare to scale scan period relative to the current period.\n\t *\t == NUMA_PERIOD_THRESHOLD scan period stays the same\n\t *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)\n\t *\t >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)\n\t */\n\tperiod_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);\n\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);\n\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared);\n\n\tif (ps_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are local. There is no need to\n\t\t * do fast NUMA scanning, since memory is already local.\n\t\t */\n\t\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else if (lr_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are shared with other tasks.\n\t\t * There is no point in continuing fast NUMA scanning,\n\t\t * since other tasks may just move the memory elsewhere.\n\t\t */\n\t\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else {\n\t\t/*\n\t\t * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS,\n\t\t * yet they are not on the local NUMA node. Speed up\n\t\t * NUMA scanning to get the memory moved over.\n\t\t */\n\t\tint ratio = max(lr_ratio, ps_ratio);\n\t\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;\n\t}\n\n\tp->numa_scan_period = clamp(p->numa_scan_period + diff,\n\t\t\ttask_scan_min(p), task_scan_max(p));\n\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_setnuma",
          "args": [
            "p",
            "max_nid"
          ],
          "line": 2199
        },
        "resolved": true,
        "details": {
          "function_name": "sched_setnuma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "5493-5515",
          "snippet": "void sched_setnuma(struct task_struct *p, int nid)\n{\n\tbool queued, running;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(p, &rf);\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\n\tif (queued)\n\t\tdequeue_task(rq, p, DEQUEUE_SAVE);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tp->numa_preferred_nid = nid;\n\n\tif (queued)\n\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);\n\tif (running)\n\t\tset_curr_task(rq, p);\n\ttask_rq_unlock(rq, p, &rf);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid sched_setnuma(struct task_struct *p, int nid)\n{\n\tbool queued, running;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(p, &rf);\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\n\tif (queued)\n\t\tdequeue_task(rq, p, DEQUEUE_SAVE);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tp->numa_preferred_nid = nid;\n\n\tif (queued)\n\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);\n\tif (running)\n\t\tset_curr_task(rq, p);\n\ttask_rq_unlock(rq, p, &rf);\n}"
        }
      },
      {
        "call_info": {
          "callee": "preferred_group_nid",
          "args": [
            "p",
            "max_nid"
          ],
          "line": 2193
        },
        "resolved": true,
        "details": {
          "function_name": "preferred_group_nid",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "2014-2095",
          "snippet": "static int preferred_group_nid(struct task_struct *p, int nid)\n{\n\tnodemask_t nodes;\n\tint dist;\n\n\t/* Direct connections between all NUMA nodes. */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn nid;\n\n\t/*\n\t * On a system with glueless mesh NUMA topology, group_weight\n\t * scores nodes according to the number of NUMA hinting faults on\n\t * both the node itself, and on nearby nodes.\n\t */\n\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\tunsigned long score, max_score = 0;\n\t\tint node, max_node = nid;\n\n\t\tdist = sched_max_numa_distance;\n\n\t\tfor_each_online_node(node) {\n\t\t\tscore = group_weight(p, node, dist);\n\t\t\tif (score > max_score) {\n\t\t\t\tmax_score = score;\n\t\t\t\tmax_node = node;\n\t\t\t}\n\t\t}\n\t\treturn max_node;\n\t}\n\n\t/*\n\t * Finding the preferred nid in a system with NUMA backplane\n\t * interconnect topology is more involved. The goal is to locate\n\t * tasks from numa_groups near each other in the system, and\n\t * untangle workloads from different sides of the system. This requires\n\t * searching down the hierarchy of node groups, recursively searching\n\t * inside the highest scoring group of nodes. The nodemask tricks\n\t * keep the complexity of the search down.\n\t */\n\tnodes = node_online_map;\n\tfor (dist = sched_max_numa_distance; dist > LOCAL_DISTANCE; dist--) {\n\t\tunsigned long max_faults = 0;\n\t\tnodemask_t max_group = NODE_MASK_NONE;\n\t\tint a, b;\n\n\t\t/* Are there nodes at this distance from each other? */\n\t\tif (!find_numa_distance(dist))\n\t\t\tcontinue;\n\n\t\tfor_each_node_mask(a, nodes) {\n\t\t\tunsigned long faults = 0;\n\t\t\tnodemask_t this_group;\n\t\t\tnodes_clear(this_group);\n\n\t\t\t/* Sum group's NUMA faults; includes a==b case. */\n\t\t\tfor_each_node_mask(b, nodes) {\n\t\t\t\tif (node_distance(a, b) < dist) {\n\t\t\t\t\tfaults += group_faults(p, b);\n\t\t\t\t\tnode_set(b, this_group);\n\t\t\t\t\tnode_clear(b, nodes);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Remember the top group. */\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_group = this_group;\n\t\t\t\t/*\n\t\t\t\t * subtle: at the smallest distance there is\n\t\t\t\t * just one node left in each \"group\", the\n\t\t\t\t * winner is the preferred nid.\n\t\t\t\t */\n\t\t\t\tnid = a;\n\t\t\t}\n\t\t}\n\t\t/* Next round, evaluate the nodes within max_group. */\n\t\tif (!max_faults)\n\t\t\tbreak;\n\t\tnodes = max_group;\n\t}\n\treturn nid;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int preferred_group_nid(struct task_struct *p, int nid)\n{\n\tnodemask_t nodes;\n\tint dist;\n\n\t/* Direct connections between all NUMA nodes. */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn nid;\n\n\t/*\n\t * On a system with glueless mesh NUMA topology, group_weight\n\t * scores nodes according to the number of NUMA hinting faults on\n\t * both the node itself, and on nearby nodes.\n\t */\n\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\tunsigned long score, max_score = 0;\n\t\tint node, max_node = nid;\n\n\t\tdist = sched_max_numa_distance;\n\n\t\tfor_each_online_node(node) {\n\t\t\tscore = group_weight(p, node, dist);\n\t\t\tif (score > max_score) {\n\t\t\t\tmax_score = score;\n\t\t\t\tmax_node = node;\n\t\t\t}\n\t\t}\n\t\treturn max_node;\n\t}\n\n\t/*\n\t * Finding the preferred nid in a system with NUMA backplane\n\t * interconnect topology is more involved. The goal is to locate\n\t * tasks from numa_groups near each other in the system, and\n\t * untangle workloads from different sides of the system. This requires\n\t * searching down the hierarchy of node groups, recursively searching\n\t * inside the highest scoring group of nodes. The nodemask tricks\n\t * keep the complexity of the search down.\n\t */\n\tnodes = node_online_map;\n\tfor (dist = sched_max_numa_distance; dist > LOCAL_DISTANCE; dist--) {\n\t\tunsigned long max_faults = 0;\n\t\tnodemask_t max_group = NODE_MASK_NONE;\n\t\tint a, b;\n\n\t\t/* Are there nodes at this distance from each other? */\n\t\tif (!find_numa_distance(dist))\n\t\t\tcontinue;\n\n\t\tfor_each_node_mask(a, nodes) {\n\t\t\tunsigned long faults = 0;\n\t\t\tnodemask_t this_group;\n\t\t\tnodes_clear(this_group);\n\n\t\t\t/* Sum group's NUMA faults; includes a==b case. */\n\t\t\tfor_each_node_mask(b, nodes) {\n\t\t\t\tif (node_distance(a, b) < dist) {\n\t\t\t\t\tfaults += group_faults(p, b);\n\t\t\t\t\tnode_set(b, this_group);\n\t\t\t\t\tnode_clear(b, nodes);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Remember the top group. */\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_group = this_group;\n\t\t\t\t/*\n\t\t\t\t * subtle: at the smallest distance there is\n\t\t\t\t * just one node left in each \"group\", the\n\t\t\t\t * winner is the preferred nid.\n\t\t\t\t */\n\t\t\t\tnid = a;\n\t\t\t}\n\t\t}\n\t\t/* Next round, evaluate the nodes within max_group. */\n\t\tif (!max_faults)\n\t\t\tbreak;\n\t\tnodes = max_group;\n\t}\n\treturn nid;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock_irq",
          "args": [
            "group_lock"
          ],
          "line": 2192
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "numa_group_count_active_nodes",
          "args": [
            "p->numa_group"
          ],
          "line": 2191
        },
        "resolved": true,
        "details": {
          "function_name": "numa_group_count_active_nodes",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1873-1892",
          "snippet": "static void numa_group_count_active_nodes(struct numa_group *numa_group)\n{\n\tunsigned long faults, max_faults = 0;\n\tint nid, active_nodes = 0;\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults > max_faults)\n\t\t\tmax_faults = faults;\n\t}\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults * ACTIVE_NODE_FRACTION > max_faults)\n\t\t\tactive_nodes++;\n\t}\n\n\tnuma_group->max_faults_cpu = max_faults;\n\tnuma_group->active_nodes = active_nodes;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define ACTIVE_NODE_FRACTION 3"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define ACTIVE_NODE_FRACTION 3\n\nstatic void numa_group_count_active_nodes(struct numa_group *numa_group)\n{\n\tunsigned long faults, max_faults = 0;\n\tint nid, active_nodes = 0;\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults > max_faults)\n\t\t\tmax_faults = faults;\n\t}\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults * ACTIVE_NODE_FRACTION > max_faults)\n\t\t\tactive_nodes++;\n\t}\n\n\tnuma_group->max_faults_cpu = max_faults;\n\tnuma_group->active_nodes = active_nodes;\n}"
        }
      },
      {
        "call_info": {
          "callee": "div64_u64",
          "args": [
            "runtime << 16",
            "period + 1"
          ],
          "line": 2154
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_CPUBUF",
            "nid",
            "priv"
          ],
          "line": 2140
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock_irq",
          "args": [
            "group_lock"
          ],
          "line": 2124
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "numa_get_avg_runtime",
          "args": [
            "p",
            "&period"
          ],
          "line": 2119
        },
        "resolved": true,
        "details": {
          "function_name": "numa_get_avg_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1988-2007",
          "snippet": "static u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)\n{\n\tu64 runtime, delta, now;\n\t/* Use the start of this time slice to avoid calculations. */\n\tnow = p->se.exec_start;\n\truntime = p->se.sum_exec_runtime;\n\n\tif (p->last_task_numa_placement) {\n\t\tdelta = runtime - p->last_sum_exec_runtime;\n\t\t*period = now - p->last_task_numa_placement;\n\t} else {\n\t\tdelta = p->se.avg.load_sum;\n\t\t*period = LOAD_AVG_MAX;\n\t}\n\n\tp->last_sum_exec_runtime = runtime;\n\tp->last_task_numa_placement = now;\n\n\treturn delta;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)\n{\n\tu64 runtime, delta, now;\n\t/* Use the start of this time slice to avoid calculations. */\n\tnow = p->se.exec_start;\n\truntime = p->se.sum_exec_runtime;\n\n\tif (p->last_task_numa_placement) {\n\t\tdelta = runtime - p->last_sum_exec_runtime;\n\t\t*period = now - p->last_task_numa_placement;\n\t} else {\n\t\tdelta = p->se.avg.load_sum;\n\t\t*period = LOAD_AVG_MAX;\n\t}\n\n\tp->last_sum_exec_runtime = runtime;\n\tp->last_task_numa_placement = now;\n\n\treturn delta;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_scan_max",
          "args": [
            "p"
          ],
          "line": 2115
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_max",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1116-1139",
          "snippet": "static unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "p->mm->numa_scan_seq"
          ],
          "line": 2111
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic void task_numa_placement(struct task_struct *p)\n{\n\tint seq, nid, max_nid = -1;\n\tunsigned long max_faults = 0;\n\tunsigned long fault_types[2] = { 0, 0 };\n\tunsigned long total_faults;\n\tu64 runtime, period;\n\tspinlock_t *group_lock = NULL;\n\n\t/*\n\t * The p->mm->numa_scan_seq field gets updated without\n\t * exclusive access. Use READ_ONCE() here to ensure\n\t * that the field is read in a single access:\n\t */\n\tseq = READ_ONCE(p->mm->numa_scan_seq);\n\tif (p->numa_scan_seq == seq)\n\t\treturn;\n\tp->numa_scan_seq = seq;\n\tp->numa_scan_period_max = task_scan_max(p);\n\n\ttotal_faults = p->numa_faults_locality[0] +\n\t\t       p->numa_faults_locality[1];\n\truntime = numa_get_avg_runtime(p, &period);\n\n\t/* If the task is part of a group prevent parallel updates to group stats */\n\tif (p->numa_group) {\n\t\tgroup_lock = &p->numa_group->lock;\n\t\tspin_lock_irq(group_lock);\n\t}\n\n\t/* Find the node with the highest number of faults */\n\tfor_each_online_node(nid) {\n\t\t/* Keep track of the offsets in numa_faults array */\n\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;\n\t\tunsigned long faults = 0, group_faults = 0;\n\t\tint priv;\n\n\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {\n\t\t\tlong diff, f_diff, f_weight;\n\n\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);\n\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);\n\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);\n\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);\n\n\t\t\t/* Decay existing window, copy faults since last scan */\n\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;\n\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];\n\t\t\tp->numa_faults[membuf_idx] = 0;\n\n\t\t\t/*\n\t\t\t * Normalize the faults_from, so all tasks in a group\n\t\t\t * count according to CPU use, instead of by the raw\n\t\t\t * number of faults. Tasks with little runtime have\n\t\t\t * little over-all impact on throughput, and thus their\n\t\t\t * faults are less important.\n\t\t\t */\n\t\t\tf_weight = div64_u64(runtime << 16, period + 1);\n\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /\n\t\t\t\t   (total_faults + 1);\n\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;\n\t\t\tp->numa_faults[cpubuf_idx] = 0;\n\n\t\t\tp->numa_faults[mem_idx] += diff;\n\t\t\tp->numa_faults[cpu_idx] += f_diff;\n\t\t\tfaults += p->numa_faults[mem_idx];\n\t\t\tp->total_numa_faults += diff;\n\t\t\tif (p->numa_group) {\n\t\t\t\t/*\n\t\t\t\t * safe because we can only change our own group\n\t\t\t\t *\n\t\t\t\t * mem_idx represents the offset for a given\n\t\t\t\t * nid and priv in a specific region because it\n\t\t\t\t * is at the beginning of the numa_faults array.\n\t\t\t\t */\n\t\t\t\tp->numa_group->faults[mem_idx] += diff;\n\t\t\t\tp->numa_group->faults_cpu[mem_idx] += f_diff;\n\t\t\t\tp->numa_group->total_faults += diff;\n\t\t\t\tgroup_faults += p->numa_group->faults[mem_idx];\n\t\t\t}\n\t\t}\n\n\t\tif (!p->numa_group) {\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_nid = nid;\n\t\t\t}\n\t\t} else if (group_faults > max_faults) {\n\t\t\tmax_faults = group_faults;\n\t\t\tmax_nid = nid;\n\t\t}\n\t}\n\n\tif (p->numa_group) {\n\t\tnuma_group_count_active_nodes(p->numa_group);\n\t\tspin_unlock_irq(group_lock);\n\t\tmax_nid = preferred_group_nid(p, max_nid);\n\t}\n\n\tif (max_faults) {\n\t\t/* Set the new preferred node */\n\t\tif (max_nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, max_nid);\n\t}\n\n\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);\n}"
  },
  {
    "function_name": "preferred_group_nid",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "2014-2095",
    "snippet": "static int preferred_group_nid(struct task_struct *p, int nid)\n{\n\tnodemask_t nodes;\n\tint dist;\n\n\t/* Direct connections between all NUMA nodes. */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn nid;\n\n\t/*\n\t * On a system with glueless mesh NUMA topology, group_weight\n\t * scores nodes according to the number of NUMA hinting faults on\n\t * both the node itself, and on nearby nodes.\n\t */\n\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\tunsigned long score, max_score = 0;\n\t\tint node, max_node = nid;\n\n\t\tdist = sched_max_numa_distance;\n\n\t\tfor_each_online_node(node) {\n\t\t\tscore = group_weight(p, node, dist);\n\t\t\tif (score > max_score) {\n\t\t\t\tmax_score = score;\n\t\t\t\tmax_node = node;\n\t\t\t}\n\t\t}\n\t\treturn max_node;\n\t}\n\n\t/*\n\t * Finding the preferred nid in a system with NUMA backplane\n\t * interconnect topology is more involved. The goal is to locate\n\t * tasks from numa_groups near each other in the system, and\n\t * untangle workloads from different sides of the system. This requires\n\t * searching down the hierarchy of node groups, recursively searching\n\t * inside the highest scoring group of nodes. The nodemask tricks\n\t * keep the complexity of the search down.\n\t */\n\tnodes = node_online_map;\n\tfor (dist = sched_max_numa_distance; dist > LOCAL_DISTANCE; dist--) {\n\t\tunsigned long max_faults = 0;\n\t\tnodemask_t max_group = NODE_MASK_NONE;\n\t\tint a, b;\n\n\t\t/* Are there nodes at this distance from each other? */\n\t\tif (!find_numa_distance(dist))\n\t\t\tcontinue;\n\n\t\tfor_each_node_mask(a, nodes) {\n\t\t\tunsigned long faults = 0;\n\t\t\tnodemask_t this_group;\n\t\t\tnodes_clear(this_group);\n\n\t\t\t/* Sum group's NUMA faults; includes a==b case. */\n\t\t\tfor_each_node_mask(b, nodes) {\n\t\t\t\tif (node_distance(a, b) < dist) {\n\t\t\t\t\tfaults += group_faults(p, b);\n\t\t\t\t\tnode_set(b, this_group);\n\t\t\t\t\tnode_clear(b, nodes);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Remember the top group. */\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_group = this_group;\n\t\t\t\t/*\n\t\t\t\t * subtle: at the smallest distance there is\n\t\t\t\t * just one node left in each \"group\", the\n\t\t\t\t * winner is the preferred nid.\n\t\t\t\t */\n\t\t\t\tnid = a;\n\t\t\t}\n\t\t}\n\t\t/* Next round, evaluate the nodes within max_group. */\n\t\tif (!max_faults)\n\t\t\tbreak;\n\t\tnodes = max_group;\n\t}\n\treturn nid;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "node_clear",
          "args": [
            "b",
            "nodes"
          ],
          "line": 2073
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "node_set",
          "args": [
            "b",
            "this_group"
          ],
          "line": 2072
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_faults",
          "args": [
            "p",
            "b"
          ],
          "line": 2071
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1237-1241",
          "snippet": "static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      },
      {
        "call_info": {
          "callee": "node_distance",
          "args": [
            "a",
            "b"
          ],
          "line": 2070
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_node_mask",
          "args": [
            "b",
            "nodes"
          ],
          "line": 2069
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "nodes_clear",
          "args": [
            "this_group"
          ],
          "line": 2066
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_node_mask",
          "args": [
            "a",
            "nodes"
          ],
          "line": 2063
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "find_numa_distance",
          "args": [
            "dist"
          ],
          "line": 2060
        },
        "resolved": true,
        "details": {
          "function_name": "find_numa_distance",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/topology.c",
          "lines": "1266-1279",
          "snippet": "bool find_numa_distance(int distance)\n{\n\tint i;\n\n\tif (distance == node_distance(0, 0))\n\t\treturn true;\n\n\tfor (i = 0; i < sched_domains_numa_levels; i++) {\n\t\tif (sched_domains_numa_distance[i] == distance)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}",
          "includes": [
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched.h\"\n\nbool find_numa_distance(int distance)\n{\n\tint i;\n\n\tif (distance == node_distance(0, 0))\n\t\treturn true;\n\n\tfor (i = 0; i < sched_domains_numa_levels; i++) {\n\t\tif (sched_domains_numa_distance[i] == distance)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_weight",
          "args": [
            "p",
            "node",
            "dist"
          ],
          "line": 2035
        },
        "resolved": true,
        "details": {
          "function_name": "group_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1369-1386",
          "snippet": "static inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int preferred_group_nid(struct task_struct *p, int nid)\n{\n\tnodemask_t nodes;\n\tint dist;\n\n\t/* Direct connections between all NUMA nodes. */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn nid;\n\n\t/*\n\t * On a system with glueless mesh NUMA topology, group_weight\n\t * scores nodes according to the number of NUMA hinting faults on\n\t * both the node itself, and on nearby nodes.\n\t */\n\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\tunsigned long score, max_score = 0;\n\t\tint node, max_node = nid;\n\n\t\tdist = sched_max_numa_distance;\n\n\t\tfor_each_online_node(node) {\n\t\t\tscore = group_weight(p, node, dist);\n\t\t\tif (score > max_score) {\n\t\t\t\tmax_score = score;\n\t\t\t\tmax_node = node;\n\t\t\t}\n\t\t}\n\t\treturn max_node;\n\t}\n\n\t/*\n\t * Finding the preferred nid in a system with NUMA backplane\n\t * interconnect topology is more involved. The goal is to locate\n\t * tasks from numa_groups near each other in the system, and\n\t * untangle workloads from different sides of the system. This requires\n\t * searching down the hierarchy of node groups, recursively searching\n\t * inside the highest scoring group of nodes. The nodemask tricks\n\t * keep the complexity of the search down.\n\t */\n\tnodes = node_online_map;\n\tfor (dist = sched_max_numa_distance; dist > LOCAL_DISTANCE; dist--) {\n\t\tunsigned long max_faults = 0;\n\t\tnodemask_t max_group = NODE_MASK_NONE;\n\t\tint a, b;\n\n\t\t/* Are there nodes at this distance from each other? */\n\t\tif (!find_numa_distance(dist))\n\t\t\tcontinue;\n\n\t\tfor_each_node_mask(a, nodes) {\n\t\t\tunsigned long faults = 0;\n\t\t\tnodemask_t this_group;\n\t\t\tnodes_clear(this_group);\n\n\t\t\t/* Sum group's NUMA faults; includes a==b case. */\n\t\t\tfor_each_node_mask(b, nodes) {\n\t\t\t\tif (node_distance(a, b) < dist) {\n\t\t\t\t\tfaults += group_faults(p, b);\n\t\t\t\t\tnode_set(b, this_group);\n\t\t\t\t\tnode_clear(b, nodes);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Remember the top group. */\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_group = this_group;\n\t\t\t\t/*\n\t\t\t\t * subtle: at the smallest distance there is\n\t\t\t\t * just one node left in each \"group\", the\n\t\t\t\t * winner is the preferred nid.\n\t\t\t\t */\n\t\t\t\tnid = a;\n\t\t\t}\n\t\t}\n\t\t/* Next round, evaluate the nodes within max_group. */\n\t\tif (!max_faults)\n\t\t\tbreak;\n\t\tnodes = max_group;\n\t}\n\treturn nid;\n}"
  },
  {
    "function_name": "numa_get_avg_runtime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1988-2007",
    "snippet": "static u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)\n{\n\tu64 runtime, delta, now;\n\t/* Use the start of this time slice to avoid calculations. */\n\tnow = p->se.exec_start;\n\truntime = p->se.sum_exec_runtime;\n\n\tif (p->last_task_numa_placement) {\n\t\tdelta = runtime - p->last_sum_exec_runtime;\n\t\t*period = now - p->last_task_numa_placement;\n\t} else {\n\t\tdelta = p->se.avg.load_sum;\n\t\t*period = LOAD_AVG_MAX;\n\t}\n\n\tp->last_sum_exec_runtime = runtime;\n\tp->last_task_numa_placement = now;\n\n\treturn delta;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)\n{\n\tu64 runtime, delta, now;\n\t/* Use the start of this time slice to avoid calculations. */\n\tnow = p->se.exec_start;\n\truntime = p->se.sum_exec_runtime;\n\n\tif (p->last_task_numa_placement) {\n\t\tdelta = runtime - p->last_sum_exec_runtime;\n\t\t*period = now - p->last_task_numa_placement;\n\t} else {\n\t\tdelta = p->se.avg.load_sum;\n\t\t*period = LOAD_AVG_MAX;\n\t}\n\n\tp->last_sum_exec_runtime = runtime;\n\tp->last_task_numa_placement = now;\n\n\treturn delta;\n}"
  },
  {
    "function_name": "update_task_scan_period",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1910-1979",
    "snippet": "static void update_task_scan_period(struct task_struct *p,\n\t\t\tunsigned long shared, unsigned long private)\n{\n\tunsigned int period_slot;\n\tint lr_ratio, ps_ratio;\n\tint diff;\n\n\tunsigned long remote = p->numa_faults_locality[0];\n\tunsigned long local = p->numa_faults_locality[1];\n\n\t/*\n\t * If there were no record hinting faults then either the task is\n\t * completely idle or all activity is areas that are not of interest\n\t * to automatic numa balancing. Related to that, if there were failed\n\t * migration then it implies we are migrating too quickly or the local\n\t * node is overloaded. In either case, scan slower\n\t */\n\tif (local + shared == 0 || p->numa_faults_locality[2]) {\n\t\tp->numa_scan_period = min(p->numa_scan_period_max,\n\t\t\tp->numa_scan_period << 1);\n\n\t\tp->mm->numa_next_scan = jiffies +\n\t\t\tmsecs_to_jiffies(p->numa_scan_period);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Prepare to scale scan period relative to the current period.\n\t *\t == NUMA_PERIOD_THRESHOLD scan period stays the same\n\t *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)\n\t *\t >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)\n\t */\n\tperiod_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);\n\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);\n\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared);\n\n\tif (ps_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are local. There is no need to\n\t\t * do fast NUMA scanning, since memory is already local.\n\t\t */\n\t\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else if (lr_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are shared with other tasks.\n\t\t * There is no point in continuing fast NUMA scanning,\n\t\t * since other tasks may just move the memory elsewhere.\n\t\t */\n\t\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else {\n\t\t/*\n\t\t * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS,\n\t\t * yet they are not on the local NUMA node. Speed up\n\t\t * NUMA scanning to get the memory moved over.\n\t\t */\n\t\tint ratio = max(lr_ratio, ps_ratio);\n\t\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;\n\t}\n\n\tp->numa_scan_period = clamp(p->numa_scan_period + diff,\n\t\t\ttask_scan_min(p), task_scan_max(p));\n\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define NUMA_PERIOD_THRESHOLD 7",
      "#define NUMA_PERIOD_SLOTS 10"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "p->numa_faults_locality",
            "0",
            "sizeof(p->numa_faults_locality)"
          ],
          "line": 1978
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "clamp",
          "args": [
            "p->numa_scan_period + diff",
            "task_scan_min(p)",
            "task_scan_max(p)"
          ],
          "line": 1976
        },
        "resolved": true,
        "details": {
          "function_name": "wq_clamp_max_active",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/workqueue.c",
          "lines": "4004-4014",
          "snippet": "static int wq_clamp_max_active(int max_active, unsigned int flags,\n\t\t\t       const char *name)\n{\n\tint lim = flags & WQ_UNBOUND ? WQ_UNBOUND_MAX_ACTIVE : WQ_MAX_ACTIVE;\n\n\tif (max_active < 1 || max_active > lim)\n\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",\n\t\t\tmax_active, name, 1, lim);\n\n\treturn clamp_val(max_active, 1, lim);\n}",
          "includes": [
            "#include <trace/events/workqueue.h>",
            "#include \"workqueue_internal.h\"",
            "#include <linux/nmi.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/rculist.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/jhash.h>",
            "#include <linux/idr.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/freezer.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/kthread.h>",
            "#include <linux/notifier.h>",
            "#include <linux/cpu.h>",
            "#include <linux/slab.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/completion.h>",
            "#include <linux/signal.h>",
            "#include <linux/init.h>",
            "#include <linux/sched.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/workqueue.h>\n#include \"workqueue_internal.h\"\n#include <linux/nmi.h>\n#include <linux/sched/isolation.h>\n#include <linux/uaccess.h>\n#include <linux/moduleparam.h>\n#include <linux/nodemask.h>\n#include <linux/rculist.h>\n#include <linux/hashtable.h>\n#include <linux/jhash.h>\n#include <linux/idr.h>\n#include <linux/lockdep.h>\n#include <linux/debug_locks.h>\n#include <linux/freezer.h>\n#include <linux/mempolicy.h>\n#include <linux/hardirq.h>\n#include <linux/kthread.h>\n#include <linux/notifier.h>\n#include <linux/cpu.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/completion.h>\n#include <linux/signal.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nstatic int wq_clamp_max_active(int max_active, unsigned int flags,\n\t\t\t       const char *name)\n{\n\tint lim = flags & WQ_UNBOUND ? WQ_UNBOUND_MAX_ACTIVE : WQ_MAX_ACTIVE;\n\n\tif (max_active < 1 || max_active > lim)\n\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",\n\t\t\tmax_active, name, 1, lim);\n\n\treturn clamp_val(max_active, 1, lim);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_scan_max",
          "args": [
            "p"
          ],
          "line": 1977
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_max",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1116-1139",
          "snippet": "static unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_scan_min",
          "args": [
            "p"
          ],
          "line": 1977
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_min",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1083-1095",
          "snippet": "static unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define MAX_SCAN_WINDOW 2560"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define MAX_SCAN_WINDOW 2560\n\nstatic unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}"
        }
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "lr_ratio",
            "ps_ratio"
          ],
          "line": 1972
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DIV_ROUND_UP",
          "args": [
            "p->numa_scan_period",
            "NUMA_PERIOD_SLOTS"
          ],
          "line": 1943
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "msecs_to_jiffies",
          "args": [
            "p->numa_scan_period"
          ],
          "line": 1932
        },
        "resolved": true,
        "details": {
          "function_name": "__msecs_to_jiffies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/time.c",
          "lines": "565-573",
          "snippet": "unsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}",
          "includes": [
            "#include \"timekeeping.h\"",
            "#include <generated/timeconst.h>",
            "#include <asm/unistd.h>",
            "#include <linux/compat.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/math64.h>",
            "#include <linux/fs.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/errno.h>",
            "#include <linux/timekeeper_internal.h>",
            "#include <linux/capability.h>",
            "#include <linux/timex.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"timekeeping.h\"\n#include <generated/timeconst.h>\n#include <asm/unistd.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/math64.h>\n#include <linux/fs.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/errno.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/capability.h>\n#include <linux/timex.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nunsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}"
        }
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "p->numa_scan_period_max",
            "p->numa_scan_period << 1"
          ],
          "line": 1928
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NUMA_PERIOD_THRESHOLD 7\n#define NUMA_PERIOD_SLOTS 10\n\nstatic void update_task_scan_period(struct task_struct *p,\n\t\t\tunsigned long shared, unsigned long private)\n{\n\tunsigned int period_slot;\n\tint lr_ratio, ps_ratio;\n\tint diff;\n\n\tunsigned long remote = p->numa_faults_locality[0];\n\tunsigned long local = p->numa_faults_locality[1];\n\n\t/*\n\t * If there were no record hinting faults then either the task is\n\t * completely idle or all activity is areas that are not of interest\n\t * to automatic numa balancing. Related to that, if there were failed\n\t * migration then it implies we are migrating too quickly or the local\n\t * node is overloaded. In either case, scan slower\n\t */\n\tif (local + shared == 0 || p->numa_faults_locality[2]) {\n\t\tp->numa_scan_period = min(p->numa_scan_period_max,\n\t\t\tp->numa_scan_period << 1);\n\n\t\tp->mm->numa_next_scan = jiffies +\n\t\t\tmsecs_to_jiffies(p->numa_scan_period);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * Prepare to scale scan period relative to the current period.\n\t *\t == NUMA_PERIOD_THRESHOLD scan period stays the same\n\t *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)\n\t *\t >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)\n\t */\n\tperiod_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);\n\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);\n\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared);\n\n\tif (ps_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are local. There is no need to\n\t\t * do fast NUMA scanning, since memory is already local.\n\t\t */\n\t\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else if (lr_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t/*\n\t\t * Most memory accesses are shared with other tasks.\n\t\t * There is no point in continuing fast NUMA scanning,\n\t\t * since other tasks may just move the memory elsewhere.\n\t\t */\n\t\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else {\n\t\t/*\n\t\t * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS,\n\t\t * yet they are not on the local NUMA node. Speed up\n\t\t * NUMA scanning to get the memory moved over.\n\t\t */\n\t\tint ratio = max(lr_ratio, ps_ratio);\n\t\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;\n\t}\n\n\tp->numa_scan_period = clamp(p->numa_scan_period + diff,\n\t\t\ttask_scan_min(p), task_scan_max(p));\n\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n}"
  },
  {
    "function_name": "numa_group_count_active_nodes",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1873-1892",
    "snippet": "static void numa_group_count_active_nodes(struct numa_group *numa_group)\n{\n\tunsigned long faults, max_faults = 0;\n\tint nid, active_nodes = 0;\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults > max_faults)\n\t\t\tmax_faults = faults;\n\t}\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults * ACTIVE_NODE_FRACTION > max_faults)\n\t\t\tactive_nodes++;\n\t}\n\n\tnuma_group->max_faults_cpu = max_faults;\n\tnuma_group->active_nodes = active_nodes;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define ACTIVE_NODE_FRACTION 3"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "group_faults_cpu",
          "args": [
            "numa_group",
            "nid"
          ],
          "line": 1885
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1237-1241",
          "snippet": "static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define ACTIVE_NODE_FRACTION 3\n\nstatic void numa_group_count_active_nodes(struct numa_group *numa_group)\n{\n\tunsigned long faults, max_faults = 0;\n\tint nid, active_nodes = 0;\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults > max_faults)\n\t\t\tmax_faults = faults;\n\t}\n\n\tfor_each_online_node(nid) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults * ACTIVE_NODE_FRACTION > max_faults)\n\t\t\tactive_nodes++;\n\t}\n\n\tnuma_group->max_faults_cpu = max_faults;\n\tnuma_group->active_nodes = active_nodes;\n}"
  },
  {
    "function_name": "numa_migrate_preferred",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1847-1865",
    "snippet": "static void numa_migrate_preferred(struct task_struct *p)\n{\n\tunsigned long interval = HZ;\n\n\t/* This task has no NUMA fault statistics yet */\n\tif (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))\n\t\treturn;\n\n\t/* Periodically retry migrating the task to the preferred node */\n\tinterval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);\n\tp->numa_migrate_retry = jiffies + interval;\n\n\t/* Success if task is already running on preferred CPU */\n\tif (task_node(p) == p->numa_preferred_nid)\n\t\treturn;\n\n\t/* Otherwise, try migrate to a CPU on the preferred node */\n\ttask_numa_migrate(p);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_numa_migrate",
          "args": [
            "p"
          ],
          "line": 1864
        },
        "resolved": true,
        "details": {
          "function_name": "task_numa_migrate",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1717-1844",
          "snippet": "static int task_numa_migrate(struct task_struct *p)\n{\n\tstruct task_numa_env env = {\n\t\t.p = p,\n\n\t\t.src_cpu = task_cpu(p),\n\t\t.src_nid = task_node(p),\n\n\t\t.imbalance_pct = 112,\n\n\t\t.best_task = NULL,\n\t\t.best_imp = 0,\n\t\t.best_cpu = -1,\n\t};\n\tstruct sched_domain *sd;\n\tstruct rq *best_rq;\n\tunsigned long taskweight, groupweight;\n\tint nid, ret, dist;\n\tlong taskimp, groupimp;\n\n\t/*\n\t * Pick the lowest SD_NUMA domain, as that would have the smallest\n\t * imbalance and would be the first to start moving tasks about.\n\t *\n\t * And we want to avoid any moving of tasks about, as that would create\n\t * random movement of tasks -- counter the numa conditions we're trying\n\t * to satisfy here.\n\t */\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));\n\tif (sd)\n\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;\n\trcu_read_unlock();\n\n\t/*\n\t * Cpusets can break the scheduler domain tree into smaller\n\t * balance domains, some of which do not cross NUMA boundaries.\n\t * Tasks that are \"trapped\" in such domains cannot be migrated\n\t * elsewhere, so there is no point in (re)trying.\n\t */\n\tif (unlikely(!sd)) {\n\t\tsched_setnuma(p, task_node(p));\n\t\treturn -EINVAL;\n\t}\n\n\tenv.dst_nid = p->numa_preferred_nid;\n\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);\n\ttaskweight = task_weight(p, env.src_nid, dist);\n\tgroupweight = group_weight(p, env.src_nid, dist);\n\tupdate_numa_stats(&env.src_stats, env.src_nid);\n\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;\n\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;\n\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\n\t/* Try to find a spot on the preferred nid. */\n\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\n\t/*\n\t * Look at other nodes in these cases:\n\t * - there is no space available on the preferred_nid\n\t * - the task is part of a numa_group that is interleaved across\n\t *   multiple NUMA nodes; in order to better consolidate the group,\n\t *   we need to check other locations.\n\t */\n\tif (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {\n\t\tfor_each_online_node(nid) {\n\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)\n\t\t\t\tcontinue;\n\n\t\t\tdist = node_distance(env.src_nid, env.dst_nid);\n\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\t\tdist != env.dist) {\n\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);\n\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);\n\t\t\t}\n\n\t\t\t/* Only consider nodes where both task and groups benefit */\n\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;\n\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;\n\t\t\tif (taskimp < 0 && groupimp < 0)\n\t\t\t\tcontinue;\n\n\t\t\tenv.dist = dist;\n\t\t\tenv.dst_nid = nid;\n\t\t\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\t\t}\n\t}\n\n\t/*\n\t * If the task is part of a workload that spans multiple NUMA nodes,\n\t * and is migrating into one of the workload's active nodes, remember\n\t * this node as the task's preferred numa node, so the workload can\n\t * settle down.\n\t * A task that migrated to a second choice node will be better off\n\t * trying for a better one later. Do not set the preferred node here.\n\t */\n\tif (p->numa_group) {\n\t\tif (env.best_cpu == -1)\n\t\t\tnid = env.src_nid;\n\t\telse\n\t\t\tnid = cpu_to_node(env.best_cpu);\n\n\t\tif (nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, nid);\n\t}\n\n\t/* No better CPU than the current one was found. */\n\tif (env.best_cpu == -1)\n\t\treturn -EAGAIN;\n\n\tbest_rq = cpu_rq(env.best_cpu);\n\tif (env.best_task == NULL) {\n\t\tret = migrate_task_to(p, env.best_cpu);\n\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\t\tif (ret != 0)\n\t\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_cpu);\n\t\treturn ret;\n\t}\n\n\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);\n\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\n\tif (ret != 0)\n\t\ttrace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));\n\tput_task_struct(env.best_task);\n\treturn ret;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int task_numa_migrate(struct task_struct *p)\n{\n\tstruct task_numa_env env = {\n\t\t.p = p,\n\n\t\t.src_cpu = task_cpu(p),\n\t\t.src_nid = task_node(p),\n\n\t\t.imbalance_pct = 112,\n\n\t\t.best_task = NULL,\n\t\t.best_imp = 0,\n\t\t.best_cpu = -1,\n\t};\n\tstruct sched_domain *sd;\n\tstruct rq *best_rq;\n\tunsigned long taskweight, groupweight;\n\tint nid, ret, dist;\n\tlong taskimp, groupimp;\n\n\t/*\n\t * Pick the lowest SD_NUMA domain, as that would have the smallest\n\t * imbalance and would be the first to start moving tasks about.\n\t *\n\t * And we want to avoid any moving of tasks about, as that would create\n\t * random movement of tasks -- counter the numa conditions we're trying\n\t * to satisfy here.\n\t */\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));\n\tif (sd)\n\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;\n\trcu_read_unlock();\n\n\t/*\n\t * Cpusets can break the scheduler domain tree into smaller\n\t * balance domains, some of which do not cross NUMA boundaries.\n\t * Tasks that are \"trapped\" in such domains cannot be migrated\n\t * elsewhere, so there is no point in (re)trying.\n\t */\n\tif (unlikely(!sd)) {\n\t\tsched_setnuma(p, task_node(p));\n\t\treturn -EINVAL;\n\t}\n\n\tenv.dst_nid = p->numa_preferred_nid;\n\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);\n\ttaskweight = task_weight(p, env.src_nid, dist);\n\tgroupweight = group_weight(p, env.src_nid, dist);\n\tupdate_numa_stats(&env.src_stats, env.src_nid);\n\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;\n\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;\n\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\n\t/* Try to find a spot on the preferred nid. */\n\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\n\t/*\n\t * Look at other nodes in these cases:\n\t * - there is no space available on the preferred_nid\n\t * - the task is part of a numa_group that is interleaved across\n\t *   multiple NUMA nodes; in order to better consolidate the group,\n\t *   we need to check other locations.\n\t */\n\tif (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {\n\t\tfor_each_online_node(nid) {\n\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)\n\t\t\t\tcontinue;\n\n\t\t\tdist = node_distance(env.src_nid, env.dst_nid);\n\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\t\tdist != env.dist) {\n\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);\n\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);\n\t\t\t}\n\n\t\t\t/* Only consider nodes where both task and groups benefit */\n\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;\n\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;\n\t\t\tif (taskimp < 0 && groupimp < 0)\n\t\t\t\tcontinue;\n\n\t\t\tenv.dist = dist;\n\t\t\tenv.dst_nid = nid;\n\t\t\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\t\t}\n\t}\n\n\t/*\n\t * If the task is part of a workload that spans multiple NUMA nodes,\n\t * and is migrating into one of the workload's active nodes, remember\n\t * this node as the task's preferred numa node, so the workload can\n\t * settle down.\n\t * A task that migrated to a second choice node will be better off\n\t * trying for a better one later. Do not set the preferred node here.\n\t */\n\tif (p->numa_group) {\n\t\tif (env.best_cpu == -1)\n\t\t\tnid = env.src_nid;\n\t\telse\n\t\t\tnid = cpu_to_node(env.best_cpu);\n\n\t\tif (nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, nid);\n\t}\n\n\t/* No better CPU than the current one was found. */\n\tif (env.best_cpu == -1)\n\t\treturn -EAGAIN;\n\n\tbest_rq = cpu_rq(env.best_cpu);\n\tif (env.best_task == NULL) {\n\t\tret = migrate_task_to(p, env.best_cpu);\n\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\t\tif (ret != 0)\n\t\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_cpu);\n\t\treturn ret;\n\t}\n\n\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);\n\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\n\tif (ret != 0)\n\t\ttrace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));\n\tput_task_struct(env.best_task);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_node",
          "args": [
            "p"
          ],
          "line": 1860
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "min",
          "args": [
            "interval",
            "msecs_to_jiffies(p->numa_scan_period) / 16"
          ],
          "line": 1856
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "msecs_to_jiffies",
          "args": [
            "p->numa_scan_period"
          ],
          "line": 1856
        },
        "resolved": true,
        "details": {
          "function_name": "__msecs_to_jiffies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/time.c",
          "lines": "565-573",
          "snippet": "unsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}",
          "includes": [
            "#include \"timekeeping.h\"",
            "#include <generated/timeconst.h>",
            "#include <asm/unistd.h>",
            "#include <linux/compat.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/math64.h>",
            "#include <linux/fs.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/errno.h>",
            "#include <linux/timekeeper_internal.h>",
            "#include <linux/capability.h>",
            "#include <linux/timex.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"timekeeping.h\"\n#include <generated/timeconst.h>\n#include <asm/unistd.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/math64.h>\n#include <linux/fs.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/errno.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/capability.h>\n#include <linux/timex.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nunsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "p->numa_preferred_nid == -1 || !p->numa_faults"
          ],
          "line": 1852
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void numa_migrate_preferred(struct task_struct *p)\n{\n\tunsigned long interval = HZ;\n\n\t/* This task has no NUMA fault statistics yet */\n\tif (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))\n\t\treturn;\n\n\t/* Periodically retry migrating the task to the preferred node */\n\tinterval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);\n\tp->numa_migrate_retry = jiffies + interval;\n\n\t/* Success if task is already running on preferred CPU */\n\tif (task_node(p) == p->numa_preferred_nid)\n\t\treturn;\n\n\t/* Otherwise, try migrate to a CPU on the preferred node */\n\ttask_numa_migrate(p);\n}"
  },
  {
    "function_name": "task_numa_migrate",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1717-1844",
    "snippet": "static int task_numa_migrate(struct task_struct *p)\n{\n\tstruct task_numa_env env = {\n\t\t.p = p,\n\n\t\t.src_cpu = task_cpu(p),\n\t\t.src_nid = task_node(p),\n\n\t\t.imbalance_pct = 112,\n\n\t\t.best_task = NULL,\n\t\t.best_imp = 0,\n\t\t.best_cpu = -1,\n\t};\n\tstruct sched_domain *sd;\n\tstruct rq *best_rq;\n\tunsigned long taskweight, groupweight;\n\tint nid, ret, dist;\n\tlong taskimp, groupimp;\n\n\t/*\n\t * Pick the lowest SD_NUMA domain, as that would have the smallest\n\t * imbalance and would be the first to start moving tasks about.\n\t *\n\t * And we want to avoid any moving of tasks about, as that would create\n\t * random movement of tasks -- counter the numa conditions we're trying\n\t * to satisfy here.\n\t */\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));\n\tif (sd)\n\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;\n\trcu_read_unlock();\n\n\t/*\n\t * Cpusets can break the scheduler domain tree into smaller\n\t * balance domains, some of which do not cross NUMA boundaries.\n\t * Tasks that are \"trapped\" in such domains cannot be migrated\n\t * elsewhere, so there is no point in (re)trying.\n\t */\n\tif (unlikely(!sd)) {\n\t\tsched_setnuma(p, task_node(p));\n\t\treturn -EINVAL;\n\t}\n\n\tenv.dst_nid = p->numa_preferred_nid;\n\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);\n\ttaskweight = task_weight(p, env.src_nid, dist);\n\tgroupweight = group_weight(p, env.src_nid, dist);\n\tupdate_numa_stats(&env.src_stats, env.src_nid);\n\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;\n\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;\n\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\n\t/* Try to find a spot on the preferred nid. */\n\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\n\t/*\n\t * Look at other nodes in these cases:\n\t * - there is no space available on the preferred_nid\n\t * - the task is part of a numa_group that is interleaved across\n\t *   multiple NUMA nodes; in order to better consolidate the group,\n\t *   we need to check other locations.\n\t */\n\tif (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {\n\t\tfor_each_online_node(nid) {\n\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)\n\t\t\t\tcontinue;\n\n\t\t\tdist = node_distance(env.src_nid, env.dst_nid);\n\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\t\tdist != env.dist) {\n\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);\n\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);\n\t\t\t}\n\n\t\t\t/* Only consider nodes where both task and groups benefit */\n\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;\n\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;\n\t\t\tif (taskimp < 0 && groupimp < 0)\n\t\t\t\tcontinue;\n\n\t\t\tenv.dist = dist;\n\t\t\tenv.dst_nid = nid;\n\t\t\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\t\t}\n\t}\n\n\t/*\n\t * If the task is part of a workload that spans multiple NUMA nodes,\n\t * and is migrating into one of the workload's active nodes, remember\n\t * this node as the task's preferred numa node, so the workload can\n\t * settle down.\n\t * A task that migrated to a second choice node will be better off\n\t * trying for a better one later. Do not set the preferred node here.\n\t */\n\tif (p->numa_group) {\n\t\tif (env.best_cpu == -1)\n\t\t\tnid = env.src_nid;\n\t\telse\n\t\t\tnid = cpu_to_node(env.best_cpu);\n\n\t\tif (nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, nid);\n\t}\n\n\t/* No better CPU than the current one was found. */\n\tif (env.best_cpu == -1)\n\t\treturn -EAGAIN;\n\n\tbest_rq = cpu_rq(env.best_cpu);\n\tif (env.best_task == NULL) {\n\t\tret = migrate_task_to(p, env.best_cpu);\n\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\t\tif (ret != 0)\n\t\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_cpu);\n\t\treturn ret;\n\t}\n\n\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);\n\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\n\tif (ret != 0)\n\t\ttrace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));\n\tput_task_struct(env.best_task);\n\treturn ret;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "put_task_struct",
          "args": [
            "env.best_task"
          ],
          "line": 1842
        },
        "resolved": true,
        "details": {
          "function_name": "__put_task_struct",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/fork.c",
          "lines": "716-731",
          "snippet": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}",
          "includes": [
            "#include <linux/init_task.h>",
            "#include <trace/events/task.h>",
            "#include <trace/events/sched.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/pgtable.h>",
            "#include <linux/stackleak.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/kcov.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/compiler.h>",
            "#include <linux/aio.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/oom.h>",
            "#include <linux/user-return-notifier.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/magic.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/tty.h>",
            "#include <linux/random.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/acct.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/profile.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/audit.h>",
            "#include <linux/mount.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/kthread.h>",
            "#include <linux/compat.h>",
            "#include <linux/futex.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/seccomp.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/cpu.h>",
            "#include <linux/capability.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>",
            "#include <linux/hmm.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/mman.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/sem.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/module.h>",
            "#include <linux/unistd.h>",
            "#include <linux/init.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __latent_entropy struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/init_task.h>\n#include <trace/events/task.h>\n#include <trace/events/sched.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/pgtable.h>\n#include <linux/stackleak.h>\n#include <linux/thread_info.h>\n#include <linux/livepatch.h>\n#include <linux/kcov.h>\n#include <linux/sysctl.h>\n#include <linux/compiler.h>\n#include <linux/aio.h>\n#include <linux/uprobes.h>\n#include <linux/signalfd.h>\n#include <linux/khugepaged.h>\n#include <linux/oom.h>\n#include <linux/user-return-notifier.h>\n#include <linux/posix-timers.h>\n#include <linux/perf_event.h>\n#include <linux/sched/mm.h>\n#include <linux/magic.h>\n#include <linux/fs_struct.h>\n#include <linux/blkdev.h>\n#include <linux/tty.h>\n#include <linux/random.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/freezer.h>\n#include <linux/cn_proc.h>\n#include <linux/tsacct_kern.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/acct.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/profile.h>\n#include <linux/proc_fs.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/audit.h>\n#include <linux/mount.h>\n#include <linux/ptrace.h>\n#include <linux/rcupdate.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/kthread.h>\n#include <linux/compat.h>\n#include <linux/futex.h>\n#include <linux/jiffies.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/seccomp.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/cgroup.h>\n#include <linux/cpu.h>\n#include <linux/capability.h>\n#include <linux/nsproxy.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/hmm.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mman.h>\n#include <linux/binfmts.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/sem.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/vmalloc.h>\n#include <linux/module.h>\n#include <linux/unistd.h>\n#include <linux/init.h>\n#include <linux/rtmutex.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/user.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n\nstatic __latent_entropy struct;\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "trace_sched_stick_numa",
          "args": [
            "p",
            "env.src_cpu",
            "task_cpu(env.best_task)"
          ],
          "line": 1841
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_cpu",
          "args": [
            "env.best_task"
          ],
          "line": 1841
        },
        "resolved": true,
        "details": {
          "function_name": "ignore_task_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/ftrace.c",
          "lines": "6542-6556",
          "snippet": "static void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}",
          "includes": [
            "#include \"trace_stat.h\"",
            "#include \"trace_output.h\"",
            "#include <asm/setup.h>",
            "#include <asm/sections.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/hash.h>",
            "#include <linux/list.h>",
            "#include <linux/sort.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/module.h>",
            "#include <linux/bsearch.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/kthread.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/suspend.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/clocksource.h>",
            "#include <linux/stop_machine.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_MUTEX(ftrace_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_stat.h\"\n#include \"trace_output.h\"\n#include <asm/setup.h>\n#include <asm/sections.h>\n#include <trace/events/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/sort.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/bsearch.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/tracefs.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/sched/task.h>\n#include <linux/clocksource.h>\n#include <linux/stop_machine.h>\n\nstatic DEFINE_MUTEX(ftrace_lock);\n\nstatic void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\n\t/*\n\t * This function is called by on_each_cpu() while the\n\t * event_mutex is held.\n\t */\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\n\tthis_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, current));\n}"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "best_rq->numa_migrate_on",
            "0"
          ],
          "line": 1838
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "migrate_swap",
          "args": [
            "p",
            "env.best_task",
            "env.best_cpu",
            "env.src_cpu"
          ],
          "line": 1837
        },
        "resolved": true,
        "details": {
          "function_name": "migrate_swap",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "1266-1300",
          "snippet": "int migrate_swap(struct task_struct *cur, struct task_struct *p,\n\t\tint target_cpu, int curr_cpu)\n{\n\tstruct migration_swap_arg arg;\n\tint ret = -EINVAL;\n\n\targ = (struct migration_swap_arg){\n\t\t.src_task = cur,\n\t\t.src_cpu = curr_cpu,\n\t\t.dst_task = p,\n\t\t.dst_cpu = target_cpu,\n\t};\n\n\tif (arg.src_cpu == arg.dst_cpu)\n\t\tgoto out;\n\n\t/*\n\t * These three tests are all lockless; this is OK since all of them\n\t * will be re-checked with proper locks held further down the line.\n\t */\n\tif (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))\n\t\tgoto out;\n\n\tif (!cpumask_test_cpu(arg.dst_cpu, &arg.src_task->cpus_allowed))\n\t\tgoto out;\n\n\tif (!cpumask_test_cpu(arg.src_cpu, &arg.dst_task->cpus_allowed))\n\t\tgoto out;\n\n\ttrace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);\n\tret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);\n\nout:\n\treturn ret;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nint migrate_swap(struct task_struct *cur, struct task_struct *p,\n\t\tint target_cpu, int curr_cpu)\n{\n\tstruct migration_swap_arg arg;\n\tint ret = -EINVAL;\n\n\targ = (struct migration_swap_arg){\n\t\t.src_task = cur,\n\t\t.src_cpu = curr_cpu,\n\t\t.dst_task = p,\n\t\t.dst_cpu = target_cpu,\n\t};\n\n\tif (arg.src_cpu == arg.dst_cpu)\n\t\tgoto out;\n\n\t/*\n\t * These three tests are all lockless; this is OK since all of them\n\t * will be re-checked with proper locks held further down the line.\n\t */\n\tif (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))\n\t\tgoto out;\n\n\tif (!cpumask_test_cpu(arg.dst_cpu, &arg.src_task->cpus_allowed))\n\t\tgoto out;\n\n\tif (!cpumask_test_cpu(arg.src_cpu, &arg.dst_task->cpus_allowed))\n\t\tgoto out;\n\n\ttrace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);\n\tret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);\n\nout:\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "trace_sched_stick_numa",
          "args": [
            "p",
            "env.src_cpu",
            "env.best_cpu"
          ],
          "line": 1833
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "best_rq->numa_migrate_on",
            "0"
          ],
          "line": 1831
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "migrate_task_to",
          "args": [
            "p",
            "env.best_cpu"
          ],
          "line": 1830
        },
        "resolved": true,
        "details": {
          "function_name": "migrate_task_to",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "5472-5487",
          "snippet": "int migrate_task_to(struct task_struct *p, int target_cpu)\n{\n\tstruct migration_arg arg = { p, target_cpu };\n\tint curr_cpu = task_cpu(p);\n\n\tif (curr_cpu == target_cpu)\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(target_cpu, &p->cpus_allowed))\n\t\treturn -EINVAL;\n\n\t/* TODO: This is not properly updating schedstats */\n\n\ttrace_sched_move_numa(p, curr_cpu, target_cpu);\n\treturn stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nint migrate_task_to(struct task_struct *p, int target_cpu)\n{\n\tstruct migration_arg arg = { p, target_cpu };\n\tint curr_cpu = task_cpu(p);\n\n\tif (curr_cpu == target_cpu)\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(target_cpu, &p->cpus_allowed))\n\t\treturn -EINVAL;\n\n\t/* TODO: This is not properly updating schedstats */\n\n\ttrace_sched_move_numa(p, curr_cpu, target_cpu);\n\treturn stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "env.best_cpu"
          ],
          "line": 1828
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "sched_setnuma",
          "args": [
            "p",
            "nid"
          ],
          "line": 1821
        },
        "resolved": true,
        "details": {
          "function_name": "sched_setnuma",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "5493-5515",
          "snippet": "void sched_setnuma(struct task_struct *p, int nid)\n{\n\tbool queued, running;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(p, &rf);\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\n\tif (queued)\n\t\tdequeue_task(rq, p, DEQUEUE_SAVE);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tp->numa_preferred_nid = nid;\n\n\tif (queued)\n\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);\n\tif (running)\n\t\tset_curr_task(rq, p);\n\ttask_rq_unlock(rq, p, &rf);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid sched_setnuma(struct task_struct *p, int nid)\n{\n\tbool queued, running;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(p, &rf);\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\n\tif (queued)\n\t\tdequeue_task(rq, p, DEQUEUE_SAVE);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tp->numa_preferred_nid = nid;\n\n\tif (queued)\n\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);\n\tif (running)\n\t\tset_curr_task(rq, p);\n\ttask_rq_unlock(rq, p, &rf);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "env.best_cpu"
          ],
          "line": 1818
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_numa_find_cpu",
          "args": [
            "&env",
            "taskimp",
            "groupimp"
          ],
          "line": 1802
        },
        "resolved": true,
        "details": {
          "function_name": "task_numa_find_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1690-1715",
          "snippet": "static void task_numa_find_cpu(struct task_numa_env *env,\n\t\t\t\tlong taskimp, long groupimp)\n{\n\tlong src_load, dst_load, load;\n\tbool maymove = false;\n\tint cpu;\n\n\tload = task_h_load(env->p);\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\t/*\n\t * If the improvement from just moving env->p direction is better\n\t * than swapping tasks around, check if a move is possible.\n\t */\n\tmaymove = !load_too_imbalanced(src_load, dst_load, env);\n\n\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {\n\t\t/* Skip this CPU if the source task cannot migrate */\n\t\tif (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tenv->dst_cpu = cpu;\n\t\ttask_numa_compare(env, taskimp, groupimp, maymove);\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void task_numa_find_cpu(struct task_numa_env *env,\n\t\t\t\tlong taskimp, long groupimp)\n{\n\tlong src_load, dst_load, load;\n\tbool maymove = false;\n\tint cpu;\n\n\tload = task_h_load(env->p);\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\t/*\n\t * If the improvement from just moving env->p direction is better\n\t * than swapping tasks around, check if a move is possible.\n\t */\n\tmaymove = !load_too_imbalanced(src_load, dst_load, env);\n\n\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {\n\t\t/* Skip this CPU if the source task cannot migrate */\n\t\tif (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tenv->dst_cpu = cpu;\n\t\ttask_numa_compare(env, taskimp, groupimp, maymove);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_numa_stats",
          "args": [
            "&env.dst_stats",
            "env.dst_nid"
          ],
          "line": 1801
        },
        "resolved": true,
        "details": {
          "function_name": "update_numa_stats",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1472-1484",
          "snippet": "static void update_numa_stats(struct numa_stats *ns, int nid)\n{\n\tint cpu;\n\n\tmemset(ns, 0, sizeof(*ns));\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\tns->load += weighted_cpuload(rq);\n\t\tns->compute_capacity += capacity_of(cpu);\n\t}\n\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void update_numa_stats(struct numa_stats *ns, int nid)\n{\n\tint cpu;\n\n\tmemset(ns, 0, sizeof(*ns));\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\tns->load += weighted_cpuload(rq);\n\t\tns->compute_capacity += capacity_of(cpu);\n\t}\n\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_weight",
          "args": [
            "p",
            "nid",
            "dist"
          ],
          "line": 1795
        },
        "resolved": true,
        "details": {
          "function_name": "group_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1369-1386",
          "snippet": "static inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_weight",
          "args": [
            "p",
            "nid",
            "dist"
          ],
          "line": 1794
        },
        "resolved": true,
        "details": {
          "function_name": "task_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1350-1367",
          "snippet": "static inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "node_distance",
          "args": [
            "env.src_nid",
            "env.dst_nid"
          ],
          "line": 1786
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "node_distance",
          "args": [
            "env.src_nid",
            "env.dst_nid"
          ],
          "line": 1763
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_node",
          "args": [
            "p"
          ],
          "line": 1758
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!sd"
          ],
          "line": 1757
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 1749
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "per_cpu(sd_numa, env.src_cpu)"
          ],
          "line": 1746
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "per_cpu",
          "args": [
            "sd_numa",
            "env.src_cpu"
          ],
          "line": 1746
        },
        "resolved": true,
        "details": {
          "function_name": "kdb_per_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/debug/kdb/kdb_main.c",
          "lines": "2575-2640",
          "snippet": "static int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}",
          "includes": [
            "#include \"kdb_private.h\"",
            "#include <linux/slab.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/cpu.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/time.h>",
            "#include <linux/nmi.h>",
            "#include <linux/delay.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kdb.h>",
            "#include <linux/kgdb.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/init.h>",
            "#include <linux/mm.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/atomic.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/utsname.h>",
            "#include <linux/smp.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched.h>",
            "#include <linux/reboot.h>",
            "#include <linux/kmsg_dump.h>",
            "#include <linux/kernel.h>",
            "#include <linux/string.h>",
            "#include <linux/types.h>",
            "#include <linux/ctype.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"kdb_private.h\"\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/proc_fs.h>\n#include <linux/kdebug.h>\n#include <linux/cpu.h>\n#include <linux/sysctl.h>\n#include <linux/ptrace.h>\n#include <linux/time.h>\n#include <linux/nmi.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n#include <linux/kdb.h>\n#include <linux/kgdb.h>\n#include <linux/kallsyms.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/atomic.h>\n#include <linux/vmalloc.h>\n#include <linux/utsname.h>\n#include <linux/smp.h>\n#include <linux/sysrq.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched.h>\n#include <linux/reboot.h>\n#include <linux/kmsg_dump.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ctype.h>\n\nstatic int kdb_per_cpu(int argc, const char **argv)\n{\n\tchar fmtstr[64];\n\tint cpu, diag, nextarg = 1;\n\tunsigned long addr, symaddr, val, bytesperword = 0, whichcpu = ~0UL;\n\n\tif (argc < 1 || argc > 3)\n\t\treturn KDB_ARGCOUNT;\n\n\tdiag = kdbgetaddrarg(argc, argv, &nextarg, &symaddr, NULL, NULL);\n\tif (diag)\n\t\treturn diag;\n\n\tif (argc >= 2) {\n\t\tdiag = kdbgetularg(argv[2], &bytesperword);\n\t\tif (diag)\n\t\t\treturn diag;\n\t}\n\tif (!bytesperword)\n\t\tbytesperword = KDB_WORD_SIZE;\n\telse if (bytesperword > KDB_WORD_SIZE)\n\t\treturn KDB_BADWIDTH;\n\tsprintf(fmtstr, \"%%0%dlx \", (int)(2*bytesperword));\n\tif (argc >= 3) {\n\t\tdiag = kdbgetularg(argv[3], &whichcpu);\n\t\tif (diag)\n\t\t\treturn diag;\n\t\tif (!cpu_online(whichcpu)) {\n\t\t\tkdb_printf(\"cpu %ld is not online\\n\", whichcpu);\n\t\t\treturn KDB_BADCPUNUM;\n\t\t}\n\t}\n\n\t/* Most architectures use __per_cpu_offset[cpu], some use\n\t * __per_cpu_offset(cpu), smp has no __per_cpu_offset.\n\t */\n#ifdef\t__per_cpu_offset\n#define KDB_PCU(cpu) __per_cpu_offset(cpu)\n#else\n#ifdef\tCONFIG_SMP\n#define KDB_PCU(cpu) __per_cpu_offset[cpu]\n#else\n#define KDB_PCU(cpu) 0\n#endif\n#endif\n\tfor_each_online_cpu(cpu) {\n\t\tif (KDB_FLAG(CMD_INTERRUPT))\n\t\t\treturn 0;\n\n\t\tif (whichcpu != ~0UL && whichcpu != cpu)\n\t\t\tcontinue;\n\t\taddr = symaddr + KDB_PCU(cpu);\n\t\tdiag = kdb_getword(&val, addr, bytesperword);\n\t\tif (diag) {\n\t\t\tkdb_printf(\"%5d \" kdb_bfd_vma_fmt0 \" - unable to \"\n\t\t\t\t   \"read, diag=%d\\n\", cpu, addr, diag);\n\t\t\tcontinue;\n\t\t}\n\t\tkdb_printf(\"%5d \", cpu);\n\t\tkdb_md_line(fmtstr, addr,\n\t\t\tbytesperword == KDB_WORD_SIZE,\n\t\t\t1, bytesperword, 1, 1, 0);\n\t}\n#undef KDB_PCU\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 1745
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_node",
          "args": [
            "p"
          ],
          "line": 1723
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int task_numa_migrate(struct task_struct *p)\n{\n\tstruct task_numa_env env = {\n\t\t.p = p,\n\n\t\t.src_cpu = task_cpu(p),\n\t\t.src_nid = task_node(p),\n\n\t\t.imbalance_pct = 112,\n\n\t\t.best_task = NULL,\n\t\t.best_imp = 0,\n\t\t.best_cpu = -1,\n\t};\n\tstruct sched_domain *sd;\n\tstruct rq *best_rq;\n\tunsigned long taskweight, groupweight;\n\tint nid, ret, dist;\n\tlong taskimp, groupimp;\n\n\t/*\n\t * Pick the lowest SD_NUMA domain, as that would have the smallest\n\t * imbalance and would be the first to start moving tasks about.\n\t *\n\t * And we want to avoid any moving of tasks about, as that would create\n\t * random movement of tasks -- counter the numa conditions we're trying\n\t * to satisfy here.\n\t */\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));\n\tif (sd)\n\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;\n\trcu_read_unlock();\n\n\t/*\n\t * Cpusets can break the scheduler domain tree into smaller\n\t * balance domains, some of which do not cross NUMA boundaries.\n\t * Tasks that are \"trapped\" in such domains cannot be migrated\n\t * elsewhere, so there is no point in (re)trying.\n\t */\n\tif (unlikely(!sd)) {\n\t\tsched_setnuma(p, task_node(p));\n\t\treturn -EINVAL;\n\t}\n\n\tenv.dst_nid = p->numa_preferred_nid;\n\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);\n\ttaskweight = task_weight(p, env.src_nid, dist);\n\tgroupweight = group_weight(p, env.src_nid, dist);\n\tupdate_numa_stats(&env.src_stats, env.src_nid);\n\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;\n\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;\n\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\n\t/* Try to find a spot on the preferred nid. */\n\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\n\t/*\n\t * Look at other nodes in these cases:\n\t * - there is no space available on the preferred_nid\n\t * - the task is part of a numa_group that is interleaved across\n\t *   multiple NUMA nodes; in order to better consolidate the group,\n\t *   we need to check other locations.\n\t */\n\tif (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {\n\t\tfor_each_online_node(nid) {\n\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)\n\t\t\t\tcontinue;\n\n\t\t\tdist = node_distance(env.src_nid, env.dst_nid);\n\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\t\tdist != env.dist) {\n\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);\n\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);\n\t\t\t}\n\n\t\t\t/* Only consider nodes where both task and groups benefit */\n\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;\n\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;\n\t\t\tif (taskimp < 0 && groupimp < 0)\n\t\t\t\tcontinue;\n\n\t\t\tenv.dist = dist;\n\t\t\tenv.dst_nid = nid;\n\t\t\tupdate_numa_stats(&env.dst_stats, env.dst_nid);\n\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\t\t}\n\t}\n\n\t/*\n\t * If the task is part of a workload that spans multiple NUMA nodes,\n\t * and is migrating into one of the workload's active nodes, remember\n\t * this node as the task's preferred numa node, so the workload can\n\t * settle down.\n\t * A task that migrated to a second choice node will be better off\n\t * trying for a better one later. Do not set the preferred node here.\n\t */\n\tif (p->numa_group) {\n\t\tif (env.best_cpu == -1)\n\t\t\tnid = env.src_nid;\n\t\telse\n\t\t\tnid = cpu_to_node(env.best_cpu);\n\n\t\tif (nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, nid);\n\t}\n\n\t/* No better CPU than the current one was found. */\n\tif (env.best_cpu == -1)\n\t\treturn -EAGAIN;\n\n\tbest_rq = cpu_rq(env.best_cpu);\n\tif (env.best_task == NULL) {\n\t\tret = migrate_task_to(p, env.best_cpu);\n\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\t\tif (ret != 0)\n\t\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_cpu);\n\t\treturn ret;\n\t}\n\n\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);\n\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\n\tif (ret != 0)\n\t\ttrace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));\n\tput_task_struct(env.best_task);\n\treturn ret;\n}"
  },
  {
    "function_name": "task_numa_find_cpu",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1690-1715",
    "snippet": "static void task_numa_find_cpu(struct task_numa_env *env,\n\t\t\t\tlong taskimp, long groupimp)\n{\n\tlong src_load, dst_load, load;\n\tbool maymove = false;\n\tint cpu;\n\n\tload = task_h_load(env->p);\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\t/*\n\t * If the improvement from just moving env->p direction is better\n\t * than swapping tasks around, check if a move is possible.\n\t */\n\tmaymove = !load_too_imbalanced(src_load, dst_load, env);\n\n\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {\n\t\t/* Skip this CPU if the source task cannot migrate */\n\t\tif (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tenv->dst_cpu = cpu;\n\t\ttask_numa_compare(env, taskimp, groupimp, maymove);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_numa_compare",
          "args": [
            "env",
            "taskimp",
            "groupimp",
            "maymove"
          ],
          "line": 1713
        },
        "resolved": true,
        "details": {
          "function_name": "task_numa_compare",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1571-1688",
          "snippet": "static void task_numa_compare(struct task_numa_env *env,\n\t\t\t      long taskimp, long groupimp, bool maymove)\n{\n\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);\n\tstruct task_struct *cur;\n\tlong src_load, dst_load;\n\tlong load;\n\tlong imp = env->p->numa_group ? groupimp : taskimp;\n\tlong moveimp = imp;\n\tint dist = env->dist;\n\n\tif (READ_ONCE(dst_rq->numa_migrate_on))\n\t\treturn;\n\n\trcu_read_lock();\n\tcur = task_rcu_dereference(&dst_rq->curr);\n\tif (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))\n\t\tcur = NULL;\n\n\t/*\n\t * Because we have preemption enabled we can get migrated around and\n\t * end try selecting ourselves (current == env->p) as a swap candidate.\n\t */\n\tif (cur == env->p)\n\t\tgoto unlock;\n\n\tif (!cur) {\n\t\tif (maymove && moveimp >= env->best_imp)\n\t\t\tgoto assign;\n\t\telse\n\t\t\tgoto unlock;\n\t}\n\n\t/*\n\t * \"imp\" is the fault differential for the source task between the\n\t * source and destination node. Calculate the total differential for\n\t * the source task and potential destination task. The more negative\n\t * the value is, the more remote accesses that would be expected to\n\t * be incurred if the tasks were swapped.\n\t */\n\t/* Skip this swap candidate if cannot move to the source cpu */\n\tif (!cpumask_test_cpu(env->src_cpu, &cur->cpus_allowed))\n\t\tgoto unlock;\n\n\t/*\n\t * If dst and source tasks are in the same NUMA group, or not\n\t * in any group then look only at task weights.\n\t */\n\tif (cur->numa_group == env->p->numa_group) {\n\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -\n\t\t      task_weight(cur, env->dst_nid, dist);\n\t\t/*\n\t\t * Add some hysteresis to prevent swapping the\n\t\t * tasks within a group over tiny differences.\n\t\t */\n\t\tif (cur->numa_group)\n\t\t\timp -= imp / 16;\n\t} else {\n\t\t/*\n\t\t * Compare the group weights. If a task is all by itself\n\t\t * (not part of a group), use the task weight instead.\n\t\t */\n\t\tif (cur->numa_group && env->p->numa_group)\n\t\t\timp += group_weight(cur, env->src_nid, dist) -\n\t\t\t       group_weight(cur, env->dst_nid, dist);\n\t\telse\n\t\t\timp += task_weight(cur, env->src_nid, dist) -\n\t\t\t       task_weight(cur, env->dst_nid, dist);\n\t}\n\n\tif (maymove && moveimp > imp && moveimp > env->best_imp) {\n\t\timp = moveimp;\n\t\tcur = NULL;\n\t\tgoto assign;\n\t}\n\n\t/*\n\t * If the NUMA importance is less than SMALLIMP,\n\t * task migration might only result in ping pong\n\t * of tasks and also hurt performance due to cache\n\t * misses.\n\t */\n\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)\n\t\tgoto unlock;\n\n\t/*\n\t * In the overloaded case, try and keep the load balanced.\n\t */\n\tload = task_h_load(env->p) - task_h_load(cur);\n\tif (!load)\n\t\tgoto assign;\n\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\tif (load_too_imbalanced(src_load, dst_load, env))\n\t\tgoto unlock;\n\nassign:\n\t/*\n\t * One idle CPU per node is evaluated for a task numa move.\n\t * Call select_idle_sibling to maybe find a better one.\n\t */\n\tif (!cur) {\n\t\t/*\n\t\t * select_idle_siblings() uses an per-CPU cpumask that\n\t\t * can be used from IRQ context.\n\t\t */\n\t\tlocal_irq_disable();\n\t\tenv->dst_cpu = select_idle_sibling(env->p, env->src_cpu,\n\t\t\t\t\t\t   env->dst_cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\ttask_numa_assign(env, cur, imp);\nunlock:\n\trcu_read_unlock();\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define SMALLIMP\t30"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define SMALLIMP\t30\n\nstatic void task_numa_compare(struct task_numa_env *env,\n\t\t\t      long taskimp, long groupimp, bool maymove)\n{\n\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);\n\tstruct task_struct *cur;\n\tlong src_load, dst_load;\n\tlong load;\n\tlong imp = env->p->numa_group ? groupimp : taskimp;\n\tlong moveimp = imp;\n\tint dist = env->dist;\n\n\tif (READ_ONCE(dst_rq->numa_migrate_on))\n\t\treturn;\n\n\trcu_read_lock();\n\tcur = task_rcu_dereference(&dst_rq->curr);\n\tif (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))\n\t\tcur = NULL;\n\n\t/*\n\t * Because we have preemption enabled we can get migrated around and\n\t * end try selecting ourselves (current == env->p) as a swap candidate.\n\t */\n\tif (cur == env->p)\n\t\tgoto unlock;\n\n\tif (!cur) {\n\t\tif (maymove && moveimp >= env->best_imp)\n\t\t\tgoto assign;\n\t\telse\n\t\t\tgoto unlock;\n\t}\n\n\t/*\n\t * \"imp\" is the fault differential for the source task between the\n\t * source and destination node. Calculate the total differential for\n\t * the source task and potential destination task. The more negative\n\t * the value is, the more remote accesses that would be expected to\n\t * be incurred if the tasks were swapped.\n\t */\n\t/* Skip this swap candidate if cannot move to the source cpu */\n\tif (!cpumask_test_cpu(env->src_cpu, &cur->cpus_allowed))\n\t\tgoto unlock;\n\n\t/*\n\t * If dst and source tasks are in the same NUMA group, or not\n\t * in any group then look only at task weights.\n\t */\n\tif (cur->numa_group == env->p->numa_group) {\n\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -\n\t\t      task_weight(cur, env->dst_nid, dist);\n\t\t/*\n\t\t * Add some hysteresis to prevent swapping the\n\t\t * tasks within a group over tiny differences.\n\t\t */\n\t\tif (cur->numa_group)\n\t\t\timp -= imp / 16;\n\t} else {\n\t\t/*\n\t\t * Compare the group weights. If a task is all by itself\n\t\t * (not part of a group), use the task weight instead.\n\t\t */\n\t\tif (cur->numa_group && env->p->numa_group)\n\t\t\timp += group_weight(cur, env->src_nid, dist) -\n\t\t\t       group_weight(cur, env->dst_nid, dist);\n\t\telse\n\t\t\timp += task_weight(cur, env->src_nid, dist) -\n\t\t\t       task_weight(cur, env->dst_nid, dist);\n\t}\n\n\tif (maymove && moveimp > imp && moveimp > env->best_imp) {\n\t\timp = moveimp;\n\t\tcur = NULL;\n\t\tgoto assign;\n\t}\n\n\t/*\n\t * If the NUMA importance is less than SMALLIMP,\n\t * task migration might only result in ping pong\n\t * of tasks and also hurt performance due to cache\n\t * misses.\n\t */\n\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)\n\t\tgoto unlock;\n\n\t/*\n\t * In the overloaded case, try and keep the load balanced.\n\t */\n\tload = task_h_load(env->p) - task_h_load(cur);\n\tif (!load)\n\t\tgoto assign;\n\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\tif (load_too_imbalanced(src_load, dst_load, env))\n\t\tgoto unlock;\n\nassign:\n\t/*\n\t * One idle CPU per node is evaluated for a task numa move.\n\t * Call select_idle_sibling to maybe find a better one.\n\t */\n\tif (!cur) {\n\t\t/*\n\t\t * select_idle_siblings() uses an per-CPU cpumask that\n\t\t * can be used from IRQ context.\n\t\t */\n\t\tlocal_irq_disable();\n\t\tenv->dst_cpu = select_idle_sibling(env->p, env->src_cpu,\n\t\t\t\t\t\t   env->dst_cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\ttask_numa_assign(env, cur, imp);\nunlock:\n\trcu_read_unlock();\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "cpu",
            "&env->p->cpus_allowed"
          ],
          "line": 1709
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "cpu",
            "cpumask_of_node(env->dst_nid)"
          ],
          "line": 1707
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_of_node",
          "args": [
            "env->dst_nid"
          ],
          "line": 1707
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "load_too_imbalanced",
          "args": [
            "src_load",
            "dst_load",
            "env"
          ],
          "line": 1705
        },
        "resolved": true,
        "details": {
          "function_name": "load_too_imbalanced",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1530-1556",
          "snippet": "static bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_h_load",
          "args": [
            "env->p"
          ],
          "line": 1697
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void task_numa_find_cpu(struct task_numa_env *env,\n\t\t\t\tlong taskimp, long groupimp)\n{\n\tlong src_load, dst_load, load;\n\tbool maymove = false;\n\tint cpu;\n\n\tload = task_h_load(env->p);\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\t/*\n\t * If the improvement from just moving env->p direction is better\n\t * than swapping tasks around, check if a move is possible.\n\t */\n\tmaymove = !load_too_imbalanced(src_load, dst_load, env);\n\n\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {\n\t\t/* Skip this CPU if the source task cannot migrate */\n\t\tif (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))\n\t\t\tcontinue;\n\n\t\tenv->dst_cpu = cpu;\n\t\ttask_numa_compare(env, taskimp, groupimp, maymove);\n\t}\n}"
  },
  {
    "function_name": "task_numa_compare",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1571-1688",
    "snippet": "static void task_numa_compare(struct task_numa_env *env,\n\t\t\t      long taskimp, long groupimp, bool maymove)\n{\n\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);\n\tstruct task_struct *cur;\n\tlong src_load, dst_load;\n\tlong load;\n\tlong imp = env->p->numa_group ? groupimp : taskimp;\n\tlong moveimp = imp;\n\tint dist = env->dist;\n\n\tif (READ_ONCE(dst_rq->numa_migrate_on))\n\t\treturn;\n\n\trcu_read_lock();\n\tcur = task_rcu_dereference(&dst_rq->curr);\n\tif (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))\n\t\tcur = NULL;\n\n\t/*\n\t * Because we have preemption enabled we can get migrated around and\n\t * end try selecting ourselves (current == env->p) as a swap candidate.\n\t */\n\tif (cur == env->p)\n\t\tgoto unlock;\n\n\tif (!cur) {\n\t\tif (maymove && moveimp >= env->best_imp)\n\t\t\tgoto assign;\n\t\telse\n\t\t\tgoto unlock;\n\t}\n\n\t/*\n\t * \"imp\" is the fault differential for the source task between the\n\t * source and destination node. Calculate the total differential for\n\t * the source task and potential destination task. The more negative\n\t * the value is, the more remote accesses that would be expected to\n\t * be incurred if the tasks were swapped.\n\t */\n\t/* Skip this swap candidate if cannot move to the source cpu */\n\tif (!cpumask_test_cpu(env->src_cpu, &cur->cpus_allowed))\n\t\tgoto unlock;\n\n\t/*\n\t * If dst and source tasks are in the same NUMA group, or not\n\t * in any group then look only at task weights.\n\t */\n\tif (cur->numa_group == env->p->numa_group) {\n\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -\n\t\t      task_weight(cur, env->dst_nid, dist);\n\t\t/*\n\t\t * Add some hysteresis to prevent swapping the\n\t\t * tasks within a group over tiny differences.\n\t\t */\n\t\tif (cur->numa_group)\n\t\t\timp -= imp / 16;\n\t} else {\n\t\t/*\n\t\t * Compare the group weights. If a task is all by itself\n\t\t * (not part of a group), use the task weight instead.\n\t\t */\n\t\tif (cur->numa_group && env->p->numa_group)\n\t\t\timp += group_weight(cur, env->src_nid, dist) -\n\t\t\t       group_weight(cur, env->dst_nid, dist);\n\t\telse\n\t\t\timp += task_weight(cur, env->src_nid, dist) -\n\t\t\t       task_weight(cur, env->dst_nid, dist);\n\t}\n\n\tif (maymove && moveimp > imp && moveimp > env->best_imp) {\n\t\timp = moveimp;\n\t\tcur = NULL;\n\t\tgoto assign;\n\t}\n\n\t/*\n\t * If the NUMA importance is less than SMALLIMP,\n\t * task migration might only result in ping pong\n\t * of tasks and also hurt performance due to cache\n\t * misses.\n\t */\n\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)\n\t\tgoto unlock;\n\n\t/*\n\t * In the overloaded case, try and keep the load balanced.\n\t */\n\tload = task_h_load(env->p) - task_h_load(cur);\n\tif (!load)\n\t\tgoto assign;\n\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\tif (load_too_imbalanced(src_load, dst_load, env))\n\t\tgoto unlock;\n\nassign:\n\t/*\n\t * One idle CPU per node is evaluated for a task numa move.\n\t * Call select_idle_sibling to maybe find a better one.\n\t */\n\tif (!cur) {\n\t\t/*\n\t\t * select_idle_siblings() uses an per-CPU cpumask that\n\t\t * can be used from IRQ context.\n\t\t */\n\t\tlocal_irq_disable();\n\t\tenv->dst_cpu = select_idle_sibling(env->p, env->src_cpu,\n\t\t\t\t\t\t   env->dst_cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\ttask_numa_assign(env, cur, imp);\nunlock:\n\trcu_read_unlock();\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define SMALLIMP\t30"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 1687
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_numa_assign",
          "args": [
            "env",
            "cur",
            "imp"
          ],
          "line": 1685
        },
        "resolved": true,
        "details": {
          "function_name": "task_numa_assign",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1502-1528",
          "snippet": "static void task_numa_assign(struct task_numa_env *env,\n\t\t\t     struct task_struct *p, long imp)\n{\n\tstruct rq *rq = cpu_rq(env->dst_cpu);\n\n\t/* Bail out if run-queue part of active NUMA balance. */\n\tif (xchg(&rq->numa_migrate_on, 1))\n\t\treturn;\n\n\t/*\n\t * Clear previous best_cpu/rq numa-migrate flag, since task now\n\t * found a better CPU to move/swap.\n\t */\n\tif (env->best_cpu != -1) {\n\t\trq = cpu_rq(env->best_cpu);\n\t\tWRITE_ONCE(rq->numa_migrate_on, 0);\n\t}\n\n\tif (env->best_task)\n\t\tput_task_struct(env->best_task);\n\tif (p)\n\t\tget_task_struct(p);\n\n\tenv->best_task = p;\n\tenv->best_imp = imp;\n\tenv->best_cpu = env->dst_cpu;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void task_numa_assign(struct task_numa_env *env,\n\t\t\t     struct task_struct *p, long imp)\n{\n\tstruct rq *rq = cpu_rq(env->dst_cpu);\n\n\t/* Bail out if run-queue part of active NUMA balance. */\n\tif (xchg(&rq->numa_migrate_on, 1))\n\t\treturn;\n\n\t/*\n\t * Clear previous best_cpu/rq numa-migrate flag, since task now\n\t * found a better CPU to move/swap.\n\t */\n\tif (env->best_cpu != -1) {\n\t\trq = cpu_rq(env->best_cpu);\n\t\tWRITE_ONCE(rq->numa_migrate_on, 0);\n\t}\n\n\tif (env->best_task)\n\t\tput_task_struct(env->best_task);\n\tif (p)\n\t\tget_task_struct(p);\n\n\tenv->best_task = p;\n\tenv->best_imp = imp;\n\tenv->best_cpu = env->dst_cpu;\n}"
        }
      },
      {
        "call_info": {
          "callee": "local_irq_enable",
          "args": [],
          "line": 1682
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "select_idle_sibling",
          "args": [
            "env->p",
            "env->src_cpu",
            "env->dst_cpu"
          ],
          "line": 1680
        },
        "resolved": true,
        "details": {
          "function_name": "select_idle_sibling",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "6118-6164",
          "snippet": "static int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tstruct sched_domain *sd;\n\tint i, recent_used_cpu;\n\n\tif (available_idle_cpu(target))\n\t\treturn target;\n\n\t/*\n\t * If the previous CPU is cache affine and idle, don't be stupid:\n\t */\n\tif (prev != target && cpus_share_cache(prev, target) && available_idle_cpu(prev))\n\t\treturn prev;\n\n\t/* Check a recently used CPU as a potential idle candidate: */\n\trecent_used_cpu = p->recent_used_cpu;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    available_idle_cpu(recent_used_cpu) &&\n\t    cpumask_test_cpu(p->recent_used_cpu, &p->cpus_allowed)) {\n\t\t/*\n\t\t * Replace recent_used_cpu with prev as it is a potential\n\t\t * candidate for the next wake:\n\t\t */\n\t\tp->recent_used_cpu = prev;\n\t\treturn recent_used_cpu;\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\ti = select_idle_core(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_cpu(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\ti = select_idle_smt(p, sd, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}"
        }
      },
      {
        "call_info": {
          "callee": "local_irq_disable",
          "args": [],
          "line": 1679
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "load_too_imbalanced",
          "args": [
            "src_load",
            "dst_load",
            "env"
          ],
          "line": 1666
        },
        "resolved": true,
        "details": {
          "function_name": "load_too_imbalanced",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1530-1556",
          "snippet": "static bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_h_load",
          "args": [
            "cur"
          ],
          "line": 1659
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_h_load",
          "args": [
            "env->p"
          ],
          "line": 1659
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_weight",
          "args": [
            "cur",
            "env->dst_nid",
            "dist"
          ],
          "line": 1638
        },
        "resolved": true,
        "details": {
          "function_name": "task_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1350-1367",
          "snippet": "static inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_weight",
          "args": [
            "cur",
            "env->dst_nid",
            "dist"
          ],
          "line": 1635
        },
        "resolved": true,
        "details": {
          "function_name": "group_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1369-1386",
          "snippet": "static inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpumask_test_cpu",
          "args": [
            "env->src_cpu",
            "&cur->cpus_allowed"
          ],
          "line": 1612
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "is_idle_task",
          "args": [
            "cur"
          ],
          "line": 1587
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_rcu_dereference",
          "args": [
            "&dst_rq->curr"
          ],
          "line": 1586
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 1585
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "dst_rq->numa_migrate_on"
          ],
          "line": 1582
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "env->dst_cpu"
          ],
          "line": 1574
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define SMALLIMP\t30\n\nstatic void task_numa_compare(struct task_numa_env *env,\n\t\t\t      long taskimp, long groupimp, bool maymove)\n{\n\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);\n\tstruct task_struct *cur;\n\tlong src_load, dst_load;\n\tlong load;\n\tlong imp = env->p->numa_group ? groupimp : taskimp;\n\tlong moveimp = imp;\n\tint dist = env->dist;\n\n\tif (READ_ONCE(dst_rq->numa_migrate_on))\n\t\treturn;\n\n\trcu_read_lock();\n\tcur = task_rcu_dereference(&dst_rq->curr);\n\tif (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))\n\t\tcur = NULL;\n\n\t/*\n\t * Because we have preemption enabled we can get migrated around and\n\t * end try selecting ourselves (current == env->p) as a swap candidate.\n\t */\n\tif (cur == env->p)\n\t\tgoto unlock;\n\n\tif (!cur) {\n\t\tif (maymove && moveimp >= env->best_imp)\n\t\t\tgoto assign;\n\t\telse\n\t\t\tgoto unlock;\n\t}\n\n\t/*\n\t * \"imp\" is the fault differential for the source task between the\n\t * source and destination node. Calculate the total differential for\n\t * the source task and potential destination task. The more negative\n\t * the value is, the more remote accesses that would be expected to\n\t * be incurred if the tasks were swapped.\n\t */\n\t/* Skip this swap candidate if cannot move to the source cpu */\n\tif (!cpumask_test_cpu(env->src_cpu, &cur->cpus_allowed))\n\t\tgoto unlock;\n\n\t/*\n\t * If dst and source tasks are in the same NUMA group, or not\n\t * in any group then look only at task weights.\n\t */\n\tif (cur->numa_group == env->p->numa_group) {\n\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -\n\t\t      task_weight(cur, env->dst_nid, dist);\n\t\t/*\n\t\t * Add some hysteresis to prevent swapping the\n\t\t * tasks within a group over tiny differences.\n\t\t */\n\t\tif (cur->numa_group)\n\t\t\timp -= imp / 16;\n\t} else {\n\t\t/*\n\t\t * Compare the group weights. If a task is all by itself\n\t\t * (not part of a group), use the task weight instead.\n\t\t */\n\t\tif (cur->numa_group && env->p->numa_group)\n\t\t\timp += group_weight(cur, env->src_nid, dist) -\n\t\t\t       group_weight(cur, env->dst_nid, dist);\n\t\telse\n\t\t\timp += task_weight(cur, env->src_nid, dist) -\n\t\t\t       task_weight(cur, env->dst_nid, dist);\n\t}\n\n\tif (maymove && moveimp > imp && moveimp > env->best_imp) {\n\t\timp = moveimp;\n\t\tcur = NULL;\n\t\tgoto assign;\n\t}\n\n\t/*\n\t * If the NUMA importance is less than SMALLIMP,\n\t * task migration might only result in ping pong\n\t * of tasks and also hurt performance due to cache\n\t * misses.\n\t */\n\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)\n\t\tgoto unlock;\n\n\t/*\n\t * In the overloaded case, try and keep the load balanced.\n\t */\n\tload = task_h_load(env->p) - task_h_load(cur);\n\tif (!load)\n\t\tgoto assign;\n\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\tif (load_too_imbalanced(src_load, dst_load, env))\n\t\tgoto unlock;\n\nassign:\n\t/*\n\t * One idle CPU per node is evaluated for a task numa move.\n\t * Call select_idle_sibling to maybe find a better one.\n\t */\n\tif (!cur) {\n\t\t/*\n\t\t * select_idle_siblings() uses an per-CPU cpumask that\n\t\t * can be used from IRQ context.\n\t\t */\n\t\tlocal_irq_disable();\n\t\tenv->dst_cpu = select_idle_sibling(env->p, env->src_cpu,\n\t\t\t\t\t\t   env->dst_cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\ttask_numa_assign(env, cur, imp);\nunlock:\n\trcu_read_unlock();\n}"
  },
  {
    "function_name": "load_too_imbalanced",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1530-1556",
    "snippet": "static bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "abs",
          "args": [
            "orig_dst_load * src_capacity - orig_src_load * dst_capacity"
          ],
          "line": 1552
        },
        "resolved": true,
        "details": {
          "function_name": "torture_shutdown_absorb",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/torture.c",
          "lines": "462-469",
          "snippet": "void torture_shutdown_absorb(const char *title)\n{\n\twhile (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {\n\t\tpr_notice(\"torture thread %s parking due to system shutdown\\n\",\n\t\t\t  title);\n\t\tschedule_timeout_uninterruptible(MAX_SCHEDULE_TIMEOUT);\n\t}\n}",
          "includes": [
            "#include \"rcu/rcu.h\"",
            "#include <linux/torture.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/ktime.h>",
            "#include <linux/trace_clock.h>",
            "#include <linux/slab.h>",
            "#include <linux/stat.h>",
            "#include <linux/delay.h>",
            "#include <linux/cpu.h>",
            "#include <linux/freezer.h>",
            "#include <linux/reboot.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/completion.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/err.h>",
            "#include <linux/kthread.h>",
            "#include <linux/module.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [
            "#define FULLSTOP_SHUTDOWN 1\t/* System shutdown with torture running. */"
          ],
          "globals_used": [
            "static int fullstop = FULLSTOP_RMMOD;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu/rcu.h\"\n#include <linux/torture.h>\n#include <asm/byteorder.h>\n#include <linux/ktime.h>\n#include <linux/trace_clock.h>\n#include <linux/slab.h>\n#include <linux/stat.h>\n#include <linux/delay.h>\n#include <linux/cpu.h>\n#include <linux/freezer.h>\n#include <linux/reboot.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/moduleparam.h>\n#include <linux/completion.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/clock.h>\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/err.h>\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\n#define FULLSTOP_SHUTDOWN 1\t/* System shutdown with torture running. */\n\nstatic int fullstop = FULLSTOP_RMMOD;\n\nvoid torture_shutdown_absorb(const char *title)\n{\n\twhile (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {\n\t\tpr_notice(\"torture thread %s parking due to system shutdown\\n\",\n\t\t\t  title);\n\t\tschedule_timeout_uninterruptible(MAX_SCHEDULE_TIMEOUT);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t/*\n\t * The load is corrected for the CPU capacity available on each node.\n\t *\n\t * src_load        dst_load\n\t * ------------ vs ---------\n\t * src_capacity    dst_capacity\n\t */\n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t/* Would this change make things worse? */\n\treturn (imb > old_imb);\n}"
  },
  {
    "function_name": "task_numa_assign",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1502-1528",
    "snippet": "static void task_numa_assign(struct task_numa_env *env,\n\t\t\t     struct task_struct *p, long imp)\n{\n\tstruct rq *rq = cpu_rq(env->dst_cpu);\n\n\t/* Bail out if run-queue part of active NUMA balance. */\n\tif (xchg(&rq->numa_migrate_on, 1))\n\t\treturn;\n\n\t/*\n\t * Clear previous best_cpu/rq numa-migrate flag, since task now\n\t * found a better CPU to move/swap.\n\t */\n\tif (env->best_cpu != -1) {\n\t\trq = cpu_rq(env->best_cpu);\n\t\tWRITE_ONCE(rq->numa_migrate_on, 0);\n\t}\n\n\tif (env->best_task)\n\t\tput_task_struct(env->best_task);\n\tif (p)\n\t\tget_task_struct(p);\n\n\tenv->best_task = p;\n\tenv->best_imp = imp;\n\tenv->best_cpu = env->dst_cpu;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "p"
          ],
          "line": 1523
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "put_task_struct",
          "args": [
            "env->best_task"
          ],
          "line": 1521
        },
        "resolved": true,
        "details": {
          "function_name": "__put_task_struct",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/fork.c",
          "lines": "716-731",
          "snippet": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}",
          "includes": [
            "#include <linux/init_task.h>",
            "#include <trace/events/task.h>",
            "#include <trace/events/sched.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/pgtable.h>",
            "#include <linux/stackleak.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/kcov.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/compiler.h>",
            "#include <linux/aio.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/oom.h>",
            "#include <linux/user-return-notifier.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/magic.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/tty.h>",
            "#include <linux/random.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/acct.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/profile.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/audit.h>",
            "#include <linux/mount.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/kthread.h>",
            "#include <linux/compat.h>",
            "#include <linux/futex.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/seccomp.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/cpu.h>",
            "#include <linux/capability.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>",
            "#include <linux/hmm.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/mman.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/sem.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/module.h>",
            "#include <linux/unistd.h>",
            "#include <linux/init.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __latent_entropy struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/init_task.h>\n#include <trace/events/task.h>\n#include <trace/events/sched.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/pgtable.h>\n#include <linux/stackleak.h>\n#include <linux/thread_info.h>\n#include <linux/livepatch.h>\n#include <linux/kcov.h>\n#include <linux/sysctl.h>\n#include <linux/compiler.h>\n#include <linux/aio.h>\n#include <linux/uprobes.h>\n#include <linux/signalfd.h>\n#include <linux/khugepaged.h>\n#include <linux/oom.h>\n#include <linux/user-return-notifier.h>\n#include <linux/posix-timers.h>\n#include <linux/perf_event.h>\n#include <linux/sched/mm.h>\n#include <linux/magic.h>\n#include <linux/fs_struct.h>\n#include <linux/blkdev.h>\n#include <linux/tty.h>\n#include <linux/random.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/freezer.h>\n#include <linux/cn_proc.h>\n#include <linux/tsacct_kern.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/acct.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/profile.h>\n#include <linux/proc_fs.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/audit.h>\n#include <linux/mount.h>\n#include <linux/ptrace.h>\n#include <linux/rcupdate.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/kthread.h>\n#include <linux/compat.h>\n#include <linux/futex.h>\n#include <linux/jiffies.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/seccomp.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/cgroup.h>\n#include <linux/cpu.h>\n#include <linux/capability.h>\n#include <linux/nsproxy.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/hmm.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mman.h>\n#include <linux/binfmts.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/sem.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/vmalloc.h>\n#include <linux/module.h>\n#include <linux/unistd.h>\n#include <linux/init.h>\n#include <linux/rtmutex.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/user.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n\nstatic __latent_entropy struct;\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "rq->numa_migrate_on",
            "0"
          ],
          "line": 1517
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "env->best_cpu"
          ],
          "line": 1516
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "xchg",
          "args": [
            "&rq->numa_migrate_on",
            "1"
          ],
          "line": 1508
        },
        "resolved": true,
        "details": {
          "function_name": "xchg_tail",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/qspinlock.c",
          "lines": "231-249",
          "snippet": "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}",
          "includes": [
            "#include \"qspinlock.c\"",
            "#include \"qspinlock_paravirt.h\"",
            "#include \"mcs_spinlock.h\"",
            "#include \"qspinlock_stat.h\"",
            "#include <asm/qspinlock.h>",
            "#include <asm/byteorder.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/mutex.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/percpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/bug.h>",
            "#include <linux/smp.h>"
          ],
          "macros_used": [
            "#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"qspinlock.c\"\n#include \"qspinlock_paravirt.h\"\n#include \"mcs_spinlock.h\"\n#include \"qspinlock_stat.h\"\n#include <asm/qspinlock.h>\n#include <asm/byteorder.h>\n#include <linux/prefetch.h>\n#include <linux/mutex.h>\n#include <linux/hardirq.h>\n#include <linux/percpu.h>\n#include <linux/cpumask.h>\n#include <linux/bug.h>\n#include <linux/smp.h>\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t/*\n\t\t * We can use relaxed semantics since the caller ensures that\n\t\t * the MCS node is properly initialized before updating the\n\t\t * tail.\n\t\t */\n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "env->dst_cpu"
          ],
          "line": 1505
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void task_numa_assign(struct task_numa_env *env,\n\t\t\t     struct task_struct *p, long imp)\n{\n\tstruct rq *rq = cpu_rq(env->dst_cpu);\n\n\t/* Bail out if run-queue part of active NUMA balance. */\n\tif (xchg(&rq->numa_migrate_on, 1))\n\t\treturn;\n\n\t/*\n\t * Clear previous best_cpu/rq numa-migrate flag, since task now\n\t * found a better CPU to move/swap.\n\t */\n\tif (env->best_cpu != -1) {\n\t\trq = cpu_rq(env->best_cpu);\n\t\tWRITE_ONCE(rq->numa_migrate_on, 0);\n\t}\n\n\tif (env->best_task)\n\t\tput_task_struct(env->best_task);\n\tif (p)\n\t\tget_task_struct(p);\n\n\tenv->best_task = p;\n\tenv->best_imp = imp;\n\tenv->best_cpu = env->dst_cpu;\n}"
  },
  {
    "function_name": "update_numa_stats",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1472-1484",
    "snippet": "static void update_numa_stats(struct numa_stats *ns, int nid)\n{\n\tint cpu;\n\n\tmemset(ns, 0, sizeof(*ns));\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\tns->load += weighted_cpuload(rq);\n\t\tns->compute_capacity += capacity_of(cpu);\n\t}\n\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "capacity_of",
          "args": [
            "cpu"
          ],
          "line": 1481
        },
        "resolved": true,
        "details": {
          "function_name": "capacity_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5509-5512",
          "snippet": "static unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}"
        }
      },
      {
        "call_info": {
          "callee": "weighted_cpuload",
          "args": [
            "rq"
          ],
          "line": 1480
        },
        "resolved": true,
        "details": {
          "function_name": "weighted_cpuload",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "5358-5361",
          "snippet": "static unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long weighted_cpuload(struct rq *rq)\n{\n\treturn cfs_rq_runnable_load_avg(&rq->cfs);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 1478
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_cpu",
          "args": [
            "cpu",
            "cpumask_of_node(nid)"
          ],
          "line": 1477
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpumask_of_node",
          "args": [
            "nid"
          ],
          "line": 1477
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "ns",
            "0",
            "sizeof(*ns)"
          ],
          "line": 1476
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void update_numa_stats(struct numa_stats *ns, int nid)\n{\n\tint cpu;\n\n\tmemset(ns, 0, sizeof(*ns));\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\tns->load += weighted_cpuload(rq);\n\t\tns->compute_capacity += capacity_of(cpu);\n\t}\n\n}"
  },
  {
    "function_name": "should_numa_migrate_memory",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1388-1455",
    "snippet": "bool should_numa_migrate_memory(struct task_struct *p, struct page * page,\n\t\t\t\tint src_nid, int dst_cpu)\n{\n\tstruct numa_group *ng = p->numa_group;\n\tint dst_nid = cpu_to_node(dst_cpu);\n\tint last_cpupid, this_cpupid;\n\n\tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n\tlast_cpupid = page_cpupid_xchg_last(page, this_cpupid);\n\n\t/*\n\t * Allow first faults or private faults to migrate immediately early in\n\t * the lifetime of a task. The magic number 4 is based on waiting for\n\t * two full passes of the \"multi-stage node selection\" test that is\n\t * executed below.\n\t */\n\tif ((p->numa_preferred_nid == -1 || p->numa_scan_seq <= 4) &&\n\t    (cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid)))\n\t\treturn true;\n\n\t/*\n\t * Multi-stage node selection is used in conjunction with a periodic\n\t * migration fault to build a temporal task<->page relation. By using\n\t * a two-stage filter we remove short/unlikely relations.\n\t *\n\t * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate\n\t * a task's usage of a particular page (n_p) per total usage of this\n\t * page (n_t) (in a given time-span) to a probability.\n\t *\n\t * Our periodic faults will sample this probability and getting the\n\t * same result twice in a row, given these samples are fully\n\t * independent, is then given by P(n)^2, provided our sample period\n\t * is sufficiently short compared to the usage pattern.\n\t *\n\t * This quadric squishes small probabilities, making it less likely we\n\t * act on an unlikely task<->page relation.\n\t */\n\tif (!cpupid_pid_unset(last_cpupid) &&\n\t\t\t\tcpupid_to_nid(last_cpupid) != dst_nid)\n\t\treturn false;\n\n\t/* Always allow migrate on private faults */\n\tif (cpupid_match_pid(p, last_cpupid))\n\t\treturn true;\n\n\t/* A shared fault, but p->numa_group has not been set up yet. */\n\tif (!ng)\n\t\treturn true;\n\n\t/*\n\t * Destination node is much more heavily used than the source\n\t * node? Allow migration.\n\t */\n\tif (group_faults_cpu(ng, dst_nid) > group_faults_cpu(ng, src_nid) *\n\t\t\t\t\tACTIVE_NODE_FRACTION)\n\t\treturn true;\n\n\t/*\n\t * Distribute memory according to CPU & memory use on each node,\n\t * with 3/4 hysteresis to avoid unnecessary memory migrations:\n\t *\n\t * faults_cpu(dst)   3   faults_cpu(src)\n\t * --------------- * - > ---------------\n\t * faults_mem(dst)   4   faults_mem(src)\n\t */\n\treturn group_faults_cpu(ng, dst_nid) * group_faults(p, src_nid) * 3 >\n\t       group_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define ACTIVE_NODE_FRACTION 3"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "group_faults",
          "args": [
            "p",
            "dst_nid"
          ],
          "line": 1454
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1237-1241",
          "snippet": "static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpupid_match_pid",
          "args": [
            "p",
            "last_cpupid"
          ],
          "line": 1430
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpupid_to_nid",
          "args": [
            "last_cpupid"
          ],
          "line": 1426
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpupid_pid_unset",
          "args": [
            "last_cpupid"
          ],
          "line": 1425
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpupid_match_pid",
          "args": [
            "p",
            "last_cpupid"
          ],
          "line": 1405
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpupid_pid_unset",
          "args": [
            "last_cpupid"
          ],
          "line": 1405
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "page_cpupid_xchg_last",
          "args": [
            "page",
            "this_cpupid"
          ],
          "line": 1396
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_pid_to_cpupid",
          "args": [
            "dst_cpu",
            "current->pid"
          ],
          "line": 1395
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_to_node",
          "args": [
            "dst_cpu"
          ],
          "line": 1392
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define ACTIVE_NODE_FRACTION 3\n\nbool should_numa_migrate_memory(struct task_struct *p, struct page * page,\n\t\t\t\tint src_nid, int dst_cpu)\n{\n\tstruct numa_group *ng = p->numa_group;\n\tint dst_nid = cpu_to_node(dst_cpu);\n\tint last_cpupid, this_cpupid;\n\n\tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n\tlast_cpupid = page_cpupid_xchg_last(page, this_cpupid);\n\n\t/*\n\t * Allow first faults or private faults to migrate immediately early in\n\t * the lifetime of a task. The magic number 4 is based on waiting for\n\t * two full passes of the \"multi-stage node selection\" test that is\n\t * executed below.\n\t */\n\tif ((p->numa_preferred_nid == -1 || p->numa_scan_seq <= 4) &&\n\t    (cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid)))\n\t\treturn true;\n\n\t/*\n\t * Multi-stage node selection is used in conjunction with a periodic\n\t * migration fault to build a temporal task<->page relation. By using\n\t * a two-stage filter we remove short/unlikely relations.\n\t *\n\t * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate\n\t * a task's usage of a particular page (n_p) per total usage of this\n\t * page (n_t) (in a given time-span) to a probability.\n\t *\n\t * Our periodic faults will sample this probability and getting the\n\t * same result twice in a row, given these samples are fully\n\t * independent, is then given by P(n)^2, provided our sample period\n\t * is sufficiently short compared to the usage pattern.\n\t *\n\t * This quadric squishes small probabilities, making it less likely we\n\t * act on an unlikely task<->page relation.\n\t */\n\tif (!cpupid_pid_unset(last_cpupid) &&\n\t\t\t\tcpupid_to_nid(last_cpupid) != dst_nid)\n\t\treturn false;\n\n\t/* Always allow migrate on private faults */\n\tif (cpupid_match_pid(p, last_cpupid))\n\t\treturn true;\n\n\t/* A shared fault, but p->numa_group has not been set up yet. */\n\tif (!ng)\n\t\treturn true;\n\n\t/*\n\t * Destination node is much more heavily used than the source\n\t * node? Allow migration.\n\t */\n\tif (group_faults_cpu(ng, dst_nid) > group_faults_cpu(ng, src_nid) *\n\t\t\t\t\tACTIVE_NODE_FRACTION)\n\t\treturn true;\n\n\t/*\n\t * Distribute memory according to CPU & memory use on each node,\n\t * with 3/4 hysteresis to avoid unnecessary memory migrations:\n\t *\n\t * faults_cpu(dst)   3   faults_cpu(src)\n\t * --------------- * - > ---------------\n\t * faults_mem(dst)   4   faults_mem(src)\n\t */\n\treturn group_faults_cpu(ng, dst_nid) * group_faults(p, src_nid) * 3 >\n\t       group_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4;\n}"
  },
  {
    "function_name": "group_weight",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1369-1386",
    "snippet": "static inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "score_nearby_nodes",
          "args": [
            "p",
            "nid",
            "dist",
            "false"
          ],
          "line": 1383
        },
        "resolved": true,
        "details": {
          "function_name": "score_nearby_nodes",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1280-1342",
          "snippet": "static unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_faults",
          "args": [
            "p",
            "nid"
          ],
          "line": 1382
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1237-1241",
          "snippet": "static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\ttotal_faults = p->numa_group->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}"
  },
  {
    "function_name": "task_weight",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1350-1367",
    "snippet": "static inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "score_nearby_nodes",
          "args": [
            "p",
            "nid",
            "dist",
            "true"
          ],
          "line": 1364
        },
        "resolved": true,
        "details": {
          "function_name": "score_nearby_nodes",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1280-1342",
          "snippet": "static unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_faults",
          "args": [
            "p",
            "nid"
          ],
          "line": 1363
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1219-1226",
          "snippet": "static inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}"
  },
  {
    "function_name": "score_nearby_nodes",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1280-1342",
    "snippet": "static unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "group_faults",
          "args": [
            "p",
            "node"
          ],
          "line": 1323
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1237-1241",
          "snippet": "static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_faults",
          "args": [
            "p",
            "node"
          ],
          "line": 1321
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1219-1226",
          "snippet": "static inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      },
      {
        "call_info": {
          "callee": "node_distance",
          "args": [
            "nid",
            "node"
          ],
          "line": 1299
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint maxdist, bool task)\n{\n\tunsigned long score = 0;\n\tint node;\n\n\t/*\n\t * All nodes are directly connected, and the same distance\n\t * from each other. No need for fancy placement algorithms.\n\t */\n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t/*\n\t * This code is called for each node, introducing N^2 complexity,\n\t * which should be ok given the number of nodes rarely exceeds 8.\n\t */\n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t/*\n\t\t * The furthest away nodes in the system are not interesting\n\t\t * for placement; nid was already counted.\n\t\t */\n\t\tif (dist == sched_max_numa_distance || node == nid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * On systems with a backplane NUMA topology, compare groups\n\t\t * of nodes, and move tasks towards the group with the most\n\t\t * memory accesses. When comparing two nodes at distance\n\t\t * \"hoplimit\", only nodes closer by than \"hoplimit\" are part\n\t\t * of each group. Skip other nodes.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\tdist >= maxdist)\n\t\t\tcontinue;\n\n\t\t/* Add up the faults from nearby nodes. */\n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t/*\n\t\t * On systems with a glueless mesh NUMA topology, there are\n\t\t * no fixed \"groups of nodes\". Instead, nodes that are not\n\t\t * directly connected bounce traffic through intermediate\n\t\t * nodes; a numa_group can occupy any set of nodes.\n\t\t * The further away a node is, the less the faults count.\n\t\t * This seems to result in good task placement.\n\t\t */\n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (sched_max_numa_distance - dist);\n\t\t\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}"
  },
  {
    "function_name": "numa_is_active_node",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1274-1277",
    "snippet": "static bool numa_is_active_node(int nid, struct numa_group *ng)\n{\n\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION > ng->max_faults_cpu;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define ACTIVE_NODE_FRACTION 3"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "group_faults_cpu",
          "args": [
            "ng",
            "nid"
          ],
          "line": 1276
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_cpu",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1237-1241",
          "snippet": "static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define ACTIVE_NODE_FRACTION 3\n\nstatic bool numa_is_active_node(int nid, struct numa_group *ng)\n{\n\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION > ng->max_faults_cpu;\n}"
  },
  {
    "function_name": "group_faults_shared",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1255-1265",
    "snippet": "static inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_MEM",
            "node",
            "0"
          ],
          "line": 1261
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}"
  },
  {
    "function_name": "group_faults_priv",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1243-1253",
    "snippet": "static inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_MEM",
            "node",
            "1"
          ],
          "line": 1249
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}"
  },
  {
    "function_name": "group_faults_cpu",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1237-1241",
    "snippet": "static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_MEM",
            "nid",
            "1"
          ],
          "line": 1240
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tgroup->faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
  },
  {
    "function_name": "group_faults",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1228-1235",
    "snippet": "static inline unsigned long group_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\treturn p->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_MEM",
            "nid",
            "1"
          ],
          "line": 1234
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_group)\n\t\treturn 0;\n\n\treturn p->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
  },
  {
    "function_name": "task_faults",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1219-1226",
    "snippet": "static inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_faults_idx",
          "args": [
            "NUMA_MEM",
            "nid",
            "1"
          ],
          "line": 1225
        },
        "resolved": true,
        "details": {
          "function_name": "task_faults_idx",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1214-1217",
          "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define NR_NUMA_HINT_FAULT_TYPES 2"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}"
  },
  {
    "function_name": "task_faults_idx",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1214-1217",
    "snippet": "static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define NR_NUMA_HINT_FAULT_TYPES 2"
    ],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define NR_NUMA_HINT_FAULT_TYPES 2\n\nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}"
  },
  {
    "function_name": "task_numa_group_id",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1203-1206",
    "snippet": "pid_t task_numa_group_id(struct task_struct *p)\n{\n\treturn p->numa_group ? p->numa_group->gid : 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\npid_t task_numa_group_id(struct task_struct *p)\n{\n\treturn p->numa_group ? p->numa_group->gid : 0;\n}"
  },
  {
    "function_name": "account_numa_dequeue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1188-1192",
    "snippet": "static void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running -= (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_node",
          "args": [
            "p"
          ],
          "line": 1191
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running -= (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));\n}"
  },
  {
    "function_name": "account_numa_enqueue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1182-1186",
    "snippet": "static void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running += (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_node",
          "args": [
            "p"
          ],
          "line": 1185
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running += (p->numa_preferred_nid != -1);\n\trq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));\n}"
  },
  {
    "function_name": "init_numa_balancing",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1141-1180",
    "snippet": "void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)\n{\n\tint mm_users = 0;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (mm) {\n\t\tmm_users = atomic_read(&mm->mm_users);\n\t\tif (mm_users == 1) {\n\t\t\tmm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t\t\tmm->numa_scan_seq = 0;\n\t\t}\n\t}\n\tp->node_stamp\t\t\t= 0;\n\tp->numa_scan_seq\t\t= mm ? mm->numa_scan_seq : 0;\n\tp->numa_scan_period\t\t= sysctl_numa_balancing_scan_delay;\n\tp->numa_work.next\t\t= &p->numa_work;\n\tp->numa_faults\t\t\t= NULL;\n\tp->numa_group\t\t\t= NULL;\n\tp->last_task_numa_placement\t= 0;\n\tp->last_sum_exec_runtime\t= 0;\n\n\t/* New address space, reset the preferred nid */\n\tif (!(clone_flags & CLONE_VM)) {\n\t\tp->numa_preferred_nid = -1;\n\t\treturn;\n\t}\n\n\t/*\n\t * New thread, keep existing numa_preferred_nid which should be copied\n\t * already by arch_dup_task_struct but stagger when scans start.\n\t */\n\tif (mm) {\n\t\tunsigned int delay;\n\n\t\tdelay = min_t(unsigned int, task_scan_max(current),\n\t\t\tcurrent->numa_scan_period * mm_users * NSEC_PER_MSEC);\n\t\tdelay += 2 * TICK_NSEC;\n\t\tp->node_stamp = delay;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "min_t",
          "args": [
            "unsignedint",
            "task_scan_max(current)",
            "current->numa_scan_period * mm_users * NSEC_PER_MSEC"
          ],
          "line": 1175
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_scan_max",
          "args": [
            "current"
          ],
          "line": 1175
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_max",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1116-1139",
          "snippet": "static unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}"
        }
      },
      {
        "call_info": {
          "callee": "msecs_to_jiffies",
          "args": [
            "sysctl_numa_balancing_scan_delay"
          ],
          "line": 1149
        },
        "resolved": true,
        "details": {
          "function_name": "__msecs_to_jiffies",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/time.c",
          "lines": "565-573",
          "snippet": "unsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}",
          "includes": [
            "#include \"timekeeping.h\"",
            "#include <generated/timeconst.h>",
            "#include <asm/unistd.h>",
            "#include <linux/compat.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/math64.h>",
            "#include <linux/fs.h>",
            "#include <linux/security.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/errno.h>",
            "#include <linux/timekeeper_internal.h>",
            "#include <linux/capability.h>",
            "#include <linux/timex.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"timekeeping.h\"\n#include <generated/timeconst.h>\n#include <asm/unistd.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/math64.h>\n#include <linux/fs.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/errno.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/capability.h>\n#include <linux/timex.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nunsigned long __msecs_to_jiffies(const unsigned int m)\n{\n\t/*\n\t * Negative value, means infinite timeout:\n\t */\n\tif ((int)m < 0)\n\t\treturn MAX_JIFFY_OFFSET;\n\treturn _msecs_to_jiffies(m);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&mm->mm_users"
          ],
          "line": 1147
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid init_numa_balancing(unsigned long clone_flags, struct task_struct *p)\n{\n\tint mm_users = 0;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (mm) {\n\t\tmm_users = atomic_read(&mm->mm_users);\n\t\tif (mm_users == 1) {\n\t\t\tmm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t\t\tmm->numa_scan_seq = 0;\n\t\t}\n\t}\n\tp->node_stamp\t\t\t= 0;\n\tp->numa_scan_seq\t\t= mm ? mm->numa_scan_seq : 0;\n\tp->numa_scan_period\t\t= sysctl_numa_balancing_scan_delay;\n\tp->numa_work.next\t\t= &p->numa_work;\n\tp->numa_faults\t\t\t= NULL;\n\tp->numa_group\t\t\t= NULL;\n\tp->last_task_numa_placement\t= 0;\n\tp->last_sum_exec_runtime\t= 0;\n\n\t/* New address space, reset the preferred nid */\n\tif (!(clone_flags & CLONE_VM)) {\n\t\tp->numa_preferred_nid = -1;\n\t\treturn;\n\t}\n\n\t/*\n\t * New thread, keep existing numa_preferred_nid which should be copied\n\t * already by arch_dup_task_struct but stagger when scans start.\n\t */\n\tif (mm) {\n\t\tunsigned int delay;\n\n\t\tdelay = min_t(unsigned int, task_scan_max(current),\n\t\t\tcurrent->numa_scan_period * mm_users * NSEC_PER_MSEC);\n\t\tdelay += 2 * TICK_NSEC;\n\t\tp->node_stamp = delay;\n\t}\n}"
  },
  {
    "function_name": "task_scan_max",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1116-1139",
    "snippet": "static unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "max",
          "args": [
            "smin",
            "smax"
          ],
          "line": 1138
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&ng->refcount"
          ],
          "line": 1131
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_faults_priv",
          "args": [
            "ng"
          ],
          "line": 1128
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_priv",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1243-1253",
          "snippet": "static inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_faults_shared",
          "args": [
            "ng"
          ],
          "line": 1127
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_shared",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1255-1265",
          "snippet": "static inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_nr_scan_windows",
          "args": [
            "p"
          ],
          "line": 1122
        },
        "resolved": true,
        "details": {
          "function_name": "task_nr_scan_windows",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1061-1078",
          "snippet": "static unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_scan_min",
          "args": [
            "p"
          ],
          "line": 1118
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_min",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1083-1095",
          "snippet": "static unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define MAX_SCAN_WINDOW 2560"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define MAX_SCAN_WINDOW 2560\n\nstatic unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\n\t/* Watch for min being lower than max due to floor calculations */\n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}"
  },
  {
    "function_name": "task_scan_start",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1097-1114",
    "snippet": "static unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "max",
          "args": [
            "smin",
            "period"
          ],
          "line": 1113
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&ng->refcount"
          ],
          "line": 1108
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "group_faults_priv",
          "args": [
            "ng"
          ],
          "line": 1106
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_priv",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1243-1253",
          "snippet": "static inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "group_faults_shared",
          "args": [
            "ng"
          ],
          "line": 1105
        },
        "resolved": true,
        "details": {
          "function_name": "group_faults_shared",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1255-1265",
          "snippet": "static inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_scan_min",
          "args": [
            "p"
          ],
          "line": 1099
        },
        "resolved": true,
        "details": {
          "function_name": "task_scan_min",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1083-1095",
          "snippet": "static unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define MAX_SCAN_WINDOW 2560"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define MAX_SCAN_WINDOW 2560\n\nstatic unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\n\t/* Scale the maximum scan period with the amount of shared memory. */\n\tif (p->numa_group) {\n\t\tstruct numa_group *ng = p->numa_group;\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= atomic_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\n\treturn max(smin, period);\n}"
  },
  {
    "function_name": "task_scan_min",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1083-1095",
    "snippet": "static unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define MAX_SCAN_WINDOW 2560"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "max_t",
          "args": [
            "unsignedint",
            "floor",
            "scan"
          ],
          "line": 1094
        },
        "resolved": true,
        "details": {
          "function_name": "update_max_tr_single",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace.c",
          "lines": "1397-1431",
          "snippet": "void\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}",
          "includes": [
            "#include \"trace_selftest.c\"",
            "#include \"trace_output.h\"",
            "#include \"trace.h\"",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/trace.h>",
            "#include <linux/fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/poll.h>",
            "#include <linux/init.h>",
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/mount.h>",
            "#include <linux/string.h>",
            "#include <linux/kdebug.h>",
            "#include <linux/splice.h>",
            "#include <linux/percpu.h>",
            "#include <linux/module.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/linkage.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/tracefs.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/notifier.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/writeback.h>",
            "#include <linux/stacktrace.h>",
            "#include <generated/utsrelease.h>",
            "#include <linux/ring_buffer.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_selftest.c\"\n#include \"trace_output.h\"\n#include \"trace.h\"\n#include <linux/sched/rt.h>\n#include <linux/sched/clock.h>\n#include <linux/trace.h>\n#include <linux/fs.h>\n#include <linux/nmi.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/rwsem.h>\n#include <linux/mount.h>\n#include <linux/string.h>\n#include <linux/kdebug.h>\n#include <linux/splice.h>\n#include <linux/percpu.h>\n#include <linux/module.h>\n#include <linux/ftrace.h>\n#include <linux/vmalloc.h>\n#include <linux/uaccess.h>\n#include <linux/linkage.h>\n#include <linux/hardirq.h>\n#include <linux/pagemap.h>\n#include <linux/tracefs.h>\n#include <linux/debugfs.h>\n#include <linux/irqflags.h>\n#include <linux/notifier.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n#include <linux/writeback.h>\n#include <linux/stacktrace.h>\n#include <generated/utsrelease.h>\n#include <linux/ring_buffer.h>\n\nstatic __always_inline struct;\n\nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_nr_scan_windows",
          "args": [
            "p"
          ],
          "line": 1093
        },
        "resolved": true,
        "details": {
          "function_name": "task_nr_scan_windows",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "1061-1078",
          "snippet": "static unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}"
        }
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "sysctl_numa_balancing_scan_size"
          ],
          "line": 1085
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define MAX_SCAN_WINDOW 2560\n\nstatic unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}"
  },
  {
    "function_name": "task_nr_scan_windows",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1061-1078",
    "snippet": "static unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "round_up",
          "args": [
            "rss",
            "nr_scan_pages"
          ],
          "line": 1076
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_mm_rss",
          "args": [
            "p->mm"
          ],
          "line": 1072
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t/*\n\t * Calculations based on RSS as non-present and empty pages are skipped\n\t * by the PTE scanner and NUMA hinting faults should be trapped based\n\t * on resident pages\n\t */\n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}"
  },
  {
    "function_name": "update_stats_curr_start",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "1010-1017",
    "snippet": "static inline void\nupdate_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/*\n\t * We are starting a new run period:\n\t */\n\tse->exec_start = rq_clock_task(rq_of(cfs_rq));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rq_clock_task",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 1016
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 1016
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t/*\n\t * We are starting a new run period:\n\t */\n\tse->exec_start = rq_clock_task(rq_of(cfs_rq));\n}"
  },
  {
    "function_name": "update_stats_dequeue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "981-1005",
    "snippet": "static inline void\nupdate_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Mark the end of the wait period if dequeueing a\n\t * waiting task:\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\n\tif ((flags & DEQUEUE_SLEEP) && entity_is_task(se)) {\n\t\tstruct task_struct *tsk = task_of(se);\n\n\t\tif (tsk->state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.sleep_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t\tif (tsk->state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.block_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.block_start",
            "rq_clock(rq_of(cfs_rq))"
          ],
          "line": 1002
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 1003
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 1003
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.sleep_start",
            "rq_clock(rq_of(cfs_rq))"
          ],
          "line": 999
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 996
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 995
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_stats_wait_end",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 993
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_wait_end",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "861-891",
          "snippet": "static inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_enabled",
          "args": [],
          "line": 985
        },
        "resolved": true,
        "details": {
          "function_name": "force_schedstat_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2231-2237",
          "snippet": "void force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Mark the end of the wait period if dequeueing a\n\t * waiting task:\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_end(cfs_rq, se);\n\n\tif ((flags & DEQUEUE_SLEEP) && entity_is_task(se)) {\n\t\tstruct task_struct *tsk = task_of(se);\n\n\t\tif (tsk->state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.sleep_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t\tif (tsk->state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(se->statistics.block_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t}\n}"
  },
  {
    "function_name": "update_stats_enqueue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "964-979",
    "snippet": "static inline void\nupdate_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Are we enqueueing a waiting task? (for current tasks\n\t * a dequeue/enqueue event is a NOP)\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_start(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper(cfs_rq, se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_stats_enqueue_sleeper",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 978
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_enqueue_sleeper",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "893-959",
          "snippet": "static inline void\nupdate_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *tsk = NULL;\n\tu64 sleep_start, block_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tsleep_start = schedstat_val(se->statistics.sleep_start);\n\tblock_start = schedstat_val(se->statistics.block_start);\n\n\tif (entity_is_task(se))\n\t\ttsk = task_of(se);\n\n\tif (sleep_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.sleep_max)))\n\t\t\t__schedstat_set(se->statistics.sleep_max, delta);\n\n\t\t__schedstat_set(se->statistics.sleep_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 1);\n\t\t\ttrace_sched_stat_sleep(tsk, delta);\n\t\t}\n\t}\n\tif (block_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - block_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.block_max)))\n\t\t\t__schedstat_set(se->statistics.block_max, delta);\n\n\t\t__schedstat_set(se->statistics.block_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\tif (tsk->in_iowait) {\n\t\t\t\t__schedstat_add(se->statistics.iowait_sum, delta);\n\t\t\t\t__schedstat_inc(se->statistics.iowait_count);\n\t\t\t\ttrace_sched_stat_iowait(tsk, delta);\n\t\t\t}\n\n\t\t\ttrace_sched_stat_blocked(tsk, delta);\n\n\t\t\t/*\n\t\t\t * Blocking time is in units of nanosecs, so shift by\n\t\t\t * 20 to get a milliseconds-range estimation of the\n\t\t\t * amount of time that the task spent sleeping:\n\t\t\t */\n\t\t\tif (unlikely(prof_on == SLEEP_PROFILING)) {\n\t\t\t\tprofile_hits(SLEEP_PROFILING,\n\t\t\t\t\t\t(void *)get_wchan(tsk),\n\t\t\t\t\t\tdelta >> 20);\n\t\t\t}\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 0);\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *tsk = NULL;\n\tu64 sleep_start, block_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tsleep_start = schedstat_val(se->statistics.sleep_start);\n\tblock_start = schedstat_val(se->statistics.block_start);\n\n\tif (entity_is_task(se))\n\t\ttsk = task_of(se);\n\n\tif (sleep_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.sleep_max)))\n\t\t\t__schedstat_set(se->statistics.sleep_max, delta);\n\n\t\t__schedstat_set(se->statistics.sleep_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 1);\n\t\t\ttrace_sched_stat_sleep(tsk, delta);\n\t\t}\n\t}\n\tif (block_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - block_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.block_max)))\n\t\t\t__schedstat_set(se->statistics.block_max, delta);\n\n\t\t__schedstat_set(se->statistics.block_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\tif (tsk->in_iowait) {\n\t\t\t\t__schedstat_add(se->statistics.iowait_sum, delta);\n\t\t\t\t__schedstat_inc(se->statistics.iowait_count);\n\t\t\t\ttrace_sched_stat_iowait(tsk, delta);\n\t\t\t}\n\n\t\t\ttrace_sched_stat_blocked(tsk, delta);\n\n\t\t\t/*\n\t\t\t * Blocking time is in units of nanosecs, so shift by\n\t\t\t * 20 to get a milliseconds-range estimation of the\n\t\t\t * amount of time that the task spent sleeping:\n\t\t\t */\n\t\t\tif (unlikely(prof_on == SLEEP_PROFILING)) {\n\t\t\t\tprofile_hits(SLEEP_PROFILING,\n\t\t\t\t\t\t(void *)get_wchan(tsk),\n\t\t\t\t\t\tdelta >> 20);\n\t\t\t}\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 0);\n\t\t}\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_stats_wait_start",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 975
        },
        "resolved": true,
        "details": {
          "function_name": "update_stats_wait_start",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "843-859",
          "snippet": "static inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_enabled",
          "args": [],
          "line": 967
        },
        "resolved": true,
        "details": {
          "function_name": "force_schedstat_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2231-2237",
          "snippet": "void force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t/*\n\t * Are we enqueueing a waiting task? (for current tasks\n\t * a dequeue/enqueue event is a NOP)\n\t */\n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_start(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper(cfs_rq, se);\n}"
  },
  {
    "function_name": "update_stats_enqueue_sleeper",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "893-959",
    "snippet": "static inline void\nupdate_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *tsk = NULL;\n\tu64 sleep_start, block_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tsleep_start = schedstat_val(se->statistics.sleep_start);\n\tblock_start = schedstat_val(se->statistics.block_start);\n\n\tif (entity_is_task(se))\n\t\ttsk = task_of(se);\n\n\tif (sleep_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.sleep_max)))\n\t\t\t__schedstat_set(se->statistics.sleep_max, delta);\n\n\t\t__schedstat_set(se->statistics.sleep_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 1);\n\t\t\ttrace_sched_stat_sleep(tsk, delta);\n\t\t}\n\t}\n\tif (block_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - block_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.block_max)))\n\t\t\t__schedstat_set(se->statistics.block_max, delta);\n\n\t\t__schedstat_set(se->statistics.block_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\tif (tsk->in_iowait) {\n\t\t\t\t__schedstat_add(se->statistics.iowait_sum, delta);\n\t\t\t\t__schedstat_inc(se->statistics.iowait_count);\n\t\t\t\ttrace_sched_stat_iowait(tsk, delta);\n\t\t\t}\n\n\t\t\ttrace_sched_stat_blocked(tsk, delta);\n\n\t\t\t/*\n\t\t\t * Blocking time is in units of nanosecs, so shift by\n\t\t\t * 20 to get a milliseconds-range estimation of the\n\t\t\t * amount of time that the task spent sleeping:\n\t\t\t */\n\t\t\tif (unlikely(prof_on == SLEEP_PROFILING)) {\n\t\t\t\tprofile_hits(SLEEP_PROFILING,\n\t\t\t\t\t\t(void *)get_wchan(tsk),\n\t\t\t\t\t\tdelta >> 20);\n\t\t\t}\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 0);\n\t\t}\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "account_scheduler_latency",
          "args": [
            "tsk",
            "delta >> 10",
            "0"
          ],
          "line": 956
        },
        "resolved": true,
        "details": {
          "function_name": "__account_scheduler_latency",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/latencytop.c",
          "lines": "174-238",
          "snippet": "void __sched\n__account_scheduler_latency(struct task_struct *tsk, int usecs, int inter)\n{\n\tunsigned long flags;\n\tint i, q;\n\tstruct latency_record lat;\n\n\t/* Long interruptible waits are generally user requested... */\n\tif (inter && usecs > 5000)\n\t\treturn;\n\n\t/* Negative sleeps are time going backwards */\n\t/* Zero-time sleeps are non-interesting */\n\tif (usecs <= 0)\n\t\treturn;\n\n\tmemset(&lat, 0, sizeof(lat));\n\tlat.count = 1;\n\tlat.time = usecs;\n\tlat.max = usecs;\n\tstore_stacktrace(tsk, &lat);\n\n\traw_spin_lock_irqsave(&latency_lock, flags);\n\n\taccount_global_scheduler_latency(tsk, &lat);\n\n\tfor (i = 0; i < tsk->latency_record_count; i++) {\n\t\tstruct latency_record *mylat;\n\t\tint same = 1;\n\n\t\tmylat = &tsk->latency_record[i];\n\t\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\n\t\t\tunsigned long record = lat.backtrace[q];\n\n\t\t\tif (mylat->backtrace[q] != record) {\n\t\t\t\tsame = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* 0 and ULONG_MAX entries mean end of backtrace: */\n\t\t\tif (record == 0 || record == ULONG_MAX)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (same) {\n\t\t\tmylat->count++;\n\t\t\tmylat->time += lat.time;\n\t\t\tif (lat.time > mylat->max)\n\t\t\t\tmylat->max = lat.time;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t/*\n\t * short term hack; if we're > 32 we stop; future we recycle:\n\t */\n\tif (tsk->latency_record_count >= LT_SAVECOUNT)\n\t\tgoto out_unlock;\n\n\t/* Allocated a new one: */\n\ti = tsk->latency_record_count++;\n\tmemcpy(&tsk->latency_record[i], &lat, sizeof(struct latency_record));\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&latency_lock, flags);\n}",
          "includes": [
            "#include <linux/stacktrace.h>",
            "#include <linux/list.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched.h>",
            "#include <linux/export.h>",
            "#include <linux/latencytop.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/notifier.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/kallsyms.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_RAW_SPINLOCK(latency_lock);",
            "static struct latency_record latency_record[MAXLR];"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/stacktrace.h>\n#include <linux/list.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/debug.h>\n#include <linux/sched.h>\n#include <linux/export.h>\n#include <linux/latencytop.h>\n#include <linux/proc_fs.h>\n#include <linux/spinlock.h>\n#include <linux/notifier.h>\n#include <linux/seq_file.h>\n#include <linux/kallsyms.h>\n\nstatic DEFINE_RAW_SPINLOCK(latency_lock);\nstatic struct latency_record latency_record[MAXLR];\n\nvoid __sched\n__account_scheduler_latency(struct task_struct *tsk, int usecs, int inter)\n{\n\tunsigned long flags;\n\tint i, q;\n\tstruct latency_record lat;\n\n\t/* Long interruptible waits are generally user requested... */\n\tif (inter && usecs > 5000)\n\t\treturn;\n\n\t/* Negative sleeps are time going backwards */\n\t/* Zero-time sleeps are non-interesting */\n\tif (usecs <= 0)\n\t\treturn;\n\n\tmemset(&lat, 0, sizeof(lat));\n\tlat.count = 1;\n\tlat.time = usecs;\n\tlat.max = usecs;\n\tstore_stacktrace(tsk, &lat);\n\n\traw_spin_lock_irqsave(&latency_lock, flags);\n\n\taccount_global_scheduler_latency(tsk, &lat);\n\n\tfor (i = 0; i < tsk->latency_record_count; i++) {\n\t\tstruct latency_record *mylat;\n\t\tint same = 1;\n\n\t\tmylat = &tsk->latency_record[i];\n\t\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\n\t\t\tunsigned long record = lat.backtrace[q];\n\n\t\t\tif (mylat->backtrace[q] != record) {\n\t\t\t\tsame = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* 0 and ULONG_MAX entries mean end of backtrace: */\n\t\t\tif (record == 0 || record == ULONG_MAX)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (same) {\n\t\t\tmylat->count++;\n\t\t\tmylat->time += lat.time;\n\t\t\tif (lat.time > mylat->max)\n\t\t\t\tmylat->max = lat.time;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t/*\n\t * short term hack; if we're > 32 we stop; future we recycle:\n\t */\n\tif (tsk->latency_record_count >= LT_SAVECOUNT)\n\t\tgoto out_unlock;\n\n\t/* Allocated a new one: */\n\ti = tsk->latency_record_count++;\n\tmemcpy(&tsk->latency_record[i], &lat, sizeof(struct latency_record));\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&latency_lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "profile_hits",
          "args": [
            "SLEEP_PROFILING",
            "(void *)get_wchan(tsk)",
            "delta >> 20"
          ],
          "line": 952
        },
        "resolved": true,
        "details": {
          "function_name": "profile_hits",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/profile.c",
          "lines": "393-398",
          "snippet": "void profile_hits(int type, void *__pc, unsigned int nr_hits)\n{\n\tif (prof_on != type || !prof_buffer)\n\t\treturn;\n\tdo_profile_hits(type, __pc, nr_hits);\n}",
          "includes": [
            "#include <linux/uaccess.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/proc_fs.h>",
            "#include <asm/ptrace.h>",
            "#include <asm/irq_regs.h>",
            "#include <asm/sections.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/highmem.h>",
            "#include <linux/cpu.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/mm.h>",
            "#include <linux/notifier.h>",
            "#include <linux/memblock.h>",
            "#include <linux/profile.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static atomic_t *prof_buffer;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/uaccess.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <asm/ptrace.h>\n#include <asm/irq_regs.h>\n#include <asm/sections.h>\n#include <linux/sched/stat.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/mutex.h>\n#include <linux/highmem.h>\n#include <linux/cpu.h>\n#include <linux/cpumask.h>\n#include <linux/mm.h>\n#include <linux/notifier.h>\n#include <linux/memblock.h>\n#include <linux/profile.h>\n#include <linux/export.h>\n\nstatic atomic_t *prof_buffer;\n\nvoid profile_hits(int type, void *__pc, unsigned int nr_hits)\n{\n\tif (prof_on != type || !prof_buffer)\n\t\treturn;\n\tdo_profile_hits(type, __pc, nr_hits);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_wchan",
          "args": [
            "tsk"
          ],
          "line": 953
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "prof_on == SLEEP_PROFILING"
          ],
          "line": 951
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_blocked",
          "args": [
            "tsk",
            "delta"
          ],
          "line": 944
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_iowait",
          "args": [
            "tsk",
            "delta"
          ],
          "line": 941
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_inc",
          "args": [
            "se->statistics.iowait_count"
          ],
          "line": 940
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_add",
          "args": [
            "se->statistics.iowait_sum",
            "delta"
          ],
          "line": 939
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_add",
          "args": [
            "se->statistics.sum_sleep_runtime",
            "delta"
          ],
          "line": 935
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.block_start",
            "0"
          ],
          "line": 934
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.block_max",
            "delta"
          ],
          "line": 932
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "delta > schedstat_val(se->statistics.block_max)"
          ],
          "line": 931
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.block_max"
          ],
          "line": 931
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 926
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 926
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_sleep",
          "args": [
            "tsk",
            "delta"
          ],
          "line": 922
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_add",
          "args": [
            "se->statistics.sum_sleep_runtime",
            "delta"
          ],
          "line": 918
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.sleep_start",
            "0"
          ],
          "line": 917
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.sleep_max",
            "delta"
          ],
          "line": 915
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "delta > schedstat_val(se->statistics.sleep_max)"
          ],
          "line": 914
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.sleep_max"
          ],
          "line": 914
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 906
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 905
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.block_start"
          ],
          "line": 903
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.sleep_start"
          ],
          "line": 902
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_enabled",
          "args": [],
          "line": 899
        },
        "resolved": true,
        "details": {
          "function_name": "force_schedstat_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2231-2237",
          "snippet": "void force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *tsk = NULL;\n\tu64 sleep_start, block_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tsleep_start = schedstat_val(se->statistics.sleep_start);\n\tblock_start = schedstat_val(se->statistics.block_start);\n\n\tif (entity_is_task(se))\n\t\ttsk = task_of(se);\n\n\tif (sleep_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.sleep_max)))\n\t\t\t__schedstat_set(se->statistics.sleep_max, delta);\n\n\t\t__schedstat_set(se->statistics.sleep_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 1);\n\t\t\ttrace_sched_stat_sleep(tsk, delta);\n\t\t}\n\t}\n\tif (block_start) {\n\t\tu64 delta = rq_clock(rq_of(cfs_rq)) - block_start;\n\n\t\tif ((s64)delta < 0)\n\t\t\tdelta = 0;\n\n\t\tif (unlikely(delta > schedstat_val(se->statistics.block_max)))\n\t\t\t__schedstat_set(se->statistics.block_max, delta);\n\n\t\t__schedstat_set(se->statistics.block_start, 0);\n\t\t__schedstat_add(se->statistics.sum_sleep_runtime, delta);\n\n\t\tif (tsk) {\n\t\t\tif (tsk->in_iowait) {\n\t\t\t\t__schedstat_add(se->statistics.iowait_sum, delta);\n\t\t\t\t__schedstat_inc(se->statistics.iowait_count);\n\t\t\t\ttrace_sched_stat_iowait(tsk, delta);\n\t\t\t}\n\n\t\t\ttrace_sched_stat_blocked(tsk, delta);\n\n\t\t\t/*\n\t\t\t * Blocking time is in units of nanosecs, so shift by\n\t\t\t * 20 to get a milliseconds-range estimation of the\n\t\t\t * amount of time that the task spent sleeping:\n\t\t\t */\n\t\t\tif (unlikely(prof_on == SLEEP_PROFILING)) {\n\t\t\t\tprofile_hits(SLEEP_PROFILING,\n\t\t\t\t\t\t(void *)get_wchan(tsk),\n\t\t\t\t\t\tdelta >> 20);\n\t\t\t}\n\t\t\taccount_scheduler_latency(tsk, delta >> 10, 0);\n\t\t}\n\t}\n}"
  },
  {
    "function_name": "update_stats_wait_end",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "861-891",
    "snippet": "static inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.wait_start",
            "0"
          ],
          "line": 890
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_add",
          "args": [
            "se->statistics.wait_sum",
            "delta"
          ],
          "line": 889
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_inc",
          "args": [
            "se->statistics.wait_count"
          ],
          "line": 888
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.wait_max",
            "max(schedstat_val(se->statistics.wait_max), delta)"
          ],
          "line": 886
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "schedstat_val(se->statistics.wait_max)",
            "delta"
          ],
          "line": 887
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.wait_max"
          ],
          "line": 887
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_wait",
          "args": [
            "p",
            "delta"
          ],
          "line": 883
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.wait_start",
            "delta"
          ],
          "line": 880
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_on_rq_migrating",
          "args": [
            "p"
          ],
          "line": 874
        },
        "resolved": true,
        "details": {
          "function_name": "task_on_rq_migrating",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1540-1543",
          "snippet": "static inline int task_on_rq_migrating(struct task_struct *p)\n{\n\treturn p->on_rq == TASK_ON_RQ_MIGRATING;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [
            "#define TASK_ON_RQ_MIGRATING\t2"
          ],
          "globals_used": [
            "void __dl_clear_params(struct task_struct *p);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\n#define TASK_ON_RQ_MIGRATING\t2\n\nvoid __dl_clear_params(struct task_struct *p);\n\nstatic inline int task_on_rq_migrating(struct task_struct *p)\n{\n\treturn p->on_rq == TASK_ON_RQ_MIGRATING;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 873
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 872
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.wait_start"
          ],
          "line": 870
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 870
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 870
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_enabled",
          "args": [],
          "line": 867
        },
        "resolved": true,
        "details": {
          "function_name": "force_schedstat_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2231-2237",
          "snippet": "void force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct task_struct *p;\n\tu64 delta;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se)) {\n\t\tp = task_of(se);\n\t\tif (task_on_rq_migrating(p)) {\n\t\t\t/*\n\t\t\t * Preserve migrating task's wait time so wait_start\n\t\t\t * time stamp can be adjusted to accumulate wait time\n\t\t\t * prior to migration.\n\t\t\t */\n\t\t\t__schedstat_set(se->statistics.wait_start, delta);\n\t\t\treturn;\n\t\t}\n\t\ttrace_sched_stat_wait(p, delta);\n\t}\n\n\t__schedstat_set(se->statistics.wait_max,\n\t\t      max(schedstat_val(se->statistics.wait_max), delta));\n\t__schedstat_inc(se->statistics.wait_count);\n\t__schedstat_add(se->statistics.wait_sum, delta);\n\t__schedstat_set(se->statistics.wait_start, 0);\n}"
  },
  {
    "function_name": "update_stats_wait_start",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "843-859",
    "snippet": "static inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__schedstat_set",
          "args": [
            "se->statistics.wait_start",
            "wait_start"
          ],
          "line": 858
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "wait_start > prev_wait_start"
          ],
          "line": 855
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_on_rq_migrating",
          "args": [
            "task_of(se)"
          ],
          "line": 854
        },
        "resolved": true,
        "details": {
          "function_name": "task_on_rq_migrating",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1540-1543",
          "snippet": "static inline int task_on_rq_migrating(struct task_struct *p)\n{\n\treturn p->on_rq == TASK_ON_RQ_MIGRATING;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [
            "#define TASK_ON_RQ_MIGRATING\t2"
          ],
          "globals_used": [
            "void __dl_clear_params(struct task_struct *p);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\n#define TASK_ON_RQ_MIGRATING\t2\n\nvoid __dl_clear_params(struct task_struct *p);\n\nstatic inline int task_on_rq_migrating(struct task_struct *p)\n{\n\treturn p->on_rq == TASK_ON_RQ_MIGRATING;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 854
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 854
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_val",
          "args": [
            "se->statistics.wait_start"
          ],
          "line": 852
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 851
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 851
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_enabled",
          "args": [],
          "line": 848
        },
        "resolved": true,
        "details": {
          "function_name": "force_schedstat_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2231-2237",
          "snippet": "void force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 wait_start, prev_wait_start;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\twait_start = rq_clock(rq_of(cfs_rq));\n\tprev_wait_start = schedstat_val(se->statistics.wait_start);\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)) &&\n\t    likely(wait_start > prev_wait_start))\n\t\twait_start -= prev_wait_start;\n\n\t__schedstat_set(se->statistics.wait_start, wait_start);\n}"
  },
  {
    "function_name": "update_curr_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "838-841",
    "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_curr",
          "args": [
            "cfs_rq_of(&rq->curr->se)"
          ],
          "line": 840
        },
        "resolved": true,
        "details": {
          "function_name": "update_curr_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "838-841",
          "snippet": "static void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "&rq->curr->se"
          ],
          "line": 840
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}"
  },
  {
    "function_name": "update_curr",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "803-836",
    "snippet": "static void update_curr(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tu64 now = rq_clock_task(rq_of(cfs_rq));\n\tu64 delta_exec;\n\n\tif (unlikely(!curr))\n\t\treturn;\n\n\tdelta_exec = now - curr->exec_start;\n\tif (unlikely((s64)delta_exec <= 0))\n\t\treturn;\n\n\tcurr->exec_start = now;\n\n\tschedstat_set(curr->statistics.exec_max,\n\t\t      max(delta_exec, curr->statistics.exec_max));\n\n\tcurr->sum_exec_runtime += delta_exec;\n\tschedstat_add(cfs_rq->exec_clock, delta_exec);\n\n\tcurr->vruntime += calc_delta_fair(delta_exec, curr);\n\tupdate_min_vruntime(cfs_rq);\n\n\tif (entity_is_task(curr)) {\n\t\tstruct task_struct *curtask = task_of(curr);\n\n\t\ttrace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);\n\t\tcgroup_account_cputime(curtask, delta_exec);\n\t\taccount_group_exec_runtime(curtask, delta_exec);\n\t}\n\n\taccount_cfs_rq_runtime(cfs_rq, delta_exec);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "account_cfs_rq_runtime",
          "args": [
            "cfs_rq",
            "delta_exec"
          ],
          "line": 835
        },
        "resolved": true,
        "details": {
          "function_name": "account_cfs_rq_runtime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4985-4985",
          "snippet": "static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);",
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}"
        }
      },
      {
        "call_info": {
          "callee": "account_group_exec_runtime",
          "args": [
            "curtask",
            "delta_exec"
          ],
          "line": 832
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_account_cputime",
          "args": [
            "curtask",
            "delta_exec"
          ],
          "line": 831
        },
        "resolved": true,
        "details": {
          "function_name": "__cgroup_account_cputime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/rstat.c",
          "lines": "362-369",
          "snippet": "void __cgroup_account_cputime(struct cgroup *cgrp, u64 delta_exec)\n{\n\tstruct cgroup_rstat_cpu *rstatc;\n\n\trstatc = cgroup_base_stat_cputime_account_begin(cgrp);\n\trstatc->bstat.cputime.sum_exec_runtime += delta_exec;\n\tcgroup_base_stat_cputime_account_end(cgrp, rstatc);\n}",
          "includes": [
            "#include <linux/sched/cputime.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/sched/cputime.h>\n#include \"cgroup-internal.h\"\n\nvoid __cgroup_account_cputime(struct cgroup *cgrp, u64 delta_exec)\n{\n\tstruct cgroup_rstat_cpu *rstatc;\n\n\trstatc = cgroup_base_stat_cputime_account_begin(cgrp);\n\trstatc->bstat.cputime.sum_exec_runtime += delta_exec;\n\tcgroup_base_stat_cputime_account_end(cgrp, rstatc);\n}"
        }
      },
      {
        "call_info": {
          "callee": "trace_sched_stat_runtime",
          "args": [
            "curtask",
            "delta_exec",
            "curr->vruntime"
          ],
          "line": 830
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "curr"
          ],
          "line": 828
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "curr"
          ],
          "line": 827
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "update_min_vruntime",
          "args": [
            "cfs_rq"
          ],
          "line": 825
        },
        "resolved": true,
        "details": {
          "function_name": "update_min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "496-526",
          "snippet": "static void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "calc_delta_fair",
          "args": [
            "delta_exec",
            "curr"
          ],
          "line": 824
        },
        "resolved": true,
        "details": {
          "function_name": "calc_delta_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "628-634",
          "snippet": "static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedstat_add",
          "args": [
            "cfs_rq->exec_clock",
            "delta_exec"
          ],
          "line": 822
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "schedstat_set",
          "args": [
            "curr->statistics.exec_max",
            "max(delta_exec, curr->statistics.exec_max)"
          ],
          "line": 818
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max",
          "args": [
            "delta_exec",
            "curr->statistics.exec_max"
          ],
          "line": 819
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "(s64)delta_exec <= 0"
          ],
          "line": 813
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!curr"
          ],
          "line": 809
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rq_clock_task",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 806
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 806
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void update_curr(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tu64 now = rq_clock_task(rq_of(cfs_rq));\n\tu64 delta_exec;\n\n\tif (unlikely(!curr))\n\t\treturn;\n\n\tdelta_exec = now - curr->exec_start;\n\tif (unlikely((s64)delta_exec <= 0))\n\t\treturn;\n\n\tcurr->exec_start = now;\n\n\tschedstat_set(curr->statistics.exec_max,\n\t\t      max(delta_exec, curr->statistics.exec_max));\n\n\tcurr->sum_exec_runtime += delta_exec;\n\tschedstat_add(cfs_rq->exec_clock, delta_exec);\n\n\tcurr->vruntime += calc_delta_fair(delta_exec, curr);\n\tupdate_min_vruntime(cfs_rq);\n\n\tif (entity_is_task(curr)) {\n\t\tstruct task_struct *curtask = task_of(curr);\n\n\t\ttrace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);\n\t\tcgroup_account_cputime(curtask, delta_exec);\n\t\taccount_group_exec_runtime(curtask, delta_exec);\n\t}\n\n\taccount_cfs_rq_runtime(cfs_rq, delta_exec);\n}"
  },
  {
    "function_name": "update_tg_load_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "795-797",
    "snippet": "static void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)\n{\n}"
  },
  {
    "function_name": "post_init_entity_util_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "792-794",
    "snippet": "void post_init_entity_util_avg(struct sched_entity *se)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid post_init_entity_util_avg(struct sched_entity *se)\n{\n}"
  },
  {
    "function_name": "init_entity_runnable_average",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "789-791",
    "snippet": "void init_entity_runnable_average(struct sched_entity *se)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n}"
  },
  {
    "function_name": "post_init_entity_util_avg",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "748-786",
    "snippet": "void post_init_entity_util_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct sched_avg *sa = &se->avg;\n\tlong cpu_scale = arch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\tlong cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;\n\n\tif (cap > 0) {\n\t\tif (cfs_rq->avg.util_avg != 0) {\n\t\t\tsa->util_avg  = cfs_rq->avg.util_avg * se->load.weight;\n\t\t\tsa->util_avg /= (cfs_rq->avg.load_avg + 1);\n\n\t\t\tif (sa->util_avg > cap)\n\t\t\t\tsa->util_avg = cap;\n\t\t} else {\n\t\t\tsa->util_avg = cap;\n\t\t}\n\t}\n\n\tif (entity_is_task(se)) {\n\t\tstruct task_struct *p = task_of(se);\n\t\tif (p->sched_class != &fair_sched_class) {\n\t\t\t/*\n\t\t\t * For !fair tasks do:\n\t\t\t *\n\t\t\tupdate_cfs_rq_load_avg(now, cfs_rq);\n\t\t\tattach_entity_load_avg(cfs_rq, se, 0);\n\t\t\tswitched_from_fair(rq, p);\n\t\t\t *\n\t\t\t * such that the next switched_to_fair() has the\n\t\t\t * expected state.\n\t\t\t */\n\t\t\tse->avg.last_update_time = cfs_rq_clock_task(cfs_rq);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tattach_entity_cfs_rq(se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "const struct sched_class fair_sched_class;",
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "attach_entity_cfs_rq",
          "args": [
            "se"
          ],
          "line": 785
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_clock_task",
          "args": [
            "cfs_rq"
          ],
          "line": 780
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_clock_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "4980-4983",
          "snippet": "static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_task(rq_of(cfs_rq));\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 768
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 767
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "arch_scale_cpu_capacity",
          "args": [
            "NULL",
            "cpu_of(rq_of(cfs_rq))"
          ],
          "line": 752
        },
        "resolved": true,
        "details": {
          "function_name": "arch_scale_cpu_capacity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "1872-1876",
          "snippet": "static __always_inline\nunsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void resched_cpu(int cpu);\n\nstatic __always_inline\nunsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq_of(cfs_rq)"
          ],
          "line": 752
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 752
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nconst struct sched_class fair_sched_class;\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid post_init_entity_util_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct sched_avg *sa = &se->avg;\n\tlong cpu_scale = arch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));\n\tlong cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;\n\n\tif (cap > 0) {\n\t\tif (cfs_rq->avg.util_avg != 0) {\n\t\t\tsa->util_avg  = cfs_rq->avg.util_avg * se->load.weight;\n\t\t\tsa->util_avg /= (cfs_rq->avg.load_avg + 1);\n\n\t\t\tif (sa->util_avg > cap)\n\t\t\t\tsa->util_avg = cap;\n\t\t} else {\n\t\t\tsa->util_avg = cap;\n\t\t}\n\t}\n\n\tif (entity_is_task(se)) {\n\t\tstruct task_struct *p = task_of(se);\n\t\tif (p->sched_class != &fair_sched_class) {\n\t\t\t/*\n\t\t\t * For !fair tasks do:\n\t\t\t *\n\t\t\tupdate_cfs_rq_load_avg(now, cfs_rq);\n\t\t\tattach_entity_load_avg(cfs_rq, se, 0);\n\t\t\tswitched_from_fair(rq, p);\n\t\t\t *\n\t\t\t * such that the next switched_to_fair() has the\n\t\t\t * expected state.\n\t\t\t */\n\t\t\tse->avg.last_update_time = cfs_rq_clock_task(cfs_rq);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tattach_entity_cfs_rq(se);\n}"
  },
  {
    "function_name": "init_entity_runnable_average",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "699-717",
    "snippet": "void init_entity_runnable_average(struct sched_entity *se)\n{\n\tstruct sched_avg *sa = &se->avg;\n\n\tmemset(sa, 0, sizeof(*sa));\n\n\t/*\n\t * Tasks are intialized with full load to be seen as heavy tasks until\n\t * they get a chance to stabilize to their real load level.\n\t * Group entities are intialized with zero load to reflect the fact that\n\t * nothing has been attached to the task group yet.\n\t */\n\tif (entity_is_task(se))\n\t\tsa->runnable_load_avg = sa->load_avg = scale_load_down(se->load.weight);\n\n\tse->runnable_weight = se->load.weight;\n\n\t/* when this task enqueue'ed, it will contribute to its cfs_rq's load_avg */\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "se->load.weight"
          ],
          "line": 712
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 711
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "sa",
            "0",
            "sizeof(*sa)"
          ],
          "line": 703
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n\tstruct sched_avg *sa = &se->avg;\n\n\tmemset(sa, 0, sizeof(*sa));\n\n\t/*\n\t * Tasks are intialized with full load to be seen as heavy tasks until\n\t * they get a chance to stabilize to their real load level.\n\t * Group entities are intialized with zero load to reflect the fact that\n\t * nothing has been attached to the task group yet.\n\t */\n\tif (entity_is_task(se))\n\t\tsa->runnable_load_avg = sa->load_avg = scale_load_down(se->load.weight);\n\n\tse->runnable_weight = se->load.weight;\n\n\t/* when this task enqueue'ed, it will contribute to its cfs_rq's load_avg */\n}"
  },
  {
    "function_name": "sched_vslice",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "685-688",
    "snippet": "static u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\treturn calc_delta_fair(sched_slice(cfs_rq, se), se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "calc_delta_fair",
          "args": [
            "sched_slice(cfs_rq, se)",
            "se"
          ],
          "line": 687
        },
        "resolved": true,
        "details": {
          "function_name": "calc_delta_fair",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "628-634",
          "snippet": "static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sched_slice",
          "args": [
            "cfs_rq",
            "se"
          ],
          "line": 687
        },
        "resolved": true,
        "details": {
          "function_name": "sched_slice",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "658-678",
          "snippet": "static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\treturn calc_delta_fair(sched_slice(cfs_rq, se), se);\n}"
  },
  {
    "function_name": "sched_slice",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "658-678",
    "snippet": "static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__calc_delta",
          "args": [
            "slice",
            "se->load.weight",
            "load"
          ],
          "line": 675
        },
        "resolved": true,
        "details": {
          "function_name": "__calc_delta",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "218-241",
          "snippet": "static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define WMULT_SHIFT\t32"
          ],
          "globals_used": [
            "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define WMULT_SHIFT\t32\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\n\nstatic u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}"
        }
      },
      {
        "call_info": {
          "callee": "update_load_add",
          "args": [
            "&lw",
            "se->load.weight"
          ],
          "line": 672
        },
        "resolved": true,
        "details": {
          "function_name": "update_load_add",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "121-125",
          "snippet": "static inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!se->on_rq"
          ],
          "line": 669
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cfs_rq_of",
          "args": [
            "se"
          ],
          "line": 666
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__sched_period",
          "args": [
            "cfs_rq->nr_running + !se->on_rq"
          ],
          "line": 660
        },
        "resolved": true,
        "details": {
          "function_name": "__sched_period",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "644-650",
          "snippet": "static u64 __sched_period(unsigned long nr_running)\n{\n\tif (unlikely(nr_running > sched_nr_latency))\n\t\treturn nr_running * sysctl_sched_min_granularity;\n\telse\n\t\treturn sysctl_sched_latency;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
            "unsigned int sysctl_sched_min_granularity\t\t= 750000ULL;",
            "static unsigned int sched_nr_latency = 8;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nunsigned int sysctl_sched_min_granularity\t\t= 750000ULL;\nstatic unsigned int sched_nr_latency = 8;\n\nstatic u64 __sched_period(unsigned long nr_running)\n{\n\tif (unlikely(nr_running > sched_nr_latency))\n\t\treturn nr_running * sysctl_sched_min_granularity;\n\telse\n\t\treturn sysctl_sched_latency;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tu64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);\n\n\tfor_each_sched_entity(se) {\n\t\tstruct load_weight *load;\n\t\tstruct load_weight lw;\n\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tload = &cfs_rq->load;\n\n\t\tif (unlikely(!se->on_rq)) {\n\t\t\tlw = cfs_rq->load;\n\n\t\t\tupdate_load_add(&lw, se->load.weight);\n\t\t\tload = &lw;\n\t\t}\n\t\tslice = __calc_delta(slice, se->load.weight, load);\n\t}\n\treturn slice;\n}"
  },
  {
    "function_name": "__sched_period",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "644-650",
    "snippet": "static u64 __sched_period(unsigned long nr_running)\n{\n\tif (unlikely(nr_running > sched_nr_latency))\n\t\treturn nr_running * sysctl_sched_min_granularity;\n\telse\n\t\treturn sysctl_sched_latency;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
      "unsigned int sysctl_sched_min_granularity\t\t= 750000ULL;",
      "static unsigned int sched_nr_latency = 8;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "nr_running > sched_nr_latency"
          ],
          "line": 646
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nunsigned int sysctl_sched_min_granularity\t\t= 750000ULL;\nstatic unsigned int sched_nr_latency = 8;\n\nstatic u64 __sched_period(unsigned long nr_running)\n{\n\tif (unlikely(nr_running > sched_nr_latency))\n\t\treturn nr_running * sysctl_sched_min_granularity;\n\telse\n\t\treturn sysctl_sched_latency;\n}"
  },
  {
    "function_name": "calc_delta_fair",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "628-634",
    "snippet": "static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "__calc_delta",
          "args": [
            "delta",
            "NICE_0_LOAD",
            "&se->load"
          ],
          "line": 631
        },
        "resolved": true,
        "details": {
          "function_name": "__calc_delta",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "218-241",
          "snippet": "static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define WMULT_SHIFT\t32"
          ],
          "globals_used": [
            "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define WMULT_SHIFT\t32\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\n\nstatic u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "se->load.weight != NICE_0_LOAD"
          ],
          "line": 630
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}"
  },
  {
    "function_name": "sched_proc_update_handler",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "601-622",
    "snippet": "int sched_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tunsigned int factor = get_update_sysctl_factor();\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tsched_nr_latency = DIV_ROUND_UP(sysctl_sched_latency,\n\t\t\t\t\tsysctl_sched_min_granularity);\n\n#define WRT_SYSCTL(name) \\\n\t(normalized_sysctl_##name = sysctl_##name / (factor))\n\tWRT_SYSCTL(sched_min_granularity);\n\tWRT_SYSCTL(sched_latency);\n\tWRT_SYSCTL(sched_wakeup_granularity);\n#undef WRT_SYSCTL\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "unsigned int sysctl_sched_latency\t\t\t= 6000000ULL;",
      "unsigned int sysctl_sched_min_granularity\t\t= 750000ULL;",
      "static unsigned int sched_nr_latency = 8;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRT_SYSCTL",
          "args": [
            "sched_wakeup_granularity"
          ],
          "line": 618
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRT_SYSCTL",
          "args": [
            "sched_latency"
          ],
          "line": 617
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "WRT_SYSCTL",
          "args": [
            "sched_min_granularity"
          ],
          "line": 616
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DIV_ROUND_UP",
          "args": [
            "sysctl_sched_latency",
            "sysctl_sched_min_granularity"
          ],
          "line": 611
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_update_sysctl_factor",
          "args": [],
          "line": 606
        },
        "resolved": true,
        "details": {
          "function_name": "get_update_sysctl_factor",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "148-167",
          "snippet": "static unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "enum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nenum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;\n\nstatic unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}"
        }
      },
      {
        "call_info": {
          "callee": "proc_dointvec_minmax",
          "args": [
            "table",
            "write",
            "buffer",
            "lenp",
            "ppos"
          ],
          "line": 605
        },
        "resolved": true,
        "details": {
          "function_name": "proc_dointvec_minmax",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sysctl.c",
          "lines": "3200-3204",
          "snippet": "int proc_dointvec_minmax(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}",
          "includes": [
            "#include <linux/inotify.h>",
            "#include <linux/nmi.h>",
            "#include <linux/stackleak.h>",
            "#include <scsi/sg.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/acct.h>",
            "#include <asm/setup.h>",
            "#include <asm/io.h>",
            "#include <asm/stacktrace.h>",
            "#include <asm/nmi.h>",
            "#include <asm/processor.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/mount.h>",
            "#include <linux/bpf.h>",
            "#include <linux/kexec.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/capability.h>",
            "#include <linux/kmod.h>",
            "#include <linux/oom.h>",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/reboot.h>",
            "#include <linux/acpi.h>",
            "#include <linux/nfs_fs.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/dnotify.h>",
            "#include <linux/dcache.h>",
            "#include <linux/limits.h>",
            "#include <linux/times.h>",
            "#include <linux/key.h>",
            "#include <linux/initrd.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/compaction.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/writeback.h>",
            "#include <linux/highuid.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/net.h>",
            "#include <linux/kobject.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init.h>",
            "#include <linux/fs.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/ctype.h>",
            "#include <linux/security.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/printk.h>",
            "#include <linux/signal.h>",
            "#include <linux/bitmap.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/slab.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/aio.h>",
            "#include <linux/module.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/inotify.h>\n#include <linux/nmi.h>\n#include <linux/stackleak.h>\n#include <scsi/sg.h>\n#include <linux/lockdep.h>\n#include <linux/rtmutex.h>\n#include <linux/acct.h>\n#include <asm/setup.h>\n#include <asm/io.h>\n#include <asm/stacktrace.h>\n#include <asm/nmi.h>\n#include <asm/processor.h>\n#include <linux/uaccess.h>\n#include <linux/mount.h>\n#include <linux/bpf.h>\n#include <linux/kexec.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/sysctl.h>\n#include <linux/binfmts.h>\n#include <linux/capability.h>\n#include <linux/kmod.h>\n#include <linux/oom.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/kprobes.h>\n#include <linux/perf_event.h>\n#include <linux/ftrace.h>\n#include <linux/reboot.h>\n#include <linux/acpi.h>\n#include <linux/nfs_fs.h>\n#include <linux/vmstat.h>\n#include <linux/syscalls.h>\n#include <linux/dnotify.h>\n#include <linux/dcache.h>\n#include <linux/limits.h>\n#include <linux/times.h>\n#include <linux/key.h>\n#include <linux/initrd.h>\n#include <linux/hugetlb.h>\n#include <linux/compaction.h>\n#include <linux/ratelimit.h>\n#include <linux/writeback.h>\n#include <linux/highuid.h>\n#include <linux/sysrq.h>\n#include <linux/net.h>\n#include <linux/kobject.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/kmemleak.h>\n#include <linux/ctype.h>\n#include <linux/security.h>\n#include <linux/proc_fs.h>\n#include <linux/printk.h>\n#include <linux/signal.h>\n#include <linux/bitmap.h>\n#include <linux/sysctl.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/aio.h>\n#include <linux/module.h>\n\nint proc_dointvec_minmax(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nunsigned int sysctl_sched_latency\t\t\t= 6000000ULL;\nunsigned int sysctl_sched_min_granularity\t\t= 750000ULL;\nstatic unsigned int sched_nr_latency = 8;\n\nint sched_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tunsigned int factor = get_update_sysctl_factor();\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tsched_nr_latency = DIV_ROUND_UP(sysctl_sched_latency,\n\t\t\t\t\tsysctl_sched_min_granularity);\n\n#define WRT_SYSCTL(name) \\\n\t(normalized_sysctl_##name = sysctl_##name / (factor))\n\tWRT_SYSCTL(sched_min_granularity);\n\tWRT_SYSCTL(sched_latency);\n\tWRT_SYSCTL(sched_wakeup_granularity);\n#undef WRT_SYSCTL\n\n\treturn 0;\n}"
  },
  {
    "function_name": "__pick_last_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "587-595",
    "snippet": "struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *last = rb_last(&cfs_rq->tasks_timeline.rb_root);\n\n\tif (!last)\n\t\treturn NULL;\n\n\treturn rb_entry(last, struct sched_entity, run_node);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "last",
            "structsched_entity",
            "run_node"
          ],
          "line": 594
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_last",
          "args": [
            "&cfs_rq->tasks_timeline.rb_root"
          ],
          "line": 589
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstruct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *last = rb_last(&cfs_rq->tasks_timeline.rb_root);\n\n\tif (!last)\n\t\treturn NULL;\n\n\treturn rb_entry(last, struct sched_entity, run_node);\n}"
  },
  {
    "function_name": "__pick_next_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "576-584",
    "snippet": "static struct sched_entity *__pick_next_entity(struct sched_entity *se)\n{\n\tstruct rb_node *next = rb_next(&se->run_node);\n\n\tif (!next)\n\t\treturn NULL;\n\n\treturn rb_entry(next, struct sched_entity, run_node);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "next",
            "structsched_entity",
            "run_node"
          ],
          "line": 583
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_next",
          "args": [
            "&se->run_node"
          ],
          "line": 578
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic struct sched_entity *__pick_next_entity(struct sched_entity *se)\n{\n\tstruct rb_node *next = rb_next(&se->run_node);\n\n\tif (!next)\n\t\treturn NULL;\n\n\treturn rb_entry(next, struct sched_entity, run_node);\n}"
  },
  {
    "function_name": "__pick_first_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "566-574",
    "snippet": "struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "left",
            "structsched_entity",
            "run_node"
          ],
          "line": 573
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_first_cached",
          "args": [
            "&cfs_rq->tasks_timeline"
          ],
          "line": 568
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstruct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn rb_entry(left, struct sched_entity, run_node);\n}"
  },
  {
    "function_name": "__dequeue_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "561-564",
    "snippet": "static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_erase_cached",
          "args": [
            "&se->run_node",
            "&cfs_rq->tasks_timeline"
          ],
          "line": 563
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);\n}"
  },
  {
    "function_name": "__enqueue_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "531-559",
    "snippet": "static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_insert_color_cached",
          "args": [
            "&se->run_node",
            "&cfs_rq->tasks_timeline",
            "leftmost"
          ],
          "line": 557
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_link_node",
          "args": [
            "&se->run_node",
            "parent",
            "link"
          ],
          "line": 556
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "entity_before",
          "args": [
            "se",
            "entry"
          ],
          "line": 548
        },
        "resolved": true,
        "details": {
          "function_name": "entity_before",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "490-494",
          "snippet": "static inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "parent",
            "structsched_entity",
            "run_node"
          ],
          "line": 543
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sched_entity *entry;\n\tbool leftmost = true;\n\n\t/*\n\t * Find the right place in the rbtree:\n\t */\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct sched_entity, run_node);\n\t\t/*\n\t\t * We dont care about collisions. Nodes with\n\t\t * the same key stay together.\n\t\t */\n\t\tif (entity_before(se, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&se->run_node, parent, link);\n\trb_insert_color_cached(&se->run_node,\n\t\t\t       &cfs_rq->tasks_timeline, leftmost);\n}"
  },
  {
    "function_name": "update_min_vruntime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "496-526",
    "snippet": "static void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_wmb",
          "args": [],
          "line": 523
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "max_vruntime",
          "args": [
            "cfs_rq->min_vruntime",
            "vruntime"
          ],
          "line": 521
        },
        "resolved": true,
        "details": {
          "function_name": "max_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "472-479",
          "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "min_vruntime",
          "args": [
            "vruntime",
            "se->vruntime"
          ],
          "line": 517
        },
        "resolved": true,
        "details": {
          "function_name": "min_vruntime",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "481-488",
          "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "leftmost",
            "structsched_entity",
            "run_node"
          ],
          "line": 512
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_first_cached",
          "args": [
            "&cfs_rq->tasks_timeline"
          ],
          "line": 499
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (leftmost) { /* non-empty tree */\n\t\tstruct sched_entity *se;\n\t\tse = rb_entry(leftmost, struct sched_entity, run_node);\n\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t/* ensure we never gain time by being placed backwards. */\n\tcfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);\n#ifndef CONFIG_64BIT\n\tsmp_wmb();\n\tcfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;\n#endif\n}"
  },
  {
    "function_name": "entity_before",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "490-494",
    "snippet": "static inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "",
          "args": [
            "a->vruntime - b->vruntime"
          ],
          "line": 493
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline int entity_before(struct sched_entity *a,\n\t\t\t\tstruct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}"
  },
  {
    "function_name": "min_vruntime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "481-488",
    "snippet": "static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "",
          "args": [
            "vruntime - min_vruntime"
          ],
          "line": 483
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}"
  },
  {
    "function_name": "max_vruntime",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "472-479",
    "snippet": "static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "",
          "args": [
            "vruntime - max_vruntime"
          ],
          "line": 474
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}"
  },
  {
    "function_name": "find_matching_se",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "458-461",
    "snippet": "static inline void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n}"
  },
  {
    "function_name": "parent_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "453-456",
    "snippet": "static inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}"
  },
  {
    "function_name": "list_del_leaf_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "446-448",
    "snippet": "static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
  },
  {
    "function_name": "list_add_leaf_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "442-444",
    "snippet": "static inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
  },
  {
    "function_name": "group_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "437-440",
    "snippet": "static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}"
  },
  {
    "function_name": "cfs_rq_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "428-434",
    "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_rq",
          "args": [
            "p"
          ],
          "line": 431
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_of",
          "args": [
            "se"
          ],
          "line": 430
        },
        "resolved": true,
        "details": {
          "function_name": "task_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "409-412",
          "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
  },
  {
    "function_name": "task_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "423-426",
    "snippet": "static inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn &task_rq(p)->cfs;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "task_rq",
          "args": [
            "p"
          ],
          "line": 425
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn &task_rq(p)->cfs;\n}"
  },
  {
    "function_name": "rq_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "414-417",
    "snippet": "static inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn container_of(cfs_rq, struct rq, cfs);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "cfs_rq",
            "structrq",
            "cfs"
          ],
          "line": 416
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn container_of(cfs_rq, struct rq, cfs);\n}"
  },
  {
    "function_name": "task_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "409-412",
    "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "se",
            "structtask_struct",
            "se"
          ],
          "line": 411
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\treturn container_of(se, struct task_struct, se);\n}"
  },
  {
    "function_name": "find_matching_se",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "375-405",
    "snippet": "static void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n\tint se_depth, pse_depth;\n\n\t/*\n\t * preemption test can be made between sibling entities who are in the\n\t * same cfs_rq i.e who have a common parent. Walk up the hierarchy of\n\t * both tasks until we find their ancestors who are siblings of common\n\t * parent.\n\t */\n\n\t/* First walk up until both entities are at same depth */\n\tse_depth = (*se)->depth;\n\tpse_depth = (*pse)->depth;\n\n\twhile (se_depth > pse_depth) {\n\t\tse_depth--;\n\t\t*se = parent_entity(*se);\n\t}\n\n\twhile (pse_depth > se_depth) {\n\t\tpse_depth--;\n\t\t*pse = parent_entity(*pse);\n\t}\n\n\twhile (!is_same_group(*se, *pse)) {\n\t\t*se = parent_entity(*se);\n\t\t*pse = parent_entity(*pse);\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "parent_entity",
          "args": [
            "*pse"
          ],
          "line": 403
        },
        "resolved": true,
        "details": {
          "function_name": "parent_entity",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "453-456",
          "snippet": "static inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "is_same_group",
          "args": [
            "*se",
            "*pse"
          ],
          "line": 401
        },
        "resolved": true,
        "details": {
          "function_name": "is_same_group",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "361-368",
          "snippet": "static inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n\tint se_depth, pse_depth;\n\n\t/*\n\t * preemption test can be made between sibling entities who are in the\n\t * same cfs_rq i.e who have a common parent. Walk up the hierarchy of\n\t * both tasks until we find their ancestors who are siblings of common\n\t * parent.\n\t */\n\n\t/* First walk up until both entities are at same depth */\n\tse_depth = (*se)->depth;\n\tpse_depth = (*pse)->depth;\n\n\twhile (se_depth > pse_depth) {\n\t\tse_depth--;\n\t\t*se = parent_entity(*se);\n\t}\n\n\twhile (pse_depth > se_depth) {\n\t\tpse_depth--;\n\t\t*pse = parent_entity(*pse);\n\t}\n\n\twhile (!is_same_group(*se, *pse)) {\n\t\t*se = parent_entity(*se);\n\t\t*pse = parent_entity(*pse);\n\t}\n}"
  },
  {
    "function_name": "parent_entity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "370-373",
    "snippet": "static inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn se->parent;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn se->parent;\n}"
  },
  {
    "function_name": "is_same_group",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "361-368",
    "snippet": "static inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}"
  },
  {
    "function_name": "list_del_leaf_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "347-353",
    "snippet": "static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->on_list) {\n\t\tlist_del_rcu(&cfs_rq->leaf_cfs_rq_list);\n\t\tcfs_rq->on_list = 0;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "list_del_rcu",
          "args": [
            "&cfs_rq->leaf_cfs_rq_list"
          ],
          "line": 350
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->on_list) {\n\t\tlist_del_rcu(&cfs_rq->leaf_cfs_rq_list);\n\t\tcfs_rq->on_list = 0;\n\t}\n}"
  },
  {
    "function_name": "list_add_leaf_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "285-345",
    "snippet": "static inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_rq->on_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tint cpu = cpu_of(rq);\n\t\t/*\n\t\t * Ensure we either appear before our parent (if already\n\t\t * enqueued) or force our parent to appear after us when it is\n\t\t * enqueued. The fact that we always enqueue bottom-up\n\t\t * reduces this to two cases and a special case for the root\n\t\t * cfs_rq. Furthermore, it also means that we will always reset\n\t\t * tmp_alone_branch either when the branch is connected\n\t\t * to a tree or when we reach the beg of the tree\n\t\t */\n\t\tif (cfs_rq->tg->parent &&\n\t\t    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {\n\t\t\t/*\n\t\t\t * If parent is already on the list, we add the child\n\t\t\t * just before. Thanks to circular linked property of\n\t\t\t * the list, this means to put the child at the tail\n\t\t\t * of the list that starts by parent.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));\n\t\t\t/*\n\t\t\t * The branch is now connected to its tree so we can\n\t\t\t * reset tmp_alone_branch to the beginning of the\n\t\t\t * list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else if (!cfs_rq->tg->parent) {\n\t\t\t/*\n\t\t\t * cfs rq without parent should be put\n\t\t\t * at the tail of the list.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&rq->leaf_cfs_rq_list);\n\t\t\t/*\n\t\t\t * We have reach the beg of a tree so we can reset\n\t\t\t * tmp_alone_branch to the beginning of the list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The parent has not already been added so we want to\n\t\t\t * make sure that it will be put after us.\n\t\t\t * tmp_alone_branch points to the beg of the branch\n\t\t\t * where we will add parent.\n\t\t\t */\n\t\t\tlist_add_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\trq->tmp_alone_branch);\n\t\t\t/*\n\t\t\t * update tmp_alone_branch to points to the new beg\n\t\t\t * of the branch\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;\n\t\t}\n\n\t\tcfs_rq->on_list = 1;\n\t}\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "list_add_rcu",
          "args": [
            "&cfs_rq->leaf_cfs_rq_list",
            "rq->tmp_alone_branch"
          ],
          "line": 334
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_add_tail_rcu",
          "args": [
            "&cfs_rq->leaf_cfs_rq_list",
            "&rq->leaf_cfs_rq_list"
          ],
          "line": 320
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_add_tail_rcu",
          "args": [
            "&cfs_rq->leaf_cfs_rq_list",
            "&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list)"
          ],
          "line": 307
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpu_of",
          "args": [
            "rq"
          ],
          "line": 289
        },
        "resolved": true,
        "details": {
          "function_name": "cpu_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/sched.h",
          "lines": "928-935",
          "snippet": "static inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}",
          "includes": [
            "#include \"features.h\"",
            "#include \"features.h\"",
            "# include <linux/static_key.h>",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include <linux/psi.h>",
            "#include <linux/cgroup.h>",
            "#include \"cpudeadline.h\"",
            "#include \"cpupri.h\"",
            "# include <asm/paravirt.h>",
            "#include <asm/tlb.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/task_work.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swait.h>",
            "#include <linux/suspend.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/security.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/psi.h>",
            "#include <linux/profile.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/nmi.h>",
            "#include <linux/mmu_context.h>",
            "#include <linux/migrate.h>",
            "#include <linux/membarrier.h>",
            "#include <linux/kthread.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/init_task.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ctype.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/cpuidle.h>",
            "#include <linux/cpufreq.h>",
            "#include <linux/context_tracking.h>",
            "#include <linux/compat.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/binfmts.h>",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/xacct.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/topology.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/prio.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/loadavg.h>",
            "#include <linux/sched/jobctl.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/init.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/cpufreq.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "extern bool dl_cpu_busy(unsigned int cpu);",
            "extern void update_rq_clock(struct rq *rq);",
            "extern void resched_curr(struct rq *rq);",
            "extern void resched_cpu(int cpu);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include \"features.h\"\n# include <linux/static_key.h>\n#include \"autogroup.h\"\n#include \"stats.h\"\n#include <linux/psi.h>\n#include <linux/cgroup.h>\n#include \"cpudeadline.h\"\n#include \"cpupri.h\"\n# include <asm/paravirt.h>\n#include <asm/tlb.h>\n#include <linux/tsacct_kern.h>\n#include <linux/task_work.h>\n#include <linux/syscalls.h>\n#include <linux/swait.h>\n#include <linux/suspend.h>\n#include <linux/stop_machine.h>\n#include <linux/security.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/psi.h>\n#include <linux/profile.h>\n#include <linux/prefetch.h>\n#include <linux/proc_fs.h>\n#include <linux/nmi.h>\n#include <linux/mmu_context.h>\n#include <linux/migrate.h>\n#include <linux/membarrier.h>\n#include <linux/kthread.h>\n#include <linux/kprobes.h>\n#include <linux/init_task.h>\n#include <linux/delayacct.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/cpuset.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/context_tracking.h>\n#include <linux/compat.h>\n#include <linux/blkdev.h>\n#include <linux/binfmts.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/xacct.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/user.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/init.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched.h>\n\nextern bool dl_cpu_busy(unsigned int cpu);\nextern void update_rq_clock(struct rq *rq);\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "rq_of",
          "args": [
            "cfs_rq"
          ],
          "line": 288
        },
        "resolved": true,
        "details": {
          "function_name": "cfs_rq_of",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "428-434",
          "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static void set_next_buddy(struct sched_entity *se);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\tstruct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_rq->on_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tint cpu = cpu_of(rq);\n\t\t/*\n\t\t * Ensure we either appear before our parent (if already\n\t\t * enqueued) or force our parent to appear after us when it is\n\t\t * enqueued. The fact that we always enqueue bottom-up\n\t\t * reduces this to two cases and a special case for the root\n\t\t * cfs_rq. Furthermore, it also means that we will always reset\n\t\t * tmp_alone_branch either when the branch is connected\n\t\t * to a tree or when we reach the beg of the tree\n\t\t */\n\t\tif (cfs_rq->tg->parent &&\n\t\t    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {\n\t\t\t/*\n\t\t\t * If parent is already on the list, we add the child\n\t\t\t * just before. Thanks to circular linked property of\n\t\t\t * the list, this means to put the child at the tail\n\t\t\t * of the list that starts by parent.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));\n\t\t\t/*\n\t\t\t * The branch is now connected to its tree so we can\n\t\t\t * reset tmp_alone_branch to the beginning of the\n\t\t\t * list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else if (!cfs_rq->tg->parent) {\n\t\t\t/*\n\t\t\t * cfs rq without parent should be put\n\t\t\t * at the tail of the list.\n\t\t\t */\n\t\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\t&rq->leaf_cfs_rq_list);\n\t\t\t/*\n\t\t\t * We have reach the beg of a tree so we can reset\n\t\t\t * tmp_alone_branch to the beginning of the list.\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The parent has not already been added so we want to\n\t\t\t * make sure that it will be put after us.\n\t\t\t * tmp_alone_branch points to the beg of the branch\n\t\t\t * where we will add parent.\n\t\t\t */\n\t\t\tlist_add_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t\trq->tmp_alone_branch);\n\t\t\t/*\n\t\t\t * update tmp_alone_branch to points to the new beg\n\t\t\t * of the branch\n\t\t\t */\n\t\t\trq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;\n\t\t}\n\n\t\tcfs_rq->on_list = 1;\n\t}\n}"
  },
  {
    "function_name": "group_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "280-283",
    "snippet": "static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn grp->my_q;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn grp->my_q;\n}"
  },
  {
    "function_name": "cfs_rq_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "274-277",
    "snippet": "static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\treturn se->cfs_rq;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)\n{\n\treturn se->cfs_rq;\n}"
  },
  {
    "function_name": "task_cfs_rq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "268-271",
    "snippet": "static inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn p->se.cfs_rq;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn p->se.cfs_rq;\n}"
  },
  {
    "function_name": "task_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "258-262",
    "snippet": "static inline struct task_struct *task_of(struct sched_entity *se)\n{\n\tSCHED_WARN_ON(!entity_is_task(se));\n\treturn container_of(se, struct task_struct, se);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);",
      "static void set_next_buddy(struct sched_entity *se);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "se",
            "structtask_struct",
            "se"
          ],
          "line": 261
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "SCHED_WARN_ON",
          "args": [
            "!entity_is_task(se)"
          ],
          "line": 260
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "entity_is_task",
          "args": [
            "se"
          ],
          "line": 260
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic int\nwakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);\nstatic void set_next_buddy(struct sched_entity *se);\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\tSCHED_WARN_ON(!entity_is_task(se));\n\treturn container_of(se, struct task_struct, se);\n}"
  },
  {
    "function_name": "rq_of",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "253-256",
    "snippet": "static inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->rq;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
      "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
      "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
    ],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->rq;\n}"
  },
  {
    "function_name": "__calc_delta",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "218-241",
    "snippet": "static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define WMULT_SHIFT\t32"
    ],
    "globals_used": [
      "static __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "mul_u64_u32_shr",
          "args": [
            "delta_exec",
            "fact",
            "shift"
          ],
          "line": 240
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "fact >> 32"
          ],
          "line": 225
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__update_inv_weight",
          "args": [
            "lw"
          ],
          "line": 223
        },
        "resolved": true,
        "details": {
          "function_name": "__update_inv_weight",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "189-204",
          "snippet": "static void __update_inv_weight(struct load_weight *lw)\n{\n\tunsigned long w;\n\n\tif (likely(lw->inv_weight))\n\t\treturn;\n\n\tw = scale_load_down(lw->weight);\n\n\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\tlw->inv_weight = 1;\n\telse if (unlikely(!w))\n\t\tlw->inv_weight = WMULT_CONST;\n\telse\n\t\tlw->inv_weight = WMULT_CONST / w;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [
            "#define WMULT_CONST\t(~0U)"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define WMULT_CONST\t(~0U)\n\nstatic void __update_inv_weight(struct load_weight *lw)\n{\n\tunsigned long w;\n\n\tif (likely(lw->inv_weight))\n\t\treturn;\n\n\tw = scale_load_down(lw->weight);\n\n\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\tlw->inv_weight = 1;\n\telse if (unlikely(!w))\n\t\tlw->inv_weight = WMULT_CONST;\n\telse\n\t\tlw->inv_weight = WMULT_CONST / w;\n}"
        }
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "weight"
          ],
          "line": 220
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define WMULT_SHIFT\t32\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\n\nstatic u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tint shift = WMULT_SHIFT;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact >> 32)) {\n\t\twhile (fact >> 32) {\n\t\t\tfact >>= 1;\n\t\t\tshift--;\n\t\t}\n\t}\n\n\t/* hint to use a 32x32->64 mul */\n\tfact = (u64)(u32)fact * lw->inv_weight;\n\n\twhile (fact >> 32) {\n\t\tfact >>= 1;\n\t\tshift--;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}"
  },
  {
    "function_name": "__update_inv_weight",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "189-204",
    "snippet": "static void __update_inv_weight(struct load_weight *lw)\n{\n\tunsigned long w;\n\n\tif (likely(lw->inv_weight))\n\t\treturn;\n\n\tw = scale_load_down(lw->weight);\n\n\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\tlw->inv_weight = 1;\n\telse if (unlikely(!w))\n\t\tlw->inv_weight = WMULT_CONST;\n\telse\n\t\tlw->inv_weight = WMULT_CONST / w;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define WMULT_CONST\t(~0U)"
    ],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "!w"
          ],
          "line": 200
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "w >= WMULT_CONST"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "scale_load_down",
          "args": [
            "lw->weight"
          ],
          "line": 196
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "lw->inv_weight"
          ],
          "line": 193
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\n#define WMULT_CONST\t(~0U)\n\nstatic void __update_inv_weight(struct load_weight *lw)\n{\n\tunsigned long w;\n\n\tif (likely(lw->inv_weight))\n\t\treturn;\n\n\tw = scale_load_down(lw->weight);\n\n\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\tlw->inv_weight = 1;\n\telse if (unlikely(!w))\n\t\tlw->inv_weight = WMULT_CONST;\n\telse\n\t\tlw->inv_weight = WMULT_CONST / w;\n}"
  },
  {
    "function_name": "sched_init_granularity",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "181-184",
    "snippet": "void sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "update_sysctl",
          "args": [],
          "line": 183
        },
        "resolved": true,
        "details": {
          "function_name": "update_sysctl",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "169-179",
          "snippet": "static void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nvoid sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}"
  },
  {
    "function_name": "update_sysctl",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "169-179",
    "snippet": "static void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "SET_SYSCTL",
          "args": [
            "sched_wakeup_granularity"
          ],
          "line": 177
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "SET_SYSCTL",
          "args": [
            "sched_latency"
          ],
          "line": 176
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "SET_SYSCTL",
          "args": [
            "sched_min_granularity"
          ],
          "line": 175
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_update_sysctl_factor",
          "args": [],
          "line": 171
        },
        "resolved": true,
        "details": {
          "function_name": "get_update_sysctl_factor",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "148-167",
          "snippet": "static unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "enum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nenum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;\n\nstatic unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n#undef SET_SYSCTL\n}"
  },
  {
    "function_name": "get_update_sysctl_factor",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "148-167",
    "snippet": "static unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "enum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "ilog2",
          "args": [
            "cpus"
          ],
          "line": 162
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "min_t",
          "args": [
            "unsignedint",
            "num_online_cpus()",
            "8"
          ],
          "line": 150
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "num_online_cpus",
          "args": [],
          "line": 150
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nenum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;\n\nstatic unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}"
  },
  {
    "function_name": "update_load_set",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "133-137",
    "snippet": "static inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}"
  },
  {
    "function_name": "update_load_sub",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "127-131",
    "snippet": "static inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}"
  },
  {
    "function_name": "update_load_add",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "121-125",
    "snippet": "static inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}"
  },
  {
    "function_name": "arch_asym_cpu_priority",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
    "lines": "93-96",
    "snippet": "int __weak arch_asym_cpu_priority(int cpu)\n{\n\treturn -cpu;\n}",
    "includes": [
      "#include \"sched-pelt.h\"",
      "#include \"pelt.h\"",
      "#include <trace/events/sched.h>",
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nint __weak arch_asym_cpu_priority(int cpu)\n{\n\treturn -cpu;\n}"
  }
]