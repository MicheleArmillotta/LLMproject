[
  {
    "function_name": "cgroup_no_v1",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "1283-1307",
    "snippet": "static int __init cgroup_no_v1(char *str)\n{\n\tstruct cgroup_subsys *ss;\n\tchar *token;\n\tint i;\n\n\twhile ((token = strsep(&str, \",\")) != NULL) {\n\t\tif (!*token)\n\t\t\tcontinue;\n\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\tcgroup_no_v1_mask = U16_MAX;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->name) &&\n\t\t\t    strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\n\t\t\tcgroup_no_v1_mask |= 1 << i;\n\t\t}\n\t}\n\treturn 1;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static u16 cgroup_no_v1_mask;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "ss->legacy_name"
          ],
          "line": 1300
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "ss->name"
          ],
          "line": 1299
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_subsys",
          "args": [
            "ss",
            "i"
          ],
          "line": 1298
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "\"all\""
          ],
          "line": 1293
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strsep",
          "args": [
            "&str",
            "\",\""
          ],
          "line": 1289
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic u16 cgroup_no_v1_mask;\n\nstatic int __init cgroup_no_v1(char *str)\n{\n\tstruct cgroup_subsys *ss;\n\tchar *token;\n\tint i;\n\n\twhile ((token = strsep(&str, \",\")) != NULL) {\n\t\tif (!*token)\n\t\t\tcontinue;\n\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\tcgroup_no_v1_mask = U16_MAX;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->name) &&\n\t\t\t    strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\n\t\t\tcgroup_no_v1_mask |= 1 << i;\n\t\t}\n\t}\n\treturn 1;\n}"
  },
  {
    "function_name": "cgroup1_wq_init",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "1270-1280",
    "snippet": "static int __init cgroup1_wq_init(void)\n{\n\t/*\n\t * Used to destroy pidlists and separate to serve as flush domain.\n\t * Cap @max_active to 1 too.\n\t */\n\tcgroup_pidlist_destroy_wq = alloc_workqueue(\"cgroup_pidlist_destroy\",\n\t\t\t\t\t\t    0, 1);\n\tBUG_ON(!cgroup_pidlist_destroy_wq);\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static struct workqueue_struct *cgroup_pidlist_destroy_wq;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "!cgroup_pidlist_destroy_wq"
          ],
          "line": 1278
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "alloc_workqueue",
          "args": [
            "\"cgroup_pidlist_destroy\"",
            "0",
            "1"
          ],
          "line": 1276
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic struct workqueue_struct *cgroup_pidlist_destroy_wq;\n\nstatic int __init cgroup1_wq_init(void)\n{\n\t/*\n\t * Used to destroy pidlists and separate to serve as flush domain.\n\t * Cap @max_active to 1 too.\n\t */\n\tcgroup_pidlist_destroy_wq = alloc_workqueue(\"cgroup_pidlist_destroy\",\n\t\t\t\t\t\t    0, 1);\n\tBUG_ON(!cgroup_pidlist_destroy_wq);\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup1_mount",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "1108-1268",
    "snippet": "struct dentry *cgroup1_mount(struct file_system_type *fs_type, int flags,\n\t\t\t     void *data, unsigned long magic,\n\t\t\t     struct cgroup_namespace *ns)\n{\n\tstruct super_block *pinned_sb = NULL;\n\tstruct cgroup_sb_opts opts;\n\tstruct cgroup_root *root;\n\tstruct cgroup_subsys *ss;\n\tstruct dentry *dentry;\n\tint i, ret;\n\tbool new_root = false;\n\n\tcgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);\n\n\t/* First find the desired set of subsystems */\n\tret = parse_cgroupfs_options(data, &opts);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Destruction of cgroup root is asynchronous, so subsystems may\n\t * still be dying after the previous unmount.  Let's drain the\n\t * dying subsystems.  We just need to ensure that the ones\n\t * unmounted previously finish dying and don't care about new ones\n\t * starting.  Testing ref liveliness is good enough.\n\t */\n\tfor_each_subsys(ss, i) {\n\t\tif (!(opts.subsys_mask & (1 << i)) ||\n\t\t    ss->root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\tif (!percpu_ref_tryget_live(&ss->root->cgrp.self.refcnt)) {\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tmsleep(10);\n\t\t\tret = restart_syscall();\n\t\t\tgoto out_free;\n\t\t}\n\t\tcgroup_put(&ss->root->cgrp);\n\t}\n\n\tfor_each_root(root) {\n\t\tbool name_match = false;\n\n\t\tif (root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If we asked for a name then it must match.  Also, if\n\t\t * name matches but sybsys_mask doesn't, we should fail.\n\t\t * Remember whether name matched.\n\t\t */\n\t\tif (opts.name) {\n\t\t\tif (strcmp(opts.name, root->name))\n\t\t\t\tcontinue;\n\t\t\tname_match = true;\n\t\t}\n\n\t\t/*\n\t\t * If we asked for subsystems (or explicitly for no\n\t\t * subsystems) then they must match.\n\t\t */\n\t\tif ((opts.subsys_mask || opts.none) &&\n\t\t    (opts.subsys_mask != root->subsys_mask)) {\n\t\t\tif (!name_match)\n\t\t\t\tcontinue;\n\t\t\tret = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (root->flags ^ opts.flags)\n\t\t\tpr_warn(\"new mount options do not match the existing superblock, will be ignored\\n\");\n\n\t\t/*\n\t\t * We want to reuse @root whose lifetime is governed by its\n\t\t * ->cgrp.  Let's check whether @root is alive and keep it\n\t\t * that way.  As cgroup_kill_sb() can happen anytime, we\n\t\t * want to block it by pinning the sb so that @root doesn't\n\t\t * get killed before mount is complete.\n\t\t *\n\t\t * With the sb pinned, tryget_live can reliably indicate\n\t\t * whether @root can be reused.  If it's being killed,\n\t\t * drain it.  We can use wait_queue for the wait but this\n\t\t * path is super cold.  Let's just sleep a bit and retry.\n\t\t */\n\t\tpinned_sb = kernfs_pin_sb(root->kf_root, NULL);\n\t\tif (IS_ERR(pinned_sb) ||\n\t\t    !percpu_ref_tryget_live(&root->cgrp.self.refcnt)) {\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tif (!IS_ERR_OR_NULL(pinned_sb))\n\t\t\t\tdeactivate_super(pinned_sb);\n\t\t\tmsleep(10);\n\t\t\tret = restart_syscall();\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * No such thing, create a new one.  name= matching without subsys\n\t * specification is allowed for already existing hierarchies but we\n\t * can't create new one without subsys specification.\n\t */\n\tif (!opts.subsys_mask && !opts.none) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/* Hierarchies may only be created in the initial cgroup namespace. */\n\tif (ns != &init_cgroup_ns) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\troot = kzalloc(sizeof(*root), GFP_KERNEL);\n\tif (!root) {\n\t\tret = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\tnew_root = true;\n\n\tinit_cgroup_root(root, &opts);\n\n\tret = cgroup_setup_root(root, opts.subsys_mask, PERCPU_REF_INIT_DEAD);\n\tif (ret)\n\t\tcgroup_free_root(root);\n\nout_unlock:\n\tmutex_unlock(&cgroup_mutex);\nout_free:\n\tkfree(opts.release_agent);\n\tkfree(opts.name);\n\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdentry = cgroup_do_mount(&cgroup_fs_type, flags, root,\n\t\t\t\t CGROUP_SUPER_MAGIC, ns);\n\n\t/*\n\t * There's a race window after we release cgroup_mutex and before\n\t * allocating a superblock. Make sure a concurrent process won't\n\t * be able to re-use the root during this window by delaying the\n\t * initialization of root refcnt.\n\t */\n\tif (new_root) {\n\t\tmutex_lock(&cgroup_mutex);\n\t\tpercpu_ref_reinit(&root->cgrp.self.refcnt);\n\t\tmutex_unlock(&cgroup_mutex);\n\t}\n\n\t/*\n\t * If @pinned_sb, we're reusing an existing root and holding an\n\t * extra ref on its sb.  Mount is complete.  Put the extra ref.\n\t */\n\tif (pinned_sb)\n\t\tdeactivate_super(pinned_sb);\n\n\treturn dentry;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "deactivate_super",
          "args": [
            "pinned_sb"
          ],
          "line": 1265
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 1257
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "percpu_ref_reinit",
          "args": [
            "&root->cgrp.self.refcnt"
          ],
          "line": 1256
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 1255
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_do_mount",
          "args": [
            "&cgroup_fs_type",
            "flags",
            "root",
            "CGROUP_SUPER_MAGIC",
            "ns"
          ],
          "line": 1245
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_do_mount",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1997-2031",
          "snippet": "struct dentry *cgroup_do_mount(struct file_system_type *fs_type, int flags,\n\t\t\t       struct cgroup_root *root, unsigned long magic,\n\t\t\t       struct cgroup_namespace *ns)\n{\n\tstruct dentry *dentry;\n\tbool new_sb;\n\n\tdentry = kernfs_mount(fs_type, flags, root->kf_root, magic, &new_sb);\n\n\t/*\n\t * In non-init cgroup namespace, instead of root cgroup's dentry,\n\t * we return the dentry corresponding to the cgroupns->root_cgrp.\n\t */\n\tif (!IS_ERR(dentry) && ns != &init_cgroup_ns) {\n\t\tstruct dentry *nsdentry;\n\t\tstruct cgroup *cgrp;\n\n\t\tmutex_lock(&cgroup_mutex);\n\t\tspin_lock_irq(&css_set_lock);\n\n\t\tcgrp = cset_cgroup_from_root(ns->root_cset, root);\n\n\t\tspin_unlock_irq(&css_set_lock);\n\t\tmutex_unlock(&cgroup_mutex);\n\n\t\tnsdentry = kernfs_node_dentry(cgrp->kn, dentry->d_sb);\n\t\tdput(dentry);\n\t\tdentry = nsdentry;\n\t}\n\n\tif (IS_ERR(dentry) || !new_sb)\n\t\tcgroup_put(&root->cgrp);\n\n\treturn dentry;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "struct cgroup_namespace init_cgroup_ns = {\n\t.count\t\t= REFCOUNT_INIT(2),\n\t.user_ns\t= &init_user_ns,\n\t.ns.ops\t\t= &cgroupns_operations,\n\t.ns.inum\t= PROC_CGROUP_INIT_INO,\n\t.root_cset\t= &init_css_set,\n};",
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstruct cgroup_namespace init_cgroup_ns = {\n\t.count\t\t= REFCOUNT_INIT(2),\n\t.user_ns\t= &init_user_ns,\n\t.ns.ops\t\t= &cgroupns_operations,\n\t.ns.inum\t= PROC_CGROUP_INIT_INO,\n\t.root_cset\t= &init_css_set,\n};\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nstruct dentry *cgroup_do_mount(struct file_system_type *fs_type, int flags,\n\t\t\t       struct cgroup_root *root, unsigned long magic,\n\t\t\t       struct cgroup_namespace *ns)\n{\n\tstruct dentry *dentry;\n\tbool new_sb;\n\n\tdentry = kernfs_mount(fs_type, flags, root->kf_root, magic, &new_sb);\n\n\t/*\n\t * In non-init cgroup namespace, instead of root cgroup's dentry,\n\t * we return the dentry corresponding to the cgroupns->root_cgrp.\n\t */\n\tif (!IS_ERR(dentry) && ns != &init_cgroup_ns) {\n\t\tstruct dentry *nsdentry;\n\t\tstruct cgroup *cgrp;\n\n\t\tmutex_lock(&cgroup_mutex);\n\t\tspin_lock_irq(&css_set_lock);\n\n\t\tcgrp = cset_cgroup_from_root(ns->root_cset, root);\n\n\t\tspin_unlock_irq(&css_set_lock);\n\t\tmutex_unlock(&cgroup_mutex);\n\n\t\tnsdentry = kernfs_node_dentry(cgrp->kn, dentry->d_sb);\n\t\tdput(dentry);\n\t\tdentry = nsdentry;\n\t}\n\n\tif (IS_ERR(dentry) || !new_sb)\n\t\tcgroup_put(&root->cgrp);\n\n\treturn dentry;\n}"
        }
      },
      {
        "call_info": {
          "callee": "ERR_PTR",
          "args": [
            "ret"
          ],
          "line": 1243
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "opts.name"
          ],
          "line": 1240
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/params.c",
          "lines": "73-86",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_free_root",
          "args": [
            "root"
          ],
          "line": 1234
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_free_root",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1238-1244",
          "snippet": "void cgroup_free_root(struct cgroup_root *root)\n{\n\tif (root) {\n\t\tidr_destroy(&root->cgroup_idr);\n\t\tkfree(root);\n\t}\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nvoid cgroup_free_root(struct cgroup_root *root)\n{\n\tif (root) {\n\t\tidr_destroy(&root->cgroup_idr);\n\t\tkfree(root);\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_setup_root",
          "args": [
            "root",
            "opts.subsys_mask",
            "PERCPU_REF_INIT_DEAD"
          ],
          "line": 1232
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_setup_root",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1896-1995",
          "snippet": "int cgroup_setup_root(struct cgroup_root *root, u16 ss_mask, int ref_flags)\n{\n\tLIST_HEAD(tmp_links);\n\tstruct cgroup *root_cgrp = &root->cgrp;\n\tstruct kernfs_syscall_ops *kf_sops;\n\tstruct css_set *cset;\n\tint i, ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tret = cgroup_idr_alloc(&root->cgroup_idr, root_cgrp, 1, 2, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto out;\n\troot_cgrp->id = ret;\n\troot_cgrp->ancestor_ids[0] = ret;\n\n\tret = percpu_ref_init(&root_cgrp->self.refcnt, css_release,\n\t\t\t      ref_flags, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out;\n\n\t/*\n\t * We're accessing css_set_count without locking css_set_lock here,\n\t * but that's OK - it can only be increased by someone holding\n\t * cgroup_lock, and that's us.  Later rebinding may disable\n\t * controllers on the default hierarchy and thus create new csets,\n\t * which can't be more than the existing ones.  Allocate 2x.\n\t */\n\tret = allocate_cgrp_cset_links(2 * css_set_count, &tmp_links);\n\tif (ret)\n\t\tgoto cancel_ref;\n\n\tret = cgroup_init_root_id(root);\n\tif (ret)\n\t\tgoto cancel_ref;\n\n\tkf_sops = root == &cgrp_dfl_root ?\n\t\t&cgroup_kf_syscall_ops : &cgroup1_kf_syscall_ops;\n\n\troot->kf_root = kernfs_create_root(kf_sops,\n\t\t\t\t\t   KERNFS_ROOT_CREATE_DEACTIVATED |\n\t\t\t\t\t   KERNFS_ROOT_SUPPORT_EXPORTOP,\n\t\t\t\t\t   root_cgrp);\n\tif (IS_ERR(root->kf_root)) {\n\t\tret = PTR_ERR(root->kf_root);\n\t\tgoto exit_root_id;\n\t}\n\troot_cgrp->kn = root->kf_root->kn;\n\n\tret = css_populate_dir(&root_cgrp->self);\n\tif (ret)\n\t\tgoto destroy_root;\n\n\tret = rebind_subsystems(root, ss_mask);\n\tif (ret)\n\t\tgoto destroy_root;\n\n\tret = cgroup_bpf_inherit(root_cgrp);\n\tWARN_ON_ONCE(ret);\n\n\ttrace_cgroup_setup_root(root);\n\n\t/*\n\t * There must be no failure case after here, since rebinding takes\n\t * care of subsystems' refcounts, which are explicitly dropped in\n\t * the failure exit path.\n\t */\n\tlist_add(&root->root_list, &cgroup_roots);\n\tcgroup_root_count++;\n\n\t/*\n\t * Link the root cgroup in this hierarchy into all the css_set\n\t * objects.\n\t */\n\tspin_lock_irq(&css_set_lock);\n\thash_for_each(css_set_table, i, cset, hlist) {\n\t\tlink_css_set(&tmp_links, cset, root_cgrp);\n\t\tif (css_set_populated(cset))\n\t\t\tcgroup_update_populated(root_cgrp, true);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tBUG_ON(!list_empty(&root_cgrp->self.children));\n\tBUG_ON(atomic_read(&root->nr_cgrps) != 1);\n\n\tkernfs_activate(root_cgrp->kn);\n\tret = 0;\n\tgoto out;\n\ndestroy_root:\n\tkernfs_destroy_root(root->kf_root);\n\troot->kf_root = NULL;\nexit_root_id:\n\tcgroup_exit_root_id(root);\ncancel_ref:\n\tpercpu_ref_exit(&root_cgrp->self.refcnt);\nout:\n\tfree_cgrp_cset_links(&tmp_links);\n\treturn ret;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "struct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };",
            "static int cgroup_root_count;",
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static void cgroup_finalize_control(struct cgroup *cgrp, int ret);",
            "static void css_task_iter_advance(struct css_task_iter *it);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstruct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };\nstatic int cgroup_root_count;\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret);\nstatic void css_task_iter_advance(struct css_task_iter *it);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nint cgroup_setup_root(struct cgroup_root *root, u16 ss_mask, int ref_flags)\n{\n\tLIST_HEAD(tmp_links);\n\tstruct cgroup *root_cgrp = &root->cgrp;\n\tstruct kernfs_syscall_ops *kf_sops;\n\tstruct css_set *cset;\n\tint i, ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tret = cgroup_idr_alloc(&root->cgroup_idr, root_cgrp, 1, 2, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto out;\n\troot_cgrp->id = ret;\n\troot_cgrp->ancestor_ids[0] = ret;\n\n\tret = percpu_ref_init(&root_cgrp->self.refcnt, css_release,\n\t\t\t      ref_flags, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out;\n\n\t/*\n\t * We're accessing css_set_count without locking css_set_lock here,\n\t * but that's OK - it can only be increased by someone holding\n\t * cgroup_lock, and that's us.  Later rebinding may disable\n\t * controllers on the default hierarchy and thus create new csets,\n\t * which can't be more than the existing ones.  Allocate 2x.\n\t */\n\tret = allocate_cgrp_cset_links(2 * css_set_count, &tmp_links);\n\tif (ret)\n\t\tgoto cancel_ref;\n\n\tret = cgroup_init_root_id(root);\n\tif (ret)\n\t\tgoto cancel_ref;\n\n\tkf_sops = root == &cgrp_dfl_root ?\n\t\t&cgroup_kf_syscall_ops : &cgroup1_kf_syscall_ops;\n\n\troot->kf_root = kernfs_create_root(kf_sops,\n\t\t\t\t\t   KERNFS_ROOT_CREATE_DEACTIVATED |\n\t\t\t\t\t   KERNFS_ROOT_SUPPORT_EXPORTOP,\n\t\t\t\t\t   root_cgrp);\n\tif (IS_ERR(root->kf_root)) {\n\t\tret = PTR_ERR(root->kf_root);\n\t\tgoto exit_root_id;\n\t}\n\troot_cgrp->kn = root->kf_root->kn;\n\n\tret = css_populate_dir(&root_cgrp->self);\n\tif (ret)\n\t\tgoto destroy_root;\n\n\tret = rebind_subsystems(root, ss_mask);\n\tif (ret)\n\t\tgoto destroy_root;\n\n\tret = cgroup_bpf_inherit(root_cgrp);\n\tWARN_ON_ONCE(ret);\n\n\ttrace_cgroup_setup_root(root);\n\n\t/*\n\t * There must be no failure case after here, since rebinding takes\n\t * care of subsystems' refcounts, which are explicitly dropped in\n\t * the failure exit path.\n\t */\n\tlist_add(&root->root_list, &cgroup_roots);\n\tcgroup_root_count++;\n\n\t/*\n\t * Link the root cgroup in this hierarchy into all the css_set\n\t * objects.\n\t */\n\tspin_lock_irq(&css_set_lock);\n\thash_for_each(css_set_table, i, cset, hlist) {\n\t\tlink_css_set(&tmp_links, cset, root_cgrp);\n\t\tif (css_set_populated(cset))\n\t\t\tcgroup_update_populated(root_cgrp, true);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tBUG_ON(!list_empty(&root_cgrp->self.children));\n\tBUG_ON(atomic_read(&root->nr_cgrps) != 1);\n\n\tkernfs_activate(root_cgrp->kn);\n\tret = 0;\n\tgoto out;\n\ndestroy_root:\n\tkernfs_destroy_root(root->kf_root);\n\troot->kf_root = NULL;\nexit_root_id:\n\tcgroup_exit_root_id(root);\ncancel_ref:\n\tpercpu_ref_exit(&root_cgrp->self.refcnt);\nout:\n\tfree_cgrp_cset_links(&tmp_links);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "init_cgroup_root",
          "args": [
            "root",
            "&opts"
          ],
          "line": 1230
        },
        "resolved": true,
        "details": {
          "function_name": "init_cgroup_root",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1877-1894",
          "snippet": "void init_cgroup_root(struct cgroup_root *root, struct cgroup_sb_opts *opts)\n{\n\tstruct cgroup *cgrp = &root->cgrp;\n\n\tINIT_LIST_HEAD(&root->root_list);\n\tatomic_set(&root->nr_cgrps, 1);\n\tcgrp->root = root;\n\tinit_cgroup_housekeeping(cgrp);\n\tidr_init(&root->cgroup_idr);\n\n\troot->flags = opts->flags;\n\tif (opts->release_agent)\n\t\tstrscpy(root->release_agent_path, opts->release_agent, PATH_MAX);\n\tif (opts->name)\n\t\tstrscpy(root->name, opts->name, MAX_CGROUP_ROOT_NAMELEN);\n\tif (opts->cpuset_clone_children)\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &root->cgrp.flags);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nvoid init_cgroup_root(struct cgroup_root *root, struct cgroup_sb_opts *opts)\n{\n\tstruct cgroup *cgrp = &root->cgrp;\n\n\tINIT_LIST_HEAD(&root->root_list);\n\tatomic_set(&root->nr_cgrps, 1);\n\tcgrp->root = root;\n\tinit_cgroup_housekeeping(cgrp);\n\tidr_init(&root->cgroup_idr);\n\n\troot->flags = opts->flags;\n\tif (opts->release_agent)\n\t\tstrscpy(root->release_agent_path, opts->release_agent, PATH_MAX);\n\tif (opts->name)\n\t\tstrscpy(root->name, opts->name, MAX_CGROUP_ROOT_NAMELEN);\n\tif (opts->cpuset_clone_children)\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &root->cgrp.flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "kzalloc",
          "args": [
            "sizeof(*root)",
            "GFP_KERNEL"
          ],
          "line": 1223
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "restart_syscall",
          "args": [],
          "line": 1199
        },
        "resolved": true,
        "details": {
          "function_name": "restart_syscall",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/signal.c",
          "lines": "2650-2654",
          "snippet": "SYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current->restart_block;\n\treturn restart->fn(restart);\n}",
          "includes": [
            "#include <linux/kdb.h>",
            "#include \"audit.h\"\t/* audit_signal_info() */",
            "#include <asm/cacheflush.h>",
            "#include <asm/siginfo.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/param.h>",
            "#include <trace/events/signal.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/compiler.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/compat.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/user_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/freezer.h>",
            "#include <linux/capability.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/signal.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/security.h>",
            "#include <linux/coredump.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/tty.h>",
            "#include <linux/fs.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/export.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/kdb.h>\n#include \"audit.h\"\t/* audit_signal_info() */\n#include <asm/cacheflush.h>\n#include <asm/siginfo.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <asm/param.h>\n#include <trace/events/signal.h>\n#include <linux/livepatch.h>\n#include <linux/posix-timers.h>\n#include <linux/compiler.h>\n#include <linux/cn_proc.h>\n#include <linux/compat.h>\n#include <linux/uprobes.h>\n#include <linux/user_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/pid_namespace.h>\n#include <linux/freezer.h>\n#include <linux/capability.h>\n#include <linux/tracehook.h>\n#include <linux/ratelimit.h>\n#include <linux/signalfd.h>\n#include <linux/signal.h>\n#include <linux/ptrace.h>\n#include <linux/syscalls.h>\n#include <linux/security.h>\n#include <linux/coredump.h>\n#include <linux/binfmts.h>\n#include <linux/tty.h>\n#include <linux/fs.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/user.h>\n#include <linux/sched/mm.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n\nSYSCALL_DEFINE0(restart_syscall)\n{\n\tstruct restart_block *restart = &current->restart_block;\n\treturn restart->fn(restart);\n}"
        }
      },
      {
        "call_info": {
          "callee": "msleep",
          "args": [
            "10"
          ],
          "line": 1198
        },
        "resolved": true,
        "details": {
          "function_name": "msleep_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/timer.c",
          "lines": "1965-1972",
          "snippet": "unsigned long msleep_interruptible(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout && !signal_pending(current))\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\treturn jiffies_to_msecs(timeout);\n}",
          "includes": [
            "#include <trace/events/timer.h>",
            "#include \"tick-internal.h\"",
            "#include <asm/io.h>",
            "#include <asm/timex.h>",
            "#include <asm/div64.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/tick.h>",
            "#include <linux/delay.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cpu.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/time.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/notifier.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm.h>",
            "#include <linux/init.h>",
            "#include <linux/percpu.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/export.h>",
            "#include <linux/kernel_stat.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/timer.h>\n#include \"tick-internal.h\"\n#include <asm/io.h>\n#include <asm/timex.h>\n#include <asm/div64.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/slab.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/irq_work.h>\n#include <linux/kallsyms.h>\n#include <linux/tick.h>\n#include <linux/delay.h>\n#include <linux/syscalls.h>\n#include <linux/cpu.h>\n#include <linux/posix-timers.h>\n#include <linux/jiffies.h>\n#include <linux/time.h>\n#include <linux/thread_info.h>\n#include <linux/notifier.h>\n#include <linux/pid_namespace.h>\n#include <linux/swap.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/percpu.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n#include <linux/kernel_stat.h>\n\nunsigned long msleep_interruptible(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout && !signal_pending(current))\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\treturn jiffies_to_msecs(timeout);\n}"
        }
      },
      {
        "call_info": {
          "callee": "deactivate_super",
          "args": [
            "pinned_sb"
          ],
          "line": 1197
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ERR_OR_NULL",
          "args": [
            "pinned_sb"
          ],
          "line": 1196
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "percpu_ref_tryget_live",
          "args": [
            "&root->cgrp.self.refcnt"
          ],
          "line": 1194
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ERR",
          "args": [
            "pinned_sb"
          ],
          "line": 1193
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kernfs_pin_sb",
          "args": [
            "root->kf_root",
            "NULL"
          ],
          "line": 1192
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"new mount options do not match the existing superblock, will be ignored\\n\""
          ],
          "line": 1178
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "opts.name",
            "root->name"
          ],
          "line": 1160
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_put",
          "args": [
            "&ss->root->cgrp"
          ],
          "line": 1145
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "percpu_ref_tryget_live",
          "args": [
            "&ss->root->cgrp.self.refcnt"
          ],
          "line": 1139
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_subsys",
          "args": [
            "ss",
            "i"
          ],
          "line": 1134
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "parse_cgroupfs_options",
          "args": [
            "data",
            "&opts"
          ],
          "line": 1123
        },
        "resolved": true,
        "details": {
          "function_name": "parse_cgroupfs_options",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "906-1040",
          "snippet": "static int parse_cgroupfs_options(char *data, struct cgroup_sb_opts *opts)\n{\n\tchar *token, *o = data;\n\tbool all_ss = false, one_ss = false;\n\tu16 mask = U16_MAX;\n\tstruct cgroup_subsys *ss;\n\tint nr_opts = 0;\n\tint i;\n\n#ifdef CONFIG_CPUSETS\n\tmask = ~((u16)1 << cpuset_cgrp_id);\n#endif\n\n\tmemset(opts, 0, sizeof(*opts));\n\n\twhile ((token = strsep(&o, \",\")) != NULL) {\n\t\tnr_opts++;\n\n\t\tif (!*token)\n\t\t\treturn -EINVAL;\n\t\tif (!strcmp(token, \"none\")) {\n\t\t\t/* Explicitly have no subsystems */\n\t\t\topts->none = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (one_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\tall_ss = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"noprefix\")) {\n\t\t\topts->flags |= CGRP_ROOT_NOPREFIX;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"clone_children\")) {\n\t\t\topts->cpuset_clone_children = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"cpuset_v2_mode\")) {\n\t\t\topts->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"xattr\")) {\n\t\t\topts->flags |= CGRP_ROOT_XATTR;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"release_agent=\", 14)) {\n\t\t\t/* Specifying two release agents is forbidden */\n\t\t\tif (opts->release_agent)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->release_agent =\n\t\t\t\tkstrndup(token + 14, PATH_MAX - 1, GFP_KERNEL);\n\t\t\tif (!opts->release_agent)\n\t\t\t\treturn -ENOMEM;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"name=\", 5)) {\n\t\t\tconst char *name = token + 5;\n\t\t\t/* Can't specify an empty name */\n\t\t\tif (!strlen(name))\n\t\t\t\treturn -EINVAL;\n\t\t\t/* Must match [\\w.-]+ */\n\t\t\tfor (i = 0; i < strlen(name); i++) {\n\t\t\t\tchar c = name[i];\n\t\t\t\tif (isalnum(c))\n\t\t\t\t\tcontinue;\n\t\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\t\tcontinue;\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* Specifying two names is forbidden */\n\t\t\tif (opts->name)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->name = kstrndup(name,\n\t\t\t\t\t      MAX_CGROUP_ROOT_NAMELEN - 1,\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!opts->name)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i))\n\t\t\t\tcontinue;\n\t\t\tif (cgroup1_ssid_disabled(i))\n\t\t\t\tcontinue;\n\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (all_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->subsys_mask |= (1 << i);\n\t\t\tone_ss = true;\n\n\t\t\tbreak;\n\t\t}\n\t\tif (i == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * If the 'all' option was specified select all the subsystems,\n\t * otherwise if 'none', 'name=' and a subsystem name options were\n\t * not specified, let's default to 'all'\n\t */\n\tif (all_ss || (!one_ss && !opts->none && !opts->name))\n\t\tfor_each_subsys(ss, i)\n\t\t\tif (cgroup_ssid_enabled(i) && !cgroup1_ssid_disabled(i))\n\t\t\t\topts->subsys_mask |= (1 << i);\n\n\t/*\n\t * We either have to specify by name or by subsystems. (So all\n\t * empty hierarchies must have a name).\n\t */\n\tif (!opts->subsys_mask && !opts->name)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Option noprefix was introduced just for backward compatibility\n\t * with the old cpuset, so we allow noprefix only if mounting just\n\t * the cpuset subsystem.\n\t */\n\tif ((opts->flags & CGRP_ROOT_NOPREFIX) && (opts->subsys_mask & mask))\n\t\treturn -EINVAL;\n\n\t/* Can't specify \"none\" and some subsystems */\n\tif (opts->subsys_mask && opts->none)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int parse_cgroupfs_options(char *data, struct cgroup_sb_opts *opts)\n{\n\tchar *token, *o = data;\n\tbool all_ss = false, one_ss = false;\n\tu16 mask = U16_MAX;\n\tstruct cgroup_subsys *ss;\n\tint nr_opts = 0;\n\tint i;\n\n#ifdef CONFIG_CPUSETS\n\tmask = ~((u16)1 << cpuset_cgrp_id);\n#endif\n\n\tmemset(opts, 0, sizeof(*opts));\n\n\twhile ((token = strsep(&o, \",\")) != NULL) {\n\t\tnr_opts++;\n\n\t\tif (!*token)\n\t\t\treturn -EINVAL;\n\t\tif (!strcmp(token, \"none\")) {\n\t\t\t/* Explicitly have no subsystems */\n\t\t\topts->none = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (one_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\tall_ss = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"noprefix\")) {\n\t\t\topts->flags |= CGRP_ROOT_NOPREFIX;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"clone_children\")) {\n\t\t\topts->cpuset_clone_children = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"cpuset_v2_mode\")) {\n\t\t\topts->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"xattr\")) {\n\t\t\topts->flags |= CGRP_ROOT_XATTR;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"release_agent=\", 14)) {\n\t\t\t/* Specifying two release agents is forbidden */\n\t\t\tif (opts->release_agent)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->release_agent =\n\t\t\t\tkstrndup(token + 14, PATH_MAX - 1, GFP_KERNEL);\n\t\t\tif (!opts->release_agent)\n\t\t\t\treturn -ENOMEM;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"name=\", 5)) {\n\t\t\tconst char *name = token + 5;\n\t\t\t/* Can't specify an empty name */\n\t\t\tif (!strlen(name))\n\t\t\t\treturn -EINVAL;\n\t\t\t/* Must match [\\w.-]+ */\n\t\t\tfor (i = 0; i < strlen(name); i++) {\n\t\t\t\tchar c = name[i];\n\t\t\t\tif (isalnum(c))\n\t\t\t\t\tcontinue;\n\t\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\t\tcontinue;\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* Specifying two names is forbidden */\n\t\t\tif (opts->name)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->name = kstrndup(name,\n\t\t\t\t\t      MAX_CGROUP_ROOT_NAMELEN - 1,\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!opts->name)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i))\n\t\t\t\tcontinue;\n\t\t\tif (cgroup1_ssid_disabled(i))\n\t\t\t\tcontinue;\n\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (all_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->subsys_mask |= (1 << i);\n\t\t\tone_ss = true;\n\n\t\t\tbreak;\n\t\t}\n\t\tif (i == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * If the 'all' option was specified select all the subsystems,\n\t * otherwise if 'none', 'name=' and a subsystem name options were\n\t * not specified, let's default to 'all'\n\t */\n\tif (all_ss || (!one_ss && !opts->none && !opts->name))\n\t\tfor_each_subsys(ss, i)\n\t\t\tif (cgroup_ssid_enabled(i) && !cgroup1_ssid_disabled(i))\n\t\t\t\topts->subsys_mask |= (1 << i);\n\n\t/*\n\t * We either have to specify by name or by subsystems. (So all\n\t * empty hierarchies must have a name).\n\t */\n\tif (!opts->subsys_mask && !opts->name)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Option noprefix was introduced just for backward compatibility\n\t * with the old cpuset, so we allow noprefix only if mounting just\n\t * the cpuset subsystem.\n\t */\n\tif ((opts->flags & CGRP_ROOT_NOPREFIX) && (opts->subsys_mask & mask))\n\t\treturn -EINVAL;\n\n\t/* Can't specify \"none\" and some subsystems */\n\tif (opts->subsys_mask && opts->none)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_lock_and_drain_offline",
          "args": [
            "&cgrp_dfl_root.cgrp"
          ],
          "line": 1120
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstruct dentry *cgroup1_mount(struct file_system_type *fs_type, int flags,\n\t\t\t     void *data, unsigned long magic,\n\t\t\t     struct cgroup_namespace *ns)\n{\n\tstruct super_block *pinned_sb = NULL;\n\tstruct cgroup_sb_opts opts;\n\tstruct cgroup_root *root;\n\tstruct cgroup_subsys *ss;\n\tstruct dentry *dentry;\n\tint i, ret;\n\tbool new_root = false;\n\n\tcgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);\n\n\t/* First find the desired set of subsystems */\n\tret = parse_cgroupfs_options(data, &opts);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Destruction of cgroup root is asynchronous, so subsystems may\n\t * still be dying after the previous unmount.  Let's drain the\n\t * dying subsystems.  We just need to ensure that the ones\n\t * unmounted previously finish dying and don't care about new ones\n\t * starting.  Testing ref liveliness is good enough.\n\t */\n\tfor_each_subsys(ss, i) {\n\t\tif (!(opts.subsys_mask & (1 << i)) ||\n\t\t    ss->root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\tif (!percpu_ref_tryget_live(&ss->root->cgrp.self.refcnt)) {\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tmsleep(10);\n\t\t\tret = restart_syscall();\n\t\t\tgoto out_free;\n\t\t}\n\t\tcgroup_put(&ss->root->cgrp);\n\t}\n\n\tfor_each_root(root) {\n\t\tbool name_match = false;\n\n\t\tif (root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If we asked for a name then it must match.  Also, if\n\t\t * name matches but sybsys_mask doesn't, we should fail.\n\t\t * Remember whether name matched.\n\t\t */\n\t\tif (opts.name) {\n\t\t\tif (strcmp(opts.name, root->name))\n\t\t\t\tcontinue;\n\t\t\tname_match = true;\n\t\t}\n\n\t\t/*\n\t\t * If we asked for subsystems (or explicitly for no\n\t\t * subsystems) then they must match.\n\t\t */\n\t\tif ((opts.subsys_mask || opts.none) &&\n\t\t    (opts.subsys_mask != root->subsys_mask)) {\n\t\t\tif (!name_match)\n\t\t\t\tcontinue;\n\t\t\tret = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (root->flags ^ opts.flags)\n\t\t\tpr_warn(\"new mount options do not match the existing superblock, will be ignored\\n\");\n\n\t\t/*\n\t\t * We want to reuse @root whose lifetime is governed by its\n\t\t * ->cgrp.  Let's check whether @root is alive and keep it\n\t\t * that way.  As cgroup_kill_sb() can happen anytime, we\n\t\t * want to block it by pinning the sb so that @root doesn't\n\t\t * get killed before mount is complete.\n\t\t *\n\t\t * With the sb pinned, tryget_live can reliably indicate\n\t\t * whether @root can be reused.  If it's being killed,\n\t\t * drain it.  We can use wait_queue for the wait but this\n\t\t * path is super cold.  Let's just sleep a bit and retry.\n\t\t */\n\t\tpinned_sb = kernfs_pin_sb(root->kf_root, NULL);\n\t\tif (IS_ERR(pinned_sb) ||\n\t\t    !percpu_ref_tryget_live(&root->cgrp.self.refcnt)) {\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tif (!IS_ERR_OR_NULL(pinned_sb))\n\t\t\t\tdeactivate_super(pinned_sb);\n\t\t\tmsleep(10);\n\t\t\tret = restart_syscall();\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * No such thing, create a new one.  name= matching without subsys\n\t * specification is allowed for already existing hierarchies but we\n\t * can't create new one without subsys specification.\n\t */\n\tif (!opts.subsys_mask && !opts.none) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/* Hierarchies may only be created in the initial cgroup namespace. */\n\tif (ns != &init_cgroup_ns) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\troot = kzalloc(sizeof(*root), GFP_KERNEL);\n\tif (!root) {\n\t\tret = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\tnew_root = true;\n\n\tinit_cgroup_root(root, &opts);\n\n\tret = cgroup_setup_root(root, opts.subsys_mask, PERCPU_REF_INIT_DEAD);\n\tif (ret)\n\t\tcgroup_free_root(root);\n\nout_unlock:\n\tmutex_unlock(&cgroup_mutex);\nout_free:\n\tkfree(opts.release_agent);\n\tkfree(opts.name);\n\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdentry = cgroup_do_mount(&cgroup_fs_type, flags, root,\n\t\t\t\t CGROUP_SUPER_MAGIC, ns);\n\n\t/*\n\t * There's a race window after we release cgroup_mutex and before\n\t * allocating a superblock. Make sure a concurrent process won't\n\t * be able to re-use the root during this window by delaying the\n\t * initialization of root refcnt.\n\t */\n\tif (new_root) {\n\t\tmutex_lock(&cgroup_mutex);\n\t\tpercpu_ref_reinit(&root->cgrp.self.refcnt);\n\t\tmutex_unlock(&cgroup_mutex);\n\t}\n\n\t/*\n\t * If @pinned_sb, we're reusing an existing root and holding an\n\t * extra ref on its sb.  Mount is complete.  Put the extra ref.\n\t */\n\tif (pinned_sb)\n\t\tdeactivate_super(pinned_sb);\n\n\treturn dentry;\n}"
  },
  {
    "function_name": "cgroup1_remount",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "1042-1097",
    "snippet": "static int cgroup1_remount(struct kernfs_root *kf_root, int *flags, char *data)\n{\n\tint ret = 0;\n\tstruct cgroup_root *root = cgroup_root_from_kf(kf_root);\n\tstruct cgroup_sb_opts opts;\n\tu16 added_mask, removed_mask;\n\n\tcgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);\n\n\t/* See what subsystems are wanted */\n\tret = parse_cgroupfs_options(data, &opts);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (opts.subsys_mask != root->subsys_mask || opts.release_agent)\n\t\tpr_warn(\"option changes via remount are deprecated (pid=%d comm=%s)\\n\",\n\t\t\ttask_tgid_nr(current), current->comm);\n\n\tadded_mask = opts.subsys_mask & ~root->subsys_mask;\n\tremoved_mask = root->subsys_mask & ~opts.subsys_mask;\n\n\t/* Don't allow flags or name to change at remount */\n\tif ((opts.flags ^ root->flags) ||\n\t    (opts.name && strcmp(opts.name, root->name))) {\n\t\tpr_err(\"option or name mismatch, new: 0x%x \\\"%s\\\", old: 0x%x \\\"%s\\\"\\n\",\n\t\t       opts.flags, opts.name ?: \"\", root->flags, root->name);\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/* remounting is not allowed for populated hierarchies */\n\tif (!list_empty(&root->cgrp.self.children)) {\n\t\tret = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tret = rebind_subsystems(root, added_mask);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tWARN_ON(rebind_subsystems(&cgrp_dfl_root, removed_mask));\n\n\tif (opts.release_agent) {\n\t\tspin_lock(&release_agent_path_lock);\n\t\tstrcpy(root->release_agent_path, opts.release_agent);\n\t\tspin_unlock(&release_agent_path_lock);\n\t}\n\n\ttrace_cgroup_remount(root);\n\n out_unlock:\n\tkfree(opts.release_agent);\n\tkfree(opts.name);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static DEFINE_SPINLOCK(release_agent_path_lock);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 1095
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "opts.name"
          ],
          "line": 1094
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/params.c",
          "lines": "73-86",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "trace_cgroup_remount",
          "args": [
            "root"
          ],
          "line": 1090
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 1087
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_queued_spin_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "546-560",
          "snippet": "__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}"
        }
      },
      {
        "call_info": {
          "callee": "strcpy",
          "args": [
            "root->release_agent_path",
            "opts.release_agent"
          ],
          "line": 1086
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 1085
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "rebind_subsystems(&cgrp_dfl_root, removed_mask)"
          ],
          "line": 1082
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rebind_subsystems",
          "args": [
            "&cgrp_dfl_root",
            "removed_mask"
          ],
          "line": 1082
        },
        "resolved": true,
        "details": {
          "function_name": "rebind_subsystems",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1644-1712",
          "snippet": "int rebind_subsystems(struct cgroup_root *dst_root, u16 ss_mask)\n{\n\tstruct cgroup *dcgrp = &dst_root->cgrp;\n\tstruct cgroup_subsys *ss;\n\tint ssid, i, ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\t/*\n\t\t * If @ss has non-root csses attached to it, can't move.\n\t\t * If @ss is an implicit controller, it is exempt from this\n\t\t * rule and can be stolen.\n\t\t */\n\t\tif (css_next_child(NULL, cgroup_css(&ss->root->cgrp, ss)) &&\n\t\t    !ss->implicit_on_dfl)\n\t\t\treturn -EBUSY;\n\n\t\t/* can't move between two non-dummy roots either */\n\t\tif (ss->root != &cgrp_dfl_root && dst_root != &cgrp_dfl_root)\n\t\t\treturn -EBUSY;\n\t} while_each_subsys_mask();\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tstruct cgroup_root *src_root = ss->root;\n\t\tstruct cgroup *scgrp = &src_root->cgrp;\n\t\tstruct cgroup_subsys_state *css = cgroup_css(scgrp, ss);\n\t\tstruct css_set *cset;\n\n\t\tWARN_ON(!css || cgroup_css(dcgrp, ss));\n\n\t\t/* disable from the source */\n\t\tsrc_root->subsys_mask &= ~(1 << ssid);\n\t\tWARN_ON(cgroup_apply_control(scgrp));\n\t\tcgroup_finalize_control(scgrp, 0);\n\n\t\t/* rebind */\n\t\tRCU_INIT_POINTER(scgrp->subsys[ssid], NULL);\n\t\trcu_assign_pointer(dcgrp->subsys[ssid], css);\n\t\tss->root = dst_root;\n\t\tcss->cgroup = dcgrp;\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\thash_for_each(css_set_table, i, cset, hlist)\n\t\t\tlist_move_tail(&cset->e_cset_node[ss->id],\n\t\t\t\t       &dcgrp->e_csets[ss->id]);\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\t/* default hierarchy doesn't enable controllers by default */\n\t\tdst_root->subsys_mask |= 1 << ssid;\n\t\tif (dst_root == &cgrp_dfl_root) {\n\t\t\tstatic_branch_enable(cgroup_subsys_on_dfl_key[ssid]);\n\t\t} else {\n\t\t\tdcgrp->subtree_control |= 1 << ssid;\n\t\t\tstatic_branch_disable(cgroup_subsys_on_dfl_key[ssid]);\n\t\t}\n\n\t\tret = cgroup_apply_control(dcgrp);\n\t\tif (ret)\n\t\t\tpr_warn(\"partial failure to rebind %s controller (err=%d)\\n\",\n\t\t\t\tss->name, ret);\n\n\t\tif (ss->bind)\n\t\t\tss->bind(css);\n\t} while_each_subsys_mask();\n\n\tkernfs_activate(dcgrp->kn);\n\treturn 0;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct static_key_true *cgroup_subsys_on_dfl_key[] = {\n#include <linux/cgroup_subsys.h>\n};",
            "struct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };",
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static void cgroup_finalize_control(struct cgroup *cgrp, int ret);",
            "static void css_task_iter_advance(struct css_task_iter *it);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);",
            "static struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);",
            "static void kill_css(struct cgroup_subsys_state *css);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic struct static_key_true *cgroup_subsys_on_dfl_key[] = {\n#include <linux/cgroup_subsys.h>\n};\nstruct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret);\nstatic void css_task_iter_advance(struct css_task_iter *it);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);\nstatic void kill_css(struct cgroup_subsys_state *css);\n\nint rebind_subsystems(struct cgroup_root *dst_root, u16 ss_mask)\n{\n\tstruct cgroup *dcgrp = &dst_root->cgrp;\n\tstruct cgroup_subsys *ss;\n\tint ssid, i, ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\t/*\n\t\t * If @ss has non-root csses attached to it, can't move.\n\t\t * If @ss is an implicit controller, it is exempt from this\n\t\t * rule and can be stolen.\n\t\t */\n\t\tif (css_next_child(NULL, cgroup_css(&ss->root->cgrp, ss)) &&\n\t\t    !ss->implicit_on_dfl)\n\t\t\treturn -EBUSY;\n\n\t\t/* can't move between two non-dummy roots either */\n\t\tif (ss->root != &cgrp_dfl_root && dst_root != &cgrp_dfl_root)\n\t\t\treturn -EBUSY;\n\t} while_each_subsys_mask();\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tstruct cgroup_root *src_root = ss->root;\n\t\tstruct cgroup *scgrp = &src_root->cgrp;\n\t\tstruct cgroup_subsys_state *css = cgroup_css(scgrp, ss);\n\t\tstruct css_set *cset;\n\n\t\tWARN_ON(!css || cgroup_css(dcgrp, ss));\n\n\t\t/* disable from the source */\n\t\tsrc_root->subsys_mask &= ~(1 << ssid);\n\t\tWARN_ON(cgroup_apply_control(scgrp));\n\t\tcgroup_finalize_control(scgrp, 0);\n\n\t\t/* rebind */\n\t\tRCU_INIT_POINTER(scgrp->subsys[ssid], NULL);\n\t\trcu_assign_pointer(dcgrp->subsys[ssid], css);\n\t\tss->root = dst_root;\n\t\tcss->cgroup = dcgrp;\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\thash_for_each(css_set_table, i, cset, hlist)\n\t\t\tlist_move_tail(&cset->e_cset_node[ss->id],\n\t\t\t\t       &dcgrp->e_csets[ss->id]);\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\t/* default hierarchy doesn't enable controllers by default */\n\t\tdst_root->subsys_mask |= 1 << ssid;\n\t\tif (dst_root == &cgrp_dfl_root) {\n\t\t\tstatic_branch_enable(cgroup_subsys_on_dfl_key[ssid]);\n\t\t} else {\n\t\t\tdcgrp->subtree_control |= 1 << ssid;\n\t\t\tstatic_branch_disable(cgroup_subsys_on_dfl_key[ssid]);\n\t\t}\n\n\t\tret = cgroup_apply_control(dcgrp);\n\t\tif (ret)\n\t\t\tpr_warn(\"partial failure to rebind %s controller (err=%d)\\n\",\n\t\t\t\tss->name, ret);\n\n\t\tif (ss->bind)\n\t\t\tss->bind(css);\n\t} while_each_subsys_mask();\n\n\tkernfs_activate(dcgrp->kn);\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&root->cgrp.self.children"
          ],
          "line": 1073
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "pr_err",
          "args": [
            "\"option or name mismatch, new: 0x%x \\\"%s\\\", old: 0x%x \\\"%s\\\"\\n\"",
            "opts.flags",
            "opts.name ?: \"\"",
            "root->flags",
            "root->name"
          ],
          "line": 1066
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "opts.name",
            "root->name"
          ],
          "line": 1065
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pr_warn",
          "args": [
            "\"option changes via remount are deprecated (pid=%d comm=%s)\\n\"",
            "task_tgid_nr(current)",
            "current->comm"
          ],
          "line": 1057
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_tgid_nr",
          "args": [
            "current"
          ],
          "line": 1058
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "parse_cgroupfs_options",
          "args": [
            "data",
            "&opts"
          ],
          "line": 1052
        },
        "resolved": true,
        "details": {
          "function_name": "parse_cgroupfs_options",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "906-1040",
          "snippet": "static int parse_cgroupfs_options(char *data, struct cgroup_sb_opts *opts)\n{\n\tchar *token, *o = data;\n\tbool all_ss = false, one_ss = false;\n\tu16 mask = U16_MAX;\n\tstruct cgroup_subsys *ss;\n\tint nr_opts = 0;\n\tint i;\n\n#ifdef CONFIG_CPUSETS\n\tmask = ~((u16)1 << cpuset_cgrp_id);\n#endif\n\n\tmemset(opts, 0, sizeof(*opts));\n\n\twhile ((token = strsep(&o, \",\")) != NULL) {\n\t\tnr_opts++;\n\n\t\tif (!*token)\n\t\t\treturn -EINVAL;\n\t\tif (!strcmp(token, \"none\")) {\n\t\t\t/* Explicitly have no subsystems */\n\t\t\topts->none = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (one_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\tall_ss = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"noprefix\")) {\n\t\t\topts->flags |= CGRP_ROOT_NOPREFIX;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"clone_children\")) {\n\t\t\topts->cpuset_clone_children = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"cpuset_v2_mode\")) {\n\t\t\topts->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"xattr\")) {\n\t\t\topts->flags |= CGRP_ROOT_XATTR;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"release_agent=\", 14)) {\n\t\t\t/* Specifying two release agents is forbidden */\n\t\t\tif (opts->release_agent)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->release_agent =\n\t\t\t\tkstrndup(token + 14, PATH_MAX - 1, GFP_KERNEL);\n\t\t\tif (!opts->release_agent)\n\t\t\t\treturn -ENOMEM;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"name=\", 5)) {\n\t\t\tconst char *name = token + 5;\n\t\t\t/* Can't specify an empty name */\n\t\t\tif (!strlen(name))\n\t\t\t\treturn -EINVAL;\n\t\t\t/* Must match [\\w.-]+ */\n\t\t\tfor (i = 0; i < strlen(name); i++) {\n\t\t\t\tchar c = name[i];\n\t\t\t\tif (isalnum(c))\n\t\t\t\t\tcontinue;\n\t\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\t\tcontinue;\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* Specifying two names is forbidden */\n\t\t\tif (opts->name)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->name = kstrndup(name,\n\t\t\t\t\t      MAX_CGROUP_ROOT_NAMELEN - 1,\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!opts->name)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i))\n\t\t\t\tcontinue;\n\t\t\tif (cgroup1_ssid_disabled(i))\n\t\t\t\tcontinue;\n\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (all_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->subsys_mask |= (1 << i);\n\t\t\tone_ss = true;\n\n\t\t\tbreak;\n\t\t}\n\t\tif (i == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * If the 'all' option was specified select all the subsystems,\n\t * otherwise if 'none', 'name=' and a subsystem name options were\n\t * not specified, let's default to 'all'\n\t */\n\tif (all_ss || (!one_ss && !opts->none && !opts->name))\n\t\tfor_each_subsys(ss, i)\n\t\t\tif (cgroup_ssid_enabled(i) && !cgroup1_ssid_disabled(i))\n\t\t\t\topts->subsys_mask |= (1 << i);\n\n\t/*\n\t * We either have to specify by name or by subsystems. (So all\n\t * empty hierarchies must have a name).\n\t */\n\tif (!opts->subsys_mask && !opts->name)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Option noprefix was introduced just for backward compatibility\n\t * with the old cpuset, so we allow noprefix only if mounting just\n\t * the cpuset subsystem.\n\t */\n\tif ((opts->flags & CGRP_ROOT_NOPREFIX) && (opts->subsys_mask & mask))\n\t\treturn -EINVAL;\n\n\t/* Can't specify \"none\" and some subsystems */\n\tif (opts->subsys_mask && opts->none)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int parse_cgroupfs_options(char *data, struct cgroup_sb_opts *opts)\n{\n\tchar *token, *o = data;\n\tbool all_ss = false, one_ss = false;\n\tu16 mask = U16_MAX;\n\tstruct cgroup_subsys *ss;\n\tint nr_opts = 0;\n\tint i;\n\n#ifdef CONFIG_CPUSETS\n\tmask = ~((u16)1 << cpuset_cgrp_id);\n#endif\n\n\tmemset(opts, 0, sizeof(*opts));\n\n\twhile ((token = strsep(&o, \",\")) != NULL) {\n\t\tnr_opts++;\n\n\t\tif (!*token)\n\t\t\treturn -EINVAL;\n\t\tif (!strcmp(token, \"none\")) {\n\t\t\t/* Explicitly have no subsystems */\n\t\t\topts->none = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (one_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\tall_ss = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"noprefix\")) {\n\t\t\topts->flags |= CGRP_ROOT_NOPREFIX;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"clone_children\")) {\n\t\t\topts->cpuset_clone_children = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"cpuset_v2_mode\")) {\n\t\t\topts->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"xattr\")) {\n\t\t\topts->flags |= CGRP_ROOT_XATTR;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"release_agent=\", 14)) {\n\t\t\t/* Specifying two release agents is forbidden */\n\t\t\tif (opts->release_agent)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->release_agent =\n\t\t\t\tkstrndup(token + 14, PATH_MAX - 1, GFP_KERNEL);\n\t\t\tif (!opts->release_agent)\n\t\t\t\treturn -ENOMEM;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"name=\", 5)) {\n\t\t\tconst char *name = token + 5;\n\t\t\t/* Can't specify an empty name */\n\t\t\tif (!strlen(name))\n\t\t\t\treturn -EINVAL;\n\t\t\t/* Must match [\\w.-]+ */\n\t\t\tfor (i = 0; i < strlen(name); i++) {\n\t\t\t\tchar c = name[i];\n\t\t\t\tif (isalnum(c))\n\t\t\t\t\tcontinue;\n\t\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\t\tcontinue;\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* Specifying two names is forbidden */\n\t\t\tif (opts->name)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->name = kstrndup(name,\n\t\t\t\t\t      MAX_CGROUP_ROOT_NAMELEN - 1,\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!opts->name)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i))\n\t\t\t\tcontinue;\n\t\t\tif (cgroup1_ssid_disabled(i))\n\t\t\t\tcontinue;\n\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (all_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->subsys_mask |= (1 << i);\n\t\t\tone_ss = true;\n\n\t\t\tbreak;\n\t\t}\n\t\tif (i == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * If the 'all' option was specified select all the subsystems,\n\t * otherwise if 'none', 'name=' and a subsystem name options were\n\t * not specified, let's default to 'all'\n\t */\n\tif (all_ss || (!one_ss && !opts->none && !opts->name))\n\t\tfor_each_subsys(ss, i)\n\t\t\tif (cgroup_ssid_enabled(i) && !cgroup1_ssid_disabled(i))\n\t\t\t\topts->subsys_mask |= (1 << i);\n\n\t/*\n\t * We either have to specify by name or by subsystems. (So all\n\t * empty hierarchies must have a name).\n\t */\n\tif (!opts->subsys_mask && !opts->name)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Option noprefix was introduced just for backward compatibility\n\t * with the old cpuset, so we allow noprefix only if mounting just\n\t * the cpuset subsystem.\n\t */\n\tif ((opts->flags & CGRP_ROOT_NOPREFIX) && (opts->subsys_mask & mask))\n\t\treturn -EINVAL;\n\n\t/* Can't specify \"none\" and some subsystems */\n\tif (opts->subsys_mask && opts->none)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_lock_and_drain_offline",
          "args": [
            "&cgrp_dfl_root.cgrp"
          ],
          "line": 1049
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_root_from_kf",
          "args": [
            "kf_root"
          ],
          "line": 1045
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_root_from_kf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1210-1215",
          "snippet": "struct cgroup_root *cgroup_root_from_kf(struct kernfs_root *kf_root)\n{\n\tstruct cgroup *root_cgrp = kf_root->kn->priv;\n\n\treturn root_cgrp->root;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstruct cgroup_root *cgroup_root_from_kf(struct kernfs_root *kf_root)\n{\n\tstruct cgroup *root_cgrp = kf_root->kn->priv;\n\n\treturn root_cgrp->root;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic DEFINE_SPINLOCK(release_agent_path_lock);\n\nstatic int cgroup1_remount(struct kernfs_root *kf_root, int *flags, char *data)\n{\n\tint ret = 0;\n\tstruct cgroup_root *root = cgroup_root_from_kf(kf_root);\n\tstruct cgroup_sb_opts opts;\n\tu16 added_mask, removed_mask;\n\n\tcgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);\n\n\t/* See what subsystems are wanted */\n\tret = parse_cgroupfs_options(data, &opts);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tif (opts.subsys_mask != root->subsys_mask || opts.release_agent)\n\t\tpr_warn(\"option changes via remount are deprecated (pid=%d comm=%s)\\n\",\n\t\t\ttask_tgid_nr(current), current->comm);\n\n\tadded_mask = opts.subsys_mask & ~root->subsys_mask;\n\tremoved_mask = root->subsys_mask & ~opts.subsys_mask;\n\n\t/* Don't allow flags or name to change at remount */\n\tif ((opts.flags ^ root->flags) ||\n\t    (opts.name && strcmp(opts.name, root->name))) {\n\t\tpr_err(\"option or name mismatch, new: 0x%x \\\"%s\\\", old: 0x%x \\\"%s\\\"\\n\",\n\t\t       opts.flags, opts.name ?: \"\", root->flags, root->name);\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/* remounting is not allowed for populated hierarchies */\n\tif (!list_empty(&root->cgrp.self.children)) {\n\t\tret = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tret = rebind_subsystems(root, added_mask);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tWARN_ON(rebind_subsystems(&cgrp_dfl_root, removed_mask));\n\n\tif (opts.release_agent) {\n\t\tspin_lock(&release_agent_path_lock);\n\t\tstrcpy(root->release_agent_path, opts.release_agent);\n\t\tspin_unlock(&release_agent_path_lock);\n\t}\n\n\ttrace_cgroup_remount(root);\n\n out_unlock:\n\tkfree(opts.release_agent);\n\tkfree(opts.name);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}"
  },
  {
    "function_name": "parse_cgroupfs_options",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "906-1040",
    "snippet": "static int parse_cgroupfs_options(char *data, struct cgroup_sb_opts *opts)\n{\n\tchar *token, *o = data;\n\tbool all_ss = false, one_ss = false;\n\tu16 mask = U16_MAX;\n\tstruct cgroup_subsys *ss;\n\tint nr_opts = 0;\n\tint i;\n\n#ifdef CONFIG_CPUSETS\n\tmask = ~((u16)1 << cpuset_cgrp_id);\n#endif\n\n\tmemset(opts, 0, sizeof(*opts));\n\n\twhile ((token = strsep(&o, \",\")) != NULL) {\n\t\tnr_opts++;\n\n\t\tif (!*token)\n\t\t\treturn -EINVAL;\n\t\tif (!strcmp(token, \"none\")) {\n\t\t\t/* Explicitly have no subsystems */\n\t\t\topts->none = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (one_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\tall_ss = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"noprefix\")) {\n\t\t\topts->flags |= CGRP_ROOT_NOPREFIX;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"clone_children\")) {\n\t\t\topts->cpuset_clone_children = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"cpuset_v2_mode\")) {\n\t\t\topts->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"xattr\")) {\n\t\t\topts->flags |= CGRP_ROOT_XATTR;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"release_agent=\", 14)) {\n\t\t\t/* Specifying two release agents is forbidden */\n\t\t\tif (opts->release_agent)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->release_agent =\n\t\t\t\tkstrndup(token + 14, PATH_MAX - 1, GFP_KERNEL);\n\t\t\tif (!opts->release_agent)\n\t\t\t\treturn -ENOMEM;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"name=\", 5)) {\n\t\t\tconst char *name = token + 5;\n\t\t\t/* Can't specify an empty name */\n\t\t\tif (!strlen(name))\n\t\t\t\treturn -EINVAL;\n\t\t\t/* Must match [\\w.-]+ */\n\t\t\tfor (i = 0; i < strlen(name); i++) {\n\t\t\t\tchar c = name[i];\n\t\t\t\tif (isalnum(c))\n\t\t\t\t\tcontinue;\n\t\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\t\tcontinue;\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* Specifying two names is forbidden */\n\t\t\tif (opts->name)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->name = kstrndup(name,\n\t\t\t\t\t      MAX_CGROUP_ROOT_NAMELEN - 1,\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!opts->name)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i))\n\t\t\t\tcontinue;\n\t\t\tif (cgroup1_ssid_disabled(i))\n\t\t\t\tcontinue;\n\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (all_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->subsys_mask |= (1 << i);\n\t\t\tone_ss = true;\n\n\t\t\tbreak;\n\t\t}\n\t\tif (i == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * If the 'all' option was specified select all the subsystems,\n\t * otherwise if 'none', 'name=' and a subsystem name options were\n\t * not specified, let's default to 'all'\n\t */\n\tif (all_ss || (!one_ss && !opts->none && !opts->name))\n\t\tfor_each_subsys(ss, i)\n\t\t\tif (cgroup_ssid_enabled(i) && !cgroup1_ssid_disabled(i))\n\t\t\t\topts->subsys_mask |= (1 << i);\n\n\t/*\n\t * We either have to specify by name or by subsystems. (So all\n\t * empty hierarchies must have a name).\n\t */\n\tif (!opts->subsys_mask && !opts->name)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Option noprefix was introduced just for backward compatibility\n\t * with the old cpuset, so we allow noprefix only if mounting just\n\t * the cpuset subsystem.\n\t */\n\tif ((opts->flags & CGRP_ROOT_NOPREFIX) && (opts->subsys_mask & mask))\n\t\treturn -EINVAL;\n\n\t/* Can't specify \"none\" and some subsystems */\n\tif (opts->subsys_mask && opts->none)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cgroup1_ssid_disabled",
          "args": [
            "i"
          ],
          "line": 1017
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup1_ssid_disabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "42-45",
          "snippet": "bool cgroup1_ssid_disabled(int ssid)\n{\n\treturn cgroup_no_v1_mask & (1 << ssid);\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static u16 cgroup_no_v1_mask;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic u16 cgroup_no_v1_mask;\n\nbool cgroup1_ssid_disabled(int ssid)\n{\n\treturn cgroup_no_v1_mask & (1 << ssid);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_ssid_enabled",
          "args": [
            "i"
          ],
          "line": 1017
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_ssid_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "234-240",
          "snippet": "bool cgroup_ssid_enabled(int ssid)\n{\n\tif (CGROUP_SUBSYS_COUNT == 0)\n\t\treturn false;\n\n\treturn static_key_enabled(cgroup_subsys_enabled_key[ssid]);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct static_key_true *cgroup_subsys_enabled_key[] = {\n#include <linux/cgroup_subsys.h>\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic struct static_key_true *cgroup_subsys_enabled_key[] = {\n#include <linux/cgroup_subsys.h>\n};\n\nbool cgroup_ssid_enabled(int ssid)\n{\n\tif (CGROUP_SUBSYS_COUNT == 0)\n\t\treturn false;\n\n\treturn static_key_enabled(cgroup_subsys_enabled_key[ssid]);\n}"
        }
      },
      {
        "call_info": {
          "callee": "for_each_subsys",
          "args": [
            "ss",
            "i"
          ],
          "line": 1016
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "ss->legacy_name"
          ],
          "line": 991
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_subsys",
          "args": [
            "ss",
            "i"
          ],
          "line": 990
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kstrndup",
          "args": [
            "name",
            "MAX_CGROUP_ROOT_NAMELEN - 1",
            "GFP_KERNEL"
          ],
          "line": 981
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "isalnum",
          "args": [
            "c"
          ],
          "line": 972
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strlen",
          "args": [
            "name"
          ],
          "line": 970
        },
        "resolved": true,
        "details": {
          "function_name": "fetch_store_strlen",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_uprobe.c",
          "lines": "137-146",
          "snippet": "static nokprobe_inline int\nfetch_store_strlen(unsigned long addr)\n{\n\tint len;\n\tvoid __user *vaddr = (void __force __user *) addr;\n\n\tlen = strnlen_user(vaddr, MAX_STRING_SIZE);\n\n\treturn (len > MAX_STRING_SIZE) ? 0 : len;\n}",
          "includes": [
            "#include \"trace_probe_tmpl.h\"",
            "#include \"trace_probe.h\"",
            "#include <linux/rculist.h>",
            "#include <linux/string.h>",
            "#include <linux/namei.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/module.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_probe_tmpl.h\"\n#include \"trace_probe.h\"\n#include <linux/rculist.h>\n#include <linux/string.h>\n#include <linux/namei.h>\n#include <linux/uprobes.h>\n#include <linux/uaccess.h>\n#include <linux/module.h>\n\nstatic nokprobe_inline int\nfetch_store_strlen(unsigned long addr)\n{\n\tint len;\n\tvoid __user *vaddr = (void __force __user *) addr;\n\n\tlen = strnlen_user(vaddr, MAX_STRING_SIZE);\n\n\treturn (len > MAX_STRING_SIZE) ? 0 : len;\n}"
        }
      },
      {
        "call_info": {
          "callee": "strncmp",
          "args": [
            "token",
            "\"name=\"",
            "5"
          ],
          "line": 964
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kstrndup",
          "args": [
            "token + 14",
            "PATH_MAX - 1",
            "GFP_KERNEL"
          ],
          "line": 959
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strncmp",
          "args": [
            "token",
            "\"release_agent=\"",
            "14"
          ],
          "line": 954
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "\"xattr\""
          ],
          "line": 950
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "\"cpuset_v2_mode\""
          ],
          "line": 946
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "\"clone_children\""
          ],
          "line": 942
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "\"noprefix\""
          ],
          "line": 938
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "\"all\""
          ],
          "line": 931
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strcmp",
          "args": [
            "token",
            "\"none\""
          ],
          "line": 926
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strsep",
          "args": [
            "&o",
            "\",\""
          ],
          "line": 921
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "memset",
          "args": [
            "opts",
            "0",
            "sizeof(*opts)"
          ],
          "line": 919
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int parse_cgroupfs_options(char *data, struct cgroup_sb_opts *opts)\n{\n\tchar *token, *o = data;\n\tbool all_ss = false, one_ss = false;\n\tu16 mask = U16_MAX;\n\tstruct cgroup_subsys *ss;\n\tint nr_opts = 0;\n\tint i;\n\n#ifdef CONFIG_CPUSETS\n\tmask = ~((u16)1 << cpuset_cgrp_id);\n#endif\n\n\tmemset(opts, 0, sizeof(*opts));\n\n\twhile ((token = strsep(&o, \",\")) != NULL) {\n\t\tnr_opts++;\n\n\t\tif (!*token)\n\t\t\treturn -EINVAL;\n\t\tif (!strcmp(token, \"none\")) {\n\t\t\t/* Explicitly have no subsystems */\n\t\t\topts->none = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"all\")) {\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (one_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\tall_ss = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"noprefix\")) {\n\t\t\topts->flags |= CGRP_ROOT_NOPREFIX;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"clone_children\")) {\n\t\t\topts->cpuset_clone_children = true;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"cpuset_v2_mode\")) {\n\t\t\topts->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcmp(token, \"xattr\")) {\n\t\t\topts->flags |= CGRP_ROOT_XATTR;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"release_agent=\", 14)) {\n\t\t\t/* Specifying two release agents is forbidden */\n\t\t\tif (opts->release_agent)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->release_agent =\n\t\t\t\tkstrndup(token + 14, PATH_MAX - 1, GFP_KERNEL);\n\t\t\tif (!opts->release_agent)\n\t\t\t\treturn -ENOMEM;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strncmp(token, \"name=\", 5)) {\n\t\t\tconst char *name = token + 5;\n\t\t\t/* Can't specify an empty name */\n\t\t\tif (!strlen(name))\n\t\t\t\treturn -EINVAL;\n\t\t\t/* Must match [\\w.-]+ */\n\t\t\tfor (i = 0; i < strlen(name); i++) {\n\t\t\t\tchar c = name[i];\n\t\t\t\tif (isalnum(c))\n\t\t\t\t\tcontinue;\n\t\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\t\tcontinue;\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* Specifying two names is forbidden */\n\t\t\tif (opts->name)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->name = kstrndup(name,\n\t\t\t\t\t      MAX_CGROUP_ROOT_NAMELEN - 1,\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!opts->name)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i))\n\t\t\t\tcontinue;\n\t\t\tif (cgroup1_ssid_disabled(i))\n\t\t\t\tcontinue;\n\n\t\t\t/* Mutually exclusive option 'all' + subsystem name */\n\t\t\tif (all_ss)\n\t\t\t\treturn -EINVAL;\n\t\t\topts->subsys_mask |= (1 << i);\n\t\t\tone_ss = true;\n\n\t\t\tbreak;\n\t\t}\n\t\tif (i == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * If the 'all' option was specified select all the subsystems,\n\t * otherwise if 'none', 'name=' and a subsystem name options were\n\t * not specified, let's default to 'all'\n\t */\n\tif (all_ss || (!one_ss && !opts->none && !opts->name))\n\t\tfor_each_subsys(ss, i)\n\t\t\tif (cgroup_ssid_enabled(i) && !cgroup1_ssid_disabled(i))\n\t\t\t\topts->subsys_mask |= (1 << i);\n\n\t/*\n\t * We either have to specify by name or by subsystems. (So all\n\t * empty hierarchies must have a name).\n\t */\n\tif (!opts->subsys_mask && !opts->name)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Option noprefix was introduced just for backward compatibility\n\t * with the old cpuset, so we allow noprefix only if mounting just\n\t * the cpuset subsystem.\n\t */\n\tif ((opts->flags & CGRP_ROOT_NOPREFIX) && (opts->subsys_mask & mask))\n\t\treturn -EINVAL;\n\n\t/* Can't specify \"none\" and some subsystems */\n\tif (opts->subsys_mask && opts->none)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup1_show_options",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "877-904",
    "snippet": "static int cgroup1_show_options(struct seq_file *seq, struct kernfs_root *kf_root)\n{\n\tstruct cgroup_root *root = cgroup_root_from_kf(kf_root);\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tfor_each_subsys(ss, ssid)\n\t\tif (root->subsys_mask & (1 << ssid))\n\t\t\tseq_show_option(seq, ss->legacy_name, NULL);\n\tif (root->flags & CGRP_ROOT_NOPREFIX)\n\t\tseq_puts(seq, \",noprefix\");\n\tif (root->flags & CGRP_ROOT_XATTR)\n\t\tseq_puts(seq, \",xattr\");\n\tif (root->flags & CGRP_ROOT_CPUSET_V2_MODE)\n\t\tseq_puts(seq, \",cpuset_v2_mode\");\n\n\tspin_lock(&release_agent_path_lock);\n\tif (strlen(root->release_agent_path))\n\t\tseq_show_option(seq, \"release_agent\",\n\t\t\t\troot->release_agent_path);\n\tspin_unlock(&release_agent_path_lock);\n\n\tif (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &root->cgrp.flags))\n\t\tseq_puts(seq, \",clone_children\");\n\tif (strlen(root->name))\n\t\tseq_show_option(seq, \"name\", root->name);\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static DEFINE_SPINLOCK(release_agent_path_lock);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "seq_show_option",
          "args": [
            "seq",
            "\"name\"",
            "root->name"
          ],
          "line": 902
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strlen",
          "args": [
            "root->name"
          ],
          "line": 901
        },
        "resolved": true,
        "details": {
          "function_name": "fetch_store_strlen",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_uprobe.c",
          "lines": "137-146",
          "snippet": "static nokprobe_inline int\nfetch_store_strlen(unsigned long addr)\n{\n\tint len;\n\tvoid __user *vaddr = (void __force __user *) addr;\n\n\tlen = strnlen_user(vaddr, MAX_STRING_SIZE);\n\n\treturn (len > MAX_STRING_SIZE) ? 0 : len;\n}",
          "includes": [
            "#include \"trace_probe_tmpl.h\"",
            "#include \"trace_probe.h\"",
            "#include <linux/rculist.h>",
            "#include <linux/string.h>",
            "#include <linux/namei.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/module.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"trace_probe_tmpl.h\"\n#include \"trace_probe.h\"\n#include <linux/rculist.h>\n#include <linux/string.h>\n#include <linux/namei.h>\n#include <linux/uprobes.h>\n#include <linux/uaccess.h>\n#include <linux/module.h>\n\nstatic nokprobe_inline int\nfetch_store_strlen(unsigned long addr)\n{\n\tint len;\n\tvoid __user *vaddr = (void __force __user *) addr;\n\n\tlen = strnlen_user(vaddr, MAX_STRING_SIZE);\n\n\treturn (len > MAX_STRING_SIZE) ? 0 : len;\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_puts",
          "args": [
            "seq",
            "\",clone_children\""
          ],
          "line": 900
        },
        "resolved": true,
        "details": {
          "function_name": "trace_seq_puts",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_seq.c",
          "lines": "208-223",
          "snippet": "void trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}",
          "includes": [
            "#include <linux/trace_seq.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/uaccess.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_seq.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\nvoid trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}"
        }
      },
      {
        "call_info": {
          "callee": "test_bit",
          "args": [
            "CGRP_CPUSET_CLONE_CHILDREN",
            "&root->cgrp.flags"
          ],
          "line": 899
        },
        "resolved": true,
        "details": {
          "function_name": "memory_bm_test_bit",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/power/snapshot.c",
          "lines": "811-820",
          "snippet": "static int memory_bm_test_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\treturn test_bit(bit, addr);\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <asm/io.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/ktime.h>",
            "#include <linux/compiler.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/highmem.h>",
            "#include <linux/console.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/nmi.h>",
            "#include <linux/memblock.h>",
            "#include <linux/init.h>",
            "#include <linux/device.h>",
            "#include <linux/pm.h>",
            "#include <linux/kernel.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bitops.h>",
            "#include <linux/delay.h>",
            "#include <linux/suspend.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/version.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <asm/io.h>\n#include <asm/tlbflush.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <linux/set_memory.h>\n#include <linux/ktime.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/syscalls.h>\n#include <linux/nmi.h>\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/pm.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/suspend.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/version.h>\n\nstatic int memory_bm_test_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\treturn test_bit(bit, addr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 897
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_queued_spin_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "546-560",
          "snippet": "__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_show_option",
          "args": [
            "seq",
            "\"release_agent\"",
            "root->release_agent_path"
          ],
          "line": 895
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 893
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_show_option",
          "args": [
            "seq",
            "ss->legacy_name",
            "NULL"
          ],
          "line": 885
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_subsys",
          "args": [
            "ss",
            "ssid"
          ],
          "line": 883
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_root_from_kf",
          "args": [
            "kf_root"
          ],
          "line": 879
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_root_from_kf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1210-1215",
          "snippet": "struct cgroup_root *cgroup_root_from_kf(struct kernfs_root *kf_root)\n{\n\tstruct cgroup *root_cgrp = kf_root->kn->priv;\n\n\treturn root_cgrp->root;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstruct cgroup_root *cgroup_root_from_kf(struct kernfs_root *kf_root)\n{\n\tstruct cgroup *root_cgrp = kf_root->kn->priv;\n\n\treturn root_cgrp->root;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic DEFINE_SPINLOCK(release_agent_path_lock);\n\nstatic int cgroup1_show_options(struct seq_file *seq, struct kernfs_root *kf_root)\n{\n\tstruct cgroup_root *root = cgroup_root_from_kf(kf_root);\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tfor_each_subsys(ss, ssid)\n\t\tif (root->subsys_mask & (1 << ssid))\n\t\t\tseq_show_option(seq, ss->legacy_name, NULL);\n\tif (root->flags & CGRP_ROOT_NOPREFIX)\n\t\tseq_puts(seq, \",noprefix\");\n\tif (root->flags & CGRP_ROOT_XATTR)\n\t\tseq_puts(seq, \",xattr\");\n\tif (root->flags & CGRP_ROOT_CPUSET_V2_MODE)\n\t\tseq_puts(seq, \",cpuset_v2_mode\");\n\n\tspin_lock(&release_agent_path_lock);\n\tif (strlen(root->release_agent_path))\n\t\tseq_show_option(seq, \"release_agent\",\n\t\t\t\troot->release_agent_path);\n\tspin_unlock(&release_agent_path_lock);\n\n\tif (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &root->cgrp.flags))\n\t\tseq_puts(seq, \",clone_children\");\n\tif (strlen(root->name))\n\t\tseq_show_option(seq, \"name\", root->name);\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup1_rename",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "845-875",
    "snippet": "static int cgroup1_rename(struct kernfs_node *kn, struct kernfs_node *new_parent,\n\t\t\t  const char *new_name_str)\n{\n\tstruct cgroup *cgrp = kn->priv;\n\tint ret;\n\n\tif (kernfs_type(kn) != KERNFS_DIR)\n\t\treturn -ENOTDIR;\n\tif (kn->parent != new_parent)\n\t\treturn -EIO;\n\n\t/*\n\t * We're gonna grab cgroup_mutex which nests outside kernfs\n\t * active_ref.  kernfs_rename() doesn't require active_ref\n\t * protection.  Break them before grabbing cgroup_mutex.\n\t */\n\tkernfs_break_active_protection(new_parent);\n\tkernfs_break_active_protection(kn);\n\n\tmutex_lock(&cgroup_mutex);\n\n\tret = kernfs_rename(kn, new_parent, new_name_str);\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(rename, cgrp);\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tkernfs_unbreak_active_protection(kn);\n\tkernfs_unbreak_active_protection(new_parent);\n\treturn ret;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kernfs_unbreak_active_protection",
          "args": [
            "new_parent"
          ],
          "line": 873
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kernfs_unbreak_active_protection",
          "args": [
            "kn"
          ],
          "line": 872
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 870
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "TRACE_CGROUP_PATH",
          "args": [
            "rename",
            "cgrp"
          ],
          "line": 868
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kernfs_rename",
          "args": [
            "kn",
            "new_parent",
            "new_name_str"
          ],
          "line": 866
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 864
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "kernfs_break_active_protection",
          "args": [
            "kn"
          ],
          "line": 862
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kernfs_break_active_protection",
          "args": [
            "new_parent"
          ],
          "line": 861
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kernfs_type",
          "args": [
            "kn"
          ],
          "line": 851
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup1_rename(struct kernfs_node *kn, struct kernfs_node *new_parent,\n\t\t\t  const char *new_name_str)\n{\n\tstruct cgroup *cgrp = kn->priv;\n\tint ret;\n\n\tif (kernfs_type(kn) != KERNFS_DIR)\n\t\treturn -ENOTDIR;\n\tif (kn->parent != new_parent)\n\t\treturn -EIO;\n\n\t/*\n\t * We're gonna grab cgroup_mutex which nests outside kernfs\n\t * active_ref.  kernfs_rename() doesn't require active_ref\n\t * protection.  Break them before grabbing cgroup_mutex.\n\t */\n\tkernfs_break_active_protection(new_parent);\n\tkernfs_break_active_protection(kn);\n\n\tmutex_lock(&cgroup_mutex);\n\n\tret = kernfs_rename(kn, new_parent, new_name_str);\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(rename, cgrp);\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tkernfs_unbreak_active_protection(kn);\n\tkernfs_unbreak_active_protection(new_parent);\n\treturn ret;\n}"
  },
  {
    "function_name": "cgroup1_release_agent",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "802-840",
    "snippet": "void cgroup1_release_agent(struct work_struct *work)\n{\n\tstruct cgroup *cgrp =\n\t\tcontainer_of(work, struct cgroup, release_agent_work);\n\tchar *pathbuf = NULL, *agentbuf = NULL;\n\tchar *argv[3], *envp[3];\n\tint ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tpathbuf = kmalloc(PATH_MAX, GFP_KERNEL);\n\tagentbuf = kstrdup(cgrp->root->release_agent_path, GFP_KERNEL);\n\tif (!pathbuf || !agentbuf)\n\t\tgoto out;\n\n\tspin_lock_irq(&css_set_lock);\n\tret = cgroup_path_ns_locked(cgrp, pathbuf, PATH_MAX, &init_cgroup_ns);\n\tspin_unlock_irq(&css_set_lock);\n\tif (ret < 0 || ret >= PATH_MAX)\n\t\tgoto out;\n\n\targv[0] = agentbuf;\n\targv[1] = pathbuf;\n\targv[2] = NULL;\n\n\t/* minimal command environment */\n\tenvp[0] = \"HOME=/\";\n\tenvp[1] = \"PATH=/sbin:/bin:/usr/sbin:/usr/bin\";\n\tenvp[2] = NULL;\n\n\tmutex_unlock(&cgroup_mutex);\n\tcall_usermodehelper(argv[0], argv, envp, UMH_WAIT_EXEC);\n\tgoto out_free;\nout:\n\tmutex_unlock(&cgroup_mutex);\nout_free:\n\tkfree(agentbuf);\n\tkfree(pathbuf);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "pathbuf"
          ],
          "line": 839
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/params.c",
          "lines": "73-86",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 836
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "call_usermodehelper",
          "args": [
            "argv[0]",
            "argv",
            "envp",
            "UMH_WAIT_EXEC"
          ],
          "line": 833
        },
        "resolved": true,
        "details": {
          "function_name": "call_usermodehelper",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/umh.c",
          "lines": "608-619",
          "snippet": "int call_usermodehelper(const char *path, char **argv, char **envp, int wait)\n{\n\tstruct subprocess_info *info;\n\tgfp_t gfp_mask = (wait == UMH_NO_WAIT) ? GFP_ATOMIC : GFP_KERNEL;\n\n\tinfo = call_usermodehelper_setup(path, argv, envp, gfp_mask,\n\t\t\t\t\t NULL, NULL, NULL);\n\tif (info == NULL)\n\t\treturn -ENOMEM;\n\n\treturn call_usermodehelper_exec(info, wait);\n}",
          "includes": [
            "#include <trace/events/module.h>",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/async.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/suspend.h>",
            "#include <linux/notifier.h>",
            "#include <linux/resource.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/mount.h>",
            "#include <linux/security.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/cred.h>",
            "#include <linux/completion.h>",
            "#include <linux/slab.h>",
            "#include <linux/kmod.h>",
            "#include <linux/unistd.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/module.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/module.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/shmem_fs.h>\n#include <linux/uaccess.h>\n#include <linux/async.h>\n#include <linux/ptrace.h>\n#include <linux/rwsem.h>\n#include <linux/suspend.h>\n#include <linux/notifier.h>\n#include <linux/resource.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/mount.h>\n#include <linux/security.h>\n#include <linux/workqueue.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/cred.h>\n#include <linux/completion.h>\n#include <linux/slab.h>\n#include <linux/kmod.h>\n#include <linux/unistd.h>\n#include <linux/syscalls.h>\n#include <linux/binfmts.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/module.h>\n\nint call_usermodehelper(const char *path, char **argv, char **envp, int wait)\n{\n\tstruct subprocess_info *info;\n\tgfp_t gfp_mask = (wait == UMH_NO_WAIT) ? GFP_ATOMIC : GFP_KERNEL;\n\n\tinfo = call_usermodehelper_setup(path, argv, envp, gfp_mask,\n\t\t\t\t\t NULL, NULL, NULL);\n\tif (info == NULL)\n\t\treturn -ENOMEM;\n\n\treturn call_usermodehelper_exec(info, wait);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 819
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_path_ns_locked",
          "args": [
            "cgrp",
            "pathbuf",
            "PATH_MAX",
            "&init_cgroup_ns"
          ],
          "line": 818
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_path_ns_locked",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2116-2122",
          "snippet": "int cgroup_path_ns_locked(struct cgroup *cgrp, char *buf, size_t buflen,\n\t\t\t  struct cgroup_namespace *ns)\n{\n\tstruct cgroup *root = cset_cgroup_from_root(ns->root_cset, cgrp->root);\n\n\treturn kernfs_path_from_node(cgrp->kn, root->kn, buf, buflen);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nint cgroup_path_ns_locked(struct cgroup *cgrp, char *buf, size_t buflen,\n\t\t\t  struct cgroup_namespace *ns)\n{\n\tstruct cgroup *root = cset_cgroup_from_root(ns->root_cset, cgrp->root);\n\n\treturn kernfs_path_from_node(cgrp->kn, root->kn, buf, buflen);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 817
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "kstrdup",
          "args": [
            "cgrp->root->release_agent_path",
            "GFP_KERNEL"
          ],
          "line": 813
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kmalloc",
          "args": [
            "PATH_MAX",
            "GFP_KERNEL"
          ],
          "line": 812
        },
        "resolved": true,
        "details": {
          "function_name": "debug_kmalloc",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/debug/kdb/kdb_support.c",
          "lines": "745-801",
          "snippet": "void *debug_kmalloc(size_t size, gfp_t flags)\n{\n\tunsigned int rem, h_offset;\n\tstruct debug_alloc_header *best, *bestprev, *prev, *h;\n\tvoid *p = NULL;\n\tif (!get_dap_lock()) {\n\t\t__release(dap_lock);\t/* we never actually got it */\n\t\treturn NULL;\n\t}\n\th = (struct debug_alloc_header *)(debug_alloc_pool + dah_first);\n\tif (dah_first_call) {\n\t\th->size = sizeof(debug_alloc_pool_aligned) - dah_overhead;\n\t\tdah_first_call = 0;\n\t}\n\tsize = ALIGN(size, dah_align);\n\tprev = best = bestprev = NULL;\n\twhile (1) {\n\t\tif (h->size >= size && (!best || h->size < best->size)) {\n\t\t\tbest = h;\n\t\t\tbestprev = prev;\n\t\t\tif (h->size == size)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!h->next)\n\t\t\tbreak;\n\t\tprev = h;\n\t\th = (struct debug_alloc_header *)(debug_alloc_pool + h->next);\n\t}\n\tif (!best)\n\t\tgoto out;\n\trem = best->size - size;\n\t/* The pool must always contain at least one header */\n\tif (best->next == 0 && bestprev == NULL && rem < dah_overhead)\n\t\tgoto out;\n\tif (rem >= dah_overhead) {\n\t\tbest->size = size;\n\t\th_offset = ((char *)best - debug_alloc_pool) +\n\t\t\t   dah_overhead + best->size;\n\t\th = (struct debug_alloc_header *)(debug_alloc_pool + h_offset);\n\t\th->size = rem - dah_overhead;\n\t\th->next = best->next;\n\t} else\n\t\th_offset = best->next;\n\tbest->caller = __builtin_return_address(0);\n\tdah_used += best->size;\n\tdah_used_max = max(dah_used, dah_used_max);\n\tif (bestprev)\n\t\tbestprev->next = h_offset;\n\telse\n\t\tdah_first = h_offset;\n\tp = (char *)best + dah_overhead;\n\tmemset(p, POISON_INUSE, best->size - 1);\n\t*((char *)p + best->size - 1) = POISON_END;\nout:\n\tspin_unlock(&dap_lock);\n\treturn p;\n}",
          "includes": [
            "#include \"kdb_private.h\"",
            "#include <linux/slab.h>",
            "#include <linux/kdb.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/highmem.h>",
            "#include <linux/module.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/stddef.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/mm.h>",
            "#include <linux/sched.h>",
            "#include <linux/types.h>",
            "#include <stdarg.h>"
          ],
          "macros_used": [
            "#define dah_overhead ALIGN(sizeof(struct debug_alloc_header), dah_align)",
            "#define dah_align 8"
          ],
          "globals_used": [
            "static u64 debug_alloc_pool_aligned[256*1024/dah_align];",
            "static char *debug_alloc_pool = (char *)debug_alloc_pool_aligned;",
            "static u32 dah_first, dah_first_call = 1, dah_used, dah_used_max;",
            "static DEFINE_SPINLOCK(dap_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"kdb_private.h\"\n#include <linux/slab.h>\n#include <linux/kdb.h>\n#include <linux/uaccess.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/highmem.h>\n#include <linux/module.h>\n#include <linux/ptrace.h>\n#include <linux/vmalloc.h>\n#include <linux/stddef.h>\n#include <linux/kallsyms.h>\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/types.h>\n#include <stdarg.h>\n\n#define dah_overhead ALIGN(sizeof(struct debug_alloc_header), dah_align)\n#define dah_align 8\n\nstatic u64 debug_alloc_pool_aligned[256*1024/dah_align];\nstatic char *debug_alloc_pool = (char *)debug_alloc_pool_aligned;\nstatic u32 dah_first, dah_first_call = 1, dah_used, dah_used_max;\nstatic DEFINE_SPINLOCK(dap_lock);\n\nvoid *debug_kmalloc(size_t size, gfp_t flags)\n{\n\tunsigned int rem, h_offset;\n\tstruct debug_alloc_header *best, *bestprev, *prev, *h;\n\tvoid *p = NULL;\n\tif (!get_dap_lock()) {\n\t\t__release(dap_lock);\t/* we never actually got it */\n\t\treturn NULL;\n\t}\n\th = (struct debug_alloc_header *)(debug_alloc_pool + dah_first);\n\tif (dah_first_call) {\n\t\th->size = sizeof(debug_alloc_pool_aligned) - dah_overhead;\n\t\tdah_first_call = 0;\n\t}\n\tsize = ALIGN(size, dah_align);\n\tprev = best = bestprev = NULL;\n\twhile (1) {\n\t\tif (h->size >= size && (!best || h->size < best->size)) {\n\t\t\tbest = h;\n\t\t\tbestprev = prev;\n\t\t\tif (h->size == size)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!h->next)\n\t\t\tbreak;\n\t\tprev = h;\n\t\th = (struct debug_alloc_header *)(debug_alloc_pool + h->next);\n\t}\n\tif (!best)\n\t\tgoto out;\n\trem = best->size - size;\n\t/* The pool must always contain at least one header */\n\tif (best->next == 0 && bestprev == NULL && rem < dah_overhead)\n\t\tgoto out;\n\tif (rem >= dah_overhead) {\n\t\tbest->size = size;\n\t\th_offset = ((char *)best - debug_alloc_pool) +\n\t\t\t   dah_overhead + best->size;\n\t\th = (struct debug_alloc_header *)(debug_alloc_pool + h_offset);\n\t\th->size = rem - dah_overhead;\n\t\th->next = best->next;\n\t} else\n\t\th_offset = best->next;\n\tbest->caller = __builtin_return_address(0);\n\tdah_used += best->size;\n\tdah_used_max = max(dah_used, dah_used_max);\n\tif (bestprev)\n\t\tbestprev->next = h_offset;\n\telse\n\t\tdah_first = h_offset;\n\tp = (char *)best + dah_overhead;\n\tmemset(p, POISON_INUSE, best->size - 1);\n\t*((char *)p + best->size - 1) = POISON_END;\nout:\n\tspin_unlock(&dap_lock);\n\treturn p;\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 810
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "work",
            "structcgroup",
            "release_agent_work"
          ],
          "line": 805
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nvoid cgroup1_release_agent(struct work_struct *work)\n{\n\tstruct cgroup *cgrp =\n\t\tcontainer_of(work, struct cgroup, release_agent_work);\n\tchar *pathbuf = NULL, *agentbuf = NULL;\n\tchar *argv[3], *envp[3];\n\tint ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tpathbuf = kmalloc(PATH_MAX, GFP_KERNEL);\n\tagentbuf = kstrdup(cgrp->root->release_agent_path, GFP_KERNEL);\n\tif (!pathbuf || !agentbuf)\n\t\tgoto out;\n\n\tspin_lock_irq(&css_set_lock);\n\tret = cgroup_path_ns_locked(cgrp, pathbuf, PATH_MAX, &init_cgroup_ns);\n\tspin_unlock_irq(&css_set_lock);\n\tif (ret < 0 || ret >= PATH_MAX)\n\t\tgoto out;\n\n\targv[0] = agentbuf;\n\targv[1] = pathbuf;\n\targv[2] = NULL;\n\n\t/* minimal command environment */\n\tenvp[0] = \"HOME=/\";\n\tenvp[1] = \"PATH=/sbin:/bin:/usr/sbin:/usr/bin\";\n\tenvp[2] = NULL;\n\n\tmutex_unlock(&cgroup_mutex);\n\tcall_usermodehelper(argv[0], argv, envp, UMH_WAIT_EXEC);\n\tgoto out_free;\nout:\n\tmutex_unlock(&cgroup_mutex);\nout_free:\n\tkfree(agentbuf);\n\tkfree(pathbuf);\n}"
  },
  {
    "function_name": "cgroup1_check_for_release",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "772-777",
    "snippet": "void cgroup1_check_for_release(struct cgroup *cgrp)\n{\n\tif (notify_on_release(cgrp) && !cgroup_is_populated(cgrp) &&\n\t    !css_has_online_children(&cgrp->self) && !cgroup_is_dead(cgrp))\n\t\tschedule_work(&cgrp->release_agent_work);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "schedule_work",
          "args": [
            "&cgrp->release_agent_work"
          ],
          "line": 776
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_is_dead",
          "args": [
            "cgrp"
          ],
          "line": 775
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_is_dead",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-internal.h",
          "lines": "147-150",
          "snippet": "static inline bool cgroup_is_dead(const struct cgroup *cgrp)\n{\n\treturn !(cgrp->self.flags & CSS_ONLINE);\n}",
          "includes": [
            "#include <linux/refcount.h>",
            "#include <linux/list.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/kernfs.h>",
            "#include <linux/cgroup.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/refcount.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/kernfs.h>\n#include <linux/cgroup.h>\n\nstatic inline bool cgroup_is_dead(const struct cgroup *cgrp)\n{\n\treturn !(cgrp->self.flags & CSS_ONLINE);\n}"
        }
      },
      {
        "call_info": {
          "callee": "css_has_online_children",
          "args": [
            "&cgrp->self"
          ],
          "line": 775
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_is_populated",
          "args": [
            "cgrp"
          ],
          "line": 774
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "notify_on_release",
          "args": [
            "cgrp"
          ],
          "line": 774
        },
        "resolved": true,
        "details": {
          "function_name": "notify_on_release",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-internal.h",
          "lines": "152-155",
          "snippet": "static inline bool notify_on_release(const struct cgroup *cgrp)\n{\n\treturn test_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n}",
          "includes": [
            "#include <linux/refcount.h>",
            "#include <linux/list.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/kernfs.h>",
            "#include <linux/cgroup.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/refcount.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/kernfs.h>\n#include <linux/cgroup.h>\n\nstatic inline bool notify_on_release(const struct cgroup *cgrp)\n{\n\treturn test_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nvoid cgroup1_check_for_release(struct cgroup *cgrp)\n{\n\tif (notify_on_release(cgrp) && !cgroup_is_populated(cgrp) &&\n\t    !css_has_online_children(&cgrp->self) && !cgroup_is_dead(cgrp))\n\t\tschedule_work(&cgrp->release_agent_work);\n}"
  },
  {
    "function_name": "cgroupstats_build",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "717-770",
    "snippet": "int cgroupstats_build(struct cgroupstats *stats, struct dentry *dentry)\n{\n\tstruct kernfs_node *kn = kernfs_node_from_dentry(dentry);\n\tstruct cgroup *cgrp;\n\tstruct css_task_iter it;\n\tstruct task_struct *tsk;\n\n\t/* it should be kernfs_node belonging to cgroupfs and is a directory */\n\tif (dentry->d_sb->s_type != &cgroup_fs_type || !kn ||\n\t    kernfs_type(kn) != KERNFS_DIR)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&cgroup_mutex);\n\n\t/*\n\t * We aren't being called from kernfs and there's no guarantee on\n\t * @kn->priv's validity.  For this and css_tryget_online_from_dir(),\n\t * @kn->priv is RCU safe.  Let's do the RCU dancing.\n\t */\n\trcu_read_lock();\n\tcgrp = rcu_dereference(*(void __rcu __force **)&kn->priv);\n\tif (!cgrp || cgroup_is_dead(cgrp)) {\n\t\trcu_read_unlock();\n\t\tmutex_unlock(&cgroup_mutex);\n\t\treturn -ENOENT;\n\t}\n\trcu_read_unlock();\n\n\tcss_task_iter_start(&cgrp->self, 0, &it);\n\twhile ((tsk = css_task_iter_next(&it))) {\n\t\tswitch (tsk->state) {\n\t\tcase TASK_RUNNING:\n\t\t\tstats->nr_running++;\n\t\t\tbreak;\n\t\tcase TASK_INTERRUPTIBLE:\n\t\t\tstats->nr_sleeping++;\n\t\t\tbreak;\n\t\tcase TASK_UNINTERRUPTIBLE:\n\t\t\tstats->nr_uninterruptible++;\n\t\t\tbreak;\n\t\tcase TASK_STOPPED:\n\t\t\tstats->nr_stopped++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (delayacct_is_task_waiting_on_io(tsk))\n\t\t\t\tstats->nr_io_wait++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tcss_task_iter_end(&it);\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 768
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "css_task_iter_end",
          "args": [
            "&it"
          ],
          "line": 766
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "delayacct_is_task_waiting_on_io",
          "args": [
            "tsk"
          ],
          "line": 761
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "css_task_iter_next",
          "args": [
            "&it"
          ],
          "line": 746
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "css_task_iter_start",
          "args": [
            "&cgrp->self",
            "0",
            "&it"
          ],
          "line": 745
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 743
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_is_dead",
          "args": [
            "cgrp"
          ],
          "line": 738
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_is_dead",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-internal.h",
          "lines": "147-150",
          "snippet": "static inline bool cgroup_is_dead(const struct cgroup *cgrp)\n{\n\treturn !(cgrp->self.flags & CSS_ONLINE);\n}",
          "includes": [
            "#include <linux/refcount.h>",
            "#include <linux/list.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/kernfs.h>",
            "#include <linux/cgroup.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/refcount.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/kernfs.h>\n#include <linux/cgroup.h>\n\nstatic inline bool cgroup_is_dead(const struct cgroup *cgrp)\n{\n\treturn !(cgrp->self.flags & CSS_ONLINE);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_dereference",
          "args": [
            "*(void __rcu __force **)&kn->priv"
          ],
          "line": 737
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 736
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 729
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "kernfs_type",
          "args": [
            "kn"
          ],
          "line": 726
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kernfs_node_from_dentry",
          "args": [
            "dentry"
          ],
          "line": 719
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nint cgroupstats_build(struct cgroupstats *stats, struct dentry *dentry)\n{\n\tstruct kernfs_node *kn = kernfs_node_from_dentry(dentry);\n\tstruct cgroup *cgrp;\n\tstruct css_task_iter it;\n\tstruct task_struct *tsk;\n\n\t/* it should be kernfs_node belonging to cgroupfs and is a directory */\n\tif (dentry->d_sb->s_type != &cgroup_fs_type || !kn ||\n\t    kernfs_type(kn) != KERNFS_DIR)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&cgroup_mutex);\n\n\t/*\n\t * We aren't being called from kernfs and there's no guarantee on\n\t * @kn->priv's validity.  For this and css_tryget_online_from_dir(),\n\t * @kn->priv is RCU safe.  Let's do the RCU dancing.\n\t */\n\trcu_read_lock();\n\tcgrp = rcu_dereference(*(void __rcu __force **)&kn->priv);\n\tif (!cgrp || cgroup_is_dead(cgrp)) {\n\t\trcu_read_unlock();\n\t\tmutex_unlock(&cgroup_mutex);\n\t\treturn -ENOENT;\n\t}\n\trcu_read_unlock();\n\n\tcss_task_iter_start(&cgrp->self, 0, &it);\n\twhile ((tsk = css_task_iter_next(&it))) {\n\t\tswitch (tsk->state) {\n\t\tcase TASK_RUNNING:\n\t\t\tstats->nr_running++;\n\t\t\tbreak;\n\t\tcase TASK_INTERRUPTIBLE:\n\t\t\tstats->nr_sleeping++;\n\t\t\tbreak;\n\t\tcase TASK_UNINTERRUPTIBLE:\n\t\t\tstats->nr_uninterruptible++;\n\t\t\tbreak;\n\t\tcase TASK_STOPPED:\n\t\t\tstats->nr_stopped++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (delayacct_is_task_waiting_on_io(tsk))\n\t\t\t\tstats->nr_io_wait++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tcss_task_iter_end(&it);\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn 0;\n}"
  },
  {
    "function_name": "proc_cgroupstats_show",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "685-706",
    "snippet": "int proc_cgroupstats_show(struct seq_file *m, void *v)\n{\n\tstruct cgroup_subsys *ss;\n\tint i;\n\n\tseq_puts(m, \"#subsys_name\\thierarchy\\tnum_cgroups\\tenabled\\n\");\n\t/*\n\t * ideally we don't want subsystems moving around while we do this.\n\t * cgroup_mutex is also necessary to guarantee an atomic snapshot of\n\t * subsys/hierarchy state.\n\t */\n\tmutex_lock(&cgroup_mutex);\n\n\tfor_each_subsys(ss, i)\n\t\tseq_printf(m, \"%s\\t%d\\t%d\\t%d\\n\",\n\t\t\t   ss->legacy_name, ss->root->hierarchy_id,\n\t\t\t   atomic_read(&ss->root->nr_cgrps),\n\t\t\t   cgroup_ssid_enabled(i));\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 704
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_printf",
          "args": [
            "m",
            "\"%s\\t%d\\t%d\\t%d\\n\"",
            "ss->legacy_name",
            "ss->root->hierarchy_id",
            "atomic_read(&ss->root->nr_cgrps)",
            "cgroup_ssid_enabled(i)"
          ],
          "line": 699
        },
        "resolved": true,
        "details": {
          "function_name": "trace_seq_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_seq.c",
          "lines": "84-103",
          "snippet": "void trace_seq_printf(struct trace_seq *s, const char *fmt, ...)\n{\n\tunsigned int save_len = s->seq.len;\n\tva_list ap;\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tva_start(ap, fmt);\n\tseq_buf_vprintf(&s->seq, fmt, ap);\n\tva_end(ap);\n\n\t/* If we can't write it all, don't bother writing anything */\n\tif (unlikely(seq_buf_has_overflowed(&s->seq))) {\n\t\ts->seq.len = save_len;\n\t\ts->full = 1;\n\t}\n}",
          "includes": [
            "#include <linux/trace_seq.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/uaccess.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_seq.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\nvoid trace_seq_printf(struct trace_seq *s, const char *fmt, ...)\n{\n\tunsigned int save_len = s->seq.len;\n\tva_list ap;\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tva_start(ap, fmt);\n\tseq_buf_vprintf(&s->seq, fmt, ap);\n\tva_end(ap);\n\n\t/* If we can't write it all, don't bother writing anything */\n\tif (unlikely(seq_buf_has_overflowed(&s->seq))) {\n\t\ts->seq.len = save_len;\n\t\ts->full = 1;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_ssid_enabled",
          "args": [
            "i"
          ],
          "line": 702
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_ssid_enabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "234-240",
          "snippet": "bool cgroup_ssid_enabled(int ssid)\n{\n\tif (CGROUP_SUBSYS_COUNT == 0)\n\t\treturn false;\n\n\treturn static_key_enabled(cgroup_subsys_enabled_key[ssid]);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct static_key_true *cgroup_subsys_enabled_key[] = {\n#include <linux/cgroup_subsys.h>\n};"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic struct static_key_true *cgroup_subsys_enabled_key[] = {\n#include <linux/cgroup_subsys.h>\n};\n\nbool cgroup_ssid_enabled(int ssid)\n{\n\tif (CGROUP_SUBSYS_COUNT == 0)\n\t\treturn false;\n\n\treturn static_key_enabled(cgroup_subsys_enabled_key[ssid]);\n}"
        }
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&ss->root->nr_cgrps"
          ],
          "line": 701
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "for_each_subsys",
          "args": [
            "ss",
            "i"
          ],
          "line": 698
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 696
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_puts",
          "args": [
            "m",
            "\"#subsys_name\\thierarchy\\tnum_cgroups\\tenabled\\n\""
          ],
          "line": 690
        },
        "resolved": true,
        "details": {
          "function_name": "trace_seq_puts",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_seq.c",
          "lines": "208-223",
          "snippet": "void trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}",
          "includes": [
            "#include <linux/trace_seq.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/uaccess.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_seq.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\nvoid trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nint proc_cgroupstats_show(struct seq_file *m, void *v)\n{\n\tstruct cgroup_subsys *ss;\n\tint i;\n\n\tseq_puts(m, \"#subsys_name\\thierarchy\\tnum_cgroups\\tenabled\\n\");\n\t/*\n\t * ideally we don't want subsystems moving around while we do this.\n\t * cgroup_mutex is also necessary to guarantee an atomic snapshot of\n\t * subsys/hierarchy state.\n\t */\n\tmutex_lock(&cgroup_mutex);\n\n\tfor_each_subsys(ss, i)\n\t\tseq_printf(m, \"%s\\t%d\\t%d\\t%d\\n\",\n\t\t\t   ss->legacy_name, ss->root->hierarchy_id,\n\t\t\t   atomic_read(&ss->root->nr_cgrps),\n\t\t\t   cgroup_ssid_enabled(i));\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup_clone_children_write",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "629-637",
    "snippet": "static int cgroup_clone_children_write(struct cgroup_subsys_state *css,\n\t\t\t\t       struct cftype *cft, u64 val)\n{\n\tif (val)\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);\n\telse\n\t\tclear_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "clear_bit",
          "args": [
            "CGRP_CPUSET_CLONE_CHILDREN",
            "&css->cgroup->flags"
          ],
          "line": 635
        },
        "resolved": true,
        "details": {
          "function_name": "memory_bm_clear_bit",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/power/snapshot.c",
          "lines": "792-801",
          "snippet": "static void memory_bm_clear_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\tclear_bit(bit, addr);\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <asm/io.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/ktime.h>",
            "#include <linux/compiler.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/highmem.h>",
            "#include <linux/console.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/nmi.h>",
            "#include <linux/memblock.h>",
            "#include <linux/init.h>",
            "#include <linux/device.h>",
            "#include <linux/pm.h>",
            "#include <linux/kernel.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bitops.h>",
            "#include <linux/delay.h>",
            "#include <linux/suspend.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/version.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <asm/io.h>\n#include <asm/tlbflush.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <linux/set_memory.h>\n#include <linux/ktime.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/syscalls.h>\n#include <linux/nmi.h>\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/pm.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/suspend.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/version.h>\n\nstatic void memory_bm_clear_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\tclear_bit(bit, addr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_bit",
          "args": [
            "CGRP_CPUSET_CLONE_CHILDREN",
            "&css->cgroup->flags"
          ],
          "line": 633
        },
        "resolved": true,
        "details": {
          "function_name": "mem_bm_set_bit_check",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/power/snapshot.c",
          "lines": "779-790",
          "snippet": "static int mem_bm_set_bit_check(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tif (!error)\n\t\tset_bit(bit, addr);\n\n\treturn error;\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <asm/io.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/ktime.h>",
            "#include <linux/compiler.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/highmem.h>",
            "#include <linux/console.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/nmi.h>",
            "#include <linux/memblock.h>",
            "#include <linux/init.h>",
            "#include <linux/device.h>",
            "#include <linux/pm.h>",
            "#include <linux/kernel.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bitops.h>",
            "#include <linux/delay.h>",
            "#include <linux/suspend.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/version.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <asm/io.h>\n#include <asm/tlbflush.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <linux/set_memory.h>\n#include <linux/ktime.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/syscalls.h>\n#include <linux/nmi.h>\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/pm.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/suspend.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/version.h>\n\nstatic int mem_bm_set_bit_check(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tif (!error)\n\t\tset_bit(bit, addr);\n\n\treturn error;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_clone_children_write(struct cgroup_subsys_state *css,\n\t\t\t\t       struct cftype *cft, u64 val)\n{\n\tif (val)\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);\n\telse\n\t\tclear_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup_clone_children_read",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "623-627",
    "snippet": "static u64 cgroup_clone_children_read(struct cgroup_subsys_state *css,\n\t\t\t\t      struct cftype *cft)\n{\n\treturn test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "test_bit",
          "args": [
            "CGRP_CPUSET_CLONE_CHILDREN",
            "&css->cgroup->flags"
          ],
          "line": 626
        },
        "resolved": true,
        "details": {
          "function_name": "memory_bm_test_bit",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/power/snapshot.c",
          "lines": "811-820",
          "snippet": "static int memory_bm_test_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\treturn test_bit(bit, addr);\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <asm/io.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/ktime.h>",
            "#include <linux/compiler.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/highmem.h>",
            "#include <linux/console.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/nmi.h>",
            "#include <linux/memblock.h>",
            "#include <linux/init.h>",
            "#include <linux/device.h>",
            "#include <linux/pm.h>",
            "#include <linux/kernel.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bitops.h>",
            "#include <linux/delay.h>",
            "#include <linux/suspend.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/version.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <asm/io.h>\n#include <asm/tlbflush.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <linux/set_memory.h>\n#include <linux/ktime.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/syscalls.h>\n#include <linux/nmi.h>\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/pm.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/suspend.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/version.h>\n\nstatic int memory_bm_test_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\treturn test_bit(bit, addr);\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic u64 cgroup_clone_children_read(struct cgroup_subsys_state *css,\n\t\t\t\t      struct cftype *cft)\n{\n\treturn test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);\n}"
  },
  {
    "function_name": "cgroup_write_notify_on_release",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "613-621",
    "snippet": "static int cgroup_write_notify_on_release(struct cgroup_subsys_state *css,\n\t\t\t\t\t  struct cftype *cft, u64 val)\n{\n\tif (val)\n\t\tset_bit(CGRP_NOTIFY_ON_RELEASE, &css->cgroup->flags);\n\telse\n\t\tclear_bit(CGRP_NOTIFY_ON_RELEASE, &css->cgroup->flags);\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "clear_bit",
          "args": [
            "CGRP_NOTIFY_ON_RELEASE",
            "&css->cgroup->flags"
          ],
          "line": 619
        },
        "resolved": true,
        "details": {
          "function_name": "memory_bm_clear_bit",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/power/snapshot.c",
          "lines": "792-801",
          "snippet": "static void memory_bm_clear_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\tclear_bit(bit, addr);\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <asm/io.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/ktime.h>",
            "#include <linux/compiler.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/highmem.h>",
            "#include <linux/console.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/nmi.h>",
            "#include <linux/memblock.h>",
            "#include <linux/init.h>",
            "#include <linux/device.h>",
            "#include <linux/pm.h>",
            "#include <linux/kernel.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bitops.h>",
            "#include <linux/delay.h>",
            "#include <linux/suspend.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/version.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <asm/io.h>\n#include <asm/tlbflush.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <linux/set_memory.h>\n#include <linux/ktime.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/syscalls.h>\n#include <linux/nmi.h>\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/pm.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/suspend.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/version.h>\n\nstatic void memory_bm_clear_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\tclear_bit(bit, addr);\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_bit",
          "args": [
            "CGRP_NOTIFY_ON_RELEASE",
            "&css->cgroup->flags"
          ],
          "line": 617
        },
        "resolved": true,
        "details": {
          "function_name": "mem_bm_set_bit_check",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/power/snapshot.c",
          "lines": "779-790",
          "snippet": "static int mem_bm_set_bit_check(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tif (!error)\n\t\tset_bit(bit, addr);\n\n\treturn error;\n}",
          "includes": [
            "#include \"power.h\"",
            "#include <asm/io.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/set_memory.h>",
            "#include <linux/ktime.h>",
            "#include <linux/compiler.h>",
            "#include <linux/slab.h>",
            "#include <linux/list.h>",
            "#include <linux/highmem.h>",
            "#include <linux/console.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/nmi.h>",
            "#include <linux/memblock.h>",
            "#include <linux/init.h>",
            "#include <linux/device.h>",
            "#include <linux/pm.h>",
            "#include <linux/kernel.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bitops.h>",
            "#include <linux/delay.h>",
            "#include <linux/suspend.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/version.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"power.h\"\n#include <asm/io.h>\n#include <asm/tlbflush.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <linux/set_memory.h>\n#include <linux/ktime.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/console.h>\n#include <linux/syscalls.h>\n#include <linux/nmi.h>\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/pm.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/suspend.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/version.h>\n\nstatic int mem_bm_set_bit_check(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tif (!error)\n\t\tset_bit(bit, addr);\n\n\treturn error;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_write_notify_on_release(struct cgroup_subsys_state *css,\n\t\t\t\t\t  struct cftype *cft, u64 val)\n{\n\tif (val)\n\t\tset_bit(CGRP_NOTIFY_ON_RELEASE, &css->cgroup->flags);\n\telse\n\t\tclear_bit(CGRP_NOTIFY_ON_RELEASE, &css->cgroup->flags);\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup_read_notify_on_release",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "607-611",
    "snippet": "static u64 cgroup_read_notify_on_release(struct cgroup_subsys_state *css,\n\t\t\t\t\t struct cftype *cft)\n{\n\treturn notify_on_release(css->cgroup);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "notify_on_release",
          "args": [
            "css->cgroup"
          ],
          "line": 610
        },
        "resolved": true,
        "details": {
          "function_name": "notify_on_release",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-internal.h",
          "lines": "152-155",
          "snippet": "static inline bool notify_on_release(const struct cgroup *cgrp)\n{\n\treturn test_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n}",
          "includes": [
            "#include <linux/refcount.h>",
            "#include <linux/list.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/kernfs.h>",
            "#include <linux/cgroup.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/refcount.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/kernfs.h>\n#include <linux/cgroup.h>\n\nstatic inline bool notify_on_release(const struct cgroup *cgrp)\n{\n\treturn test_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic u64 cgroup_read_notify_on_release(struct cgroup_subsys_state *css,\n\t\t\t\t\t struct cftype *cft)\n{\n\treturn notify_on_release(css->cgroup);\n}"
  },
  {
    "function_name": "cgroup_sane_behavior_show",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "601-605",
    "snippet": "static int cgroup_sane_behavior_show(struct seq_file *seq, void *v)\n{\n\tseq_puts(seq, \"0\\n\");\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "seq_puts",
          "args": [
            "seq",
            "\"0\\n\""
          ],
          "line": 603
        },
        "resolved": true,
        "details": {
          "function_name": "trace_seq_puts",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_seq.c",
          "lines": "208-223",
          "snippet": "void trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}",
          "includes": [
            "#include <linux/trace_seq.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/uaccess.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_seq.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\nvoid trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_sane_behavior_show(struct seq_file *seq, void *v)\n{\n\tseq_puts(seq, \"0\\n\");\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup_release_agent_show",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "590-599",
    "snippet": "static int cgroup_release_agent_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tspin_lock(&release_agent_path_lock);\n\tseq_puts(seq, cgrp->root->release_agent_path);\n\tspin_unlock(&release_agent_path_lock);\n\tseq_putc(seq, '\\n');\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static DEFINE_SPINLOCK(release_agent_path_lock);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "seq_putc",
          "args": [
            "seq",
            "'\\n'"
          ],
          "line": 597
        },
        "resolved": true,
        "details": {
          "function_name": "trace_seq_putc",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_seq.c",
          "lines": "236-249",
          "snippet": "void trace_seq_putc(struct trace_seq *s, unsigned char c)\n{\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (TRACE_SEQ_BUF_LEFT(s) < 1) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putc(&s->seq, c);\n}",
          "includes": [
            "#include <linux/trace_seq.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/uaccess.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_seq.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\nvoid trace_seq_putc(struct trace_seq *s, unsigned char c)\n{\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (TRACE_SEQ_BUF_LEFT(s) < 1) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putc(&s->seq, c);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 596
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_queued_spin_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "546-560",
          "snippet": "__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_puts",
          "args": [
            "seq",
            "cgrp->root->release_agent_path"
          ],
          "line": 595
        },
        "resolved": true,
        "details": {
          "function_name": "trace_seq_puts",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_seq.c",
          "lines": "208-223",
          "snippet": "void trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}",
          "includes": [
            "#include <linux/trace_seq.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/uaccess.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_seq.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\nvoid trace_seq_puts(struct trace_seq *s, const char *str)\n{\n\tunsigned int len = strlen(str);\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tif (len > TRACE_SEQ_BUF_LEFT(s)) {\n\t\ts->full = 1;\n\t\treturn;\n\t}\n\n\tseq_buf_putmem(&s->seq, str, len);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 594
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_css",
          "args": [
            "seq"
          ],
          "line": 592
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic DEFINE_SPINLOCK(release_agent_path_lock);\n\nstatic int cgroup_release_agent_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tspin_lock(&release_agent_path_lock);\n\tseq_puts(seq, cgrp->root->release_agent_path);\n\tspin_unlock(&release_agent_path_lock);\n\tseq_putc(seq, '\\n');\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup_release_agent_write",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "572-588",
    "snippet": "static ssize_t cgroup_release_agent_write(struct kernfs_open_file *of,\n\t\t\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\n\tBUILD_BUG_ON(sizeof(cgrp->root->release_agent_path) < PATH_MAX);\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\tspin_lock(&release_agent_path_lock);\n\tstrlcpy(cgrp->root->release_agent_path, strstrip(buf),\n\t\tsizeof(cgrp->root->release_agent_path));\n\tspin_unlock(&release_agent_path_lock);\n\tcgroup_kn_unlock(of->kn);\n\treturn nbytes;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static DEFINE_SPINLOCK(release_agent_path_lock);"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "cgroup_kn_unlock",
          "args": [
            "of->kn"
          ],
          "line": 586
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_kn_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1488-1501",
          "snippet": "void cgroup_kn_unlock(struct kernfs_node *kn)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tkernfs_unbreak_active_protection(kn);\n\tcgroup_put(cgrp);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nvoid cgroup_kn_unlock(struct kernfs_node *kn)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tkernfs_unbreak_active_protection(kn);\n\tcgroup_put(cgrp);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 585
        },
        "resolved": true,
        "details": {
          "function_name": "__pv_queued_spin_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/qspinlock_paravirt.h",
          "lines": "546-560",
          "snippet": "__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}",
          "includes": [
            "#include <asm/qspinlock_paravirt.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/memblock.h>",
            "#include <linux/hash.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/qspinlock_paravirt.h>\n#include <linux/debug_locks.h>\n#include <linux/memblock.h>\n#include <linux/hash.h>\n\n__visible void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t/*\n\t * We must not unlock if SLOW, because in that case we must first\n\t * unhash. Otherwise it would be possible to have multiple @lock\n\t * entries, which would be BAD.\n\t */\n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}"
        }
      },
      {
        "call_info": {
          "callee": "strlcpy",
          "args": [
            "cgrp->root->release_agent_path",
            "strstrip(buf)",
            "sizeof(cgrp->root->release_agent_path)"
          ],
          "line": 583
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "strstrip",
          "args": [
            "buf"
          ],
          "line": 583
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_lock",
          "args": [
            "&release_agent_path_lock"
          ],
          "line": 582
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_kn_lock_live",
          "args": [
            "of->kn",
            "false"
          ],
          "line": 579
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_kn_lock_live",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1520-1549",
          "snippet": "struct cgroup *cgroup_kn_lock_live(struct kernfs_node *kn, bool drain_offline)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\t/*\n\t * We're gonna grab cgroup_mutex which nests outside kernfs\n\t * active_ref.  cgroup liveliness check alone provides enough\n\t * protection against removal.  Ensure @cgrp stays accessible and\n\t * break the active_ref protection.\n\t */\n\tif (!cgroup_tryget(cgrp))\n\t\treturn NULL;\n\tkernfs_break_active_protection(kn);\n\n\tif (drain_offline)\n\t\tcgroup_lock_and_drain_offline(cgrp);\n\telse\n\t\tmutex_lock(&cgroup_mutex);\n\n\tif (!cgroup_is_dead(cgrp))\n\t\treturn cgrp;\n\n\tcgroup_kn_unlock(kn);\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nstruct cgroup *cgroup_kn_lock_live(struct kernfs_node *kn, bool drain_offline)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\t/*\n\t * We're gonna grab cgroup_mutex which nests outside kernfs\n\t * active_ref.  cgroup liveliness check alone provides enough\n\t * protection against removal.  Ensure @cgrp stays accessible and\n\t * break the active_ref protection.\n\t */\n\tif (!cgroup_tryget(cgrp))\n\t\treturn NULL;\n\tkernfs_break_active_protection(kn);\n\n\tif (drain_offline)\n\t\tcgroup_lock_and_drain_offline(cgrp);\n\telse\n\t\tmutex_lock(&cgroup_mutex);\n\n\tif (!cgroup_is_dead(cgrp))\n\t\treturn cgrp;\n\n\tcgroup_kn_unlock(kn);\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "BUILD_BUG_ON",
          "args": [
            "sizeof(cgrp->root->release_agent_path) < PATH_MAX"
          ],
          "line": 577
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic DEFINE_SPINLOCK(release_agent_path_lock);\n\nstatic ssize_t cgroup_release_agent_write(struct kernfs_open_file *of,\n\t\t\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\n\tBUILD_BUG_ON(sizeof(cgrp->root->release_agent_path) < PATH_MAX);\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\tspin_lock(&release_agent_path_lock);\n\tstrlcpy(cgrp->root->release_agent_path, strstrip(buf),\n\t\tsizeof(cgrp->root->release_agent_path));\n\tspin_unlock(&release_agent_path_lock);\n\tcgroup_kn_unlock(of->kn);\n\treturn nbytes;\n}"
  },
  {
    "function_name": "cgroup1_tasks_write",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "566-570",
    "snippet": "static ssize_t cgroup1_tasks_write(struct kernfs_open_file *of,\n\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\treturn __cgroup1_procs_write(of, buf, nbytes, off, false);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__cgroup1_procs_write",
          "args": [
            "of",
            "buf",
            "nbytes",
            "off",
            "false"
          ],
          "line": 569
        },
        "resolved": true,
        "details": {
          "function_name": "__cgroup1_procs_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "518-558",
          "snippet": "static ssize_t __cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t     char *buf, size_t nbytes, loff_t off,\n\t\t\t\t     bool threadgroup)\n{\n\tstruct cgroup *cgrp;\n\tstruct task_struct *task;\n\tconst struct cred *cred, *tcred;\n\tssize_t ret;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, threadgroup);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Even if we're attaching all tasks in the thread group, we only\n\t * need to check permissions on one of them.\n\t */\n\tcred = current_cred();\n\ttcred = get_task_cred(task);\n\tif (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&\n\t    !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->euid, tcred->suid))\n\t\tret = -EACCES;\n\tput_cred(tcred);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(cgrp, task, threadgroup);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic ssize_t __cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t     char *buf, size_t nbytes, loff_t off,\n\t\t\t\t     bool threadgroup)\n{\n\tstruct cgroup *cgrp;\n\tstruct task_struct *task;\n\tconst struct cred *cred, *tcred;\n\tssize_t ret;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, threadgroup);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Even if we're attaching all tasks in the thread group, we only\n\t * need to check permissions on one of them.\n\t */\n\tcred = current_cred();\n\ttcred = get_task_cred(task);\n\tif (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&\n\t    !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->euid, tcred->suid))\n\t\tret = -EACCES;\n\tput_cred(tcred);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(cgrp, task, threadgroup);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic ssize_t cgroup1_tasks_write(struct kernfs_open_file *of,\n\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\treturn __cgroup1_procs_write(of, buf, nbytes, off, false);\n}"
  },
  {
    "function_name": "cgroup1_procs_write",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "560-564",
    "snippet": "static ssize_t cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\treturn __cgroup1_procs_write(of, buf, nbytes, off, true);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__cgroup1_procs_write",
          "args": [
            "of",
            "buf",
            "nbytes",
            "off",
            "true"
          ],
          "line": 563
        },
        "resolved": true,
        "details": {
          "function_name": "__cgroup1_procs_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "518-558",
          "snippet": "static ssize_t __cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t     char *buf, size_t nbytes, loff_t off,\n\t\t\t\t     bool threadgroup)\n{\n\tstruct cgroup *cgrp;\n\tstruct task_struct *task;\n\tconst struct cred *cred, *tcred;\n\tssize_t ret;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, threadgroup);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Even if we're attaching all tasks in the thread group, we only\n\t * need to check permissions on one of them.\n\t */\n\tcred = current_cred();\n\ttcred = get_task_cred(task);\n\tif (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&\n\t    !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->euid, tcred->suid))\n\t\tret = -EACCES;\n\tput_cred(tcred);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(cgrp, task, threadgroup);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic ssize_t __cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t     char *buf, size_t nbytes, loff_t off,\n\t\t\t\t     bool threadgroup)\n{\n\tstruct cgroup *cgrp;\n\tstruct task_struct *task;\n\tconst struct cred *cred, *tcred;\n\tssize_t ret;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, threadgroup);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Even if we're attaching all tasks in the thread group, we only\n\t * need to check permissions on one of them.\n\t */\n\tcred = current_cred();\n\ttcred = get_task_cred(task);\n\tif (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&\n\t    !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->euid, tcred->suid))\n\t\tret = -EACCES;\n\tput_cred(tcred);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(cgrp, task, threadgroup);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic ssize_t cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\treturn __cgroup1_procs_write(of, buf, nbytes, off, true);\n}"
  },
  {
    "function_name": "__cgroup1_procs_write",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "518-558",
    "snippet": "static ssize_t __cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t     char *buf, size_t nbytes, loff_t off,\n\t\t\t\t     bool threadgroup)\n{\n\tstruct cgroup *cgrp;\n\tstruct task_struct *task;\n\tconst struct cred *cred, *tcred;\n\tssize_t ret;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, threadgroup);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Even if we're attaching all tasks in the thread group, we only\n\t * need to check permissions on one of them.\n\t */\n\tcred = current_cred();\n\ttcred = get_task_cred(task);\n\tif (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&\n\t    !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->euid, tcred->suid))\n\t\tret = -EACCES;\n\tput_cred(tcred);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(cgrp, task, threadgroup);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cgroup_kn_unlock",
          "args": [
            "of->kn"
          ],
          "line": 555
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_kn_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1488-1501",
          "snippet": "void cgroup_kn_unlock(struct kernfs_node *kn)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tkernfs_unbreak_active_protection(kn);\n\tcgroup_put(cgrp);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nvoid cgroup_kn_unlock(struct kernfs_node *kn)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tkernfs_unbreak_active_protection(kn);\n\tcgroup_put(cgrp);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_procs_write_finish",
          "args": [
            "task"
          ],
          "line": 553
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_attach_task",
          "args": [
            "cgrp",
            "task",
            "threadgroup"
          ],
          "line": 550
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_attach_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2614-5068",
          "snippet": "int cgroup_attach_task(struct cgroup *dst_cgrp, struct task_struct *leader,\n\t\t       bool threadgroup)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct task_struct *task;\n\tint ret;\n\n\tret = cgroup_migrate_vet_dst(dst_cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/* look up all src csets */\n\tspin_lock_irq(&css_set_lock);\n\trcu_read_lock();\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_src(task_css_set(task), dst_cgrp, &mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\trcu_read_unlock();\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* prepare dst csets and commit */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (!ret)\n\t\tret = cgroup_migrate(leader, threadgroup, &mgctx);\n\n\tcgroup_migrate_finish(&mgctx);\n\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(attach_task, dst_cgrp, leader, threadgroup);\n\n\treturn ret;\n}\n\nstruct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup)\n\t__acquires(&cgroup_threadgroup_rwsem)\n{\n\tstruct task_struct *tsk;\n\tpid_t pid;\n\n\tif (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\trcu_read_lock();\n\tif (pid) {\n\t\ttsk = find_task_by_vpid(pid);\n\t\tif (!tsk) {\n\t\t\ttsk = ERR_PTR(-ESRCH);\n\t\t\tgoto out_unlock_threadgroup;\n\t\t}\n\t} else {\n\t\ttsk = current;\n\t}\n\n\tif (threadgroup)\n\t\ttsk = tsk->group_leader;\n\n\t/*\n\t * kthreads may acquire PF_NO_SETAFFINITY during initialization.\n\t * If userland migrates such a kthread to a non-root cgroup, it can\n\t * become trapped in a cpuset, or RT kthread may be born in a\n\t * cgroup with no rt_runtime allocated.  Just say no.\n\t */\n\tif (tsk->no_cgroup_migration || (tsk->flags & PF_NO_SETAFFINITY)) {\n\t\ttsk = ERR_PTR(-EINVAL);\n\t\tgoto out_unlock_threadgroup;\n\t}\n\n\tget_task_struct(tsk);\n\tgoto out_unlock_rcu;\n\nout_unlock_threadgroup:\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\nout_unlock_rcu:\n\trcu_read_unlock();\n\treturn tsk;\n}\n\nvoid cgroup_procs_write_finish(struct task_struct *task)\n\t__releases(&cgroup_threadgroup_rwsem)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\t/* release reference from cgroup_procs_write_start() */\n\tput_task_struct(task);\n\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tfor_each_subsys(ss, ssid)\n\t\tif (ss->post_attach)\n\t\t\tss->post_attach();\n}\n\nstatic void cgroup_print_ss_mask(struct seq_file *seq, u16 ss_mask)\n{\n\tstruct cgroup_subsys *ss;\n\tbool printed = false;\n\tint ssid;\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tif (printed)\n\t\t\tseq_putc(seq, ' ');\n\t\tseq_printf(seq, \"%s\", ss->name);\n\t\tprinted = true;\n\t} while_each_subsys_mask();\n\tif (printed)\n\t\tseq_putc(seq, '\\n');\n}\n\n/* show controllers which are enabled from the parent */\nstatic int cgroup_controllers_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgroup_control(cgrp));\n\treturn 0;\n}\n\n/* show controllers which are enabled for a given cgroup's children */\nstatic int cgroup_subtree_control_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgrp->subtree_control);\n\treturn 0;\n}\n\n/**\n * cgroup_update_dfl_csses - update css assoc of a subtree in default hierarchy\n * @cgrp: root of the subtree to update csses for\n *\n * @cgrp's control masks have changed and its subtree's css associations\n * need to be updated accordingly.  This function looks up all css_sets\n * which are attached to the subtree, creates the matching updated css_sets\n * and migrates the tasks to the new ones.\n */\nstatic int cgroup_update_dfl_csses(struct cgroup *cgrp)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup *dsct;\n\tstruct css_set *src_cset;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\t/* look up all csses currently attached to @cgrp's subtree */\n\tspin_lock_irq(&css_set_lock);\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tstruct cgrp_cset_link *link;\n\n\t\tlist_for_each_entry(link, &dsct->cset_links, cset_link)\n\t\t\tcgroup_migrate_add_src(link->cset, dsct, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* NULL dst indicates self on default hierarchy */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(src_cset, &mgctx.preloaded_src_csets, mg_preload_node) {\n\t\tstruct task_struct *task, *ntask;\n\n\t\t/* all tasks in src_csets need to be migrated */\n\t\tlist_for_each_entry_safe(task, ntask, &src_cset->tasks, cg_list)\n\t\t\tcgroup_migrate_add_task(task, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_migrate_execute(&mgctx);\nout_finish:\n\tcgroup_migrate_finish(&mgctx);\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\treturn ret;\n}\n\n/**\n * cgroup_lock_and_drain_offline - lock cgroup_mutex and drain offlined csses\n * @cgrp: root of the target subtree\n *\n * Because css offlining is asynchronous, userland may try to re-enable a\n * controller while the previous css is still around.  This function grabs\n * cgroup_mutex and drains the previous css instances of @cgrp's subtree.\n */\nvoid cgroup_lock_and_drain_offline(struct cgroup *cgrp)\n\t__acquires(&cgroup_mutex)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\nrestart:\n\tmutex_lock(&cgroup_mutex);\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\t\t\tDEFINE_WAIT(wait);\n\n\t\t\tif (!css || !percpu_ref_is_dying(&css->refcnt))\n\t\t\t\tcontinue;\n\n\t\t\tcgroup_get_live(dsct);\n\t\t\tprepare_to_wait(&dsct->offline_waitq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tschedule();\n\t\t\tfinish_wait(&dsct->offline_waitq, &wait);\n\n\t\t\tcgroup_put(dsct);\n\t\t\tgoto restart;\n\t\t}\n\t}\n}\n\n/**\n * cgroup_save_control - save control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Save ->subtree_control, ->subtree_ss_mask and ->dom_cgrp to the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_save_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->old_subtree_control = dsct->subtree_control;\n\t\tdsct->old_subtree_ss_mask = dsct->subtree_ss_mask;\n\t\tdsct->old_dom_cgrp = dsct->dom_cgrp;\n\t}\n}\n\n/**\n * cgroup_propagate_control - refresh control masks of a subtree\n * @cgrp: root of the target subtree\n *\n * For @cgrp and its subtree, ensure ->subtree_ss_mask matches\n * ->subtree_control and propagate controller availability through the\n * subtree so that descendants don't have unavailable controllers enabled.\n */\nstatic void cgroup_propagate_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control &= cgroup_control(dsct);\n\t\tdsct->subtree_ss_mask =\n\t\t\tcgroup_calc_subtree_ss_mask(dsct->subtree_control,\n\t\t\t\t\t\t    cgroup_ss_mask(dsct));\n\t}\n}\n\n/**\n * cgroup_restore_control - restore control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Restore ->subtree_control, ->subtree_ss_mask and ->dom_cgrp from the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_restore_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control = dsct->old_subtree_control;\n\t\tdsct->subtree_ss_mask = dsct->old_subtree_ss_mask;\n\t\tdsct->dom_cgrp = dsct->old_dom_cgrp;\n\t}\n}\n\nstatic bool css_visible(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tif (cgroup_control(cgrp) & (1 << ss->id))\n\t\treturn true;\n\tif (!(cgroup_ss_mask(cgrp) & (1 << ss->id)))\n\t\treturn false;\n\treturn cgroup_on_dfl(cgrp) && ss->implicit_on_dfl;\n}\n\n/**\n * cgroup_apply_control_enable - enable or show csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and create new csses or make the existing ones\n * visible.  A css is created invisible if it's being implicitly enabled\n * through dependency.  An invisible css is made visible when the userland\n * explicitly enables it.\n *\n * Returns 0 on success, -errno on failure.  On failure, csses which have\n * been processed already aren't cleaned up.  The caller is responsible for\n * cleaning up with cgroup_apply_control_disable().\n */\nstatic int cgroup_apply_control_enable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid, ret;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!(cgroup_ss_mask(dsct) & (1 << ss->id)))\n\t\t\t\tcontinue;\n\n\t\t\tif (!css) {\n\t\t\t\tcss = css_create(dsct, ss);\n\t\t\t\tif (IS_ERR(css))\n\t\t\t\t\treturn PTR_ERR(css);\n\t\t\t}\n\n\t\t\tif (css_visible(css)) {\n\t\t\t\tret = css_populate_dir(css);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_apply_control_disable - kill or hide csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and kill and hide csses so that they match\n * cgroup_ss_mask() and cgroup_visible_mask().\n *\n * A css is hidden when the userland requests it to be disabled while other\n * subsystems are still depending on it.  The css must not actively control\n * resources and be in the vanilla state if it's made visible again later.\n * Controllers which may be depended upon should provide ->css_reset() for\n * this purpose.\n */\nstatic void cgroup_apply_control_disable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!css)\n\t\t\t\tcontinue;\n\n\t\t\tif (css->parent &&\n\t\t\t    !(cgroup_ss_mask(dsct) & (1 << ss->id))) {\n\t\t\t\tkill_css(css);\n\t\t\t} else if (!css_visible(css)) {\n\t\t\t\tcss_clear_dir(css);\n\t\t\t\tif (ss->css_reset)\n\t\t\t\t\tss->css_reset(css);\n\t\t\t}\n\t\t}\n\t}\n}\n\n/**\n * cgroup_apply_control - apply control mask updates to the subtree\n * @cgrp: root of the target subtree\n *\n * subsystems can be enabled and disabled in a subtree using the following\n * steps.\n *\n * 1. Call cgroup_save_control() to stash the current state.\n * 2. Update ->subtree_control masks in the subtree as desired.\n * 3. Call cgroup_apply_control() to apply the changes.\n * 4. Optionally perform other related operations.\n * 5. Call cgroup_finalize_control() to finish up.\n *\n * This function implements step 3 and propagates the mask changes\n * throughout @cgrp's subtree, updates csses accordingly and perform\n * process migrations.\n */\nstatic int cgroup_apply_control(struct cgroup *cgrp)\n{\n\tint ret;\n\n\tcgroup_propagate_control(cgrp);\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * At this point, cgroup_e_css() results reflect the new csses\n\t * making the following cgroup_update_dfl_csses() properly update\n\t * css associations of all tasks in the subtree.\n\t */\n\tret = cgroup_update_dfl_csses(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\n/**\n * cgroup_finalize_control - finalize control mask update\n * @cgrp: root of the target subtree\n * @ret: the result of the update\n *\n * Finalize control mask update.  See cgroup_apply_control() for more info.\n */\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret)\n{\n\tif (ret) {\n\t\tcgroup_restore_control(cgrp);\n\t\tcgroup_propagate_control(cgrp);\n\t}\n\n\tcgroup_apply_control_disable(cgrp);\n}\n\nstatic int cgroup_vet_subtree_control_enable(struct cgroup *cgrp, u16 enable)\n{\n\tu16 domain_enable = enable & ~cgrp_dfl_threaded_ss_mask;\n\n\t/* if nothing is getting enabled, nothing to worry about */\n\tif (!enable)\n\t\treturn 0;\n\n\t/* can @cgrp host any resources? */\n\tif (!cgroup_is_valid_domain(cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/* mixables don't care */\n\tif (cgroup_is_mixable(cgrp))\n\t\treturn 0;\n\n\tif (domain_enable) {\n\t\t/* can't enable domain controllers inside a thread subtree */\n\t\tif (cgroup_is_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\t/*\n\t\t * Threaded controllers can handle internal competitions\n\t\t * and are always allowed inside a (prospective) thread\n\t\t * subtree.\n\t\t */\n\t\tif (cgroup_can_be_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * Controllers can't be enabled for a cgroup with tasks to avoid\n\t * child cgroups competing against tasks.\n\t */\n\tif (cgroup_has_tasks(cgrp))\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n/* change the enabled child controllers for a cgroup in the default hierarchy */\nstatic ssize_t cgroup_subtree_control_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tu16 enable = 0, disable = 0;\n\tstruct cgroup *cgrp, *child;\n\tstruct cgroup_subsys *ss;\n\tchar *tok;\n\tint ssid, ret;\n\n\t/*\n\t * Parse input - space separated list of subsystem names prefixed\n\t * with either + or -.\n\t */\n\tbuf = strstrip(buf);\n\twhile ((tok = strsep(&buf, \" \"))) {\n\t\tif (tok[0] == '\\0')\n\t\t\tcontinue;\n\t\tdo_each_subsys_mask(ss, ssid, ~cgrp_dfl_inhibit_ss_mask) {\n\t\t\tif (!cgroup_ssid_enabled(ssid) ||\n\t\t\t    strcmp(tok + 1, ss->name))\n\t\t\t\tcontinue;\n\n\t\t\tif (*tok == '+') {\n\t\t\t\tenable |= 1 << ssid;\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t} else if (*tok == '-') {\n\t\t\t\tdisable |= 1 << ssid;\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t} else {\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t} while_each_subsys_mask();\n\t\tif (ssid == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -EINVAL;\n\t}\n\n\tcgrp = cgroup_kn_lock_live(of->kn, true);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tfor_each_subsys(ss, ssid) {\n\t\tif (enable & (1 << ssid)) {\n\t\t\tif (cgrp->subtree_control & (1 << ssid)) {\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!(cgroup_control(cgrp) & (1 << ssid))) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else if (disable & (1 << ssid)) {\n\t\t\tif (!(cgrp->subtree_control & (1 << ssid))) {\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* a child has it enabled? */\n\t\t\tcgroup_for_each_live_child(child, cgrp) {\n\t\t\t\tif (child->subtree_control & (1 << ssid)) {\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!enable && !disable) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tret = cgroup_vet_subtree_control_enable(cgrp, enable);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* save and update control masks and prepare csses */\n\tcgroup_save_control(cgrp);\n\n\tcgrp->subtree_control |= enable;\n\tcgrp->subtree_control &= ~disable;\n\n\tret = cgroup_apply_control(cgrp);\n\tcgroup_finalize_control(cgrp, ret);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tkernfs_activate(cgrp->kn);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\n/**\n * cgroup_enable_threaded - make @cgrp threaded\n * @cgrp: the target cgroup\n *\n * Called when \"threaded\" is written to the cgroup.type interface file and\n * tries to make @cgrp threaded and join the parent's resource domain.\n * This function is never called on the root cgroup as cgroup.type doesn't\n * exist on it.\n */\nstatic int cgroup_enable_threaded(struct cgroup *cgrp)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup *dom_cgrp = parent->dom_cgrp;\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* noop if already threaded */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn 0;\n\n\t/*\n\t * If @cgroup is populated or has domain controllers enabled, it\n\t * can't be switched.  While the below cgroup_can_be_thread_root()\n\t * test can catch the same conditions, that's only when @parent is\n\t * not mixable, so let's check it explicitly.\n\t */\n\tif (cgroup_is_populated(cgrp) ||\n\t    cgrp->subtree_control & ~cgrp_dfl_threaded_ss_mask)\n\t\treturn -EOPNOTSUPP;\n\n\t/* we're joining the parent's domain, ensure its validity */\n\tif (!cgroup_is_valid_domain(dom_cgrp) ||\n\t    !cgroup_can_be_thread_root(dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * The following shouldn't cause actual migrations and should\n\t * always succeed.\n\t */\n\tcgroup_save_control(cgrp);\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp)\n\t\tif (dsct == cgrp || cgroup_is_threaded(dsct))\n\t\t\tdsct->dom_cgrp = dom_cgrp;\n\n\tret = cgroup_apply_control(cgrp);\n\tif (!ret)\n\t\tparent->nr_threaded_children++;\n\n\tcgroup_finalize_control(cgrp, ret);\n\treturn ret;\n}\n\nstatic int cgroup_type_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tif (cgroup_is_threaded(cgrp))\n\t\tseq_puts(seq, \"threaded\\n\");\n\telse if (!cgroup_is_valid_domain(cgrp))\n\t\tseq_puts(seq, \"domain invalid\\n\");\n\telse if (cgroup_is_thread_root(cgrp))\n\t\tseq_puts(seq, \"domain threaded\\n\");\n\telse\n\t\tseq_puts(seq, \"domain\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_type_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\t/* only switching to threaded mode is supported */\n\tif (strcmp(strstrip(buf), \"threaded\"))\n\t\treturn -EINVAL;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\t/* threaded can only be enabled */\n\tret = cgroup_enable_threaded(cgrp);\n\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\nstatic int cgroup_max_descendants_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint descendants = READ_ONCE(cgrp->max_descendants);\n\n\tif (descendants == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", descendants);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_descendants_write(struct kernfs_open_file *of,\n\t\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint descendants;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdescendants = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &descendants);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (descendants < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_descendants = descendants;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_max_depth_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint depth = READ_ONCE(cgrp->max_depth);\n\n\tif (depth == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", depth);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_depth_write(struct kernfs_open_file *of,\n\t\t\t\t      char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tssize_t ret;\n\tint depth;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdepth = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &depth);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (depth < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_depth = depth;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_events_show(struct seq_file *seq, void *v)\n{\n\tseq_printf(seq, \"populated %d\\n\",\n\t\t   cgroup_is_populated(seq_css(seq)->cgroup));\n\treturn 0;\n}\n\nstatic int cgroup_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgroup = seq_css(seq)->cgroup;\n\n\tseq_printf(seq, \"nr_descendants %d\\n\",\n\t\t   cgroup->nr_descendants);\n\tseq_printf(seq, \"nr_dying_descendants %d\\n\",\n\t\t   cgroup->nr_dying_descendants);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused cgroup_extra_stat_show(struct seq_file *seq,\n\t\t\t\t\t\t struct cgroup *cgrp, int ssid)\n{\n\tstruct cgroup_subsys *ss = cgroup_subsys[ssid];\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\tif (!ss->css_extra_stat_show)\n\t\treturn 0;\n\n\tcss = cgroup_tryget_css(cgrp, ss);\n\tif (!css)\n\t\treturn 0;\n\n\tret = ss->css_extra_stat_show(seq, css);\n\tcss_put(css);\n\treturn ret;\n}\n\nstatic int cpu_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup __maybe_unused *cgrp = seq_css(seq)->cgroup;\n\tint ret = 0;\n\n\tcgroup_base_stat_cputime_show(seq);\n#ifdef CONFIG_CGROUP_SCHED\n\tret = cgroup_extra_stat_show(seq, cgrp, cpu_cgrp_id);\n#endif\n\treturn ret;\n}\n\n#ifdef CONFIG_PSI\nstatic int cgroup_io_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_IO);\n}\nstatic int cgroup_memory_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_MEM);\n}\nstatic int cgroup_cpu_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_CPU);\n}\n#endif\n\nstatic int cgroup_file_open(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->open)\n\t\treturn cft->open(of);\n\treturn 0;\n}\n\nstatic void cgroup_file_release(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->release)\n\t\tcft->release(of);\n}\n\nstatic ssize_t cgroup_file_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *cgrp = of->kn->parent->priv;\n\tstruct cftype *cft = of->kn->priv;\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, disallow writes to\n\t * files in an non-init namespace root from inside the namespace\n\t * except for the files explicitly marked delegatable -\n\t * cgroup.procs and cgroup.subtree_control.\n\t */\n\tif ((cgrp->root->flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    !(cft->flags & CFTYPE_NS_DELEGATABLE) &&\n\t    ns != &init_cgroup_ns && ns->root_cset->dfl_cgrp == cgrp)\n\t\treturn -EPERM;\n\n\tif (cft->write)\n\t\treturn cft->write(of, buf, nbytes, off);\n\n\t/*\n\t * kernfs guarantees that a file isn't deleted with operations in\n\t * flight, which means that the matching css is and stays alive and\n\t * doesn't need to be pinned.  The RCU locking is not necessary\n\t * either.  It's just for the convenience of using cgroup_css().\n\t */\n\trcu_read_lock();\n\tcss = cgroup_css(cgrp, cft->ss);\n\trcu_read_unlock();\n\n\tif (cft->write_u64) {\n\t\tunsigned long long v;\n\t\tret = kstrtoull(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_u64(css, cft, v);\n\t} else if (cft->write_s64) {\n\t\tlong long v;\n\t\tret = kstrtoll(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_s64(css, cft, v);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_seqfile_start(struct seq_file *seq, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_start(seq, ppos);\n}\n\nstatic void *cgroup_seqfile_next(struct seq_file *seq, void *v, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_next(seq, v, ppos);\n}\n\nstatic void cgroup_seqfile_stop(struct seq_file *seq, void *v)\n{\n\tif (seq_cft(seq)->seq_stop)\n\t\tseq_cft(seq)->seq_stop(seq, v);\n}\n\nstatic int cgroup_seqfile_show(struct seq_file *m, void *arg)\n{\n\tstruct cftype *cft = seq_cft(m);\n\tstruct cgroup_subsys_state *css = seq_css(m);\n\n\tif (cft->seq_show)\n\t\treturn cft->seq_show(m, arg);\n\n\tif (cft->read_u64)\n\t\tseq_printf(m, \"%llu\\n\", cft->read_u64(css, cft));\n\telse if (cft->read_s64)\n\t\tseq_printf(m, \"%lld\\n\", cft->read_s64(css, cft));\n\telse\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic struct kernfs_ops cgroup_kf_single_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\nstatic struct kernfs_ops cgroup_kf_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_start\t\t= cgroup_seqfile_start,\n\t.seq_next\t\t= cgroup_seqfile_next,\n\t.seq_stop\t\t= cgroup_seqfile_stop,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\n/* set uid and gid of cgroup dirs and files to that of the creator */\nstatic int cgroup_kn_set_ugid(struct kernfs_node *kn)\n{\n\tstruct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,\n\t\t\t       .ia_uid = current_fsuid(),\n\t\t\t       .ia_gid = current_fsgid(), };\n\n\tif (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&\n\t    gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))\n\t\treturn 0;\n\n\treturn kernfs_setattr(kn, &iattr);\n}\n\nstatic void cgroup_file_notify_timer(struct timer_list *timer)\n{\n\tcgroup_file_notify(container_of(timer, struct cgroup_file,\n\t\t\t\t\tnotify_timer));\n}\n\nstatic int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,\n\t\t\t   struct cftype *cft)\n{\n\tchar name[CGROUP_FILE_NAME_MAX];\n\tstruct kernfs_node *kn;\n\tstruct lock_class_key *key = NULL;\n\tint ret;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tkey = &cft->lockdep_key;\n#endif\n\tkn = __kernfs_create_file(cgrp->kn, cgroup_file_name(cgrp, cft, name),\n\t\t\t\t  cgroup_file_mode(cft),\n\t\t\t\t  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID,\n\t\t\t\t  0, cft->kf_ops, cft,\n\t\t\t\t  NULL, key);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\tkernfs_remove(kn);\n\t\treturn ret;\n\t}\n\n\tif (cft->file_offset) {\n\t\tstruct cgroup_file *cfile = (void *)css + cft->file_offset;\n\n\t\ttimer_setup(&cfile->notify_timer, cgroup_file_notify_timer, 0);\n\n\t\tspin_lock_irq(&cgroup_file_kn_lock);\n\t\tcfile->kn = kn;\n\t\tspin_unlock_irq(&cgroup_file_kn_lock);\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_addrm_files - add or remove files to a cgroup directory\n * @css: the target css\n * @cgrp: the target cgroup (usually css->cgroup)\n * @cfts: array of cftypes to be added\n * @is_add: whether to add or remove\n *\n * Depending on @is_add, add or remove files defined by @cfts on @cgrp.\n * For removals, this function never fails.\n */\nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add)\n{\n\tstruct cftype *cft, *cft_end = NULL;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\nrestart:\n\tfor (cft = cfts; cft != cft_end && cft->name[0] != '\\0'; cft++) {\n\t\t/* does cft->flags tell us to skip this file on @cgrp? */\n\t\tif ((cft->flags & __CFTYPE_ONLY_ON_DFL) && !cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & __CFTYPE_NOT_ON_DFL) && cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_NOT_ON_ROOT) && !cgroup_parent(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_ONLY_ON_ROOT) && cgroup_parent(cgrp))\n\t\t\tcontinue;\n\n\t\tif (is_add) {\n\t\t\tret = cgroup_add_file(css, cgrp, cft);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"%s: failed to add %s, err=%d\\n\",\n\t\t\t\t\t__func__, cft->name, ret);\n\t\t\t\tcft_end = cft;\n\t\t\t\tis_add = false;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t} else {\n\t\t\tcgroup_rm_file(cgrp, cft);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int cgroup_apply_cftypes(struct cftype *cfts, bool is_add)\n{\n\tstruct cgroup_subsys *ss = cfts[0].ss;\n\tstruct cgroup *root = &ss->root->cgrp;\n\tstruct cgroup_subsys_state *css;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* add/rm files for all cgroups created before */\n\tcss_for_each_descendant_pre(css, cgroup_css(root, ss)) {\n\t\tstruct cgroup *cgrp = css->cgroup;\n\n\t\tif (!(css->flags & CSS_VISIBLE))\n\t\t\tcontinue;\n\n\t\tret = cgroup_addrm_files(css, cgrp, cfts, is_add);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (is_add && !ret)\n\t\tkernfs_activate(root->kn);\n\treturn ret;\n}\n\nstatic void cgroup_exit_cftypes(struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\t/* free copy for custom atomic_write_len, see init_cftypes() */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE)\n\t\t\tkfree(cft->kf_ops);\n\t\tcft->kf_ops = NULL;\n\t\tcft->ss = NULL;\n\n\t\t/* revert flags set by cgroup core while adding @cfts */\n\t\tcft->flags &= ~(__CFTYPE_ONLY_ON_DFL | __CFTYPE_NOT_ON_DFL);\n\t}\n}\n\nstatic int cgroup_init_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\tstruct kernfs_ops *kf_ops;\n\n\t\tWARN_ON(cft->ss || cft->kf_ops);\n\n\t\tif (cft->seq_start)\n\t\t\tkf_ops = &cgroup_kf_ops;\n\t\telse\n\t\t\tkf_ops = &cgroup_kf_single_ops;\n\n\t\t/*\n\t\t * Ugh... if @cft wants a custom max_write_len, we need to\n\t\t * make a copy of kf_ops to set its atomic_write_len.\n\t\t */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE) {\n\t\t\tkf_ops = kmemdup(kf_ops, sizeof(*kf_ops), GFP_KERNEL);\n\t\t\tif (!kf_ops) {\n\t\t\t\tcgroup_exit_cftypes(cfts);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tkf_ops->atomic_write_len = cft->max_write_len;\n\t\t}\n\n\t\tcft->kf_ops = kf_ops;\n\t\tcft->ss = ss;\n\t}\n\n\treturn 0;\n}\n\nstatic int cgroup_rm_cftypes_locked(struct cftype *cfts)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!cfts || !cfts[0].ss)\n\t\treturn -ENOENT;\n\n\tlist_del(&cfts->node);\n\tcgroup_apply_cftypes(cfts, false);\n\tcgroup_exit_cftypes(cfts);\n\treturn 0;\n}\n\n/**\n * cgroup_rm_cftypes - remove an array of cftypes from a subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Unregister @cfts.  Files described by @cfts are removed from all\n * existing cgroups and all future cgroups won't have them either.  This\n * function can be called anytime whether @cfts' subsys is attached or not.\n *\n * Returns 0 on successful unregistration, -ENOENT if @cfts is not\n * registered.\n */\nint cgroup_rm_cftypes(struct cftype *cfts)\n{\n\tint ret;\n\n\tmutex_lock(&cgroup_mutex);\n\tret = cgroup_rm_cftypes_locked(cfts);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_cftypes - add an array of cftypes to a subsystem\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Register @cfts to @ss.  Files described by @cfts are created for all\n * existing cgroups to which @ss is attached and all future cgroups will\n * have them too.  This function can be called anytime whether @ss is\n * attached or not.\n *\n * Returns 0 on successful registration, -errno on failure.  Note that this\n * function currently returns 0 as long as @cfts registration is successful\n * even if some file creation attempts on existing cgroups fail.\n */\nstatic int cgroup_add_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tint ret;\n\n\tif (!cgroup_ssid_enabled(ss->id))\n\t\treturn 0;\n\n\tif (!cfts || cfts[0].name[0] == '\\0')\n\t\treturn 0;\n\n\tret = cgroup_init_cftypes(ss, cfts);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tlist_add_tail(&cfts->node, &ss->cfts);\n\tret = cgroup_apply_cftypes(cfts, true);\n\tif (ret)\n\t\tcgroup_rm_cftypes_locked(cfts);\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_dfl_cftypes - add an array of cftypes for default hierarchy\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the default hierarchy.\n */\nint cgroup_add_dfl_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_ONLY_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_add_legacy_cftypes - add an array of cftypes for legacy hierarchies\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the legacy hierarchies.\n */\nint cgroup_add_legacy_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_NOT_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_file_notify - generate a file modified event for a cgroup_file\n * @cfile: target cgroup_file\n *\n * @cfile must have been obtained by setting cftype->file_offset.\n */\nvoid cgroup_file_notify(struct cgroup_file *cfile)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cgroup_file_kn_lock, flags);\n\tif (cfile->kn) {\n\t\tunsigned long last = cfile->notified_at;\n\t\tunsigned long next = last + CGROUP_FILE_NOTIFY_MIN_INTV;\n\n\t\tif (time_in_range(jiffies, last, next)) {\n\t\t\ttimer_reduce(&cfile->notify_timer, next);\n\t\t} else {\n\t\t\tkernfs_notify(cfile->kn);\n\t\t\tcfile->notified_at = jiffies;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&cgroup_file_kn_lock, flags);\n}\n\n/**\n * css_next_child - find the next child of a given css\n * @pos: the current position (%NULL to initiate traversal)\n * @parent: css whose children to walk\n *\n * This function returns the next child of @parent and should be called\n * under either cgroup_mutex or RCU read lock.  The only requirement is\n * that @parent and @pos are accessible.  The next sibling is guaranteed to\n * be returned regardless of their states.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *css_next_child(struct cgroup_subsys_state *pos,\n\t\t\t\t\t   struct cgroup_subsys_state *parent)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/*\n\t * @pos could already have been unlinked from the sibling list.\n\t * Once a cgroup is removed, its ->sibling.next is no longer\n\t * updated when its next sibling changes.  CSS_RELEASED is set when\n\t * @pos is taken off list, at which time its next pointer is valid,\n\t * and, as releases are serialized, the one pointed to by the next\n\t * pointer is guaranteed to not have started release yet.  This\n\t * implies that if we observe !CSS_RELEASED on @pos in this RCU\n\t * critical section, the one pointed to by its next pointer is\n\t * guaranteed to not have finished its RCU grace period even if we\n\t * have dropped rcu_read_lock() inbetween iterations.\n\t *\n\t * If @pos has CSS_RELEASED set, its next pointer can't be\n\t * dereferenced; however, as each css is given a monotonically\n\t * increasing unique serial number and always appended to the\n\t * sibling list, the next one can be found by walking the parent's\n\t * children until the first css with higher serial number than\n\t * @pos's.  While this path can be slower, it happens iff iteration\n\t * races against release and the race window is very small.\n\t */\n\tif (!pos) {\n\t\tnext = list_entry_rcu(parent->children.next, struct cgroup_subsys_state, sibling);\n\t} else if (likely(!(pos->flags & CSS_RELEASED))) {\n\t\tnext = list_entry_rcu(pos->sibling.next, struct cgroup_subsys_state, sibling);\n\t} else {\n\t\tlist_for_each_entry_rcu(next, &parent->children, sibling)\n\t\t\tif (next->serial_nr > pos->serial_nr)\n\t\t\t\tbreak;\n\t}\n\n\t/*\n\t * @next, if not pointing to the head, can be dereferenced and is\n\t * the next sibling.\n\t */\n\tif (&next->sibling != &parent->children)\n\t\treturn next;\n\treturn NULL;\n}\n\n/**\n * css_next_descendant_pre - find the next descendant for pre-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_pre().  Find the next descendant\n * to visit for pre-order traversal of @root's descendants.  @root is\n * included in the iteration and the first node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @root are accessible and @pos is a descendant of @root.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_pre(struct cgroup_subsys_state *pos,\n\t\t\tstruct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit @root */\n\tif (!pos)\n\t\treturn root;\n\n\t/* visit the first child if exists */\n\tnext = css_next_child(NULL, pos);\n\tif (next)\n\t\treturn next;\n\n\t/* no child, visit my or the closest ancestor's next sibling */\n\twhile (pos != root) {\n\t\tnext = css_next_child(pos, pos->parent);\n\t\tif (next)\n\t\t\treturn next;\n\t\tpos = pos->parent;\n\t}\n\n\treturn NULL;\n}\n\n/**\n * css_rightmost_descendant - return the rightmost descendant of a css\n * @pos: css of interest\n *\n * Return the rightmost descendant of @pos.  If there's no descendant, @pos\n * is returned.  This can be used during pre-order traversal to skip\n * subtree of @pos.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct rightmost descendant as\n * long as @pos is accessible.\n */\nstruct cgroup_subsys_state *\ncss_rightmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last, *tmp;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\tdo {\n\t\tlast = pos;\n\t\t/* ->prev isn't RCU safe, walk ->next till the end */\n\t\tpos = NULL;\n\t\tcss_for_each_child(tmp, last)\n\t\t\tpos = tmp;\n\t} while (pos);\n\n\treturn last;\n}\n\nstatic struct cgroup_subsys_state *\ncss_leftmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last;\n\n\tdo {\n\t\tlast = pos;\n\t\tpos = css_next_child(NULL, pos);\n\t} while (pos);\n\n\treturn last;\n}\n\n/**\n * css_next_descendant_post - find the next descendant for post-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_post().  Find the next descendant\n * to visit for post-order traversal of @root's descendants.  @root is\n * included in the iteration and the last node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @cgroup are accessible and @pos is a descendant of\n * @cgroup.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_post(struct cgroup_subsys_state *pos,\n\t\t\t struct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit leftmost descendant which may be @root */\n\tif (!pos)\n\t\treturn css_leftmost_descendant(root);\n\n\t/* if we visited @root, we're done */\n\tif (pos == root)\n\t\treturn NULL;\n\n\t/* if there's an unvisited sibling, visit its leftmost descendant */\n\tnext = css_next_child(pos, pos->parent);\n\tif (next)\n\t\treturn css_leftmost_descendant(next);\n\n\t/* no sibling left, visit parent */\n\treturn pos->parent;\n}\n\n/**\n * css_has_online_children - does a css have online children\n * @css: the target css\n *\n * Returns %true if @css has any online children; otherwise, %false.  This\n * function can be called from any context but the caller is responsible\n * for synchronizing against on/offlining as necessary.\n */\nbool css_has_online_children(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys_state *child;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tcss_for_each_child(child, css) {\n\t\tif (child->flags & CSS_ONLINE) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic struct css_set *css_task_iter_next_css_set(struct css_task_iter *it)\n{\n\tstruct list_head *l;\n\tstruct cgrp_cset_link *link;\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* find the next threaded cset */\n\tif (it->tcset_pos) {\n\t\tl = it->tcset_pos->next;\n\n\t\tif (l != it->tcset_head) {\n\t\t\tit->tcset_pos = l;\n\t\t\treturn container_of(l, struct css_set,\n\t\t\t\t\t    threaded_csets_node);\n\t\t}\n\n\t\tit->tcset_pos = NULL;\n\t}\n\n\t/* find the next cset */\n\tl = it->cset_pos;\n\tl = l->next;\n\tif (l == it->cset_head) {\n\t\tit->cset_pos = NULL;\n\t\treturn NULL;\n\t}\n\n\tif (it->ss) {\n\t\tcset = container_of(l, struct css_set, e_cset_node[it->ss->id]);\n\t} else {\n\t\tlink = list_entry(l, struct cgrp_cset_link, cset_link);\n\t\tcset = link->cset;\n\t}\n\n\tit->cset_pos = l;\n\n\t/* initialize threaded css_set walking */\n\tif (it->flags & CSS_TASK_ITER_THREADED) {\n\t\tif (it->cur_dcset)\n\t\t\tput_css_set_locked(it->cur_dcset);\n\t\tit->cur_dcset = cset;\n\t\tget_css_set(cset);\n\n\t\tit->tcset_head = &cset->threaded_csets;\n\t\tit->tcset_pos = &cset->threaded_csets;\n\t}\n\n\treturn cset;\n}\n\n/**\n * css_task_iter_advance_css_set - advance a task itererator to the next css_set\n * @it: the iterator to advance\n *\n * Advance @it to the next css_set to walk.\n */\nstatic void css_task_iter_advance_css_set(struct css_task_iter *it)\n{\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* Advance to the next non-empty css_set */\n\tdo {\n\t\tcset = css_task_iter_next_css_set(it);\n\t\tif (!cset) {\n\t\t\tit->task_pos = NULL;\n\t\t\treturn;\n\t\t}\n\t} while (!css_set_populated(cset));\n\n\tif (!list_empty(&cset->tasks))\n\t\tit->task_pos = cset->tasks.next;\n\telse\n\t\tit->task_pos = cset->mg_tasks.next;\n\n\tit->tasks_head = &cset->tasks;\n\tit->mg_tasks_head = &cset->mg_tasks;\n\n\t/*\n\t * We don't keep css_sets locked across iteration steps and thus\n\t * need to take steps to ensure that iteration can be resumed after\n\t * the lock is re-acquired.  Iteration is performed at two levels -\n\t * css_sets and tasks in them.\n\t *\n\t * Once created, a css_set never leaves its cgroup lists, so a\n\t * pinned css_set is guaranteed to stay put and we can resume\n\t * iteration afterwards.\n\t *\n\t * Tasks may leave @cset across iteration steps.  This is resolved\n\t * by registering each iterator with the css_set currently being\n\t * walked and making css_set_move_task() advance iterators whose\n\t * next task is leaving.\n\t */\n\tif (it->cur_cset) {\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t}\n\tget_css_set(cset);\n\tit->cur_cset = cset;\n\tlist_add(&it->iters_node, &cset->task_iters);\n}\n\nstatic void css_task_iter_advance(struct css_task_iter *it)\n{\n\tstruct list_head *next;\n\n\tlockdep_assert_held(&css_set_lock);\nrepeat:\n\t/*\n\t * Advance iterator to find next entry.  cset->tasks is consumed\n\t * first and then ->mg_tasks.  After ->mg_tasks, we move onto the\n\t * next cset.\n\t */\n\tnext = it->task_pos->next;\n\n\tif (next == it->tasks_head)\n\t\tnext = it->mg_tasks_head->next;\n\n\tif (next == it->mg_tasks_head)\n\t\tcss_task_iter_advance_css_set(it);\n\telse\n\t\tit->task_pos = next;\n\n\t/* if PROCS, skip over tasks which aren't group leaders */\n\tif ((it->flags & CSS_TASK_ITER_PROCS) && it->task_pos &&\n\t    !thread_group_leader(list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t    cg_list)))\n\t\tgoto repeat;\n}\n\n/**\n * css_task_iter_start - initiate task iteration\n * @css: the css to walk tasks of\n * @flags: CSS_TASK_ITER_* flags\n * @it: the task iterator to use\n *\n * Initiate iteration through the tasks of @css.  The caller can call\n * css_task_iter_next() to walk through the tasks until the function\n * returns NULL.  On completion of iteration, css_task_iter_end() must be\n * called.\n */\nvoid css_task_iter_start(struct cgroup_subsys_state *css, unsigned int flags,\n\t\t\t struct css_task_iter *it)\n{\n\t/* no one should try to iterate before mounting cgroups */\n\tWARN_ON_ONCE(!use_task_css_set_links);\n\n\tmemset(it, 0, sizeof(*it));\n\n\tspin_lock_irq(&css_set_lock);\n\n\tit->ss = css->ss;\n\tit->flags = flags;\n\n\tif (it->ss)\n\t\tit->cset_pos = &css->cgroup->e_csets[css->ss->id];\n\telse\n\t\tit->cset_pos = &css->cgroup->cset_links;\n\n\tit->cset_head = it->cset_pos;\n\n\tcss_task_iter_advance_css_set(it);\n\n\tspin_unlock_irq(&css_set_lock);\n}\n\n/**\n * css_task_iter_next - return the next task for the iterator\n * @it: the task iterator being iterated\n *\n * The \"next\" function for task iteration.  @it should have been\n * initialized via css_task_iter_start().  Returns NULL when the iteration\n * reaches the end.\n */\nstruct task_struct *css_task_iter_next(struct css_task_iter *it)\n{\n\tif (it->cur_task) {\n\t\tput_task_struct(it->cur_task);\n\t\tit->cur_task = NULL;\n\t}\n\n\tspin_lock_irq(&css_set_lock);\n\n\tif (it->task_pos) {\n\t\tit->cur_task = list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t  cg_list);\n\t\tget_task_struct(it->cur_task);\n\t\tcss_task_iter_advance(it);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn it->cur_task;\n}\n\n/**\n * css_task_iter_end - finish task iteration\n * @it: the task iterator to finish\n *\n * Finish task iteration started by css_task_iter_start().\n */\nvoid css_task_iter_end(struct css_task_iter *it)\n{\n\tif (it->cur_cset) {\n\t\tspin_lock_irq(&css_set_lock);\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t\tspin_unlock_irq(&css_set_lock);\n\t}\n\n\tif (it->cur_dcset)\n\t\tput_css_set(it->cur_dcset);\n\n\tif (it->cur_task)\n\t\tput_task_struct(it->cur_task);\n}\n\nstatic void cgroup_procs_release(struct kernfs_open_file *of)\n{\n\tif (of->priv) {\n\t\tcss_task_iter_end(of->priv);\n\t\tkfree(of->priv);\n\t}\n}\n\nstatic void *cgroup_procs_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct css_task_iter *it = of->priv;\n\n\treturn css_task_iter_next(it);\n}\n\nstatic void *__cgroup_procs_start(struct seq_file *s, loff_t *pos,\n\t\t\t\t  unsigned int iter_flags)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\tstruct css_task_iter *it = of->priv;\n\n\t/*\n\t * When a seq_file is seeked, it's always traversed sequentially\n\t * from position 0, so we can simply keep iterating on !0 *pos.\n\t */\n\tif (!it) {\n\t\tif (WARN_ON_ONCE((*pos)++))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tit = kzalloc(sizeof(*it), GFP_KERNEL);\n\t\tif (!it)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tof->priv = it;\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t} else if (!(*pos)++) {\n\t\tcss_task_iter_end(it);\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t}\n\n\treturn cgroup_procs_next(s, NULL, NULL);\n}\n\nstatic void *cgroup_procs_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\n\t/*\n\t * All processes of a threaded subtree belong to the domain cgroup\n\t * of the subtree.  Only threads can be distributed across the\n\t * subtree.  Reject reads on cgroup.procs in the subtree proper.\n\t * They're always empty anyway.\n\t */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn __cgroup_procs_start(s, pos, CSS_TASK_ITER_PROCS |\n\t\t\t\t\t    CSS_TASK_ITER_THREADED);\n}\n\nstatic int cgroup_procs_show(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", task_pid_vnr(v));\n\treturn 0;\n}\n\nstatic int cgroup_procs_write_permission(struct cgroup *src_cgrp,\n\t\t\t\t\t struct cgroup *dst_cgrp,\n\t\t\t\t\t struct super_block *sb)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *com_cgrp = src_cgrp;\n\tstruct inode *inode;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* find the common ancestor */\n\twhile (!cgroup_is_descendant(dst_cgrp, com_cgrp))\n\t\tcom_cgrp = cgroup_parent(com_cgrp);\n\n\t/* %current should be authorized to migrate to the common ancestor */\n\tinode = kernfs_get_inode(sb, com_cgrp->procs_file.kn);\n\tif (!inode)\n\t\treturn -ENOMEM;\n\n\tret = inode_permission(inode, MAY_WRITE);\n\tiput(inode);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, %current must be able\n\t * to see both source and destination cgroups from its namespace.\n\t */\n\tif ((cgrp_dfl_root.flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    (!cgroup_is_descendant(src_cgrp, ns->root_cset->dfl_cgrp) ||\n\t     !cgroup_is_descendant(dst_cgrp, ns->root_cset->dfl_cgrp)))\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_procs_write(struct kernfs_open_file *of,\n\t\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, true);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, true);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_threads_start(struct seq_file *s, loff_t *pos)\n{\n\treturn __cgroup_procs_start(s, pos, 0);\n}\n\nstatic ssize_t cgroup_threads_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, false);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* thread migrations follow the cgroup.procs delegation rule */\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\t/* and must be contained in the same domain */\n\tret = -EOPNOTSUPP;\n\tif (src_cgrp->dom_cgrp != dst_cgrp->dom_cgrp)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, false);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\n/* cgroup core interface files for the default hierarchy */\nstatic struct cftype cgroup_base_files[] = {\n\t{\n\t\t.name = \"cgroup.type\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_type_show,\n\t\t.write = cgroup_type_write,\n\t},\n\t{\n\t\t.name = \"cgroup.procs\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.file_offset = offsetof(struct cgroup, procs_file),\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_procs_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_procs_write,\n\t},\n\t{\n\t\t.name = \"cgroup.threads\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_threads_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_threads_write,\n\t},\n\t{\n\t\t.name = \"cgroup.controllers\",\n\t\t.seq_show = cgroup_controllers_show,\n\t},\n\t{\n\t\t.name = \"cgroup.subtree_control\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.seq_show = cgroup_subtree_control_show,\n\t\t.write = cgroup_subtree_control_write,\n\t},\n\t{\n\t\t.name = \"cgroup.events\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct cgroup, events_file),\n\t\t.seq_show = cgroup_events_show,\n\t},\n\t{\n\t\t.name = \"cgroup.max.descendants\",\n\t\t.seq_show = cgroup_max_descendants_show,\n\t\t.write = cgroup_max_descendants_write,\n\t},\n\t{\n\t\t.name = \"cgroup.max.depth\",\n\t\t.seq_show = cgroup_max_depth_show,\n\t\t.write = cgroup_max_depth_write,\n\t},\n\t{\n\t\t.name = \"cgroup.stat\",\n\t\t.seq_show = cgroup_stat_show,\n\t},\n\t{\n\t\t.name = \"cpu.stat\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_stat_show,\n\t},\n#ifdef CONFIG_PSI\n\t{\n\t\t.name = \"io.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_io_pressure_show,\n\t},\n\t{\n\t\t.name = \"memory.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_memory_pressure_show,\n\t},\n\t{\n\t\t.name = \"cpu.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_cpu_pressure_show,\n\t},\n#endif\n\t{ }\t/* terminate */\n};\n\n/*\n * css destruction is four-stage process.\n *\n * 1. Destruction starts.  Killing of the percpu_ref is initiated.\n *    Implemented in kill_css().\n *\n * 2. When the percpu_ref is confirmed to be visible as killed on all CPUs\n *    and thus css_tryget_online() is guaranteed to fail, the css can be\n *    offlined by invoking offline_css().  After offlining, the base ref is\n *    put.  Implemented in css_killed_work_fn().\n *\n * 3. When the percpu_ref reaches zero, the only possible remaining\n *    accessors are inside RCU read sections.  css_release() schedules the\n *    RCU callback.\n *\n * 4. After the grace period, the css can be freed.  Implemented in\n *    css_free_work_fn().\n *\n * It is actually hairier because both step 2 and 4 require process context\n * and thus involve punting to css->destroy_work adding two additional\n * steps to the already complex sequence.\n */\nstatic void css_free_rwork_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css = container_of(to_rcu_work(work),\n\t\t\t\tstruct cgroup_subsys_state, destroy_rwork);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tpercpu_ref_exit(&css->refcnt);\n\n\tif (ss) {\n\t\t/* css free path */\n\t\tstruct cgroup_subsys_state *parent = css->parent;\n\t\tint id = css->id;\n\n\t\tss->css_free(css);\n\t\tcgroup_idr_remove(&ss->css_idr, id);\n\t\tcgroup_put(cgrp);\n\n\t\tif (parent)\n\t\t\tcss_put(parent);\n\t} else {\n\t\t/* cgroup free path */\n\t\tatomic_dec(&cgrp->root->nr_cgrps);\n\t\tcgroup1_pidlist_destroy_all(cgrp);\n\t\tcancel_work_sync(&cgrp->release_agent_work);\n\n\t\tif (cgroup_parent(cgrp)) {\n\t\t\t/*\n\t\t\t * We get a ref to the parent, and put the ref when\n\t\t\t * this cgroup is being freed, so it's guaranteed\n\t\t\t * that the parent won't be destroyed before its\n\t\t\t * children.\n\t\t\t */\n\t\t\tcgroup_put(cgroup_parent(cgrp));\n\t\t\tkernfs_put(cgrp->kn);\n\t\t\tpsi_cgroup_free(cgrp);\n\t\t\tif (cgroup_on_dfl(cgrp))\n\t\t\t\tcgroup_rstat_exit(cgrp);\n\t\t\tkfree(cgrp);\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is root cgroup's refcnt reaching zero,\n\t\t\t * which indicates that the root should be\n\t\t\t * released.\n\t\t\t */\n\t\t\tcgroup_destroy_root(cgrp->root);\n\t\t}\n\t}\n}\n\nstatic void css_release_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tcss->flags |= CSS_RELEASED;\n\tlist_del_rcu(&css->sibling);\n\n\tif (ss) {\n\t\t/* css release path */\n\t\tif (!list_empty(&css->rstat_css_node)) {\n\t\t\tcgroup_rstat_flush(cgrp);\n\t\t\tlist_del_rcu(&css->rstat_css_node);\n\t\t}\n\n\t\tcgroup_idr_replace(&ss->css_idr, NULL, css->id);\n\t\tif (ss->css_released)\n\t\t\tss->css_released(css);\n\t} else {\n\t\tstruct cgroup *tcgrp;\n\n\t\t/* cgroup release path */\n\t\tTRACE_CGROUP_PATH(release, cgrp);\n\n\t\tif (cgroup_on_dfl(cgrp))\n\t\t\tcgroup_rstat_flush(cgrp);\n\n\t\tfor (tcgrp = cgroup_parent(cgrp); tcgrp;\n\t\t     tcgrp = cgroup_parent(tcgrp))\n\t\t\ttcgrp->nr_dying_descendants--;\n\n\t\tcgroup_idr_remove(&cgrp->root->cgroup_idr, cgrp->id);\n\t\tcgrp->id = -1;\n\n\t\t/*\n\t\t * There are two control paths which try to determine\n\t\t * cgroup from dentry without going through kernfs -\n\t\t * cgroupstats_build() and css_tryget_online_from_dir().\n\t\t * Those are supported by RCU protecting clearing of\n\t\t * cgrp->kn->priv backpointer.\n\t\t */\n\t\tif (cgrp->kn)\n\t\t\tRCU_INIT_POINTER(*(void __rcu __force **)&cgrp->kn->priv,\n\t\t\t\t\t NULL);\n\n\t\tcgroup_bpf_put(cgrp);\n\t}\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n}\n\nstatic void css_release(struct percpu_ref *ref)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(ref, struct cgroup_subsys_state, refcnt);\n\n\tINIT_WORK(&css->destroy_work, css_release_work_fn);\n\tqueue_work(cgroup_destroy_wq, &css->destroy_work);\n}\n\nstatic void init_and_link_css(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcgroup_get_live(cgrp);\n\n\tmemset(css, 0, sizeof(*css));\n\tcss->cgroup = cgrp;\n\tcss->ss = ss;\n\tcss->id = -1;\n\tINIT_LIST_HEAD(&css->sibling);\n\tINIT_LIST_HEAD(&css->children);\n\tINIT_LIST_HEAD(&css->rstat_css_node);\n\tcss->serial_nr = css_serial_nr_next++;\n\tatomic_set(&css->online_cnt, 0);\n\n\tif (cgroup_parent(cgrp)) {\n\t\tcss->parent = cgroup_css(cgroup_parent(cgrp), ss);\n\t\tcss_get(css->parent);\n\t}\n\n\tif (cgroup_on_dfl(cgrp) && ss->css_rstat_flush)\n\t\tlist_add_rcu(&css->rstat_css_node, &cgrp->rstat_css_list);\n\n\tBUG_ON(cgroup_css(cgrp, ss));\n}\n\n/* invoke ->css_online() on a new CSS and mark it online if successful */\nstatic int online_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (ss->css_online)\n\t\tret = ss->css_online(css);\n\tif (!ret) {\n\t\tcss->flags |= CSS_ONLINE;\n\t\trcu_assign_pointer(css->cgroup->subsys[ss->id], css);\n\n\t\tatomic_inc(&css->online_cnt);\n\t\tif (css->parent)\n\t\t\tatomic_inc(&css->parent->online_cnt);\n\t}\n\treturn ret;\n}\n\n/* if the CSS is online, invoke ->css_offline() on it and mark it offline */\nstatic void offline_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!(css->flags & CSS_ONLINE))\n\t\treturn;\n\n\tif (ss->css_offline)\n\t\tss->css_offline(css);\n\n\tcss->flags &= ~CSS_ONLINE;\n\tRCU_INIT_POINTER(css->cgroup->subsys[ss->id], NULL);\n\n\twake_up_all(&css->cgroup->offline_waitq);\n}\n\n/**\n * css_create - create a cgroup_subsys_state\n * @cgrp: the cgroup new css will be associated with\n * @ss: the subsys of new css\n *\n * Create a new css associated with @cgrp - @ss pair.  On success, the new\n * css is online and installed in @cgrp.  This function doesn't create the\n * interface files.  Returns 0 on success, -errno on failure.\n */\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup_subsys_state *parent_css = cgroup_css(parent, ss);\n\tstruct cgroup_subsys_state *css;\n\tint err;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcss = ss->css_alloc(parent_css);\n\tif (!css)\n\t\tcss = ERR_PTR(-ENOMEM);\n\tif (IS_ERR(css))\n\t\treturn css;\n\n\tinit_and_link_css(css, ss, cgrp);\n\n\terr = percpu_ref_init(&css->refcnt, css_release, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err_free_css;\n\n\terr = cgroup_idr_alloc(&ss->css_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (err < 0)\n\t\tgoto err_free_css;\n\tcss->id = err;\n\n\t/* @css is ready to be brought online now, make it visible */\n\tlist_add_tail_rcu(&css->sibling, &parent_css->children);\n\tcgroup_idr_replace(&ss->css_idr, css, css->id);\n\n\terr = online_css(css);\n\tif (err)\n\t\tgoto err_list_del;\n\n\tif (ss->broken_hierarchy && !ss->warned_broken_hierarchy &&\n\t    cgroup_parent(parent)) {\n\t\tpr_warn(\"%s (%d) created nested cgroup for controller \\\"%s\\\" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.\\n\",\n\t\t\tcurrent->comm, current->pid, ss->name);\n\t\tif (!strcmp(ss->name, \"memory\"))\n\t\t\tpr_warn(\"\\\"memory\\\" requires setting use_hierarchy to 1 on the root\\n\");\n\t\tss->warned_broken_hierarchy = true;\n\t}\n\n\treturn css;\n\nerr_list_del:\n\tlist_del_rcu(&css->sibling);\nerr_free_css:\n\tlist_del_rcu(&css->rstat_css_node);\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n\treturn ERR_PTR(err);\n}\n\n/*\n * The returned cgroup is fully initialized including its control mask, but\n * it isn't associated with its kernfs_node and doesn't have the control\n * mask applied.\n */\nstatic struct cgroup *cgroup_create(struct cgroup *parent)\n{\n\tstruct cgroup_root *root = parent->root;\n\tstruct cgroup *cgrp, *tcgrp;\n\tint level = parent->level + 1;\n\tint ret;\n\n\t/* allocate the cgroup and its ID, 0 is reserved for the root */\n\tcgrp = kzalloc(struct_size(cgrp, ancestor_ids, (level + 1)),\n\t\t       GFP_KERNEL);\n\tif (!cgrp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = percpu_ref_init(&cgrp->self.refcnt, css_release, 0, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_free_cgrp;\n\n\tif (cgroup_on_dfl(parent)) {\n\t\tret = cgroup_rstat_init(cgrp);\n\t\tif (ret)\n\t\t\tgoto out_cancel_ref;\n\t}\n\n\t/*\n\t * Temporarily set the pointer to NULL, so idr_find() won't return\n\t * a half-baked cgroup.\n\t */\n\tcgrp->id = cgroup_idr_alloc(&root->cgroup_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (cgrp->id < 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out_stat_exit;\n\t}\n\n\tinit_cgroup_housekeeping(cgrp);\n\n\tcgrp->self.parent = &parent->self;\n\tcgrp->root = root;\n\tcgrp->level = level;\n\n\tret = psi_cgroup_alloc(cgrp);\n\tif (ret)\n\t\tgoto out_idr_free;\n\n\tret = cgroup_bpf_inherit(cgrp);\n\tif (ret)\n\t\tgoto out_psi_free;\n\n\tfor (tcgrp = cgrp; tcgrp; tcgrp = cgroup_parent(tcgrp)) {\n\t\tcgrp->ancestor_ids[tcgrp->level] = tcgrp->id;\n\n\t\tif (tcgrp != cgrp)\n\t\t\ttcgrp->nr_descendants++;\n\t}\n\n\tif (notify_on_release(parent))\n\t\tset_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n\n\tif (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &parent->flags))\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags);\n\n\tcgrp->self.serial_nr = css_serial_nr_next++;\n\n\t/* allocation complete, commit to creation */\n\tlist_add_tail_rcu(&cgrp->self.sibling, &cgroup_parent(cgrp)->self.children);\n\tatomic_inc(&root->nr_cgrps);\n\tcgroup_get_live(parent);\n\n\t/*\n\t * @cgrp is now fully operational.  If something fails after this\n\t * point, it'll be released via the normal destruction path.\n\t */\n\tcgroup_idr_replace(&root->cgroup_idr, cgrp, cgrp->id);\n\n\t/*\n\t * On the default hierarchy, a child doesn't automatically inherit\n\t * subtree_control from the parent.  Each is configured manually.\n\t */\n\tif (!cgroup_on_dfl(cgrp))\n\t\tcgrp->subtree_control = cgroup_control(cgrp);\n\n\tcgroup_propagate_control(cgrp);\n\n\treturn cgrp;\n\nout_psi_free:\n\tpsi_cgroup_free(cgrp);\nout_idr_free:\n\tcgroup_idr_remove(&root->cgroup_idr, cgrp->id);\nout_stat_exit:\n\tif (cgroup_on_dfl(parent))\n\t\tcgroup_rstat_exit(cgrp);\nout_cancel_ref:\n\tpercpu_ref_exit(&cgrp->self.refcnt);\nout_free_cgrp:\n\tkfree(cgrp);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool cgroup_check_hierarchy_limits(struct cgroup *parent)\n{\n\tstruct cgroup *cgroup;\n\tint ret = false;\n\tint level = 1;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tfor (cgroup = parent; cgroup; cgroup = cgroup_parent(cgroup)) {\n\t\tif (cgroup->nr_descendants >= cgroup->max_descendants)\n\t\t\tgoto fail;\n\n\t\tif (level > cgroup->max_depth)\n\t\t\tgoto fail;\n\n\t\tlevel++;\n\t}\n\n\tret = true;\nfail:\n\treturn ret;\n}\n\nint cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)\n{\n\tstruct cgroup *parent, *cgrp;\n\tstruct kernfs_node *kn;\n\tint ret;\n\n\t/* do not accept '\\n' to prevent making /proc/<pid>/cgroup unparsable */\n\tif (strchr(name, '\\n'))\n\t\treturn -EINVAL;\n\n\tparent = cgroup_kn_lock_live(parent_kn, false);\n\tif (!parent)\n\t\treturn -ENODEV;\n\n\tif (!cgroup_check_hierarchy_limits(parent)) {\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tcgrp = cgroup_create(parent);\n\tif (IS_ERR(cgrp)) {\n\t\tret = PTR_ERR(cgrp);\n\t\tgoto out_unlock;\n\t}\n\n\t/* create the directory */\n\tkn = kernfs_create_dir(parent->kn, name, mode, cgrp);\n\tif (IS_ERR(kn)) {\n\t\tret = PTR_ERR(kn);\n\t\tgoto out_destroy;\n\t}\n\tcgrp->kn = kn;\n\n\t/*\n\t * This extra ref will be put in cgroup_free_fn() and guarantees\n\t * that @cgrp->kn is always accessible.\n\t */\n\tkernfs_get(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = css_populate_dir(&cgrp->self);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tTRACE_CGROUP_PATH(mkdir, cgrp);\n\n\t/* let's create and online css's */\n\tkernfs_activate(kn);\n\n\tret = 0;\n\tgoto out_unlock;\n\nout_destroy:\n\tcgroup_destroy_locked(cgrp);\nout_unlock:\n\tcgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\n/*\n * This is called when the refcnt of a css is confirmed to be killed.\n * css_tryget_online() is now guaranteed to fail.  Tell the subsystem to\n * initate destruction and put the css ref from kill_css().\n */\nstatic void css_killed_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\n\tmutex_lock(&cgroup_mutex);\n\n\tdo {\n\t\toffline_css(css);\n\t\tcss_put(css);\n\t\t/* @css can't go away while we're holding cgroup_mutex */\n\t\tcss = css->parent;\n\t} while (css && atomic_dec_and_test(&css->online_cnt));\n\n\tmutex_unlock(&cgroup_mutex);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [
            "#define CGROUP_FILE_NOTIFY_MIN_INTV\tDIV_ROUND_UP(HZ, 100)",
            "#define CGROUP_FILE_NAME_MAX\t\t(MAX_CGROUP_TYPE_NAMELEN +\t\\\n\t\t\t\t\t MAX_CFTYPE_NAME + 2)"
          ],
          "globals_used": [
            "static DEFINE_SPINLOCK(cgroup_file_kn_lock);",
            "struct percpu_rw_semaphore cgroup_threadgroup_rwsem;",
            "static struct workqueue_struct *cgroup_destroy_wq;",
            "struct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };",
            "static u16 cgrp_dfl_inhibit_ss_mask;",
            "static u16 cgrp_dfl_threaded_ss_mask;",
            "static u64 css_serial_nr_next = 1;",
            "struct cgroup_namespace init_cgroup_ns = {\n\t.count\t\t= REFCOUNT_INIT(2),\n\t.user_ns\t= &init_user_ns,\n\t.ns.ops\t\t= &cgroupns_operations,\n\t.ns.inum\t= PROC_CGROUP_INIT_INO,\n\t.root_cset\t= &init_css_set,\n};",
            "static struct cftype cgroup_base_files[];",
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static void cgroup_finalize_control(struct cgroup *cgrp, int ret);",
            "static void css_task_iter_advance(struct css_task_iter *it);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);",
            "static struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);",
            "static void css_release(struct percpu_ref *ref);",
            "static void kill_css(struct cgroup_subsys_state *css);",
            "static int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\n#define CGROUP_FILE_NOTIFY_MIN_INTV\tDIV_ROUND_UP(HZ, 100)\n#define CGROUP_FILE_NAME_MAX\t\t(MAX_CGROUP_TYPE_NAMELEN +\t\\\n\t\t\t\t\t MAX_CFTYPE_NAME + 2)\n\nstatic DEFINE_SPINLOCK(cgroup_file_kn_lock);\nstruct percpu_rw_semaphore cgroup_threadgroup_rwsem;\nstatic struct workqueue_struct *cgroup_destroy_wq;\nstruct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };\nstatic u16 cgrp_dfl_inhibit_ss_mask;\nstatic u16 cgrp_dfl_threaded_ss_mask;\nstatic u64 css_serial_nr_next = 1;\nstruct cgroup_namespace init_cgroup_ns = {\n\t.count\t\t= REFCOUNT_INIT(2),\n\t.user_ns\t= &init_user_ns,\n\t.ns.ops\t\t= &cgroupns_operations,\n\t.ns.inum\t= PROC_CGROUP_INIT_INO,\n\t.root_cset\t= &init_css_set,\n};\nstatic struct cftype cgroup_base_files[];\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret);\nstatic void css_task_iter_advance(struct css_task_iter *it);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);\nstatic void css_release(struct percpu_ref *ref);\nstatic void kill_css(struct cgroup_subsys_state *css);\nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add);\n\nint cgroup_attach_task(struct cgroup *dst_cgrp, struct task_struct *leader,\n\t\t       bool threadgroup)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct task_struct *task;\n\tint ret;\n\n\tret = cgroup_migrate_vet_dst(dst_cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/* look up all src csets */\n\tspin_lock_irq(&css_set_lock);\n\trcu_read_lock();\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_src(task_css_set(task), dst_cgrp, &mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\trcu_read_unlock();\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* prepare dst csets and commit */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (!ret)\n\t\tret = cgroup_migrate(leader, threadgroup, &mgctx);\n\n\tcgroup_migrate_finish(&mgctx);\n\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(attach_task, dst_cgrp, leader, threadgroup);\n\n\treturn ret;\n}\n\nstruct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup)\n\t__acquires(&cgroup_threadgroup_rwsem)\n{\n\tstruct task_struct *tsk;\n\tpid_t pid;\n\n\tif (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\trcu_read_lock();\n\tif (pid) {\n\t\ttsk = find_task_by_vpid(pid);\n\t\tif (!tsk) {\n\t\t\ttsk = ERR_PTR(-ESRCH);\n\t\t\tgoto out_unlock_threadgroup;\n\t\t}\n\t} else {\n\t\ttsk = current;\n\t}\n\n\tif (threadgroup)\n\t\ttsk = tsk->group_leader;\n\n\t/*\n\t * kthreads may acquire PF_NO_SETAFFINITY during initialization.\n\t * If userland migrates such a kthread to a non-root cgroup, it can\n\t * become trapped in a cpuset, or RT kthread may be born in a\n\t * cgroup with no rt_runtime allocated.  Just say no.\n\t */\n\tif (tsk->no_cgroup_migration || (tsk->flags & PF_NO_SETAFFINITY)) {\n\t\ttsk = ERR_PTR(-EINVAL);\n\t\tgoto out_unlock_threadgroup;\n\t}\n\n\tget_task_struct(tsk);\n\tgoto out_unlock_rcu;\n\nout_unlock_threadgroup:\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\nout_unlock_rcu:\n\trcu_read_unlock();\n\treturn tsk;\n}\n\nvoid cgroup_procs_write_finish(struct task_struct *task)\n\t__releases(&cgroup_threadgroup_rwsem)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\t/* release reference from cgroup_procs_write_start() */\n\tput_task_struct(task);\n\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tfor_each_subsys(ss, ssid)\n\t\tif (ss->post_attach)\n\t\t\tss->post_attach();\n}\n\nstatic void cgroup_print_ss_mask(struct seq_file *seq, u16 ss_mask)\n{\n\tstruct cgroup_subsys *ss;\n\tbool printed = false;\n\tint ssid;\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tif (printed)\n\t\t\tseq_putc(seq, ' ');\n\t\tseq_printf(seq, \"%s\", ss->name);\n\t\tprinted = true;\n\t} while_each_subsys_mask();\n\tif (printed)\n\t\tseq_putc(seq, '\\n');\n}\n\n/* show controllers which are enabled from the parent */\nstatic int cgroup_controllers_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgroup_control(cgrp));\n\treturn 0;\n}\n\n/* show controllers which are enabled for a given cgroup's children */\nstatic int cgroup_subtree_control_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgrp->subtree_control);\n\treturn 0;\n}\n\n/**\n * cgroup_update_dfl_csses - update css assoc of a subtree in default hierarchy\n * @cgrp: root of the subtree to update csses for\n *\n * @cgrp's control masks have changed and its subtree's css associations\n * need to be updated accordingly.  This function looks up all css_sets\n * which are attached to the subtree, creates the matching updated css_sets\n * and migrates the tasks to the new ones.\n */\nstatic int cgroup_update_dfl_csses(struct cgroup *cgrp)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup *dsct;\n\tstruct css_set *src_cset;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\t/* look up all csses currently attached to @cgrp's subtree */\n\tspin_lock_irq(&css_set_lock);\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tstruct cgrp_cset_link *link;\n\n\t\tlist_for_each_entry(link, &dsct->cset_links, cset_link)\n\t\t\tcgroup_migrate_add_src(link->cset, dsct, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* NULL dst indicates self on default hierarchy */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(src_cset, &mgctx.preloaded_src_csets, mg_preload_node) {\n\t\tstruct task_struct *task, *ntask;\n\n\t\t/* all tasks in src_csets need to be migrated */\n\t\tlist_for_each_entry_safe(task, ntask, &src_cset->tasks, cg_list)\n\t\t\tcgroup_migrate_add_task(task, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_migrate_execute(&mgctx);\nout_finish:\n\tcgroup_migrate_finish(&mgctx);\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\treturn ret;\n}\n\n/**\n * cgroup_lock_and_drain_offline - lock cgroup_mutex and drain offlined csses\n * @cgrp: root of the target subtree\n *\n * Because css offlining is asynchronous, userland may try to re-enable a\n * controller while the previous css is still around.  This function grabs\n * cgroup_mutex and drains the previous css instances of @cgrp's subtree.\n */\nvoid cgroup_lock_and_drain_offline(struct cgroup *cgrp)\n\t__acquires(&cgroup_mutex)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\nrestart:\n\tmutex_lock(&cgroup_mutex);\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\t\t\tDEFINE_WAIT(wait);\n\n\t\t\tif (!css || !percpu_ref_is_dying(&css->refcnt))\n\t\t\t\tcontinue;\n\n\t\t\tcgroup_get_live(dsct);\n\t\t\tprepare_to_wait(&dsct->offline_waitq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tschedule();\n\t\t\tfinish_wait(&dsct->offline_waitq, &wait);\n\n\t\t\tcgroup_put(dsct);\n\t\t\tgoto restart;\n\t\t}\n\t}\n}\n\n/**\n * cgroup_save_control - save control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Save ->subtree_control, ->subtree_ss_mask and ->dom_cgrp to the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_save_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->old_subtree_control = dsct->subtree_control;\n\t\tdsct->old_subtree_ss_mask = dsct->subtree_ss_mask;\n\t\tdsct->old_dom_cgrp = dsct->dom_cgrp;\n\t}\n}\n\n/**\n * cgroup_propagate_control - refresh control masks of a subtree\n * @cgrp: root of the target subtree\n *\n * For @cgrp and its subtree, ensure ->subtree_ss_mask matches\n * ->subtree_control and propagate controller availability through the\n * subtree so that descendants don't have unavailable controllers enabled.\n */\nstatic void cgroup_propagate_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control &= cgroup_control(dsct);\n\t\tdsct->subtree_ss_mask =\n\t\t\tcgroup_calc_subtree_ss_mask(dsct->subtree_control,\n\t\t\t\t\t\t    cgroup_ss_mask(dsct));\n\t}\n}\n\n/**\n * cgroup_restore_control - restore control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Restore ->subtree_control, ->subtree_ss_mask and ->dom_cgrp from the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_restore_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control = dsct->old_subtree_control;\n\t\tdsct->subtree_ss_mask = dsct->old_subtree_ss_mask;\n\t\tdsct->dom_cgrp = dsct->old_dom_cgrp;\n\t}\n}\n\nstatic bool css_visible(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tif (cgroup_control(cgrp) & (1 << ss->id))\n\t\treturn true;\n\tif (!(cgroup_ss_mask(cgrp) & (1 << ss->id)))\n\t\treturn false;\n\treturn cgroup_on_dfl(cgrp) && ss->implicit_on_dfl;\n}\n\n/**\n * cgroup_apply_control_enable - enable or show csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and create new csses or make the existing ones\n * visible.  A css is created invisible if it's being implicitly enabled\n * through dependency.  An invisible css is made visible when the userland\n * explicitly enables it.\n *\n * Returns 0 on success, -errno on failure.  On failure, csses which have\n * been processed already aren't cleaned up.  The caller is responsible for\n * cleaning up with cgroup_apply_control_disable().\n */\nstatic int cgroup_apply_control_enable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid, ret;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!(cgroup_ss_mask(dsct) & (1 << ss->id)))\n\t\t\t\tcontinue;\n\n\t\t\tif (!css) {\n\t\t\t\tcss = css_create(dsct, ss);\n\t\t\t\tif (IS_ERR(css))\n\t\t\t\t\treturn PTR_ERR(css);\n\t\t\t}\n\n\t\t\tif (css_visible(css)) {\n\t\t\t\tret = css_populate_dir(css);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_apply_control_disable - kill or hide csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and kill and hide csses so that they match\n * cgroup_ss_mask() and cgroup_visible_mask().\n *\n * A css is hidden when the userland requests it to be disabled while other\n * subsystems are still depending on it.  The css must not actively control\n * resources and be in the vanilla state if it's made visible again later.\n * Controllers which may be depended upon should provide ->css_reset() for\n * this purpose.\n */\nstatic void cgroup_apply_control_disable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!css)\n\t\t\t\tcontinue;\n\n\t\t\tif (css->parent &&\n\t\t\t    !(cgroup_ss_mask(dsct) & (1 << ss->id))) {\n\t\t\t\tkill_css(css);\n\t\t\t} else if (!css_visible(css)) {\n\t\t\t\tcss_clear_dir(css);\n\t\t\t\tif (ss->css_reset)\n\t\t\t\t\tss->css_reset(css);\n\t\t\t}\n\t\t}\n\t}\n}\n\n/**\n * cgroup_apply_control - apply control mask updates to the subtree\n * @cgrp: root of the target subtree\n *\n * subsystems can be enabled and disabled in a subtree using the following\n * steps.\n *\n * 1. Call cgroup_save_control() to stash the current state.\n * 2. Update ->subtree_control masks in the subtree as desired.\n * 3. Call cgroup_apply_control() to apply the changes.\n * 4. Optionally perform other related operations.\n * 5. Call cgroup_finalize_control() to finish up.\n *\n * This function implements step 3 and propagates the mask changes\n * throughout @cgrp's subtree, updates csses accordingly and perform\n * process migrations.\n */\nstatic int cgroup_apply_control(struct cgroup *cgrp)\n{\n\tint ret;\n\n\tcgroup_propagate_control(cgrp);\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * At this point, cgroup_e_css() results reflect the new csses\n\t * making the following cgroup_update_dfl_csses() properly update\n\t * css associations of all tasks in the subtree.\n\t */\n\tret = cgroup_update_dfl_csses(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\n/**\n * cgroup_finalize_control - finalize control mask update\n * @cgrp: root of the target subtree\n * @ret: the result of the update\n *\n * Finalize control mask update.  See cgroup_apply_control() for more info.\n */\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret)\n{\n\tif (ret) {\n\t\tcgroup_restore_control(cgrp);\n\t\tcgroup_propagate_control(cgrp);\n\t}\n\n\tcgroup_apply_control_disable(cgrp);\n}\n\nstatic int cgroup_vet_subtree_control_enable(struct cgroup *cgrp, u16 enable)\n{\n\tu16 domain_enable = enable & ~cgrp_dfl_threaded_ss_mask;\n\n\t/* if nothing is getting enabled, nothing to worry about */\n\tif (!enable)\n\t\treturn 0;\n\n\t/* can @cgrp host any resources? */\n\tif (!cgroup_is_valid_domain(cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/* mixables don't care */\n\tif (cgroup_is_mixable(cgrp))\n\t\treturn 0;\n\n\tif (domain_enable) {\n\t\t/* can't enable domain controllers inside a thread subtree */\n\t\tif (cgroup_is_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\t/*\n\t\t * Threaded controllers can handle internal competitions\n\t\t * and are always allowed inside a (prospective) thread\n\t\t * subtree.\n\t\t */\n\t\tif (cgroup_can_be_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * Controllers can't be enabled for a cgroup with tasks to avoid\n\t * child cgroups competing against tasks.\n\t */\n\tif (cgroup_has_tasks(cgrp))\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n/* change the enabled child controllers for a cgroup in the default hierarchy */\nstatic ssize_t cgroup_subtree_control_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tu16 enable = 0, disable = 0;\n\tstruct cgroup *cgrp, *child;\n\tstruct cgroup_subsys *ss;\n\tchar *tok;\n\tint ssid, ret;\n\n\t/*\n\t * Parse input - space separated list of subsystem names prefixed\n\t * with either + or -.\n\t */\n\tbuf = strstrip(buf);\n\twhile ((tok = strsep(&buf, \" \"))) {\n\t\tif (tok[0] == '\\0')\n\t\t\tcontinue;\n\t\tdo_each_subsys_mask(ss, ssid, ~cgrp_dfl_inhibit_ss_mask) {\n\t\t\tif (!cgroup_ssid_enabled(ssid) ||\n\t\t\t    strcmp(tok + 1, ss->name))\n\t\t\t\tcontinue;\n\n\t\t\tif (*tok == '+') {\n\t\t\t\tenable |= 1 << ssid;\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t} else if (*tok == '-') {\n\t\t\t\tdisable |= 1 << ssid;\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t} else {\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t} while_each_subsys_mask();\n\t\tif (ssid == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -EINVAL;\n\t}\n\n\tcgrp = cgroup_kn_lock_live(of->kn, true);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tfor_each_subsys(ss, ssid) {\n\t\tif (enable & (1 << ssid)) {\n\t\t\tif (cgrp->subtree_control & (1 << ssid)) {\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!(cgroup_control(cgrp) & (1 << ssid))) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else if (disable & (1 << ssid)) {\n\t\t\tif (!(cgrp->subtree_control & (1 << ssid))) {\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* a child has it enabled? */\n\t\t\tcgroup_for_each_live_child(child, cgrp) {\n\t\t\t\tif (child->subtree_control & (1 << ssid)) {\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!enable && !disable) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tret = cgroup_vet_subtree_control_enable(cgrp, enable);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* save and update control masks and prepare csses */\n\tcgroup_save_control(cgrp);\n\n\tcgrp->subtree_control |= enable;\n\tcgrp->subtree_control &= ~disable;\n\n\tret = cgroup_apply_control(cgrp);\n\tcgroup_finalize_control(cgrp, ret);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tkernfs_activate(cgrp->kn);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\n/**\n * cgroup_enable_threaded - make @cgrp threaded\n * @cgrp: the target cgroup\n *\n * Called when \"threaded\" is written to the cgroup.type interface file and\n * tries to make @cgrp threaded and join the parent's resource domain.\n * This function is never called on the root cgroup as cgroup.type doesn't\n * exist on it.\n */\nstatic int cgroup_enable_threaded(struct cgroup *cgrp)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup *dom_cgrp = parent->dom_cgrp;\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* noop if already threaded */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn 0;\n\n\t/*\n\t * If @cgroup is populated or has domain controllers enabled, it\n\t * can't be switched.  While the below cgroup_can_be_thread_root()\n\t * test can catch the same conditions, that's only when @parent is\n\t * not mixable, so let's check it explicitly.\n\t */\n\tif (cgroup_is_populated(cgrp) ||\n\t    cgrp->subtree_control & ~cgrp_dfl_threaded_ss_mask)\n\t\treturn -EOPNOTSUPP;\n\n\t/* we're joining the parent's domain, ensure its validity */\n\tif (!cgroup_is_valid_domain(dom_cgrp) ||\n\t    !cgroup_can_be_thread_root(dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * The following shouldn't cause actual migrations and should\n\t * always succeed.\n\t */\n\tcgroup_save_control(cgrp);\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp)\n\t\tif (dsct == cgrp || cgroup_is_threaded(dsct))\n\t\t\tdsct->dom_cgrp = dom_cgrp;\n\n\tret = cgroup_apply_control(cgrp);\n\tif (!ret)\n\t\tparent->nr_threaded_children++;\n\n\tcgroup_finalize_control(cgrp, ret);\n\treturn ret;\n}\n\nstatic int cgroup_type_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tif (cgroup_is_threaded(cgrp))\n\t\tseq_puts(seq, \"threaded\\n\");\n\telse if (!cgroup_is_valid_domain(cgrp))\n\t\tseq_puts(seq, \"domain invalid\\n\");\n\telse if (cgroup_is_thread_root(cgrp))\n\t\tseq_puts(seq, \"domain threaded\\n\");\n\telse\n\t\tseq_puts(seq, \"domain\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_type_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\t/* only switching to threaded mode is supported */\n\tif (strcmp(strstrip(buf), \"threaded\"))\n\t\treturn -EINVAL;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\t/* threaded can only be enabled */\n\tret = cgroup_enable_threaded(cgrp);\n\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\nstatic int cgroup_max_descendants_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint descendants = READ_ONCE(cgrp->max_descendants);\n\n\tif (descendants == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", descendants);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_descendants_write(struct kernfs_open_file *of,\n\t\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint descendants;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdescendants = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &descendants);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (descendants < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_descendants = descendants;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_max_depth_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint depth = READ_ONCE(cgrp->max_depth);\n\n\tif (depth == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", depth);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_depth_write(struct kernfs_open_file *of,\n\t\t\t\t      char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tssize_t ret;\n\tint depth;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdepth = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &depth);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (depth < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_depth = depth;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_events_show(struct seq_file *seq, void *v)\n{\n\tseq_printf(seq, \"populated %d\\n\",\n\t\t   cgroup_is_populated(seq_css(seq)->cgroup));\n\treturn 0;\n}\n\nstatic int cgroup_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgroup = seq_css(seq)->cgroup;\n\n\tseq_printf(seq, \"nr_descendants %d\\n\",\n\t\t   cgroup->nr_descendants);\n\tseq_printf(seq, \"nr_dying_descendants %d\\n\",\n\t\t   cgroup->nr_dying_descendants);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused cgroup_extra_stat_show(struct seq_file *seq,\n\t\t\t\t\t\t struct cgroup *cgrp, int ssid)\n{\n\tstruct cgroup_subsys *ss = cgroup_subsys[ssid];\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\tif (!ss->css_extra_stat_show)\n\t\treturn 0;\n\n\tcss = cgroup_tryget_css(cgrp, ss);\n\tif (!css)\n\t\treturn 0;\n\n\tret = ss->css_extra_stat_show(seq, css);\n\tcss_put(css);\n\treturn ret;\n}\n\nstatic int cpu_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup __maybe_unused *cgrp = seq_css(seq)->cgroup;\n\tint ret = 0;\n\n\tcgroup_base_stat_cputime_show(seq);\n#ifdef CONFIG_CGROUP_SCHED\n\tret = cgroup_extra_stat_show(seq, cgrp, cpu_cgrp_id);\n#endif\n\treturn ret;\n}\n\n#ifdef CONFIG_PSI\nstatic int cgroup_io_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_IO);\n}\nstatic int cgroup_memory_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_MEM);\n}\nstatic int cgroup_cpu_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_CPU);\n}\n#endif\n\nstatic int cgroup_file_open(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->open)\n\t\treturn cft->open(of);\n\treturn 0;\n}\n\nstatic void cgroup_file_release(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->release)\n\t\tcft->release(of);\n}\n\nstatic ssize_t cgroup_file_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *cgrp = of->kn->parent->priv;\n\tstruct cftype *cft = of->kn->priv;\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, disallow writes to\n\t * files in an non-init namespace root from inside the namespace\n\t * except for the files explicitly marked delegatable -\n\t * cgroup.procs and cgroup.subtree_control.\n\t */\n\tif ((cgrp->root->flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    !(cft->flags & CFTYPE_NS_DELEGATABLE) &&\n\t    ns != &init_cgroup_ns && ns->root_cset->dfl_cgrp == cgrp)\n\t\treturn -EPERM;\n\n\tif (cft->write)\n\t\treturn cft->write(of, buf, nbytes, off);\n\n\t/*\n\t * kernfs guarantees that a file isn't deleted with operations in\n\t * flight, which means that the matching css is and stays alive and\n\t * doesn't need to be pinned.  The RCU locking is not necessary\n\t * either.  It's just for the convenience of using cgroup_css().\n\t */\n\trcu_read_lock();\n\tcss = cgroup_css(cgrp, cft->ss);\n\trcu_read_unlock();\n\n\tif (cft->write_u64) {\n\t\tunsigned long long v;\n\t\tret = kstrtoull(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_u64(css, cft, v);\n\t} else if (cft->write_s64) {\n\t\tlong long v;\n\t\tret = kstrtoll(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_s64(css, cft, v);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_seqfile_start(struct seq_file *seq, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_start(seq, ppos);\n}\n\nstatic void *cgroup_seqfile_next(struct seq_file *seq, void *v, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_next(seq, v, ppos);\n}\n\nstatic void cgroup_seqfile_stop(struct seq_file *seq, void *v)\n{\n\tif (seq_cft(seq)->seq_stop)\n\t\tseq_cft(seq)->seq_stop(seq, v);\n}\n\nstatic int cgroup_seqfile_show(struct seq_file *m, void *arg)\n{\n\tstruct cftype *cft = seq_cft(m);\n\tstruct cgroup_subsys_state *css = seq_css(m);\n\n\tif (cft->seq_show)\n\t\treturn cft->seq_show(m, arg);\n\n\tif (cft->read_u64)\n\t\tseq_printf(m, \"%llu\\n\", cft->read_u64(css, cft));\n\telse if (cft->read_s64)\n\t\tseq_printf(m, \"%lld\\n\", cft->read_s64(css, cft));\n\telse\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic struct kernfs_ops cgroup_kf_single_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\nstatic struct kernfs_ops cgroup_kf_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_start\t\t= cgroup_seqfile_start,\n\t.seq_next\t\t= cgroup_seqfile_next,\n\t.seq_stop\t\t= cgroup_seqfile_stop,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\n/* set uid and gid of cgroup dirs and files to that of the creator */\nstatic int cgroup_kn_set_ugid(struct kernfs_node *kn)\n{\n\tstruct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,\n\t\t\t       .ia_uid = current_fsuid(),\n\t\t\t       .ia_gid = current_fsgid(), };\n\n\tif (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&\n\t    gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))\n\t\treturn 0;\n\n\treturn kernfs_setattr(kn, &iattr);\n}\n\nstatic void cgroup_file_notify_timer(struct timer_list *timer)\n{\n\tcgroup_file_notify(container_of(timer, struct cgroup_file,\n\t\t\t\t\tnotify_timer));\n}\n\nstatic int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,\n\t\t\t   struct cftype *cft)\n{\n\tchar name[CGROUP_FILE_NAME_MAX];\n\tstruct kernfs_node *kn;\n\tstruct lock_class_key *key = NULL;\n\tint ret;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tkey = &cft->lockdep_key;\n#endif\n\tkn = __kernfs_create_file(cgrp->kn, cgroup_file_name(cgrp, cft, name),\n\t\t\t\t  cgroup_file_mode(cft),\n\t\t\t\t  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID,\n\t\t\t\t  0, cft->kf_ops, cft,\n\t\t\t\t  NULL, key);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\tkernfs_remove(kn);\n\t\treturn ret;\n\t}\n\n\tif (cft->file_offset) {\n\t\tstruct cgroup_file *cfile = (void *)css + cft->file_offset;\n\n\t\ttimer_setup(&cfile->notify_timer, cgroup_file_notify_timer, 0);\n\n\t\tspin_lock_irq(&cgroup_file_kn_lock);\n\t\tcfile->kn = kn;\n\t\tspin_unlock_irq(&cgroup_file_kn_lock);\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_addrm_files - add or remove files to a cgroup directory\n * @css: the target css\n * @cgrp: the target cgroup (usually css->cgroup)\n * @cfts: array of cftypes to be added\n * @is_add: whether to add or remove\n *\n * Depending on @is_add, add or remove files defined by @cfts on @cgrp.\n * For removals, this function never fails.\n */\nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add)\n{\n\tstruct cftype *cft, *cft_end = NULL;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\nrestart:\n\tfor (cft = cfts; cft != cft_end && cft->name[0] != '\\0'; cft++) {\n\t\t/* does cft->flags tell us to skip this file on @cgrp? */\n\t\tif ((cft->flags & __CFTYPE_ONLY_ON_DFL) && !cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & __CFTYPE_NOT_ON_DFL) && cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_NOT_ON_ROOT) && !cgroup_parent(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_ONLY_ON_ROOT) && cgroup_parent(cgrp))\n\t\t\tcontinue;\n\n\t\tif (is_add) {\n\t\t\tret = cgroup_add_file(css, cgrp, cft);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"%s: failed to add %s, err=%d\\n\",\n\t\t\t\t\t__func__, cft->name, ret);\n\t\t\t\tcft_end = cft;\n\t\t\t\tis_add = false;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t} else {\n\t\t\tcgroup_rm_file(cgrp, cft);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int cgroup_apply_cftypes(struct cftype *cfts, bool is_add)\n{\n\tstruct cgroup_subsys *ss = cfts[0].ss;\n\tstruct cgroup *root = &ss->root->cgrp;\n\tstruct cgroup_subsys_state *css;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* add/rm files for all cgroups created before */\n\tcss_for_each_descendant_pre(css, cgroup_css(root, ss)) {\n\t\tstruct cgroup *cgrp = css->cgroup;\n\n\t\tif (!(css->flags & CSS_VISIBLE))\n\t\t\tcontinue;\n\n\t\tret = cgroup_addrm_files(css, cgrp, cfts, is_add);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (is_add && !ret)\n\t\tkernfs_activate(root->kn);\n\treturn ret;\n}\n\nstatic void cgroup_exit_cftypes(struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\t/* free copy for custom atomic_write_len, see init_cftypes() */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE)\n\t\t\tkfree(cft->kf_ops);\n\t\tcft->kf_ops = NULL;\n\t\tcft->ss = NULL;\n\n\t\t/* revert flags set by cgroup core while adding @cfts */\n\t\tcft->flags &= ~(__CFTYPE_ONLY_ON_DFL | __CFTYPE_NOT_ON_DFL);\n\t}\n}\n\nstatic int cgroup_init_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\tstruct kernfs_ops *kf_ops;\n\n\t\tWARN_ON(cft->ss || cft->kf_ops);\n\n\t\tif (cft->seq_start)\n\t\t\tkf_ops = &cgroup_kf_ops;\n\t\telse\n\t\t\tkf_ops = &cgroup_kf_single_ops;\n\n\t\t/*\n\t\t * Ugh... if @cft wants a custom max_write_len, we need to\n\t\t * make a copy of kf_ops to set its atomic_write_len.\n\t\t */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE) {\n\t\t\tkf_ops = kmemdup(kf_ops, sizeof(*kf_ops), GFP_KERNEL);\n\t\t\tif (!kf_ops) {\n\t\t\t\tcgroup_exit_cftypes(cfts);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tkf_ops->atomic_write_len = cft->max_write_len;\n\t\t}\n\n\t\tcft->kf_ops = kf_ops;\n\t\tcft->ss = ss;\n\t}\n\n\treturn 0;\n}\n\nstatic int cgroup_rm_cftypes_locked(struct cftype *cfts)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!cfts || !cfts[0].ss)\n\t\treturn -ENOENT;\n\n\tlist_del(&cfts->node);\n\tcgroup_apply_cftypes(cfts, false);\n\tcgroup_exit_cftypes(cfts);\n\treturn 0;\n}\n\n/**\n * cgroup_rm_cftypes - remove an array of cftypes from a subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Unregister @cfts.  Files described by @cfts are removed from all\n * existing cgroups and all future cgroups won't have them either.  This\n * function can be called anytime whether @cfts' subsys is attached or not.\n *\n * Returns 0 on successful unregistration, -ENOENT if @cfts is not\n * registered.\n */\nint cgroup_rm_cftypes(struct cftype *cfts)\n{\n\tint ret;\n\n\tmutex_lock(&cgroup_mutex);\n\tret = cgroup_rm_cftypes_locked(cfts);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_cftypes - add an array of cftypes to a subsystem\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Register @cfts to @ss.  Files described by @cfts are created for all\n * existing cgroups to which @ss is attached and all future cgroups will\n * have them too.  This function can be called anytime whether @ss is\n * attached or not.\n *\n * Returns 0 on successful registration, -errno on failure.  Note that this\n * function currently returns 0 as long as @cfts registration is successful\n * even if some file creation attempts on existing cgroups fail.\n */\nstatic int cgroup_add_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tint ret;\n\n\tif (!cgroup_ssid_enabled(ss->id))\n\t\treturn 0;\n\n\tif (!cfts || cfts[0].name[0] == '\\0')\n\t\treturn 0;\n\n\tret = cgroup_init_cftypes(ss, cfts);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tlist_add_tail(&cfts->node, &ss->cfts);\n\tret = cgroup_apply_cftypes(cfts, true);\n\tif (ret)\n\t\tcgroup_rm_cftypes_locked(cfts);\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_dfl_cftypes - add an array of cftypes for default hierarchy\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the default hierarchy.\n */\nint cgroup_add_dfl_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_ONLY_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_add_legacy_cftypes - add an array of cftypes for legacy hierarchies\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the legacy hierarchies.\n */\nint cgroup_add_legacy_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_NOT_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_file_notify - generate a file modified event for a cgroup_file\n * @cfile: target cgroup_file\n *\n * @cfile must have been obtained by setting cftype->file_offset.\n */\nvoid cgroup_file_notify(struct cgroup_file *cfile)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cgroup_file_kn_lock, flags);\n\tif (cfile->kn) {\n\t\tunsigned long last = cfile->notified_at;\n\t\tunsigned long next = last + CGROUP_FILE_NOTIFY_MIN_INTV;\n\n\t\tif (time_in_range(jiffies, last, next)) {\n\t\t\ttimer_reduce(&cfile->notify_timer, next);\n\t\t} else {\n\t\t\tkernfs_notify(cfile->kn);\n\t\t\tcfile->notified_at = jiffies;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&cgroup_file_kn_lock, flags);\n}\n\n/**\n * css_next_child - find the next child of a given css\n * @pos: the current position (%NULL to initiate traversal)\n * @parent: css whose children to walk\n *\n * This function returns the next child of @parent and should be called\n * under either cgroup_mutex or RCU read lock.  The only requirement is\n * that @parent and @pos are accessible.  The next sibling is guaranteed to\n * be returned regardless of their states.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *css_next_child(struct cgroup_subsys_state *pos,\n\t\t\t\t\t   struct cgroup_subsys_state *parent)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/*\n\t * @pos could already have been unlinked from the sibling list.\n\t * Once a cgroup is removed, its ->sibling.next is no longer\n\t * updated when its next sibling changes.  CSS_RELEASED is set when\n\t * @pos is taken off list, at which time its next pointer is valid,\n\t * and, as releases are serialized, the one pointed to by the next\n\t * pointer is guaranteed to not have started release yet.  This\n\t * implies that if we observe !CSS_RELEASED on @pos in this RCU\n\t * critical section, the one pointed to by its next pointer is\n\t * guaranteed to not have finished its RCU grace period even if we\n\t * have dropped rcu_read_lock() inbetween iterations.\n\t *\n\t * If @pos has CSS_RELEASED set, its next pointer can't be\n\t * dereferenced; however, as each css is given a monotonically\n\t * increasing unique serial number and always appended to the\n\t * sibling list, the next one can be found by walking the parent's\n\t * children until the first css with higher serial number than\n\t * @pos's.  While this path can be slower, it happens iff iteration\n\t * races against release and the race window is very small.\n\t */\n\tif (!pos) {\n\t\tnext = list_entry_rcu(parent->children.next, struct cgroup_subsys_state, sibling);\n\t} else if (likely(!(pos->flags & CSS_RELEASED))) {\n\t\tnext = list_entry_rcu(pos->sibling.next, struct cgroup_subsys_state, sibling);\n\t} else {\n\t\tlist_for_each_entry_rcu(next, &parent->children, sibling)\n\t\t\tif (next->serial_nr > pos->serial_nr)\n\t\t\t\tbreak;\n\t}\n\n\t/*\n\t * @next, if not pointing to the head, can be dereferenced and is\n\t * the next sibling.\n\t */\n\tif (&next->sibling != &parent->children)\n\t\treturn next;\n\treturn NULL;\n}\n\n/**\n * css_next_descendant_pre - find the next descendant for pre-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_pre().  Find the next descendant\n * to visit for pre-order traversal of @root's descendants.  @root is\n * included in the iteration and the first node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @root are accessible and @pos is a descendant of @root.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_pre(struct cgroup_subsys_state *pos,\n\t\t\tstruct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit @root */\n\tif (!pos)\n\t\treturn root;\n\n\t/* visit the first child if exists */\n\tnext = css_next_child(NULL, pos);\n\tif (next)\n\t\treturn next;\n\n\t/* no child, visit my or the closest ancestor's next sibling */\n\twhile (pos != root) {\n\t\tnext = css_next_child(pos, pos->parent);\n\t\tif (next)\n\t\t\treturn next;\n\t\tpos = pos->parent;\n\t}\n\n\treturn NULL;\n}\n\n/**\n * css_rightmost_descendant - return the rightmost descendant of a css\n * @pos: css of interest\n *\n * Return the rightmost descendant of @pos.  If there's no descendant, @pos\n * is returned.  This can be used during pre-order traversal to skip\n * subtree of @pos.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct rightmost descendant as\n * long as @pos is accessible.\n */\nstruct cgroup_subsys_state *\ncss_rightmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last, *tmp;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\tdo {\n\t\tlast = pos;\n\t\t/* ->prev isn't RCU safe, walk ->next till the end */\n\t\tpos = NULL;\n\t\tcss_for_each_child(tmp, last)\n\t\t\tpos = tmp;\n\t} while (pos);\n\n\treturn last;\n}\n\nstatic struct cgroup_subsys_state *\ncss_leftmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last;\n\n\tdo {\n\t\tlast = pos;\n\t\tpos = css_next_child(NULL, pos);\n\t} while (pos);\n\n\treturn last;\n}\n\n/**\n * css_next_descendant_post - find the next descendant for post-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_post().  Find the next descendant\n * to visit for post-order traversal of @root's descendants.  @root is\n * included in the iteration and the last node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @cgroup are accessible and @pos is a descendant of\n * @cgroup.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_post(struct cgroup_subsys_state *pos,\n\t\t\t struct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit leftmost descendant which may be @root */\n\tif (!pos)\n\t\treturn css_leftmost_descendant(root);\n\n\t/* if we visited @root, we're done */\n\tif (pos == root)\n\t\treturn NULL;\n\n\t/* if there's an unvisited sibling, visit its leftmost descendant */\n\tnext = css_next_child(pos, pos->parent);\n\tif (next)\n\t\treturn css_leftmost_descendant(next);\n\n\t/* no sibling left, visit parent */\n\treturn pos->parent;\n}\n\n/**\n * css_has_online_children - does a css have online children\n * @css: the target css\n *\n * Returns %true if @css has any online children; otherwise, %false.  This\n * function can be called from any context but the caller is responsible\n * for synchronizing against on/offlining as necessary.\n */\nbool css_has_online_children(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys_state *child;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tcss_for_each_child(child, css) {\n\t\tif (child->flags & CSS_ONLINE) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic struct css_set *css_task_iter_next_css_set(struct css_task_iter *it)\n{\n\tstruct list_head *l;\n\tstruct cgrp_cset_link *link;\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* find the next threaded cset */\n\tif (it->tcset_pos) {\n\t\tl = it->tcset_pos->next;\n\n\t\tif (l != it->tcset_head) {\n\t\t\tit->tcset_pos = l;\n\t\t\treturn container_of(l, struct css_set,\n\t\t\t\t\t    threaded_csets_node);\n\t\t}\n\n\t\tit->tcset_pos = NULL;\n\t}\n\n\t/* find the next cset */\n\tl = it->cset_pos;\n\tl = l->next;\n\tif (l == it->cset_head) {\n\t\tit->cset_pos = NULL;\n\t\treturn NULL;\n\t}\n\n\tif (it->ss) {\n\t\tcset = container_of(l, struct css_set, e_cset_node[it->ss->id]);\n\t} else {\n\t\tlink = list_entry(l, struct cgrp_cset_link, cset_link);\n\t\tcset = link->cset;\n\t}\n\n\tit->cset_pos = l;\n\n\t/* initialize threaded css_set walking */\n\tif (it->flags & CSS_TASK_ITER_THREADED) {\n\t\tif (it->cur_dcset)\n\t\t\tput_css_set_locked(it->cur_dcset);\n\t\tit->cur_dcset = cset;\n\t\tget_css_set(cset);\n\n\t\tit->tcset_head = &cset->threaded_csets;\n\t\tit->tcset_pos = &cset->threaded_csets;\n\t}\n\n\treturn cset;\n}\n\n/**\n * css_task_iter_advance_css_set - advance a task itererator to the next css_set\n * @it: the iterator to advance\n *\n * Advance @it to the next css_set to walk.\n */\nstatic void css_task_iter_advance_css_set(struct css_task_iter *it)\n{\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* Advance to the next non-empty css_set */\n\tdo {\n\t\tcset = css_task_iter_next_css_set(it);\n\t\tif (!cset) {\n\t\t\tit->task_pos = NULL;\n\t\t\treturn;\n\t\t}\n\t} while (!css_set_populated(cset));\n\n\tif (!list_empty(&cset->tasks))\n\t\tit->task_pos = cset->tasks.next;\n\telse\n\t\tit->task_pos = cset->mg_tasks.next;\n\n\tit->tasks_head = &cset->tasks;\n\tit->mg_tasks_head = &cset->mg_tasks;\n\n\t/*\n\t * We don't keep css_sets locked across iteration steps and thus\n\t * need to take steps to ensure that iteration can be resumed after\n\t * the lock is re-acquired.  Iteration is performed at two levels -\n\t * css_sets and tasks in them.\n\t *\n\t * Once created, a css_set never leaves its cgroup lists, so a\n\t * pinned css_set is guaranteed to stay put and we can resume\n\t * iteration afterwards.\n\t *\n\t * Tasks may leave @cset across iteration steps.  This is resolved\n\t * by registering each iterator with the css_set currently being\n\t * walked and making css_set_move_task() advance iterators whose\n\t * next task is leaving.\n\t */\n\tif (it->cur_cset) {\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t}\n\tget_css_set(cset);\n\tit->cur_cset = cset;\n\tlist_add(&it->iters_node, &cset->task_iters);\n}\n\nstatic void css_task_iter_advance(struct css_task_iter *it)\n{\n\tstruct list_head *next;\n\n\tlockdep_assert_held(&css_set_lock);\nrepeat:\n\t/*\n\t * Advance iterator to find next entry.  cset->tasks is consumed\n\t * first and then ->mg_tasks.  After ->mg_tasks, we move onto the\n\t * next cset.\n\t */\n\tnext = it->task_pos->next;\n\n\tif (next == it->tasks_head)\n\t\tnext = it->mg_tasks_head->next;\n\n\tif (next == it->mg_tasks_head)\n\t\tcss_task_iter_advance_css_set(it);\n\telse\n\t\tit->task_pos = next;\n\n\t/* if PROCS, skip over tasks which aren't group leaders */\n\tif ((it->flags & CSS_TASK_ITER_PROCS) && it->task_pos &&\n\t    !thread_group_leader(list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t    cg_list)))\n\t\tgoto repeat;\n}\n\n/**\n * css_task_iter_start - initiate task iteration\n * @css: the css to walk tasks of\n * @flags: CSS_TASK_ITER_* flags\n * @it: the task iterator to use\n *\n * Initiate iteration through the tasks of @css.  The caller can call\n * css_task_iter_next() to walk through the tasks until the function\n * returns NULL.  On completion of iteration, css_task_iter_end() must be\n * called.\n */\nvoid css_task_iter_start(struct cgroup_subsys_state *css, unsigned int flags,\n\t\t\t struct css_task_iter *it)\n{\n\t/* no one should try to iterate before mounting cgroups */\n\tWARN_ON_ONCE(!use_task_css_set_links);\n\n\tmemset(it, 0, sizeof(*it));\n\n\tspin_lock_irq(&css_set_lock);\n\n\tit->ss = css->ss;\n\tit->flags = flags;\n\n\tif (it->ss)\n\t\tit->cset_pos = &css->cgroup->e_csets[css->ss->id];\n\telse\n\t\tit->cset_pos = &css->cgroup->cset_links;\n\n\tit->cset_head = it->cset_pos;\n\n\tcss_task_iter_advance_css_set(it);\n\n\tspin_unlock_irq(&css_set_lock);\n}\n\n/**\n * css_task_iter_next - return the next task for the iterator\n * @it: the task iterator being iterated\n *\n * The \"next\" function for task iteration.  @it should have been\n * initialized via css_task_iter_start().  Returns NULL when the iteration\n * reaches the end.\n */\nstruct task_struct *css_task_iter_next(struct css_task_iter *it)\n{\n\tif (it->cur_task) {\n\t\tput_task_struct(it->cur_task);\n\t\tit->cur_task = NULL;\n\t}\n\n\tspin_lock_irq(&css_set_lock);\n\n\tif (it->task_pos) {\n\t\tit->cur_task = list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t  cg_list);\n\t\tget_task_struct(it->cur_task);\n\t\tcss_task_iter_advance(it);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn it->cur_task;\n}\n\n/**\n * css_task_iter_end - finish task iteration\n * @it: the task iterator to finish\n *\n * Finish task iteration started by css_task_iter_start().\n */\nvoid css_task_iter_end(struct css_task_iter *it)\n{\n\tif (it->cur_cset) {\n\t\tspin_lock_irq(&css_set_lock);\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t\tspin_unlock_irq(&css_set_lock);\n\t}\n\n\tif (it->cur_dcset)\n\t\tput_css_set(it->cur_dcset);\n\n\tif (it->cur_task)\n\t\tput_task_struct(it->cur_task);\n}\n\nstatic void cgroup_procs_release(struct kernfs_open_file *of)\n{\n\tif (of->priv) {\n\t\tcss_task_iter_end(of->priv);\n\t\tkfree(of->priv);\n\t}\n}\n\nstatic void *cgroup_procs_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct css_task_iter *it = of->priv;\n\n\treturn css_task_iter_next(it);\n}\n\nstatic void *__cgroup_procs_start(struct seq_file *s, loff_t *pos,\n\t\t\t\t  unsigned int iter_flags)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\tstruct css_task_iter *it = of->priv;\n\n\t/*\n\t * When a seq_file is seeked, it's always traversed sequentially\n\t * from position 0, so we can simply keep iterating on !0 *pos.\n\t */\n\tif (!it) {\n\t\tif (WARN_ON_ONCE((*pos)++))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tit = kzalloc(sizeof(*it), GFP_KERNEL);\n\t\tif (!it)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tof->priv = it;\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t} else if (!(*pos)++) {\n\t\tcss_task_iter_end(it);\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t}\n\n\treturn cgroup_procs_next(s, NULL, NULL);\n}\n\nstatic void *cgroup_procs_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\n\t/*\n\t * All processes of a threaded subtree belong to the domain cgroup\n\t * of the subtree.  Only threads can be distributed across the\n\t * subtree.  Reject reads on cgroup.procs in the subtree proper.\n\t * They're always empty anyway.\n\t */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn __cgroup_procs_start(s, pos, CSS_TASK_ITER_PROCS |\n\t\t\t\t\t    CSS_TASK_ITER_THREADED);\n}\n\nstatic int cgroup_procs_show(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", task_pid_vnr(v));\n\treturn 0;\n}\n\nstatic int cgroup_procs_write_permission(struct cgroup *src_cgrp,\n\t\t\t\t\t struct cgroup *dst_cgrp,\n\t\t\t\t\t struct super_block *sb)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *com_cgrp = src_cgrp;\n\tstruct inode *inode;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* find the common ancestor */\n\twhile (!cgroup_is_descendant(dst_cgrp, com_cgrp))\n\t\tcom_cgrp = cgroup_parent(com_cgrp);\n\n\t/* %current should be authorized to migrate to the common ancestor */\n\tinode = kernfs_get_inode(sb, com_cgrp->procs_file.kn);\n\tif (!inode)\n\t\treturn -ENOMEM;\n\n\tret = inode_permission(inode, MAY_WRITE);\n\tiput(inode);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, %current must be able\n\t * to see both source and destination cgroups from its namespace.\n\t */\n\tif ((cgrp_dfl_root.flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    (!cgroup_is_descendant(src_cgrp, ns->root_cset->dfl_cgrp) ||\n\t     !cgroup_is_descendant(dst_cgrp, ns->root_cset->dfl_cgrp)))\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_procs_write(struct kernfs_open_file *of,\n\t\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, true);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, true);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_threads_start(struct seq_file *s, loff_t *pos)\n{\n\treturn __cgroup_procs_start(s, pos, 0);\n}\n\nstatic ssize_t cgroup_threads_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, false);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* thread migrations follow the cgroup.procs delegation rule */\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\t/* and must be contained in the same domain */\n\tret = -EOPNOTSUPP;\n\tif (src_cgrp->dom_cgrp != dst_cgrp->dom_cgrp)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, false);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\n/* cgroup core interface files for the default hierarchy */\nstatic struct cftype cgroup_base_files[] = {\n\t{\n\t\t.name = \"cgroup.type\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_type_show,\n\t\t.write = cgroup_type_write,\n\t},\n\t{\n\t\t.name = \"cgroup.procs\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.file_offset = offsetof(struct cgroup, procs_file),\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_procs_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_procs_write,\n\t},\n\t{\n\t\t.name = \"cgroup.threads\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_threads_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_threads_write,\n\t},\n\t{\n\t\t.name = \"cgroup.controllers\",\n\t\t.seq_show = cgroup_controllers_show,\n\t},\n\t{\n\t\t.name = \"cgroup.subtree_control\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.seq_show = cgroup_subtree_control_show,\n\t\t.write = cgroup_subtree_control_write,\n\t},\n\t{\n\t\t.name = \"cgroup.events\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct cgroup, events_file),\n\t\t.seq_show = cgroup_events_show,\n\t},\n\t{\n\t\t.name = \"cgroup.max.descendants\",\n\t\t.seq_show = cgroup_max_descendants_show,\n\t\t.write = cgroup_max_descendants_write,\n\t},\n\t{\n\t\t.name = \"cgroup.max.depth\",\n\t\t.seq_show = cgroup_max_depth_show,\n\t\t.write = cgroup_max_depth_write,\n\t},\n\t{\n\t\t.name = \"cgroup.stat\",\n\t\t.seq_show = cgroup_stat_show,\n\t},\n\t{\n\t\t.name = \"cpu.stat\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_stat_show,\n\t},\n#ifdef CONFIG_PSI\n\t{\n\t\t.name = \"io.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_io_pressure_show,\n\t},\n\t{\n\t\t.name = \"memory.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_memory_pressure_show,\n\t},\n\t{\n\t\t.name = \"cpu.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_cpu_pressure_show,\n\t},\n#endif\n\t{ }\t/* terminate */\n};\n\n/*\n * css destruction is four-stage process.\n *\n * 1. Destruction starts.  Killing of the percpu_ref is initiated.\n *    Implemented in kill_css().\n *\n * 2. When the percpu_ref is confirmed to be visible as killed on all CPUs\n *    and thus css_tryget_online() is guaranteed to fail, the css can be\n *    offlined by invoking offline_css().  After offlining, the base ref is\n *    put.  Implemented in css_killed_work_fn().\n *\n * 3. When the percpu_ref reaches zero, the only possible remaining\n *    accessors are inside RCU read sections.  css_release() schedules the\n *    RCU callback.\n *\n * 4. After the grace period, the css can be freed.  Implemented in\n *    css_free_work_fn().\n *\n * It is actually hairier because both step 2 and 4 require process context\n * and thus involve punting to css->destroy_work adding two additional\n * steps to the already complex sequence.\n */\nstatic void css_free_rwork_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css = container_of(to_rcu_work(work),\n\t\t\t\tstruct cgroup_subsys_state, destroy_rwork);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tpercpu_ref_exit(&css->refcnt);\n\n\tif (ss) {\n\t\t/* css free path */\n\t\tstruct cgroup_subsys_state *parent = css->parent;\n\t\tint id = css->id;\n\n\t\tss->css_free(css);\n\t\tcgroup_idr_remove(&ss->css_idr, id);\n\t\tcgroup_put(cgrp);\n\n\t\tif (parent)\n\t\t\tcss_put(parent);\n\t} else {\n\t\t/* cgroup free path */\n\t\tatomic_dec(&cgrp->root->nr_cgrps);\n\t\tcgroup1_pidlist_destroy_all(cgrp);\n\t\tcancel_work_sync(&cgrp->release_agent_work);\n\n\t\tif (cgroup_parent(cgrp)) {\n\t\t\t/*\n\t\t\t * We get a ref to the parent, and put the ref when\n\t\t\t * this cgroup is being freed, so it's guaranteed\n\t\t\t * that the parent won't be destroyed before its\n\t\t\t * children.\n\t\t\t */\n\t\t\tcgroup_put(cgroup_parent(cgrp));\n\t\t\tkernfs_put(cgrp->kn);\n\t\t\tpsi_cgroup_free(cgrp);\n\t\t\tif (cgroup_on_dfl(cgrp))\n\t\t\t\tcgroup_rstat_exit(cgrp);\n\t\t\tkfree(cgrp);\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is root cgroup's refcnt reaching zero,\n\t\t\t * which indicates that the root should be\n\t\t\t * released.\n\t\t\t */\n\t\t\tcgroup_destroy_root(cgrp->root);\n\t\t}\n\t}\n}\n\nstatic void css_release_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tcss->flags |= CSS_RELEASED;\n\tlist_del_rcu(&css->sibling);\n\n\tif (ss) {\n\t\t/* css release path */\n\t\tif (!list_empty(&css->rstat_css_node)) {\n\t\t\tcgroup_rstat_flush(cgrp);\n\t\t\tlist_del_rcu(&css->rstat_css_node);\n\t\t}\n\n\t\tcgroup_idr_replace(&ss->css_idr, NULL, css->id);\n\t\tif (ss->css_released)\n\t\t\tss->css_released(css);\n\t} else {\n\t\tstruct cgroup *tcgrp;\n\n\t\t/* cgroup release path */\n\t\tTRACE_CGROUP_PATH(release, cgrp);\n\n\t\tif (cgroup_on_dfl(cgrp))\n\t\t\tcgroup_rstat_flush(cgrp);\n\n\t\tfor (tcgrp = cgroup_parent(cgrp); tcgrp;\n\t\t     tcgrp = cgroup_parent(tcgrp))\n\t\t\ttcgrp->nr_dying_descendants--;\n\n\t\tcgroup_idr_remove(&cgrp->root->cgroup_idr, cgrp->id);\n\t\tcgrp->id = -1;\n\n\t\t/*\n\t\t * There are two control paths which try to determine\n\t\t * cgroup from dentry without going through kernfs -\n\t\t * cgroupstats_build() and css_tryget_online_from_dir().\n\t\t * Those are supported by RCU protecting clearing of\n\t\t * cgrp->kn->priv backpointer.\n\t\t */\n\t\tif (cgrp->kn)\n\t\t\tRCU_INIT_POINTER(*(void __rcu __force **)&cgrp->kn->priv,\n\t\t\t\t\t NULL);\n\n\t\tcgroup_bpf_put(cgrp);\n\t}\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n}\n\nstatic void css_release(struct percpu_ref *ref)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(ref, struct cgroup_subsys_state, refcnt);\n\n\tINIT_WORK(&css->destroy_work, css_release_work_fn);\n\tqueue_work(cgroup_destroy_wq, &css->destroy_work);\n}\n\nstatic void init_and_link_css(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcgroup_get_live(cgrp);\n\n\tmemset(css, 0, sizeof(*css));\n\tcss->cgroup = cgrp;\n\tcss->ss = ss;\n\tcss->id = -1;\n\tINIT_LIST_HEAD(&css->sibling);\n\tINIT_LIST_HEAD(&css->children);\n\tINIT_LIST_HEAD(&css->rstat_css_node);\n\tcss->serial_nr = css_serial_nr_next++;\n\tatomic_set(&css->online_cnt, 0);\n\n\tif (cgroup_parent(cgrp)) {\n\t\tcss->parent = cgroup_css(cgroup_parent(cgrp), ss);\n\t\tcss_get(css->parent);\n\t}\n\n\tif (cgroup_on_dfl(cgrp) && ss->css_rstat_flush)\n\t\tlist_add_rcu(&css->rstat_css_node, &cgrp->rstat_css_list);\n\n\tBUG_ON(cgroup_css(cgrp, ss));\n}\n\n/* invoke ->css_online() on a new CSS and mark it online if successful */\nstatic int online_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (ss->css_online)\n\t\tret = ss->css_online(css);\n\tif (!ret) {\n\t\tcss->flags |= CSS_ONLINE;\n\t\trcu_assign_pointer(css->cgroup->subsys[ss->id], css);\n\n\t\tatomic_inc(&css->online_cnt);\n\t\tif (css->parent)\n\t\t\tatomic_inc(&css->parent->online_cnt);\n\t}\n\treturn ret;\n}\n\n/* if the CSS is online, invoke ->css_offline() on it and mark it offline */\nstatic void offline_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!(css->flags & CSS_ONLINE))\n\t\treturn;\n\n\tif (ss->css_offline)\n\t\tss->css_offline(css);\n\n\tcss->flags &= ~CSS_ONLINE;\n\tRCU_INIT_POINTER(css->cgroup->subsys[ss->id], NULL);\n\n\twake_up_all(&css->cgroup->offline_waitq);\n}\n\n/**\n * css_create - create a cgroup_subsys_state\n * @cgrp: the cgroup new css will be associated with\n * @ss: the subsys of new css\n *\n * Create a new css associated with @cgrp - @ss pair.  On success, the new\n * css is online and installed in @cgrp.  This function doesn't create the\n * interface files.  Returns 0 on success, -errno on failure.\n */\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup_subsys_state *parent_css = cgroup_css(parent, ss);\n\tstruct cgroup_subsys_state *css;\n\tint err;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcss = ss->css_alloc(parent_css);\n\tif (!css)\n\t\tcss = ERR_PTR(-ENOMEM);\n\tif (IS_ERR(css))\n\t\treturn css;\n\n\tinit_and_link_css(css, ss, cgrp);\n\n\terr = percpu_ref_init(&css->refcnt, css_release, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err_free_css;\n\n\terr = cgroup_idr_alloc(&ss->css_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (err < 0)\n\t\tgoto err_free_css;\n\tcss->id = err;\n\n\t/* @css is ready to be brought online now, make it visible */\n\tlist_add_tail_rcu(&css->sibling, &parent_css->children);\n\tcgroup_idr_replace(&ss->css_idr, css, css->id);\n\n\terr = online_css(css);\n\tif (err)\n\t\tgoto err_list_del;\n\n\tif (ss->broken_hierarchy && !ss->warned_broken_hierarchy &&\n\t    cgroup_parent(parent)) {\n\t\tpr_warn(\"%s (%d) created nested cgroup for controller \\\"%s\\\" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.\\n\",\n\t\t\tcurrent->comm, current->pid, ss->name);\n\t\tif (!strcmp(ss->name, \"memory\"))\n\t\t\tpr_warn(\"\\\"memory\\\" requires setting use_hierarchy to 1 on the root\\n\");\n\t\tss->warned_broken_hierarchy = true;\n\t}\n\n\treturn css;\n\nerr_list_del:\n\tlist_del_rcu(&css->sibling);\nerr_free_css:\n\tlist_del_rcu(&css->rstat_css_node);\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n\treturn ERR_PTR(err);\n}\n\n/*\n * The returned cgroup is fully initialized including its control mask, but\n * it isn't associated with its kernfs_node and doesn't have the control\n * mask applied.\n */\nstatic struct cgroup *cgroup_create(struct cgroup *parent)\n{\n\tstruct cgroup_root *root = parent->root;\n\tstruct cgroup *cgrp, *tcgrp;\n\tint level = parent->level + 1;\n\tint ret;\n\n\t/* allocate the cgroup and its ID, 0 is reserved for the root */\n\tcgrp = kzalloc(struct_size(cgrp, ancestor_ids, (level + 1)),\n\t\t       GFP_KERNEL);\n\tif (!cgrp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = percpu_ref_init(&cgrp->self.refcnt, css_release, 0, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_free_cgrp;\n\n\tif (cgroup_on_dfl(parent)) {\n\t\tret = cgroup_rstat_init(cgrp);\n\t\tif (ret)\n\t\t\tgoto out_cancel_ref;\n\t}\n\n\t/*\n\t * Temporarily set the pointer to NULL, so idr_find() won't return\n\t * a half-baked cgroup.\n\t */\n\tcgrp->id = cgroup_idr_alloc(&root->cgroup_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (cgrp->id < 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out_stat_exit;\n\t}\n\n\tinit_cgroup_housekeeping(cgrp);\n\n\tcgrp->self.parent = &parent->self;\n\tcgrp->root = root;\n\tcgrp->level = level;\n\n\tret = psi_cgroup_alloc(cgrp);\n\tif (ret)\n\t\tgoto out_idr_free;\n\n\tret = cgroup_bpf_inherit(cgrp);\n\tif (ret)\n\t\tgoto out_psi_free;\n\n\tfor (tcgrp = cgrp; tcgrp; tcgrp = cgroup_parent(tcgrp)) {\n\t\tcgrp->ancestor_ids[tcgrp->level] = tcgrp->id;\n\n\t\tif (tcgrp != cgrp)\n\t\t\ttcgrp->nr_descendants++;\n\t}\n\n\tif (notify_on_release(parent))\n\t\tset_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n\n\tif (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &parent->flags))\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags);\n\n\tcgrp->self.serial_nr = css_serial_nr_next++;\n\n\t/* allocation complete, commit to creation */\n\tlist_add_tail_rcu(&cgrp->self.sibling, &cgroup_parent(cgrp)->self.children);\n\tatomic_inc(&root->nr_cgrps);\n\tcgroup_get_live(parent);\n\n\t/*\n\t * @cgrp is now fully operational.  If something fails after this\n\t * point, it'll be released via the normal destruction path.\n\t */\n\tcgroup_idr_replace(&root->cgroup_idr, cgrp, cgrp->id);\n\n\t/*\n\t * On the default hierarchy, a child doesn't automatically inherit\n\t * subtree_control from the parent.  Each is configured manually.\n\t */\n\tif (!cgroup_on_dfl(cgrp))\n\t\tcgrp->subtree_control = cgroup_control(cgrp);\n\n\tcgroup_propagate_control(cgrp);\n\n\treturn cgrp;\n\nout_psi_free:\n\tpsi_cgroup_free(cgrp);\nout_idr_free:\n\tcgroup_idr_remove(&root->cgroup_idr, cgrp->id);\nout_stat_exit:\n\tif (cgroup_on_dfl(parent))\n\t\tcgroup_rstat_exit(cgrp);\nout_cancel_ref:\n\tpercpu_ref_exit(&cgrp->self.refcnt);\nout_free_cgrp:\n\tkfree(cgrp);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool cgroup_check_hierarchy_limits(struct cgroup *parent)\n{\n\tstruct cgroup *cgroup;\n\tint ret = false;\n\tint level = 1;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tfor (cgroup = parent; cgroup; cgroup = cgroup_parent(cgroup)) {\n\t\tif (cgroup->nr_descendants >= cgroup->max_descendants)\n\t\t\tgoto fail;\n\n\t\tif (level > cgroup->max_depth)\n\t\t\tgoto fail;\n\n\t\tlevel++;\n\t}\n\n\tret = true;\nfail:\n\treturn ret;\n}\n\nint cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)\n{\n\tstruct cgroup *parent, *cgrp;\n\tstruct kernfs_node *kn;\n\tint ret;\n\n\t/* do not accept '\\n' to prevent making /proc/<pid>/cgroup unparsable */\n\tif (strchr(name, '\\n'))\n\t\treturn -EINVAL;\n\n\tparent = cgroup_kn_lock_live(parent_kn, false);\n\tif (!parent)\n\t\treturn -ENODEV;\n\n\tif (!cgroup_check_hierarchy_limits(parent)) {\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tcgrp = cgroup_create(parent);\n\tif (IS_ERR(cgrp)) {\n\t\tret = PTR_ERR(cgrp);\n\t\tgoto out_unlock;\n\t}\n\n\t/* create the directory */\n\tkn = kernfs_create_dir(parent->kn, name, mode, cgrp);\n\tif (IS_ERR(kn)) {\n\t\tret = PTR_ERR(kn);\n\t\tgoto out_destroy;\n\t}\n\tcgrp->kn = kn;\n\n\t/*\n\t * This extra ref will be put in cgroup_free_fn() and guarantees\n\t * that @cgrp->kn is always accessible.\n\t */\n\tkernfs_get(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = css_populate_dir(&cgrp->self);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tTRACE_CGROUP_PATH(mkdir, cgrp);\n\n\t/* let's create and online css's */\n\tkernfs_activate(kn);\n\n\tret = 0;\n\tgoto out_unlock;\n\nout_destroy:\n\tcgroup_destroy_locked(cgrp);\nout_unlock:\n\tcgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\n/*\n * This is called when the refcnt of a css is confirmed to be killed.\n * css_tryget_online() is now guaranteed to fail.  Tell the subsystem to\n * initate destruction and put the css ref from kill_css().\n */\nstatic void css_killed_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\n\tmutex_lock(&cgroup_mutex);\n\n\tdo {\n\t\toffline_css(css);\n\t\tcss_put(css);\n\t\t/* @css can't go away while we're holding cgroup_mutex */\n\t\tcss = css->parent;\n\t} while (css && atomic_dec_and_test(&css->online_cnt));\n\n\tmutex_unlock(&cgroup_mutex);\n}"
        }
      },
      {
        "call_info": {
          "callee": "put_cred",
          "args": [
            "tcred"
          ],
          "line": 546
        },
        "resolved": true,
        "details": {
          "function_name": "__put_cred",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cred.c",
          "lines": "135-151",
          "snippet": "void __put_cred(struct cred *cred)\n{\n\tkdebug(\"__put_cred(%p{%d,%d})\", cred,\n\t       atomic_read(&cred->usage),\n\t       read_cred_subscribers(cred));\n\n\tBUG_ON(atomic_read(&cred->usage) != 0);\n#ifdef CONFIG_DEBUG_CREDENTIALS\n\tBUG_ON(read_cred_subscribers(cred) != 0);\n\tcred->magic = CRED_MAGIC_DEAD;\n\tcred->put_addr = __builtin_return_address(0);\n#endif\n\tBUG_ON(cred == current->cred);\n\tBUG_ON(cred == current->real_cred);\n\n\tcall_rcu(&cred->rcu, put_cred_rcu);\n}",
          "includes": [
            "#include <linux/cn_proc.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/security.h>",
            "#include <linux/init_task.h>",
            "#include <linux/keyctl.h>",
            "#include <linux/key.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/cred.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cn_proc.h>\n#include <linux/binfmts.h>\n#include <linux/security.h>\n#include <linux/init_task.h>\n#include <linux/keyctl.h>\n#include <linux/key.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/cred.h>\n#include <linux/export.h>\n\nvoid __put_cred(struct cred *cred)\n{\n\tkdebug(\"__put_cred(%p{%d,%d})\", cred,\n\t       atomic_read(&cred->usage),\n\t       read_cred_subscribers(cred));\n\n\tBUG_ON(atomic_read(&cred->usage) != 0);\n#ifdef CONFIG_DEBUG_CREDENTIALS\n\tBUG_ON(read_cred_subscribers(cred) != 0);\n\tcred->magic = CRED_MAGIC_DEAD;\n\tcred->put_addr = __builtin_return_address(0);\n#endif\n\tBUG_ON(cred == current->cred);\n\tBUG_ON(cred == current->real_cred);\n\n\tcall_rcu(&cred->rcu, put_cred_rcu);\n}"
        }
      },
      {
        "call_info": {
          "callee": "uid_eq",
          "args": [
            "cred->euid",
            "tcred->suid"
          ],
          "line": 544
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "uid_eq",
          "args": [
            "cred->euid",
            "tcred->uid"
          ],
          "line": 543
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "uid_eq",
          "args": [
            "cred->euid",
            "GLOBAL_ROOT_UID"
          ],
          "line": 542
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_task_cred",
          "args": [
            "task"
          ],
          "line": 541
        },
        "resolved": true,
        "details": {
          "function_name": "get_task_cred",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cred.c",
          "lines": "188-201",
          "snippet": "const struct cred *get_task_cred(struct task_struct *task)\n{\n\tconst struct cred *cred;\n\n\trcu_read_lock();\n\n\tdo {\n\t\tcred = __task_cred((task));\n\t\tBUG_ON(!cred);\n\t} while (!atomic_inc_not_zero(&((struct cred *)cred)->usage));\n\n\trcu_read_unlock();\n\treturn cred;\n}",
          "includes": [
            "#include <linux/cn_proc.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/security.h>",
            "#include <linux/init_task.h>",
            "#include <linux/keyctl.h>",
            "#include <linux/key.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/cred.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cn_proc.h>\n#include <linux/binfmts.h>\n#include <linux/security.h>\n#include <linux/init_task.h>\n#include <linux/keyctl.h>\n#include <linux/key.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/cred.h>\n#include <linux/export.h>\n\nconst struct cred *get_task_cred(struct task_struct *task)\n{\n\tconst struct cred *cred;\n\n\trcu_read_lock();\n\n\tdo {\n\t\tcred = __task_cred((task));\n\t\tBUG_ON(!cred);\n\t} while (!atomic_inc_not_zero(&((struct cred *)cred)->usage));\n\n\trcu_read_unlock();\n\treturn cred;\n}"
        }
      },
      {
        "call_info": {
          "callee": "current_cred",
          "args": [],
          "line": 540
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "PTR_ERR_OR_ZERO",
          "args": [
            "task"
          ],
          "line": 532
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_procs_write_start",
          "args": [
            "buf",
            "threadgroup"
          ],
          "line": 531
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_kn_lock_live",
          "args": [
            "of->kn",
            "false"
          ],
          "line": 527
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_kn_lock_live",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1520-1549",
          "snippet": "struct cgroup *cgroup_kn_lock_live(struct kernfs_node *kn, bool drain_offline)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\t/*\n\t * We're gonna grab cgroup_mutex which nests outside kernfs\n\t * active_ref.  cgroup liveliness check alone provides enough\n\t * protection against removal.  Ensure @cgrp stays accessible and\n\t * break the active_ref protection.\n\t */\n\tif (!cgroup_tryget(cgrp))\n\t\treturn NULL;\n\tkernfs_break_active_protection(kn);\n\n\tif (drain_offline)\n\t\tcgroup_lock_and_drain_offline(cgrp);\n\telse\n\t\tmutex_lock(&cgroup_mutex);\n\n\tif (!cgroup_is_dead(cgrp))\n\t\treturn cgrp;\n\n\tcgroup_kn_unlock(kn);\n\treturn NULL;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nstruct cgroup *cgroup_kn_lock_live(struct kernfs_node *kn, bool drain_offline)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\t/*\n\t * We're gonna grab cgroup_mutex which nests outside kernfs\n\t * active_ref.  cgroup liveliness check alone provides enough\n\t * protection against removal.  Ensure @cgrp stays accessible and\n\t * break the active_ref protection.\n\t */\n\tif (!cgroup_tryget(cgrp))\n\t\treturn NULL;\n\tkernfs_break_active_protection(kn);\n\n\tif (drain_offline)\n\t\tcgroup_lock_and_drain_offline(cgrp);\n\telse\n\t\tmutex_lock(&cgroup_mutex);\n\n\tif (!cgroup_is_dead(cgrp))\n\t\treturn cgrp;\n\n\tcgroup_kn_unlock(kn);\n\treturn NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic ssize_t __cgroup1_procs_write(struct kernfs_open_file *of,\n\t\t\t\t     char *buf, size_t nbytes, loff_t off,\n\t\t\t\t     bool threadgroup)\n{\n\tstruct cgroup *cgrp;\n\tstruct task_struct *task;\n\tconst struct cred *cred, *tcred;\n\tssize_t ret;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, threadgroup);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Even if we're attaching all tasks in the thread group, we only\n\t * need to check permissions on one of them.\n\t */\n\tcred = current_cred();\n\ttcred = get_task_cred(task);\n\tif (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&\n\t    !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->euid, tcred->suid))\n\t\tret = -EACCES;\n\tput_cred(tcred);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(cgrp, task, threadgroup);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}"
  },
  {
    "function_name": "cgroup_pidlist_show",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "511-516",
    "snippet": "static int cgroup_pidlist_show(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", *(int *)v);\n\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "seq_printf",
          "args": [
            "s",
            "\"%d\\n\"",
            "*(int *)v"
          ],
          "line": 513
        },
        "resolved": true,
        "details": {
          "function_name": "trace_seq_printf",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/trace_seq.c",
          "lines": "84-103",
          "snippet": "void trace_seq_printf(struct trace_seq *s, const char *fmt, ...)\n{\n\tunsigned int save_len = s->seq.len;\n\tva_list ap;\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tva_start(ap, fmt);\n\tseq_buf_vprintf(&s->seq, fmt, ap);\n\tva_end(ap);\n\n\t/* If we can't write it all, don't bother writing anything */\n\tif (unlikely(seq_buf_has_overflowed(&s->seq))) {\n\t\ts->seq.len = save_len;\n\t\ts->full = 1;\n\t}\n}",
          "includes": [
            "#include <linux/trace_seq.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/uaccess.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/trace_seq.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\nvoid trace_seq_printf(struct trace_seq *s, const char *fmt, ...)\n{\n\tunsigned int save_len = s->seq.len;\n\tva_list ap;\n\n\tif (s->full)\n\t\treturn;\n\n\t__trace_seq_init(s);\n\n\tva_start(ap, fmt);\n\tseq_buf_vprintf(&s->seq, fmt, ap);\n\tva_end(ap);\n\n\t/* If we can't write it all, don't bother writing anything */\n\tif (unlikely(seq_buf_has_overflowed(&s->seq))) {\n\t\ts->seq.len = save_len;\n\t\ts->full = 1;\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int cgroup_pidlist_show(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", *(int *)v);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup_pidlist_next",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "492-509",
    "snippet": "static void *cgroup_pidlist_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup_pidlist *l = of->priv;\n\tpid_t *p = v;\n\tpid_t *end = l->list + l->length;\n\t/*\n\t * Advance to the next pid in the array. If this goes off the\n\t * end, we're done\n\t */\n\tp++;\n\tif (p >= end) {\n\t\treturn NULL;\n\t} else {\n\t\t*pos = *p;\n\t\treturn p;\n\t}\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void *cgroup_pidlist_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup_pidlist *l = of->priv;\n\tpid_t *p = v;\n\tpid_t *end = l->list + l->length;\n\t/*\n\t * Advance to the next pid in the array. If this goes off the\n\t * end, we're done\n\t */\n\tp++;\n\tif (p >= end) {\n\t\treturn NULL;\n\t} else {\n\t\t*pos = *p;\n\t\treturn p;\n\t}\n}"
  },
  {
    "function_name": "cgroup_pidlist_stop",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "481-490",
    "snippet": "static void cgroup_pidlist_stop(struct seq_file *s, void *v)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup_pidlist *l = of->priv;\n\n\tif (l)\n\t\tmod_delayed_work(cgroup_pidlist_destroy_wq, &l->destroy_dwork,\n\t\t\t\t CGROUP_PIDLIST_DESTROY_DELAY);\n\tmutex_unlock(&seq_css(s)->cgroup->pidlist_mutex);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [
      "#define CGROUP_PIDLIST_DESTROY_DELAY\tHZ"
    ],
    "globals_used": [
      "static struct workqueue_struct *cgroup_pidlist_destroy_wq;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&seq_css(s)->cgroup->pidlist_mutex"
          ],
          "line": 489
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_css",
          "args": [
            "s"
          ],
          "line": 489
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mod_delayed_work",
          "args": [
            "cgroup_pidlist_destroy_wq",
            "&l->destroy_dwork",
            "CGROUP_PIDLIST_DESTROY_DELAY"
          ],
          "line": 487
        },
        "resolved": true,
        "details": {
          "function_name": "kthread_mod_delayed_work",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/kthread.c",
          "lines": "1040-1067",
          "snippet": "bool kthread_mod_delayed_work(struct kthread_worker *worker,\n\t\t\t      struct kthread_delayed_work *dwork,\n\t\t\t      unsigned long delay)\n{\n\tstruct kthread_work *work = &dwork->work;\n\tunsigned long flags;\n\tint ret = false;\n\n\tspin_lock_irqsave(&worker->lock, flags);\n\n\t/* Do not bother with canceling when never queued. */\n\tif (!work->worker)\n\t\tgoto fast_queue;\n\n\t/* Work must not be used with >1 worker, see kthread_queue_work() */\n\tWARN_ON_ONCE(work->worker != worker);\n\n\t/* Do not fight with another command that is canceling this work. */\n\tif (work->canceling)\n\t\tgoto out;\n\n\tret = __kthread_cancel_work(work, true, &flags);\nfast_queue:\n\t__kthread_queue_delayed_work(worker, dwork, delay);\nout:\n\tspin_unlock_irqrestore(&worker->lock, flags);\n\treturn ret;\n}",
          "includes": [
            "#include <trace/events/sched.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/freezer.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/export.h>",
            "#include <linux/file.h>",
            "#include <linux/unistd.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/err.h>",
            "#include <linux/completion.h>",
            "#include <linux/kthread.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <uapi/linux/sched/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/sched.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/freezer.h>\n#include <linux/slab.h>\n#include <linux/mutex.h>\n#include <linux/export.h>\n#include <linux/file.h>\n#include <linux/unistd.h>\n#include <linux/cpuset.h>\n#include <linux/err.h>\n#include <linux/completion.h>\n#include <linux/kthread.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <uapi/linux/sched/types.h>\n\nbool kthread_mod_delayed_work(struct kthread_worker *worker,\n\t\t\t      struct kthread_delayed_work *dwork,\n\t\t\t      unsigned long delay)\n{\n\tstruct kthread_work *work = &dwork->work;\n\tunsigned long flags;\n\tint ret = false;\n\n\tspin_lock_irqsave(&worker->lock, flags);\n\n\t/* Do not bother with canceling when never queued. */\n\tif (!work->worker)\n\t\tgoto fast_queue;\n\n\t/* Work must not be used with >1 worker, see kthread_queue_work() */\n\tWARN_ON_ONCE(work->worker != worker);\n\n\t/* Do not fight with another command that is canceling this work. */\n\tif (work->canceling)\n\t\tgoto out;\n\n\tret = __kthread_cancel_work(work, true, &flags);\nfast_queue:\n\t__kthread_queue_delayed_work(worker, dwork, delay);\nout:\n\tspin_unlock_irqrestore(&worker->lock, flags);\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\n#define CGROUP_PIDLIST_DESTROY_DELAY\tHZ\n\nstatic struct workqueue_struct *cgroup_pidlist_destroy_wq;\n\nstatic void cgroup_pidlist_stop(struct seq_file *s, void *v)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup_pidlist *l = of->priv;\n\n\tif (l)\n\t\tmod_delayed_work(cgroup_pidlist_destroy_wq, &l->destroy_dwork,\n\t\t\t\t CGROUP_PIDLIST_DESTROY_DELAY);\n\tmutex_unlock(&seq_css(s)->cgroup->pidlist_mutex);\n}"
  },
  {
    "function_name": "cgroup_pidlist_start",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "420-479",
    "snippet": "static void *cgroup_pidlist_start(struct seq_file *s, loff_t *pos)\n{\n\t/*\n\t * Initially we receive a position value that corresponds to\n\t * one more than the last pid shown (or 0 on the first call or\n\t * after a seek to the start). Use a binary-search to find the\n\t * next pid to display, if any\n\t */\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\tstruct cgroup_pidlist *l;\n\tenum cgroup_filetype type = seq_cft(s)->private;\n\tint index = 0, pid = *pos;\n\tint *iter, ret;\n\n\tmutex_lock(&cgrp->pidlist_mutex);\n\n\t/*\n\t * !NULL @of->priv indicates that this isn't the first start()\n\t * after open.  If the matching pidlist is around, we can use that.\n\t * Look for it.  Note that @of->priv can't be used directly.  It\n\t * could already have been destroyed.\n\t */\n\tif (of->priv)\n\t\tof->priv = cgroup_pidlist_find(cgrp, type);\n\n\t/*\n\t * Either this is the first start() after open or the matching\n\t * pidlist has been destroyed inbetween.  Create a new one.\n\t */\n\tif (!of->priv) {\n\t\tret = pidlist_array_load(cgrp, type,\n\t\t\t\t\t (struct cgroup_pidlist **)&of->priv);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\tl = of->priv;\n\n\tif (pid) {\n\t\tint end = l->length;\n\n\t\twhile (index < end) {\n\t\t\tint mid = (index + end) / 2;\n\t\t\tif (l->list[mid] == pid) {\n\t\t\t\tindex = mid;\n\t\t\t\tbreak;\n\t\t\t} else if (l->list[mid] <= pid)\n\t\t\t\tindex = mid + 1;\n\t\t\telse\n\t\t\t\tend = mid;\n\t\t}\n\t}\n\t/* If we're off the end of the array, we're done */\n\tif (index >= l->length)\n\t\treturn NULL;\n\t/* Update the abstract position to be the actual pid that we found */\n\titer = l->list + index;\n\t*pos = *iter;\n\treturn iter;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "ERR_PTR",
          "args": [
            "ret"
          ],
          "line": 454
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pidlist_array_load",
          "args": [
            "cgrp",
            "type",
            "(struct cgroup_pidlist **)&of->priv"
          ],
          "line": 451
        },
        "resolved": true,
        "details": {
          "function_name": "pidlist_array_load",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "358-412",
          "snippet": "static int pidlist_array_load(struct cgroup *cgrp, enum cgroup_filetype type,\n\t\t\t      struct cgroup_pidlist **lp)\n{\n\tpid_t *array;\n\tint length;\n\tint pid, n = 0; /* used for populating the array */\n\tstruct css_task_iter it;\n\tstruct task_struct *tsk;\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\t/*\n\t * If cgroup gets more users after we read count, we won't have\n\t * enough space - tough.  This race is indistinguishable to the\n\t * caller from the case that the additional cgroup users didn't\n\t * show up until sometime later on.\n\t */\n\tlength = cgroup_task_count(cgrp);\n\tarray = pidlist_allocate(length);\n\tif (!array)\n\t\treturn -ENOMEM;\n\t/* now, populate the array */\n\tcss_task_iter_start(&cgrp->self, 0, &it);\n\twhile ((tsk = css_task_iter_next(&it))) {\n\t\tif (unlikely(n == length))\n\t\t\tbreak;\n\t\t/* get tgid or pid for procs or tasks file respectively */\n\t\tif (type == CGROUP_FILE_PROCS)\n\t\t\tpid = task_tgid_vnr(tsk);\n\t\telse\n\t\t\tpid = task_pid_vnr(tsk);\n\t\tif (pid > 0) /* make sure to only use valid results */\n\t\t\tarray[n++] = pid;\n\t}\n\tcss_task_iter_end(&it);\n\tlength = n;\n\t/* now sort & (if procs) strip out duplicates */\n\tsort(array, length, sizeof(pid_t), cmppid, NULL);\n\tif (type == CGROUP_FILE_PROCS)\n\t\tlength = pidlist_uniq(array, length);\n\n\tl = cgroup_pidlist_find_create(cgrp, type);\n\tif (!l) {\n\t\tpidlist_free(array);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* store array, freeing old if necessary */\n\tpidlist_free(l->list);\n\tl->list = array;\n\tl->length = length;\n\t*lp = l;\n\treturn 0;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int pidlist_array_load(struct cgroup *cgrp, enum cgroup_filetype type,\n\t\t\t      struct cgroup_pidlist **lp)\n{\n\tpid_t *array;\n\tint length;\n\tint pid, n = 0; /* used for populating the array */\n\tstruct css_task_iter it;\n\tstruct task_struct *tsk;\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\t/*\n\t * If cgroup gets more users after we read count, we won't have\n\t * enough space - tough.  This race is indistinguishable to the\n\t * caller from the case that the additional cgroup users didn't\n\t * show up until sometime later on.\n\t */\n\tlength = cgroup_task_count(cgrp);\n\tarray = pidlist_allocate(length);\n\tif (!array)\n\t\treturn -ENOMEM;\n\t/* now, populate the array */\n\tcss_task_iter_start(&cgrp->self, 0, &it);\n\twhile ((tsk = css_task_iter_next(&it))) {\n\t\tif (unlikely(n == length))\n\t\t\tbreak;\n\t\t/* get tgid or pid for procs or tasks file respectively */\n\t\tif (type == CGROUP_FILE_PROCS)\n\t\t\tpid = task_tgid_vnr(tsk);\n\t\telse\n\t\t\tpid = task_pid_vnr(tsk);\n\t\tif (pid > 0) /* make sure to only use valid results */\n\t\t\tarray[n++] = pid;\n\t}\n\tcss_task_iter_end(&it);\n\tlength = n;\n\t/* now sort & (if procs) strip out duplicates */\n\tsort(array, length, sizeof(pid_t), cmppid, NULL);\n\tif (type == CGROUP_FILE_PROCS)\n\t\tlength = pidlist_uniq(array, length);\n\n\tl = cgroup_pidlist_find_create(cgrp, type);\n\tif (!l) {\n\t\tpidlist_free(array);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* store array, freeing old if necessary */\n\tpidlist_free(l->list);\n\tl->list = array;\n\tl->length = length;\n\t*lp = l;\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_pidlist_find",
          "args": [
            "cgrp",
            "type"
          ],
          "line": 444
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_pidlist_find_create",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "314-337",
          "snippet": "static struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,\n\t\t\t\t\t\tenum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tl = cgroup_pidlist_find(cgrp, type);\n\tif (l)\n\t\treturn l;\n\n\t/* entry not found; create a new one */\n\tl = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);\n\tif (!l)\n\t\treturn l;\n\n\tINIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);\n\tl->key.type = type;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tl->key.ns = get_pid_ns(task_active_pid_ns(current));\n\tl->owner = cgrp;\n\tlist_add(&l->links, &cgrp->pidlists);\n\treturn l;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,\n\t\t\t\t\t\tenum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tl = cgroup_pidlist_find(cgrp, type);\n\tif (l)\n\t\treturn l;\n\n\t/* entry not found; create a new one */\n\tl = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);\n\tif (!l)\n\t\treturn l;\n\n\tINIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);\n\tl->key.type = type;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tl->key.ns = get_pid_ns(task_active_pid_ns(current));\n\tl->owner = cgrp;\n\tlist_add(&l->links, &cgrp->pidlists);\n\treturn l;\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgrp->pidlist_mutex"
          ],
          "line": 435
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "seq_cft",
          "args": [
            "s"
          ],
          "line": 431
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "seq_css",
          "args": [
            "s"
          ],
          "line": 429
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void *cgroup_pidlist_start(struct seq_file *s, loff_t *pos)\n{\n\t/*\n\t * Initially we receive a position value that corresponds to\n\t * one more than the last pid shown (or 0 on the first call or\n\t * after a seek to the start). Use a binary-search to find the\n\t * next pid to display, if any\n\t */\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\tstruct cgroup_pidlist *l;\n\tenum cgroup_filetype type = seq_cft(s)->private;\n\tint index = 0, pid = *pos;\n\tint *iter, ret;\n\n\tmutex_lock(&cgrp->pidlist_mutex);\n\n\t/*\n\t * !NULL @of->priv indicates that this isn't the first start()\n\t * after open.  If the matching pidlist is around, we can use that.\n\t * Look for it.  Note that @of->priv can't be used directly.  It\n\t * could already have been destroyed.\n\t */\n\tif (of->priv)\n\t\tof->priv = cgroup_pidlist_find(cgrp, type);\n\n\t/*\n\t * Either this is the first start() after open or the matching\n\t * pidlist has been destroyed inbetween.  Create a new one.\n\t */\n\tif (!of->priv) {\n\t\tret = pidlist_array_load(cgrp, type,\n\t\t\t\t\t (struct cgroup_pidlist **)&of->priv);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\tl = of->priv;\n\n\tif (pid) {\n\t\tint end = l->length;\n\n\t\twhile (index < end) {\n\t\t\tint mid = (index + end) / 2;\n\t\t\tif (l->list[mid] == pid) {\n\t\t\t\tindex = mid;\n\t\t\t\tbreak;\n\t\t\t} else if (l->list[mid] <= pid)\n\t\t\t\tindex = mid + 1;\n\t\t\telse\n\t\t\t\tend = mid;\n\t\t}\n\t}\n\t/* If we're off the end of the array, we're done */\n\tif (index >= l->length)\n\t\treturn NULL;\n\t/* Update the abstract position to be the actual pid that we found */\n\titer = l->list + index;\n\t*pos = *iter;\n\treturn iter;\n}"
  },
  {
    "function_name": "pidlist_array_load",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "358-412",
    "snippet": "static int pidlist_array_load(struct cgroup *cgrp, enum cgroup_filetype type,\n\t\t\t      struct cgroup_pidlist **lp)\n{\n\tpid_t *array;\n\tint length;\n\tint pid, n = 0; /* used for populating the array */\n\tstruct css_task_iter it;\n\tstruct task_struct *tsk;\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\t/*\n\t * If cgroup gets more users after we read count, we won't have\n\t * enough space - tough.  This race is indistinguishable to the\n\t * caller from the case that the additional cgroup users didn't\n\t * show up until sometime later on.\n\t */\n\tlength = cgroup_task_count(cgrp);\n\tarray = pidlist_allocate(length);\n\tif (!array)\n\t\treturn -ENOMEM;\n\t/* now, populate the array */\n\tcss_task_iter_start(&cgrp->self, 0, &it);\n\twhile ((tsk = css_task_iter_next(&it))) {\n\t\tif (unlikely(n == length))\n\t\t\tbreak;\n\t\t/* get tgid or pid for procs or tasks file respectively */\n\t\tif (type == CGROUP_FILE_PROCS)\n\t\t\tpid = task_tgid_vnr(tsk);\n\t\telse\n\t\t\tpid = task_pid_vnr(tsk);\n\t\tif (pid > 0) /* make sure to only use valid results */\n\t\t\tarray[n++] = pid;\n\t}\n\tcss_task_iter_end(&it);\n\tlength = n;\n\t/* now sort & (if procs) strip out duplicates */\n\tsort(array, length, sizeof(pid_t), cmppid, NULL);\n\tif (type == CGROUP_FILE_PROCS)\n\t\tlength = pidlist_uniq(array, length);\n\n\tl = cgroup_pidlist_find_create(cgrp, type);\n\tif (!l) {\n\t\tpidlist_free(array);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* store array, freeing old if necessary */\n\tpidlist_free(l->list);\n\tl->list = array;\n\tl->length = length;\n\t*lp = l;\n\treturn 0;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "pidlist_free",
          "args": [
            "l->list"
          ],
          "line": 407
        },
        "resolved": true,
        "details": {
          "function_name": "pidlist_free",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "203-206",
          "snippet": "static void pidlist_free(void *p)\n{\n\tkvfree(p);\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void pidlist_free(void *p)\n{\n\tkvfree(p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_pidlist_find_create",
          "args": [
            "cgrp",
            "type"
          ],
          "line": 400
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_pidlist_find_create",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "314-337",
          "snippet": "static struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,\n\t\t\t\t\t\tenum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tl = cgroup_pidlist_find(cgrp, type);\n\tif (l)\n\t\treturn l;\n\n\t/* entry not found; create a new one */\n\tl = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);\n\tif (!l)\n\t\treturn l;\n\n\tINIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);\n\tl->key.type = type;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tl->key.ns = get_pid_ns(task_active_pid_ns(current));\n\tl->owner = cgrp;\n\tlist_add(&l->links, &cgrp->pidlists);\n\treturn l;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,\n\t\t\t\t\t\tenum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tl = cgroup_pidlist_find(cgrp, type);\n\tif (l)\n\t\treturn l;\n\n\t/* entry not found; create a new one */\n\tl = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);\n\tif (!l)\n\t\treturn l;\n\n\tINIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);\n\tl->key.type = type;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tl->key.ns = get_pid_ns(task_active_pid_ns(current));\n\tl->owner = cgrp;\n\tlist_add(&l->links, &cgrp->pidlists);\n\treturn l;\n}"
        }
      },
      {
        "call_info": {
          "callee": "pidlist_uniq",
          "args": [
            "array",
            "length"
          ],
          "line": 398
        },
        "resolved": true,
        "details": {
          "function_name": "pidlist_uniq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "253-277",
          "snippet": "static int pidlist_uniq(pid_t *list, int length)\n{\n\tint src, dest = 1;\n\n\t/*\n\t * we presume the 0th element is unique, so i starts at 1. trivial\n\t * edge cases first; no work needs to be done for either\n\t */\n\tif (length == 0 || length == 1)\n\t\treturn length;\n\t/* src and dest walk down the list; dest counts unique elements */\n\tfor (src = 1; src < length; src++) {\n\t\t/* find next unique element */\n\t\twhile (list[src] == list[src-1]) {\n\t\t\tsrc++;\n\t\t\tif (src == length)\n\t\t\t\tgoto after;\n\t\t}\n\t\t/* dest always points to where the next unique element goes */\n\t\tlist[dest] = list[src];\n\t\tdest++;\n\t}\nafter:\n\treturn dest;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int pidlist_uniq(pid_t *list, int length)\n{\n\tint src, dest = 1;\n\n\t/*\n\t * we presume the 0th element is unique, so i starts at 1. trivial\n\t * edge cases first; no work needs to be done for either\n\t */\n\tif (length == 0 || length == 1)\n\t\treturn length;\n\t/* src and dest walk down the list; dest counts unique elements */\n\tfor (src = 1; src < length; src++) {\n\t\t/* find next unique element */\n\t\twhile (list[src] == list[src-1]) {\n\t\t\tsrc++;\n\t\t\tif (src == length)\n\t\t\t\tgoto after;\n\t\t}\n\t\t/* dest always points to where the next unique element goes */\n\t\tlist[dest] = list[src];\n\t\tdest++;\n\t}\nafter:\n\treturn dest;\n}"
        }
      },
      {
        "call_info": {
          "callee": "sort",
          "args": [
            "array",
            "length",
            "sizeof(pid_t)",
            "cmppid",
            "NULL"
          ],
          "line": 396
        },
        "resolved": true,
        "details": {
          "function_name": "sort_secondary",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/trace/tracing_map.c",
          "lines": "986-1033",
          "snippet": "static void sort_secondary(struct tracing_map *map,\n\t\t\t   const struct tracing_map_sort_entry **entries,\n\t\t\t   unsigned int n_entries,\n\t\t\t   struct tracing_map_sort_key *primary_key,\n\t\t\t   struct tracing_map_sort_key *secondary_key)\n{\n\tint (*primary_fn)(const struct tracing_map_sort_entry **,\n\t\t\t  const struct tracing_map_sort_entry **);\n\tint (*secondary_fn)(const struct tracing_map_sort_entry **,\n\t\t\t    const struct tracing_map_sort_entry **);\n\tunsigned i, start = 0, n_sub = 1;\n\n\tif (is_key(map, primary_key->field_idx))\n\t\tprimary_fn = cmp_entries_key;\n\telse\n\t\tprimary_fn = cmp_entries_sum;\n\n\tif (is_key(map, secondary_key->field_idx))\n\t\tsecondary_fn = cmp_entries_key;\n\telse\n\t\tsecondary_fn = cmp_entries_sum;\n\n\tfor (i = 0; i < n_entries - 1; i++) {\n\t\tconst struct tracing_map_sort_entry **a = &entries[i];\n\t\tconst struct tracing_map_sort_entry **b = &entries[i + 1];\n\n\t\tif (primary_fn(a, b) == 0) {\n\t\t\tn_sub++;\n\t\t\tif (i < n_entries - 2)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (n_sub < 2) {\n\t\t\tstart = i + 1;\n\t\t\tn_sub = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tset_sort_key(map, secondary_key);\n\t\tsort(&entries[start], n_sub,\n\t\t     sizeof(struct tracing_map_sort_entry *),\n\t\t     (int (*)(const void *, const void *))secondary_fn, NULL);\n\t\tset_sort_key(map, primary_key);\n\n\t\tstart = i + 1;\n\t\tn_sub = 1;\n\t}\n}",
          "includes": [
            "#include \"trace.h\"",
            "#include \"tracing_map.h\"",
            "#include <linux/sort.h>",
            "#include <linux/slab.h>",
            "#include <linux/jhash.h>",
            "#include <linux/vmalloc.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"trace.h\"\n#include \"tracing_map.h\"\n#include <linux/sort.h>\n#include <linux/slab.h>\n#include <linux/jhash.h>\n#include <linux/vmalloc.h>\n\nstatic void sort_secondary(struct tracing_map *map,\n\t\t\t   const struct tracing_map_sort_entry **entries,\n\t\t\t   unsigned int n_entries,\n\t\t\t   struct tracing_map_sort_key *primary_key,\n\t\t\t   struct tracing_map_sort_key *secondary_key)\n{\n\tint (*primary_fn)(const struct tracing_map_sort_entry **,\n\t\t\t  const struct tracing_map_sort_entry **);\n\tint (*secondary_fn)(const struct tracing_map_sort_entry **,\n\t\t\t    const struct tracing_map_sort_entry **);\n\tunsigned i, start = 0, n_sub = 1;\n\n\tif (is_key(map, primary_key->field_idx))\n\t\tprimary_fn = cmp_entries_key;\n\telse\n\t\tprimary_fn = cmp_entries_sum;\n\n\tif (is_key(map, secondary_key->field_idx))\n\t\tsecondary_fn = cmp_entries_key;\n\telse\n\t\tsecondary_fn = cmp_entries_sum;\n\n\tfor (i = 0; i < n_entries - 1; i++) {\n\t\tconst struct tracing_map_sort_entry **a = &entries[i];\n\t\tconst struct tracing_map_sort_entry **b = &entries[i + 1];\n\n\t\tif (primary_fn(a, b) == 0) {\n\t\t\tn_sub++;\n\t\t\tif (i < n_entries - 2)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (n_sub < 2) {\n\t\t\tstart = i + 1;\n\t\t\tn_sub = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tset_sort_key(map, secondary_key);\n\t\tsort(&entries[start], n_sub,\n\t\t     sizeof(struct tracing_map_sort_entry *),\n\t\t     (int (*)(const void *, const void *))secondary_fn, NULL);\n\t\tset_sort_key(map, primary_key);\n\n\t\tstart = i + 1;\n\t\tn_sub = 1;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "css_task_iter_end",
          "args": [
            "&it"
          ],
          "line": 393
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_pid_vnr",
          "args": [
            "tsk"
          ],
          "line": 389
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_tgid_vnr",
          "args": [
            "tsk"
          ],
          "line": 387
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "n == length"
          ],
          "line": 383
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "css_task_iter_next",
          "args": [
            "&it"
          ],
          "line": 382
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "css_task_iter_start",
          "args": [
            "&cgrp->self",
            "0",
            "&it"
          ],
          "line": 381
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "pidlist_allocate",
          "args": [
            "length"
          ],
          "line": 377
        },
        "resolved": true,
        "details": {
          "function_name": "pidlist_allocate",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "195-201",
          "snippet": "static void *pidlist_allocate(int count)\n{\n\tif (PIDLIST_TOO_LARGE(count))\n\t\treturn vmalloc(array_size(count, sizeof(pid_t)));\n\telse\n\t\treturn kmalloc_array(count, sizeof(pid_t), GFP_KERNEL);\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void *pidlist_allocate(int count)\n{\n\tif (PIDLIST_TOO_LARGE(count))\n\t\treturn vmalloc(array_size(count, sizeof(pid_t)));\n\telse\n\t\treturn kmalloc_array(count, sizeof(pid_t), GFP_KERNEL);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_task_count",
          "args": [
            "cgrp"
          ],
          "line": 376
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_task_count",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "343-353",
          "snippet": "int cgroup_task_count(const struct cgroup *cgrp)\n{\n\tint count = 0;\n\tstruct cgrp_cset_link *link;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(link, &cgrp->cset_links, cset_link)\n\t\tcount += link->cset->nr_tasks;\n\tspin_unlock_irq(&css_set_lock);\n\treturn count;\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nint cgroup_task_count(const struct cgroup *cgrp)\n{\n\tint count = 0;\n\tstruct cgrp_cset_link *link;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(link, &cgrp->cset_links, cset_link)\n\t\tcount += link->cset->nr_tasks;\n\tspin_unlock_irq(&css_set_lock);\n\treturn count;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&cgrp->pidlist_mutex"
          ],
          "line": 368
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int pidlist_array_load(struct cgroup *cgrp, enum cgroup_filetype type,\n\t\t\t      struct cgroup_pidlist **lp)\n{\n\tpid_t *array;\n\tint length;\n\tint pid, n = 0; /* used for populating the array */\n\tstruct css_task_iter it;\n\tstruct task_struct *tsk;\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\t/*\n\t * If cgroup gets more users after we read count, we won't have\n\t * enough space - tough.  This race is indistinguishable to the\n\t * caller from the case that the additional cgroup users didn't\n\t * show up until sometime later on.\n\t */\n\tlength = cgroup_task_count(cgrp);\n\tarray = pidlist_allocate(length);\n\tif (!array)\n\t\treturn -ENOMEM;\n\t/* now, populate the array */\n\tcss_task_iter_start(&cgrp->self, 0, &it);\n\twhile ((tsk = css_task_iter_next(&it))) {\n\t\tif (unlikely(n == length))\n\t\t\tbreak;\n\t\t/* get tgid or pid for procs or tasks file respectively */\n\t\tif (type == CGROUP_FILE_PROCS)\n\t\t\tpid = task_tgid_vnr(tsk);\n\t\telse\n\t\t\tpid = task_pid_vnr(tsk);\n\t\tif (pid > 0) /* make sure to only use valid results */\n\t\t\tarray[n++] = pid;\n\t}\n\tcss_task_iter_end(&it);\n\tlength = n;\n\t/* now sort & (if procs) strip out duplicates */\n\tsort(array, length, sizeof(pid_t), cmppid, NULL);\n\tif (type == CGROUP_FILE_PROCS)\n\t\tlength = pidlist_uniq(array, length);\n\n\tl = cgroup_pidlist_find_create(cgrp, type);\n\tif (!l) {\n\t\tpidlist_free(array);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* store array, freeing old if necessary */\n\tpidlist_free(l->list);\n\tl->list = array;\n\tl->length = length;\n\t*lp = l;\n\treturn 0;\n}"
  },
  {
    "function_name": "cgroup_task_count",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "343-353",
    "snippet": "int cgroup_task_count(const struct cgroup *cgrp)\n{\n\tint count = 0;\n\tstruct cgrp_cset_link *link;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(link, &cgrp->cset_links, cset_link)\n\t\tcount += link->cset->nr_tasks;\n\tspin_unlock_irq(&css_set_lock);\n\treturn count;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "spin_unlock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 351
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_for_each_entry",
          "args": [
            "link",
            "&cgrp->cset_links",
            "cset_link"
          ],
          "line": 349
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_lock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 348
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nint cgroup_task_count(const struct cgroup *cgrp)\n{\n\tint count = 0;\n\tstruct cgrp_cset_link *link;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(link, &cgrp->cset_links, cset_link)\n\t\tcount += link->cset->nr_tasks;\n\tspin_unlock_irq(&css_set_lock);\n\treturn count;\n}"
  },
  {
    "function_name": "cgroup_pidlist_find_create",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "314-337",
    "snippet": "static struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,\n\t\t\t\t\t\tenum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tl = cgroup_pidlist_find(cgrp, type);\n\tif (l)\n\t\treturn l;\n\n\t/* entry not found; create a new one */\n\tl = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);\n\tif (!l)\n\t\treturn l;\n\n\tINIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);\n\tl->key.type = type;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tl->key.ns = get_pid_ns(task_active_pid_ns(current));\n\tl->owner = cgrp;\n\tlist_add(&l->links, &cgrp->pidlists);\n\treturn l;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "list_add",
          "args": [
            "&l->links",
            "&cgrp->pidlists"
          ],
          "line": 335
        },
        "resolved": true,
        "details": {
          "function_name": "list_add_event",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/events/core.c",
          "lines": "1662-1690",
          "snippet": "static void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\tevent->tstamp = perf_event_time(event);\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tevent->group_caps = event->event_caps;\n\t\tadd_event_to_groups(event, ctx);\n\t}\n\n\tlist_update_cgroup_event(event, ctx, true);\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n\n\tctx->generation++;\n}",
          "includes": [
            "#include <asm/irq_regs.h>",
            "#include \"internal.h\"",
            "#include <linux/mount.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/clock.h>",
            "#include <linux/parser.h>",
            "#include <linux/namei.h>",
            "#include <linux/filter.h>",
            "#include <linux/bpf.h>",
            "#include <linux/compat.h>",
            "#include <linux/mman.h>",
            "#include <linux/module.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <linux/trace_events.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/anon_inodes.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/rculist.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/export.h>",
            "#include <linux/device.h>",
            "#include <linux/vmstat.h>",
            "#include <linux/reboot.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/percpu.h>",
            "#include <linux/dcache.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/tick.h>",
            "#include <linux/hash.h>",
            "#include <linux/slab.h>",
            "#include <linux/poll.h>",
            "#include <linux/file.h>",
            "#include <linux/idr.h>",
            "#include <linux/smp.h>",
            "#include <linux/cpu.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static void update_context_time(struct perf_event_context *ctx);",
            "static u64 perf_event_time(struct perf_event *event);",
            "static __must_check struct",
            "static void perf_log_itrace_start(struct perf_event *event);",
            "static void perf_event_free_filter(struct perf_event *event);",
            "static void perf_event_free_bpf_prog(struct perf_event *event);",
            "static int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr);",
            "static void perf_pmu_output_stop(struct perf_event *event);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <asm/irq_regs.h>\n#include \"internal.h\"\n#include <linux/mount.h>\n#include <linux/proc_ns.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/clock.h>\n#include <linux/parser.h>\n#include <linux/namei.h>\n#include <linux/filter.h>\n#include <linux/bpf.h>\n#include <linux/compat.h>\n#include <linux/mman.h>\n#include <linux/module.h>\n#include <linux/mm_types.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/trace_events.h>\n#include <linux/perf_event.h>\n#include <linux/cgroup.h>\n#include <linux/kernel_stat.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/rculist.h>\n#include <linux/hardirq.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <linux/device.h>\n#include <linux/vmstat.h>\n#include <linux/reboot.h>\n#include <linux/ptrace.h>\n#include <linux/percpu.h>\n#include <linux/dcache.h>\n#include <linux/sysfs.h>\n#include <linux/tick.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/idr.h>\n#include <linux/smp.h>\n#include <linux/cpu.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n\nstatic void update_context_time(struct perf_event_context *ctx);\nstatic u64 perf_event_time(struct perf_event *event);\nstatic __must_check struct;\nstatic void perf_log_itrace_start(struct perf_event *event);\nstatic void perf_event_free_filter(struct perf_event *event);\nstatic void perf_event_free_bpf_prog(struct perf_event *event);\nstatic int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr);\nstatic void perf_pmu_output_stop(struct perf_event *event);\n\nstatic void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\tevent->tstamp = perf_event_time(event);\n\n\t/*\n\t * If we're a stand alone event or group leader, we go to the context\n\t * list, group events are kept attached to the group so that\n\t * perf_group_detach can, at all times, locate all siblings.\n\t */\n\tif (event->group_leader == event) {\n\t\tevent->group_caps = event->event_caps;\n\t\tadd_event_to_groups(event, ctx);\n\t}\n\n\tlist_update_cgroup_event(event, ctx, true);\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tctx->nr_events++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n\n\tctx->generation++;\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_pid_ns",
          "args": [
            "task_active_pid_ns(current)"
          ],
          "line": 333
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_active_pid_ns",
          "args": [
            "current"
          ],
          "line": 333
        },
        "resolved": true,
        "details": {
          "function_name": "task_active_pid_ns",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/pid.c",
          "lines": "436-439",
          "snippet": "struct pid_namespace *task_active_pid_ns(struct task_struct *tsk)\n{\n\treturn ns_of_pid(task_pid(tsk));\n}",
          "includes": [
            "#include <linux/idr.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/init_task.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/hash.h>",
            "#include <linux/memblock.h>",
            "#include <linux/rculist.h>",
            "#include <linux/init.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/idr.h>\n#include <linux/sched/task.h>\n#include <linux/proc_fs.h>\n#include <linux/proc_ns.h>\n#include <linux/syscalls.h>\n#include <linux/init_task.h>\n#include <linux/pid_namespace.h>\n#include <linux/hash.h>\n#include <linux/memblock.h>\n#include <linux/rculist.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n\nstruct pid_namespace *task_active_pid_ns(struct task_struct *tsk)\n{\n\treturn ns_of_pid(task_pid(tsk));\n}"
        }
      },
      {
        "call_info": {
          "callee": "INIT_DELAYED_WORK",
          "args": [
            "&l->destroy_dwork",
            "cgroup_pidlist_destroy_work_fn"
          ],
          "line": 330
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "kzalloc",
          "args": [
            "sizeof(struct cgroup_pidlist)",
            "GFP_KERNEL"
          ],
          "line": 326
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_pidlist_find",
          "args": [
            "cgrp",
            "type"
          ],
          "line": 321
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_pidlist_find_create",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "314-337",
          "snippet": "static struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,\n\t\t\t\t\t\tenum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tl = cgroup_pidlist_find(cgrp, type);\n\tif (l)\n\t\treturn l;\n\n\t/* entry not found; create a new one */\n\tl = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);\n\tif (!l)\n\t\treturn l;\n\n\tINIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);\n\tl->key.type = type;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tl->key.ns = get_pid_ns(task_active_pid_ns(current));\n\tl->owner = cgrp;\n\tlist_add(&l->links, &cgrp->pidlists);\n\treturn l;\n}",
          "note": "cyclic_reference_detected"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&cgrp->pidlist_mutex"
          ],
          "line": 319
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,\n\t\t\t\t\t\tenum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tl = cgroup_pidlist_find(cgrp, type);\n\tif (l)\n\t\treturn l;\n\n\t/* entry not found; create a new one */\n\tl = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);\n\tif (!l)\n\t\treturn l;\n\n\tINIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);\n\tl->key.type = type;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tl->key.ns = get_pid_ns(task_active_pid_ns(current));\n\tl->owner = cgrp;\n\tlist_add(&l->links, &cgrp->pidlists);\n\treturn l;\n}"
  },
  {
    "function_name": "cgroup_pidlist_find",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "293-306",
    "snippet": "static struct cgroup_pidlist *cgroup_pidlist_find(struct cgroup *cgrp,\n\t\t\t\t\t\t  enum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tstruct pid_namespace *ns = task_active_pid_ns(current);\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tlist_for_each_entry(l, &cgrp->pidlists, links)\n\t\tif (l->key.type == type && l->key.ns == ns)\n\t\t\treturn l;\n\treturn NULL;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "list_for_each_entry",
          "args": [
            "l",
            "&cgrp->pidlists",
            "links"
          ],
          "line": 302
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&cgrp->pidlist_mutex"
          ],
          "line": 300
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_active_pid_ns",
          "args": [
            "current"
          ],
          "line": 298
        },
        "resolved": true,
        "details": {
          "function_name": "task_active_pid_ns",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/pid.c",
          "lines": "436-439",
          "snippet": "struct pid_namespace *task_active_pid_ns(struct task_struct *tsk)\n{\n\treturn ns_of_pid(task_pid(tsk));\n}",
          "includes": [
            "#include <linux/idr.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/init_task.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/hash.h>",
            "#include <linux/memblock.h>",
            "#include <linux/rculist.h>",
            "#include <linux/init.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/idr.h>\n#include <linux/sched/task.h>\n#include <linux/proc_fs.h>\n#include <linux/proc_ns.h>\n#include <linux/syscalls.h>\n#include <linux/init_task.h>\n#include <linux/pid_namespace.h>\n#include <linux/hash.h>\n#include <linux/memblock.h>\n#include <linux/rculist.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n\nstruct pid_namespace *task_active_pid_ns(struct task_struct *tsk)\n{\n\treturn ns_of_pid(task_pid(tsk));\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic struct cgroup_pidlist *cgroup_pidlist_find(struct cgroup *cgrp,\n\t\t\t\t\t\t  enum cgroup_filetype type)\n{\n\tstruct cgroup_pidlist *l;\n\t/* don't need task_nsproxy() if we're looking at ourself */\n\tstruct pid_namespace *ns = task_active_pid_ns(current);\n\n\tlockdep_assert_held(&cgrp->pidlist_mutex);\n\n\tlist_for_each_entry(l, &cgrp->pidlists, links)\n\t\tif (l->key.type == type && l->key.ns == ns)\n\t\t\treturn l;\n\treturn NULL;\n}"
  },
  {
    "function_name": "cmppid",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "288-291",
    "snippet": "static int cmppid(const void *a, const void *b)\n{\n\treturn *(pid_t *)a - *(pid_t *)b;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int cmppid(const void *a, const void *b)\n{\n\treturn *(pid_t *)a - *(pid_t *)b;\n}"
  },
  {
    "function_name": "pidlist_uniq",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "253-277",
    "snippet": "static int pidlist_uniq(pid_t *list, int length)\n{\n\tint src, dest = 1;\n\n\t/*\n\t * we presume the 0th element is unique, so i starts at 1. trivial\n\t * edge cases first; no work needs to be done for either\n\t */\n\tif (length == 0 || length == 1)\n\t\treturn length;\n\t/* src and dest walk down the list; dest counts unique elements */\n\tfor (src = 1; src < length; src++) {\n\t\t/* find next unique element */\n\t\twhile (list[src] == list[src-1]) {\n\t\t\tsrc++;\n\t\t\tif (src == length)\n\t\t\t\tgoto after;\n\t\t}\n\t\t/* dest always points to where the next unique element goes */\n\t\tlist[dest] = list[src];\n\t\tdest++;\n\t}\nafter:\n\treturn dest;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic int pidlist_uniq(pid_t *list, int length)\n{\n\tint src, dest = 1;\n\n\t/*\n\t * we presume the 0th element is unique, so i starts at 1. trivial\n\t * edge cases first; no work needs to be done for either\n\t */\n\tif (length == 0 || length == 1)\n\t\treturn length;\n\t/* src and dest walk down the list; dest counts unique elements */\n\tfor (src = 1; src < length; src++) {\n\t\t/* find next unique element */\n\t\twhile (list[src] == list[src-1]) {\n\t\t\tsrc++;\n\t\t\tif (src == length)\n\t\t\t\tgoto after;\n\t\t}\n\t\t/* dest always points to where the next unique element goes */\n\t\tlist[dest] = list[src];\n\t\tdest++;\n\t}\nafter:\n\treturn dest;\n}"
  },
  {
    "function_name": "cgroup_pidlist_destroy_work_fn",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "225-247",
    "snippet": "static void cgroup_pidlist_destroy_work_fn(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct cgroup_pidlist *l = container_of(dwork, struct cgroup_pidlist,\n\t\t\t\t\t\tdestroy_dwork);\n\tstruct cgroup_pidlist *tofree = NULL;\n\n\tmutex_lock(&l->owner->pidlist_mutex);\n\n\t/*\n\t * Destroy iff we didn't get queued again.  The state won't change\n\t * as destroy_dwork can only be queued while locked.\n\t */\n\tif (!delayed_work_pending(dwork)) {\n\t\tlist_del(&l->links);\n\t\tpidlist_free(l->list);\n\t\tput_pid_ns(l->key.ns);\n\t\ttofree = l;\n\t}\n\n\tmutex_unlock(&l->owner->pidlist_mutex);\n\tkfree(tofree);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kfree",
          "args": [
            "tofree"
          ],
          "line": 246
        },
        "resolved": true,
        "details": {
          "function_name": "maybe_kfree_parameter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/params.c",
          "lines": "73-86",
          "snippet": "static void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}",
          "includes": [
            "#include <linux/ctype.h>",
            "#include <linux/slab.h>",
            "#include <linux/err.h>",
            "#include <linux/device.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/module.h>",
            "#include <linux/errno.h>",
            "#include <linux/string.h>",
            "#include <linux/kernel.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static LIST_HEAD(kmalloced_params);",
            "static DEFINE_SPINLOCK(kmalloced_params_lock);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/ctype.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/device.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/kernel.h>\n\nstatic LIST_HEAD(kmalloced_params);\nstatic DEFINE_SPINLOCK(kmalloced_params_lock);\n\nstatic void maybe_kfree_parameter(void *param)\n{\n\tstruct kmalloced_param *p;\n\n\tspin_lock(&kmalloced_params_lock);\n\tlist_for_each_entry(p, &kmalloced_params, list) {\n\t\tif (p->val == param) {\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&kmalloced_params_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&l->owner->pidlist_mutex"
          ],
          "line": 245
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "put_pid_ns",
          "args": [
            "l->key.ns"
          ],
          "line": 241
        },
        "resolved": true,
        "details": {
          "function_name": "put_pid_ns",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/pid_namespace.c",
          "lines": "167-177",
          "snippet": "void put_pid_ns(struct pid_namespace *ns)\n{\n\tstruct pid_namespace *parent;\n\n\twhile (ns != &init_pid_ns) {\n\t\tparent = ns->parent;\n\t\tif (!kref_put(&ns->kref, free_pid_ns))\n\t\t\tbreak;\n\t\tns = parent;\n\t}\n}",
          "includes": [
            "#include <linux/idr.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/export.h>",
            "#include <linux/reboot.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/slab.h>",
            "#include <linux/acct.h>",
            "#include <linux/err.h>",
            "#include <linux/cred.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/user_namespace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/pid.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/idr.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/export.h>\n#include <linux/reboot.h>\n#include <linux/proc_ns.h>\n#include <linux/slab.h>\n#include <linux/acct.h>\n#include <linux/err.h>\n#include <linux/cred.h>\n#include <linux/syscalls.h>\n#include <linux/user_namespace.h>\n#include <linux/pid_namespace.h>\n#include <linux/pid.h>\n\nvoid put_pid_ns(struct pid_namespace *ns)\n{\n\tstruct pid_namespace *parent;\n\n\twhile (ns != &init_pid_ns) {\n\t\tparent = ns->parent;\n\t\tif (!kref_put(&ns->kref, free_pid_ns))\n\t\t\tbreak;\n\t\tns = parent;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "pidlist_free",
          "args": [
            "l->list"
          ],
          "line": 240
        },
        "resolved": true,
        "details": {
          "function_name": "pidlist_free",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
          "lines": "203-206",
          "snippet": "static void pidlist_free(void *p)\n{\n\tkvfree(p);\n}",
          "includes": [
            "#include <trace/events/cgroup.h>",
            "#include <linux/cgroupstats.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/slab.h>",
            "#include <linux/magic.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/mm.h>",
            "#include <linux/delay.h>",
            "#include <linux/sort.h>",
            "#include <linux/kmod.h>",
            "#include <linux/ctype.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void pidlist_free(void *p)\n{\n\tkvfree(p);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_del",
          "args": [
            "&l->links"
          ],
          "line": 239
        },
        "resolved": true,
        "details": {
          "function_name": "list_del_leaf_cfs_rq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/fair.c",
          "lines": "446-448",
          "snippet": "static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}",
          "includes": [
            "#include \"sched-pelt.h\"",
            "#include \"pelt.h\"",
            "#include <trace/events/sched.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void check_enqueue_throttle(struct cfs_rq *cfs_rq);",
            "static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);",
            "static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"sched-pelt.h\"\n#include \"pelt.h\"\n#include <trace/events/sched.h>\n#include \"sched.h\"\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "delayed_work_pending",
          "args": [
            "dwork"
          ],
          "line": 238
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&l->owner->pidlist_mutex"
          ],
          "line": 232
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "container_of",
          "args": [
            "dwork",
            "structcgroup_pidlist",
            "destroy_dwork"
          ],
          "line": 228
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "to_delayed_work",
          "args": [
            "work"
          ],
          "line": 227
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void cgroup_pidlist_destroy_work_fn(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct cgroup_pidlist *l = container_of(dwork, struct cgroup_pidlist,\n\t\t\t\t\t\tdestroy_dwork);\n\tstruct cgroup_pidlist *tofree = NULL;\n\n\tmutex_lock(&l->owner->pidlist_mutex);\n\n\t/*\n\t * Destroy iff we didn't get queued again.  The state won't change\n\t * as destroy_dwork can only be queued while locked.\n\t */\n\tif (!delayed_work_pending(dwork)) {\n\t\tlist_del(&l->links);\n\t\tpidlist_free(l->list);\n\t\tput_pid_ns(l->key.ns);\n\t\ttofree = l;\n\t}\n\n\tmutex_unlock(&l->owner->pidlist_mutex);\n\tkfree(tofree);\n}"
  },
  {
    "function_name": "cgroup1_pidlist_destroy_all",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "212-223",
    "snippet": "void cgroup1_pidlist_destroy_all(struct cgroup *cgrp)\n{\n\tstruct cgroup_pidlist *l, *tmp_l;\n\n\tmutex_lock(&cgrp->pidlist_mutex);\n\tlist_for_each_entry_safe(l, tmp_l, &cgrp->pidlists, links)\n\t\tmod_delayed_work(cgroup_pidlist_destroy_wq, &l->destroy_dwork, 0);\n\tmutex_unlock(&cgrp->pidlist_mutex);\n\n\tflush_workqueue(cgroup_pidlist_destroy_wq);\n\tBUG_ON(!list_empty(&cgrp->pidlists));\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static struct workqueue_struct *cgroup_pidlist_destroy_wq;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "BUG_ON",
          "args": [
            "!list_empty(&cgrp->pidlists)"
          ],
          "line": 222
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "list_empty",
          "args": [
            "&cgrp->pidlists"
          ],
          "line": 222
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_segcblist_empty",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/rcu_segcblist.h",
          "lines": "50-53",
          "snippet": "static inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}",
          "includes": [
            "#include <linux/rcu_segcblist.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "void rcu_segcblist_init(struct rcu_segcblist *rsclp);",
            "void rcu_segcblist_disable(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);",
            "bool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);",
            "struct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/rcu_segcblist.h>\n\nvoid rcu_segcblist_init(struct rcu_segcblist *rsclp);\nvoid rcu_segcblist_disable(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_ready_cbs(struct rcu_segcblist *rsclp);\nbool rcu_segcblist_pend_cbs(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_cb(struct rcu_segcblist *rsclp);\nstruct rcu_head *rcu_segcblist_first_pend_cb(struct rcu_segcblist *rsclp);\n\nstatic inline bool rcu_segcblist_empty(struct rcu_segcblist *rsclp)\n{\n\treturn !rsclp->head;\n}"
        }
      },
      {
        "call_info": {
          "callee": "flush_workqueue",
          "args": [
            "cgroup_pidlist_destroy_wq"
          ],
          "line": 221
        },
        "resolved": true,
        "details": {
          "function_name": "flush_workqueue",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/workqueue.c",
          "lines": "2643-2791",
          "snippet": "void flush_workqueue(struct workqueue_struct *wq)\n{\n\tstruct wq_flusher this_flusher = {\n\t\t.list = LIST_HEAD_INIT(this_flusher.list),\n\t\t.flush_color = -1,\n\t\t.done = COMPLETION_INITIALIZER_ONSTACK_MAP(this_flusher.done, wq->lockdep_map),\n\t};\n\tint next_color;\n\n\tif (WARN_ON(!wq_online))\n\t\treturn;\n\n\tlock_map_acquire(&wq->lockdep_map);\n\tlock_map_release(&wq->lockdep_map);\n\n\tmutex_lock(&wq->mutex);\n\n\t/*\n\t * Start-to-wait phase\n\t */\n\tnext_color = work_next_color(wq->work_color);\n\n\tif (next_color != wq->flush_color) {\n\t\t/*\n\t\t * Color space is not full.  The current work_color\n\t\t * becomes our flush_color and work_color is advanced\n\t\t * by one.\n\t\t */\n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow));\n\t\tthis_flusher.flush_color = wq->work_color;\n\t\twq->work_color = next_color;\n\n\t\tif (!wq->first_flusher) {\n\t\t\t/* no flush in progress, become the first flusher */\n\t\t\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\t\t\twq->first_flusher = &this_flusher;\n\n\t\t\tif (!flush_workqueue_prep_pwqs(wq, wq->flush_color,\n\t\t\t\t\t\t       wq->work_color)) {\n\t\t\t\t/* nothing to flush, done */\n\t\t\t\twq->flush_color = next_color;\n\t\t\t\twq->first_flusher = NULL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else {\n\t\t\t/* wait in queue */\n\t\t\tWARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);\n\t\t\tlist_add_tail(&this_flusher.list, &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Oops, color space is full, wait on overflow queue.\n\t\t * The next flush completion will assign us\n\t\t * flush_color and transfer to flusher_queue.\n\t\t */\n\t\tlist_add_tail(&this_flusher.list, &wq->flusher_overflow);\n\t}\n\n\tcheck_flush_dependency(wq, NULL);\n\n\tmutex_unlock(&wq->mutex);\n\n\twait_for_completion(&this_flusher.done);\n\n\t/*\n\t * Wake-up-and-cascade phase\n\t *\n\t * First flushers are responsible for cascading flushes and\n\t * handling overflow.  Non-first flushers can simply return.\n\t */\n\tif (wq->first_flusher != &this_flusher)\n\t\treturn;\n\n\tmutex_lock(&wq->mutex);\n\n\t/* we might have raced, check again with mutex held */\n\tif (wq->first_flusher != &this_flusher)\n\t\tgoto out_unlock;\n\n\twq->first_flusher = NULL;\n\n\tWARN_ON_ONCE(!list_empty(&this_flusher.list));\n\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\twhile (true) {\n\t\tstruct wq_flusher *next, *tmp;\n\n\t\t/* complete all the flushers sharing the current flush color */\n\t\tlist_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {\n\t\t\tif (next->flush_color != wq->flush_color)\n\t\t\t\tbreak;\n\t\t\tlist_del_init(&next->list);\n\t\t\tcomplete(&next->done);\n\t\t}\n\n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&\n\t\t\t     wq->flush_color != work_next_color(wq->work_color));\n\n\t\t/* this flush_color is finished, advance by one */\n\t\twq->flush_color = work_next_color(wq->flush_color);\n\n\t\t/* one color has been freed, handle overflow queue */\n\t\tif (!list_empty(&wq->flusher_overflow)) {\n\t\t\t/*\n\t\t\t * Assign the same color to all overflowed\n\t\t\t * flushers, advance work_color and append to\n\t\t\t * flusher_queue.  This is the start-to-wait\n\t\t\t * phase for these overflowed flushers.\n\t\t\t */\n\t\t\tlist_for_each_entry(tmp, &wq->flusher_overflow, list)\n\t\t\t\ttmp->flush_color = wq->work_color;\n\n\t\t\twq->work_color = work_next_color(wq->work_color);\n\n\t\t\tlist_splice_tail_init(&wq->flusher_overflow,\n\t\t\t\t\t      &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\n\t\tif (list_empty(&wq->flusher_queue)) {\n\t\t\tWARN_ON_ONCE(wq->flush_color != wq->work_color);\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Need to flush more colors.  Make the next flusher\n\t\t * the new first flusher and arm pwqs.\n\t\t */\n\t\tWARN_ON_ONCE(wq->flush_color == wq->work_color);\n\t\tWARN_ON_ONCE(wq->flush_color != next->flush_color);\n\n\t\tlist_del_init(&next->list);\n\t\twq->first_flusher = next;\n\n\t\tif (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Meh... this color is already done, clear first\n\t\t * flusher and repeat cascading.\n\t\t */\n\t\twq->first_flusher = NULL;\n\t}\n\nout_unlock:\n\tmutex_unlock(&wq->mutex);\n}",
          "includes": [
            "#include <trace/events/workqueue.h>",
            "#include \"workqueue_internal.h\"",
            "#include <linux/nmi.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/nodemask.h>",
            "#include <linux/rculist.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/jhash.h>",
            "#include <linux/idr.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/freezer.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/kthread.h>",
            "#include <linux/notifier.h>",
            "#include <linux/cpu.h>",
            "#include <linux/slab.h>",
            "#include <linux/workqueue.h>",
            "#include <linux/completion.h>",
            "#include <linux/signal.h>",
            "#include <linux/init.h>",
            "#include <linux/sched.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static bool wq_online;",
            "static void workqueue_sysfs_unregister(struct workqueue_struct *wq);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/workqueue.h>\n#include \"workqueue_internal.h\"\n#include <linux/nmi.h>\n#include <linux/sched/isolation.h>\n#include <linux/uaccess.h>\n#include <linux/moduleparam.h>\n#include <linux/nodemask.h>\n#include <linux/rculist.h>\n#include <linux/hashtable.h>\n#include <linux/jhash.h>\n#include <linux/idr.h>\n#include <linux/lockdep.h>\n#include <linux/debug_locks.h>\n#include <linux/freezer.h>\n#include <linux/mempolicy.h>\n#include <linux/hardirq.h>\n#include <linux/kthread.h>\n#include <linux/notifier.h>\n#include <linux/cpu.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/completion.h>\n#include <linux/signal.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n\nstatic bool wq_online;\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq);\n\nvoid flush_workqueue(struct workqueue_struct *wq)\n{\n\tstruct wq_flusher this_flusher = {\n\t\t.list = LIST_HEAD_INIT(this_flusher.list),\n\t\t.flush_color = -1,\n\t\t.done = COMPLETION_INITIALIZER_ONSTACK_MAP(this_flusher.done, wq->lockdep_map),\n\t};\n\tint next_color;\n\n\tif (WARN_ON(!wq_online))\n\t\treturn;\n\n\tlock_map_acquire(&wq->lockdep_map);\n\tlock_map_release(&wq->lockdep_map);\n\n\tmutex_lock(&wq->mutex);\n\n\t/*\n\t * Start-to-wait phase\n\t */\n\tnext_color = work_next_color(wq->work_color);\n\n\tif (next_color != wq->flush_color) {\n\t\t/*\n\t\t * Color space is not full.  The current work_color\n\t\t * becomes our flush_color and work_color is advanced\n\t\t * by one.\n\t\t */\n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow));\n\t\tthis_flusher.flush_color = wq->work_color;\n\t\twq->work_color = next_color;\n\n\t\tif (!wq->first_flusher) {\n\t\t\t/* no flush in progress, become the first flusher */\n\t\t\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\t\t\twq->first_flusher = &this_flusher;\n\n\t\t\tif (!flush_workqueue_prep_pwqs(wq, wq->flush_color,\n\t\t\t\t\t\t       wq->work_color)) {\n\t\t\t\t/* nothing to flush, done */\n\t\t\t\twq->flush_color = next_color;\n\t\t\t\twq->first_flusher = NULL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else {\n\t\t\t/* wait in queue */\n\t\t\tWARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);\n\t\t\tlist_add_tail(&this_flusher.list, &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Oops, color space is full, wait on overflow queue.\n\t\t * The next flush completion will assign us\n\t\t * flush_color and transfer to flusher_queue.\n\t\t */\n\t\tlist_add_tail(&this_flusher.list, &wq->flusher_overflow);\n\t}\n\n\tcheck_flush_dependency(wq, NULL);\n\n\tmutex_unlock(&wq->mutex);\n\n\twait_for_completion(&this_flusher.done);\n\n\t/*\n\t * Wake-up-and-cascade phase\n\t *\n\t * First flushers are responsible for cascading flushes and\n\t * handling overflow.  Non-first flushers can simply return.\n\t */\n\tif (wq->first_flusher != &this_flusher)\n\t\treturn;\n\n\tmutex_lock(&wq->mutex);\n\n\t/* we might have raced, check again with mutex held */\n\tif (wq->first_flusher != &this_flusher)\n\t\tgoto out_unlock;\n\n\twq->first_flusher = NULL;\n\n\tWARN_ON_ONCE(!list_empty(&this_flusher.list));\n\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\twhile (true) {\n\t\tstruct wq_flusher *next, *tmp;\n\n\t\t/* complete all the flushers sharing the current flush color */\n\t\tlist_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {\n\t\t\tif (next->flush_color != wq->flush_color)\n\t\t\t\tbreak;\n\t\t\tlist_del_init(&next->list);\n\t\t\tcomplete(&next->done);\n\t\t}\n\n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&\n\t\t\t     wq->flush_color != work_next_color(wq->work_color));\n\n\t\t/* this flush_color is finished, advance by one */\n\t\twq->flush_color = work_next_color(wq->flush_color);\n\n\t\t/* one color has been freed, handle overflow queue */\n\t\tif (!list_empty(&wq->flusher_overflow)) {\n\t\t\t/*\n\t\t\t * Assign the same color to all overflowed\n\t\t\t * flushers, advance work_color and append to\n\t\t\t * flusher_queue.  This is the start-to-wait\n\t\t\t * phase for these overflowed flushers.\n\t\t\t */\n\t\t\tlist_for_each_entry(tmp, &wq->flusher_overflow, list)\n\t\t\t\ttmp->flush_color = wq->work_color;\n\n\t\t\twq->work_color = work_next_color(wq->work_color);\n\n\t\t\tlist_splice_tail_init(&wq->flusher_overflow,\n\t\t\t\t\t      &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\n\t\tif (list_empty(&wq->flusher_queue)) {\n\t\t\tWARN_ON_ONCE(wq->flush_color != wq->work_color);\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Need to flush more colors.  Make the next flusher\n\t\t * the new first flusher and arm pwqs.\n\t\t */\n\t\tWARN_ON_ONCE(wq->flush_color == wq->work_color);\n\t\tWARN_ON_ONCE(wq->flush_color != next->flush_color);\n\n\t\tlist_del_init(&next->list);\n\t\twq->first_flusher = next;\n\n\t\tif (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Meh... this color is already done, clear first\n\t\t * flusher and repeat cascading.\n\t\t */\n\t\twq->first_flusher = NULL;\n\t}\n\nout_unlock:\n\tmutex_unlock(&wq->mutex);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgrp->pidlist_mutex"
          ],
          "line": 219
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mod_delayed_work",
          "args": [
            "cgroup_pidlist_destroy_wq",
            "&l->destroy_dwork",
            "0"
          ],
          "line": 218
        },
        "resolved": true,
        "details": {
          "function_name": "kthread_mod_delayed_work",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/kthread.c",
          "lines": "1040-1067",
          "snippet": "bool kthread_mod_delayed_work(struct kthread_worker *worker,\n\t\t\t      struct kthread_delayed_work *dwork,\n\t\t\t      unsigned long delay)\n{\n\tstruct kthread_work *work = &dwork->work;\n\tunsigned long flags;\n\tint ret = false;\n\n\tspin_lock_irqsave(&worker->lock, flags);\n\n\t/* Do not bother with canceling when never queued. */\n\tif (!work->worker)\n\t\tgoto fast_queue;\n\n\t/* Work must not be used with >1 worker, see kthread_queue_work() */\n\tWARN_ON_ONCE(work->worker != worker);\n\n\t/* Do not fight with another command that is canceling this work. */\n\tif (work->canceling)\n\t\tgoto out;\n\n\tret = __kthread_cancel_work(work, true, &flags);\nfast_queue:\n\t__kthread_queue_delayed_work(worker, dwork, delay);\nout:\n\tspin_unlock_irqrestore(&worker->lock, flags);\n\treturn ret;\n}",
          "includes": [
            "#include <trace/events/sched.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/freezer.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/export.h>",
            "#include <linux/file.h>",
            "#include <linux/unistd.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/err.h>",
            "#include <linux/completion.h>",
            "#include <linux/kthread.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <uapi/linux/sched/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <trace/events/sched.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/freezer.h>\n#include <linux/slab.h>\n#include <linux/mutex.h>\n#include <linux/export.h>\n#include <linux/file.h>\n#include <linux/unistd.h>\n#include <linux/cpuset.h>\n#include <linux/err.h>\n#include <linux/completion.h>\n#include <linux/kthread.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <uapi/linux/sched/types.h>\n\nbool kthread_mod_delayed_work(struct kthread_worker *worker,\n\t\t\t      struct kthread_delayed_work *dwork,\n\t\t\t      unsigned long delay)\n{\n\tstruct kthread_work *work = &dwork->work;\n\tunsigned long flags;\n\tint ret = false;\n\n\tspin_lock_irqsave(&worker->lock, flags);\n\n\t/* Do not bother with canceling when never queued. */\n\tif (!work->worker)\n\t\tgoto fast_queue;\n\n\t/* Work must not be used with >1 worker, see kthread_queue_work() */\n\tWARN_ON_ONCE(work->worker != worker);\n\n\t/* Do not fight with another command that is canceling this work. */\n\tif (work->canceling)\n\t\tgoto out;\n\n\tret = __kthread_cancel_work(work, true, &flags);\nfast_queue:\n\t__kthread_queue_delayed_work(worker, dwork, delay);\nout:\n\tspin_unlock_irqrestore(&worker->lock, flags);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_for_each_entry_safe",
          "args": [
            "l",
            "tmp_l",
            "&cgrp->pidlists",
            "links"
          ],
          "line": 217
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgrp->pidlist_mutex"
          ],
          "line": 216
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic struct workqueue_struct *cgroup_pidlist_destroy_wq;\n\nvoid cgroup1_pidlist_destroy_all(struct cgroup *cgrp)\n{\n\tstruct cgroup_pidlist *l, *tmp_l;\n\n\tmutex_lock(&cgrp->pidlist_mutex);\n\tlist_for_each_entry_safe(l, tmp_l, &cgrp->pidlists, links)\n\t\tmod_delayed_work(cgroup_pidlist_destroy_wq, &l->destroy_dwork, 0);\n\tmutex_unlock(&cgrp->pidlist_mutex);\n\n\tflush_workqueue(cgroup_pidlist_destroy_wq);\n\tBUG_ON(!list_empty(&cgrp->pidlists));\n}"
  },
  {
    "function_name": "pidlist_free",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "203-206",
    "snippet": "static void pidlist_free(void *p)\n{\n\tkvfree(p);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kvfree",
          "args": [
            "p"
          ],
          "line": 205
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void pidlist_free(void *p)\n{\n\tkvfree(p);\n}"
  },
  {
    "function_name": "pidlist_allocate",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "195-201",
    "snippet": "static void *pidlist_allocate(int count)\n{\n\tif (PIDLIST_TOO_LARGE(count))\n\t\treturn vmalloc(array_size(count, sizeof(pid_t)));\n\telse\n\t\treturn kmalloc_array(count, sizeof(pid_t), GFP_KERNEL);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "kmalloc_array",
          "args": [
            "count",
            "sizeof(pid_t)",
            "GFP_KERNEL"
          ],
          "line": 200
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "vmalloc",
          "args": [
            "array_size(count, sizeof(pid_t))"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "array_size",
          "args": [
            "count",
            "sizeof(pid_t)"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "PIDLIST_TOO_LARGE",
          "args": [
            "count"
          ],
          "line": 197
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic void *pidlist_allocate(int count)\n{\n\tif (PIDLIST_TOO_LARGE(count))\n\t\treturn vmalloc(array_size(count, sizeof(pid_t)));\n\telse\n\t\treturn kmalloc_array(count, sizeof(pid_t), GFP_KERNEL);\n}"
  },
  {
    "function_name": "cgroup_transfer_tasks",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "91-147",
    "snippet": "int cgroup_transfer_tasks(struct cgroup *to, struct cgroup *from)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct cgrp_cset_link *link;\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\tint ret;\n\n\tif (cgroup_on_dfl(to))\n\t\treturn -EINVAL;\n\n\tret = cgroup_migrate_vet_dst(to);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\t/* all tasks in @from are being moved, all csets are source */\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(link, &from->cset_links, cset_link)\n\t\tcgroup_migrate_add_src(link->cset, to, &mgctx);\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (ret)\n\t\tgoto out_err;\n\n\t/*\n\t * Migrate tasks one-by-one until @from is empty.  This fails iff\n\t * ->can_attach() fails.\n\t */\n\tdo {\n\t\tcss_task_iter_start(&from->self, 0, &it);\n\n\t\tdo {\n\t\t\ttask = css_task_iter_next(&it);\n\t\t} while (task && (task->flags & PF_EXITING));\n\n\t\tif (task)\n\t\t\tget_task_struct(task);\n\t\tcss_task_iter_end(&it);\n\n\t\tif (task) {\n\t\t\tret = cgroup_migrate(task, false, &mgctx);\n\t\t\tif (!ret)\n\t\t\t\tTRACE_CGROUP_PATH(transfer_tasks, to, task, false);\n\t\t\tput_task_struct(task);\n\t\t}\n\t} while (task && !ret);\nout_err:\n\tcgroup_migrate_finish(&mgctx);\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 145
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "percpu_up_write",
          "args": [
            "&cgroup_threadgroup_rwsem"
          ],
          "line": 144
        },
        "resolved": true,
        "details": {
          "function_name": "percpu_up_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "166-191",
          "snippet": "void percpu_up_write(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * Signal the writer is done, no fast path yet.\n\t *\n\t * One reason that we cannot just immediately flip to readers_fast is\n\t * that new readers might fail to see the results of this writer's\n\t * critical section.\n\t *\n\t * Therefore we force it through the slow path which guarantees an\n\t * acquire and thereby guarantees the critical section's consistency.\n\t */\n\tsmp_store_release(&sem->readers_block, 0);\n\n\t/*\n\t * Release the write lock, this will allow readers back in the game.\n\t */\n\tup_write(&sem->rw_sem);\n\n\t/*\n\t * Once this completes (at least one RCU-sched grace period hence) the\n\t * reader fast path will be available again. Safe to use outside the\n\t * exclusive write lock because its counting.\n\t */\n\trcu_sync_exit(&sem->rss);\n}",
          "includes": [
            "#include <linux/errno.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/percpu.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid percpu_up_write(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * Signal the writer is done, no fast path yet.\n\t *\n\t * One reason that we cannot just immediately flip to readers_fast is\n\t * that new readers might fail to see the results of this writer's\n\t * critical section.\n\t *\n\t * Therefore we force it through the slow path which guarantees an\n\t * acquire and thereby guarantees the critical section's consistency.\n\t */\n\tsmp_store_release(&sem->readers_block, 0);\n\n\t/*\n\t * Release the write lock, this will allow readers back in the game.\n\t */\n\tup_write(&sem->rw_sem);\n\n\t/*\n\t * Once this completes (at least one RCU-sched grace period hence) the\n\t * reader fast path will be available again. Safe to use outside the\n\t * exclusive write lock because its counting.\n\t */\n\trcu_sync_exit(&sem->rss);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_migrate_finish",
          "args": [
            "&mgctx"
          ],
          "line": 143
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_migrate_finish",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2426-2447",
          "snippet": "void cgroup_migrate_finish(struct cgroup_mgctx *mgctx)\n{\n\tLIST_HEAD(preloaded);\n\tstruct css_set *cset, *tmp_cset;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tspin_lock_irq(&css_set_lock);\n\n\tlist_splice_tail_init(&mgctx->preloaded_src_csets, &preloaded);\n\tlist_splice_tail_init(&mgctx->preloaded_dst_csets, &preloaded);\n\n\tlist_for_each_entry_safe(cset, tmp_cset, &preloaded, mg_preload_node) {\n\t\tcset->mg_src_cgrp = NULL;\n\t\tcset->mg_dst_cgrp = NULL;\n\t\tcset->mg_dst_cset = NULL;\n\t\tlist_del_init(&cset->mg_preload_node);\n\t\tput_css_set_locked(cset);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nvoid cgroup_migrate_finish(struct cgroup_mgctx *mgctx)\n{\n\tLIST_HEAD(preloaded);\n\tstruct css_set *cset, *tmp_cset;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tspin_lock_irq(&css_set_lock);\n\n\tlist_splice_tail_init(&mgctx->preloaded_src_csets, &preloaded);\n\tlist_splice_tail_init(&mgctx->preloaded_dst_csets, &preloaded);\n\n\tlist_for_each_entry_safe(cset, tmp_cset, &preloaded, mg_preload_node) {\n\t\tcset->mg_src_cgrp = NULL;\n\t\tcset->mg_dst_cgrp = NULL;\n\t\tcset->mg_dst_cset = NULL;\n\t\tlist_del_init(&cset->mg_preload_node);\n\t\tput_css_set_locked(cset);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "put_task_struct",
          "args": [
            "task"
          ],
          "line": 139
        },
        "resolved": true,
        "details": {
          "function_name": "__put_task_struct",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/fork.c",
          "lines": "716-731",
          "snippet": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}",
          "includes": [
            "#include <linux/init_task.h>",
            "#include <trace/events/task.h>",
            "#include <trace/events/sched.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/pgtable.h>",
            "#include <linux/stackleak.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/kcov.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/compiler.h>",
            "#include <linux/aio.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/oom.h>",
            "#include <linux/user-return-notifier.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/magic.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/tty.h>",
            "#include <linux/random.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/acct.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/profile.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/audit.h>",
            "#include <linux/mount.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/kthread.h>",
            "#include <linux/compat.h>",
            "#include <linux/futex.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/seccomp.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/cpu.h>",
            "#include <linux/capability.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>",
            "#include <linux/hmm.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/mman.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/sem.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/module.h>",
            "#include <linux/unistd.h>",
            "#include <linux/init.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __latent_entropy struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/init_task.h>\n#include <trace/events/task.h>\n#include <trace/events/sched.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/pgtable.h>\n#include <linux/stackleak.h>\n#include <linux/thread_info.h>\n#include <linux/livepatch.h>\n#include <linux/kcov.h>\n#include <linux/sysctl.h>\n#include <linux/compiler.h>\n#include <linux/aio.h>\n#include <linux/uprobes.h>\n#include <linux/signalfd.h>\n#include <linux/khugepaged.h>\n#include <linux/oom.h>\n#include <linux/user-return-notifier.h>\n#include <linux/posix-timers.h>\n#include <linux/perf_event.h>\n#include <linux/sched/mm.h>\n#include <linux/magic.h>\n#include <linux/fs_struct.h>\n#include <linux/blkdev.h>\n#include <linux/tty.h>\n#include <linux/random.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/freezer.h>\n#include <linux/cn_proc.h>\n#include <linux/tsacct_kern.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/acct.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/profile.h>\n#include <linux/proc_fs.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/audit.h>\n#include <linux/mount.h>\n#include <linux/ptrace.h>\n#include <linux/rcupdate.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/kthread.h>\n#include <linux/compat.h>\n#include <linux/futex.h>\n#include <linux/jiffies.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/seccomp.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/cgroup.h>\n#include <linux/cpu.h>\n#include <linux/capability.h>\n#include <linux/nsproxy.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/hmm.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mman.h>\n#include <linux/binfmts.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/sem.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/vmalloc.h>\n#include <linux/module.h>\n#include <linux/unistd.h>\n#include <linux/init.h>\n#include <linux/rtmutex.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/user.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n\nstatic __latent_entropy struct;\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "TRACE_CGROUP_PATH",
          "args": [
            "transfer_tasks",
            "to",
            "task",
            "false"
          ],
          "line": 138
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_migrate",
          "args": [
            "task",
            "false",
            "&mgctx"
          ],
          "line": 136
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_migrate",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2582-2604",
          "snippet": "int cgroup_migrate(struct task_struct *leader, bool threadgroup,\n\t\t   struct cgroup_mgctx *mgctx)\n{\n\tstruct task_struct *task;\n\n\t/*\n\t * Prevent freeing of tasks while we take a snapshot. Tasks that are\n\t * already PF_EXITING could be freed from underneath us unless we\n\t * take an rcu_read_lock.\n\t */\n\tspin_lock_irq(&css_set_lock);\n\trcu_read_lock();\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_task(task, mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\trcu_read_unlock();\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn cgroup_migrate_execute(mgctx);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nint cgroup_migrate(struct task_struct *leader, bool threadgroup,\n\t\t   struct cgroup_mgctx *mgctx)\n{\n\tstruct task_struct *task;\n\n\t/*\n\t * Prevent freeing of tasks while we take a snapshot. Tasks that are\n\t * already PF_EXITING could be freed from underneath us unless we\n\t * take an rcu_read_lock.\n\t */\n\tspin_lock_irq(&css_set_lock);\n\trcu_read_lock();\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_task(task, mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\trcu_read_unlock();\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn cgroup_migrate_execute(mgctx);\n}"
        }
      },
      {
        "call_info": {
          "callee": "css_task_iter_end",
          "args": [
            "&it"
          ],
          "line": 133
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "task"
          ],
          "line": 132
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "css_task_iter_next",
          "args": [
            "&it"
          ],
          "line": 128
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "css_task_iter_start",
          "args": [
            "&from->self",
            "0",
            "&it"
          ],
          "line": 125
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cgroup_migrate_prepare_dst",
          "args": [
            "&mgctx"
          ],
          "line": 116
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_migrate_prepare_dst",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2512-2562",
          "snippet": "int cgroup_migrate_prepare_dst(struct cgroup_mgctx *mgctx)\n{\n\tstruct css_set *src_cset, *tmp_cset;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* look up the dst cset for each src cset and link it to src */\n\tlist_for_each_entry_safe(src_cset, tmp_cset, &mgctx->preloaded_src_csets,\n\t\t\t\t mg_preload_node) {\n\t\tstruct css_set *dst_cset;\n\t\tstruct cgroup_subsys *ss;\n\t\tint ssid;\n\n\t\tdst_cset = find_css_set(src_cset, src_cset->mg_dst_cgrp);\n\t\tif (!dst_cset)\n\t\t\tgoto err;\n\n\t\tWARN_ON_ONCE(src_cset->mg_dst_cset || dst_cset->mg_dst_cset);\n\n\t\t/*\n\t\t * If src cset equals dst, it's noop.  Drop the src.\n\t\t * cgroup_migrate() will skip the cset too.  Note that we\n\t\t * can't handle src == dst as some nodes are used by both.\n\t\t */\n\t\tif (src_cset == dst_cset) {\n\t\t\tsrc_cset->mg_src_cgrp = NULL;\n\t\t\tsrc_cset->mg_dst_cgrp = NULL;\n\t\t\tlist_del_init(&src_cset->mg_preload_node);\n\t\t\tput_css_set(src_cset);\n\t\t\tput_css_set(dst_cset);\n\t\t\tcontinue;\n\t\t}\n\n\t\tsrc_cset->mg_dst_cset = dst_cset;\n\n\t\tif (list_empty(&dst_cset->mg_preload_node))\n\t\t\tlist_add_tail(&dst_cset->mg_preload_node,\n\t\t\t\t      &mgctx->preloaded_dst_csets);\n\t\telse\n\t\t\tput_css_set(dst_cset);\n\n\t\tfor_each_subsys(ss, ssid)\n\t\t\tif (src_cset->subsys[ssid] != dst_cset->subsys[ssid])\n\t\t\t\tmgctx->ss_mask |= 1 << ssid;\n\t}\n\n\treturn 0;\nerr:\n\tcgroup_migrate_finish(mgctx);\n\treturn -ENOMEM;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void css_task_iter_advance(struct css_task_iter *it);",
            "static struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic void css_task_iter_advance(struct css_task_iter *it);\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);\n\nint cgroup_migrate_prepare_dst(struct cgroup_mgctx *mgctx)\n{\n\tstruct css_set *src_cset, *tmp_cset;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* look up the dst cset for each src cset and link it to src */\n\tlist_for_each_entry_safe(src_cset, tmp_cset, &mgctx->preloaded_src_csets,\n\t\t\t\t mg_preload_node) {\n\t\tstruct css_set *dst_cset;\n\t\tstruct cgroup_subsys *ss;\n\t\tint ssid;\n\n\t\tdst_cset = find_css_set(src_cset, src_cset->mg_dst_cgrp);\n\t\tif (!dst_cset)\n\t\t\tgoto err;\n\n\t\tWARN_ON_ONCE(src_cset->mg_dst_cset || dst_cset->mg_dst_cset);\n\n\t\t/*\n\t\t * If src cset equals dst, it's noop.  Drop the src.\n\t\t * cgroup_migrate() will skip the cset too.  Note that we\n\t\t * can't handle src == dst as some nodes are used by both.\n\t\t */\n\t\tif (src_cset == dst_cset) {\n\t\t\tsrc_cset->mg_src_cgrp = NULL;\n\t\t\tsrc_cset->mg_dst_cgrp = NULL;\n\t\t\tlist_del_init(&src_cset->mg_preload_node);\n\t\t\tput_css_set(src_cset);\n\t\t\tput_css_set(dst_cset);\n\t\t\tcontinue;\n\t\t}\n\n\t\tsrc_cset->mg_dst_cset = dst_cset;\n\n\t\tif (list_empty(&dst_cset->mg_preload_node))\n\t\t\tlist_add_tail(&dst_cset->mg_preload_node,\n\t\t\t\t      &mgctx->preloaded_dst_csets);\n\t\telse\n\t\t\tput_css_set(dst_cset);\n\n\t\tfor_each_subsys(ss, ssid)\n\t\t\tif (src_cset->subsys[ssid] != dst_cset->subsys[ssid])\n\t\t\t\tmgctx->ss_mask |= 1 << ssid;\n\t}\n\n\treturn 0;\nerr:\n\tcgroup_migrate_finish(mgctx);\n\treturn -ENOMEM;\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 114
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_migrate_add_src",
          "args": [
            "link->cset",
            "to",
            "&mgctx"
          ],
          "line": 113
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_migrate_add_src",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2465-2496",
          "snippet": "void cgroup_migrate_add_src(struct css_set *src_cset,\n\t\t\t    struct cgroup *dst_cgrp,\n\t\t\t    struct cgroup_mgctx *mgctx)\n{\n\tstruct cgroup *src_cgrp;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\tlockdep_assert_held(&css_set_lock);\n\n\t/*\n\t * If ->dead, @src_set is associated with one or more dead cgroups\n\t * and doesn't contain any migratable tasks.  Ignore it early so\n\t * that the rest of migration path doesn't get confused by it.\n\t */\n\tif (src_cset->dead)\n\t\treturn;\n\n\tsrc_cgrp = cset_cgroup_from_root(src_cset, dst_cgrp->root);\n\n\tif (!list_empty(&src_cset->mg_preload_node))\n\t\treturn;\n\n\tWARN_ON(src_cset->mg_src_cgrp);\n\tWARN_ON(src_cset->mg_dst_cgrp);\n\tWARN_ON(!list_empty(&src_cset->mg_tasks));\n\tWARN_ON(!list_empty(&src_cset->mg_node));\n\n\tsrc_cset->mg_src_cgrp = src_cgrp;\n\tsrc_cset->mg_dst_cgrp = dst_cgrp;\n\tget_css_set(src_cset);\n\tlist_add_tail(&src_cset->mg_preload_node, &mgctx->preloaded_src_csets);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void css_task_iter_advance(struct css_task_iter *it);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic void css_task_iter_advance(struct css_task_iter *it);\n\nvoid cgroup_migrate_add_src(struct css_set *src_cset,\n\t\t\t    struct cgroup *dst_cgrp,\n\t\t\t    struct cgroup_mgctx *mgctx)\n{\n\tstruct cgroup *src_cgrp;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\tlockdep_assert_held(&css_set_lock);\n\n\t/*\n\t * If ->dead, @src_set is associated with one or more dead cgroups\n\t * and doesn't contain any migratable tasks.  Ignore it early so\n\t * that the rest of migration path doesn't get confused by it.\n\t */\n\tif (src_cset->dead)\n\t\treturn;\n\n\tsrc_cgrp = cset_cgroup_from_root(src_cset, dst_cgrp->root);\n\n\tif (!list_empty(&src_cset->mg_preload_node))\n\t\treturn;\n\n\tWARN_ON(src_cset->mg_src_cgrp);\n\tWARN_ON(src_cset->mg_dst_cgrp);\n\tWARN_ON(!list_empty(&src_cset->mg_tasks));\n\tWARN_ON(!list_empty(&src_cset->mg_node));\n\n\tsrc_cset->mg_src_cgrp = src_cgrp;\n\tsrc_cset->mg_dst_cgrp = dst_cgrp;\n\tget_css_set(src_cset);\n\tlist_add_tail(&src_cset->mg_preload_node, &mgctx->preloaded_src_csets);\n}"
        }
      },
      {
        "call_info": {
          "callee": "list_for_each_entry",
          "args": [
            "link",
            "&from->cset_links",
            "cset_link"
          ],
          "line": 112
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "spin_lock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 111
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "percpu_down_write",
          "args": [
            "&cgroup_threadgroup_rwsem"
          ],
          "line": 108
        },
        "resolved": true,
        "details": {
          "function_name": "percpu_down_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "140-163",
          "snippet": "void percpu_down_write(struct percpu_rw_semaphore *sem)\n{\n\t/* Notify readers to take the slow path. */\n\trcu_sync_enter(&sem->rss);\n\n\tdown_write(&sem->rw_sem);\n\n\t/*\n\t * Notify new readers to block; up until now, and thus throughout the\n\t * longish rcu_sync_enter() above, new readers could still come in.\n\t */\n\tWRITE_ONCE(sem->readers_block, 1);\n\n\tsmp_mb(); /* D matches A */\n\n\t/*\n\t * If they don't see our writer of readers_block, then we are\n\t * guaranteed to see their sem->read_count increment, and therefore\n\t * will wait for them.\n\t */\n\n\t/* Wait for all now active readers to complete. */\n\trcuwait_wait_event(&sem->writer, readers_active_check(sem));\n}",
          "includes": [
            "#include <linux/errno.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/percpu.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid percpu_down_write(struct percpu_rw_semaphore *sem)\n{\n\t/* Notify readers to take the slow path. */\n\trcu_sync_enter(&sem->rss);\n\n\tdown_write(&sem->rw_sem);\n\n\t/*\n\t * Notify new readers to block; up until now, and thus throughout the\n\t * longish rcu_sync_enter() above, new readers could still come in.\n\t */\n\tWRITE_ONCE(sem->readers_block, 1);\n\n\tsmp_mb(); /* D matches A */\n\n\t/*\n\t * If they don't see our writer of readers_block, then we are\n\t * guaranteed to see their sem->read_count increment, and therefore\n\t * will wait for them.\n\t */\n\n\t/* Wait for all now active readers to complete. */\n\trcuwait_wait_event(&sem->writer, readers_active_check(sem));\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 106
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_migrate_vet_dst",
          "args": [
            "to"
          ],
          "line": 102
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_migrate_vet_dst",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2391-2417",
          "snippet": "int cgroup_migrate_vet_dst(struct cgroup *dst_cgrp)\n{\n\t/* v1 doesn't have any restriction */\n\tif (!cgroup_on_dfl(dst_cgrp))\n\t\treturn 0;\n\n\t/* verify @dst_cgrp can host resources */\n\tif (!cgroup_is_valid_domain(dst_cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/* mixables don't care */\n\tif (cgroup_is_mixable(dst_cgrp))\n\t\treturn 0;\n\n\t/*\n\t * If @dst_cgrp is already or can become a thread root or is\n\t * threaded, it doesn't matter.\n\t */\n\tif (cgroup_can_be_thread_root(dst_cgrp) || cgroup_is_threaded(dst_cgrp))\n\t\treturn 0;\n\n\t/* apply no-internal-process constraint */\n\tif (dst_cgrp->subtree_control)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void css_task_iter_advance(struct css_task_iter *it);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic void css_task_iter_advance(struct css_task_iter *it);\n\nint cgroup_migrate_vet_dst(struct cgroup *dst_cgrp)\n{\n\t/* v1 doesn't have any restriction */\n\tif (!cgroup_on_dfl(dst_cgrp))\n\t\treturn 0;\n\n\t/* verify @dst_cgrp can host resources */\n\tif (!cgroup_is_valid_domain(dst_cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/* mixables don't care */\n\tif (cgroup_is_mixable(dst_cgrp))\n\t\treturn 0;\n\n\t/*\n\t * If @dst_cgrp is already or can become a thread root or is\n\t * threaded, it doesn't matter.\n\t */\n\tif (cgroup_can_be_thread_root(dst_cgrp) || cgroup_is_threaded(dst_cgrp))\n\t\treturn 0;\n\n\t/* apply no-internal-process constraint */\n\tif (dst_cgrp->subtree_control)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_on_dfl",
          "args": [
            "to"
          ],
          "line": 99
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_on_dfl",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "295-298",
          "snippet": "bool cgroup_on_dfl(const struct cgroup *cgrp)\n{\n\treturn cgrp->root == &cgrp_dfl_root;\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "struct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };",
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstruct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\n\nbool cgroup_on_dfl(const struct cgroup *cgrp)\n{\n\treturn cgrp->root == &cgrp_dfl_root;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_CGROUP_MGCTX",
          "args": [
            "mgctx"
          ],
          "line": 93
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nint cgroup_transfer_tasks(struct cgroup *to, struct cgroup *from)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct cgrp_cset_link *link;\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\tint ret;\n\n\tif (cgroup_on_dfl(to))\n\t\treturn -EINVAL;\n\n\tret = cgroup_migrate_vet_dst(to);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\t/* all tasks in @from are being moved, all csets are source */\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(link, &from->cset_links, cset_link)\n\t\tcgroup_migrate_add_src(link->cset, to, &mgctx);\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (ret)\n\t\tgoto out_err;\n\n\t/*\n\t * Migrate tasks one-by-one until @from is empty.  This fails iff\n\t * ->can_attach() fails.\n\t */\n\tdo {\n\t\tcss_task_iter_start(&from->self, 0, &it);\n\n\t\tdo {\n\t\t\ttask = css_task_iter_next(&it);\n\t\t} while (task && (task->flags & PF_EXITING));\n\n\t\tif (task)\n\t\t\tget_task_struct(task);\n\t\tcss_task_iter_end(&it);\n\n\t\tif (task) {\n\t\t\tret = cgroup_migrate(task, false, &mgctx);\n\t\t\tif (!ret)\n\t\t\t\tTRACE_CGROUP_PATH(transfer_tasks, to, task, false);\n\t\t\tput_task_struct(task);\n\t\t}\n\t} while (task && !ret);\nout_err:\n\tcgroup_migrate_finish(&mgctx);\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}"
  },
  {
    "function_name": "cgroup_attach_task_all",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "52-77",
    "snippet": "int cgroup_attach_task_all(struct task_struct *from, struct task_struct *tsk)\n{\n\tstruct cgroup_root *root;\n\tint retval = 0;\n\n\tmutex_lock(&cgroup_mutex);\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\tfor_each_root(root) {\n\t\tstruct cgroup *from_cgrp;\n\n\t\tif (root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\tfrom_cgrp = task_cgroup_from_root(from, root);\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\tretval = cgroup_attach_task(from_cgrp, tsk, false);\n\t\tif (retval)\n\t\t\tbreak;\n\t}\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tmutex_unlock(&cgroup_mutex);\n\n\treturn retval;\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_unlock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 74
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1602-1606",
          "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "percpu_up_write",
          "args": [
            "&cgroup_threadgroup_rwsem"
          ],
          "line": 73
        },
        "resolved": true,
        "details": {
          "function_name": "percpu_up_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "166-191",
          "snippet": "void percpu_up_write(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * Signal the writer is done, no fast path yet.\n\t *\n\t * One reason that we cannot just immediately flip to readers_fast is\n\t * that new readers might fail to see the results of this writer's\n\t * critical section.\n\t *\n\t * Therefore we force it through the slow path which guarantees an\n\t * acquire and thereby guarantees the critical section's consistency.\n\t */\n\tsmp_store_release(&sem->readers_block, 0);\n\n\t/*\n\t * Release the write lock, this will allow readers back in the game.\n\t */\n\tup_write(&sem->rw_sem);\n\n\t/*\n\t * Once this completes (at least one RCU-sched grace period hence) the\n\t * reader fast path will be available again. Safe to use outside the\n\t * exclusive write lock because its counting.\n\t */\n\trcu_sync_exit(&sem->rss);\n}",
          "includes": [
            "#include <linux/errno.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/percpu.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid percpu_up_write(struct percpu_rw_semaphore *sem)\n{\n\t/*\n\t * Signal the writer is done, no fast path yet.\n\t *\n\t * One reason that we cannot just immediately flip to readers_fast is\n\t * that new readers might fail to see the results of this writer's\n\t * critical section.\n\t *\n\t * Therefore we force it through the slow path which guarantees an\n\t * acquire and thereby guarantees the critical section's consistency.\n\t */\n\tsmp_store_release(&sem->readers_block, 0);\n\n\t/*\n\t * Release the write lock, this will allow readers back in the game.\n\t */\n\tup_write(&sem->rw_sem);\n\n\t/*\n\t * Once this completes (at least one RCU-sched grace period hence) the\n\t * reader fast path will be available again. Safe to use outside the\n\t * exclusive write lock because its counting.\n\t */\n\trcu_sync_exit(&sem->rss);\n}"
        }
      },
      {
        "call_info": {
          "callee": "cgroup_attach_task",
          "args": [
            "from_cgrp",
            "tsk",
            "false"
          ],
          "line": 69
        },
        "resolved": true,
        "details": {
          "function_name": "cgroup_attach_task",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "2614-5068",
          "snippet": "int cgroup_attach_task(struct cgroup *dst_cgrp, struct task_struct *leader,\n\t\t       bool threadgroup)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct task_struct *task;\n\tint ret;\n\n\tret = cgroup_migrate_vet_dst(dst_cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/* look up all src csets */\n\tspin_lock_irq(&css_set_lock);\n\trcu_read_lock();\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_src(task_css_set(task), dst_cgrp, &mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\trcu_read_unlock();\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* prepare dst csets and commit */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (!ret)\n\t\tret = cgroup_migrate(leader, threadgroup, &mgctx);\n\n\tcgroup_migrate_finish(&mgctx);\n\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(attach_task, dst_cgrp, leader, threadgroup);\n\n\treturn ret;\n}\n\nstruct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup)\n\t__acquires(&cgroup_threadgroup_rwsem)\n{\n\tstruct task_struct *tsk;\n\tpid_t pid;\n\n\tif (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\trcu_read_lock();\n\tif (pid) {\n\t\ttsk = find_task_by_vpid(pid);\n\t\tif (!tsk) {\n\t\t\ttsk = ERR_PTR(-ESRCH);\n\t\t\tgoto out_unlock_threadgroup;\n\t\t}\n\t} else {\n\t\ttsk = current;\n\t}\n\n\tif (threadgroup)\n\t\ttsk = tsk->group_leader;\n\n\t/*\n\t * kthreads may acquire PF_NO_SETAFFINITY during initialization.\n\t * If userland migrates such a kthread to a non-root cgroup, it can\n\t * become trapped in a cpuset, or RT kthread may be born in a\n\t * cgroup with no rt_runtime allocated.  Just say no.\n\t */\n\tif (tsk->no_cgroup_migration || (tsk->flags & PF_NO_SETAFFINITY)) {\n\t\ttsk = ERR_PTR(-EINVAL);\n\t\tgoto out_unlock_threadgroup;\n\t}\n\n\tget_task_struct(tsk);\n\tgoto out_unlock_rcu;\n\nout_unlock_threadgroup:\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\nout_unlock_rcu:\n\trcu_read_unlock();\n\treturn tsk;\n}\n\nvoid cgroup_procs_write_finish(struct task_struct *task)\n\t__releases(&cgroup_threadgroup_rwsem)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\t/* release reference from cgroup_procs_write_start() */\n\tput_task_struct(task);\n\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tfor_each_subsys(ss, ssid)\n\t\tif (ss->post_attach)\n\t\t\tss->post_attach();\n}\n\nstatic void cgroup_print_ss_mask(struct seq_file *seq, u16 ss_mask)\n{\n\tstruct cgroup_subsys *ss;\n\tbool printed = false;\n\tint ssid;\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tif (printed)\n\t\t\tseq_putc(seq, ' ');\n\t\tseq_printf(seq, \"%s\", ss->name);\n\t\tprinted = true;\n\t} while_each_subsys_mask();\n\tif (printed)\n\t\tseq_putc(seq, '\\n');\n}\n\n/* show controllers which are enabled from the parent */\nstatic int cgroup_controllers_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgroup_control(cgrp));\n\treturn 0;\n}\n\n/* show controllers which are enabled for a given cgroup's children */\nstatic int cgroup_subtree_control_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgrp->subtree_control);\n\treturn 0;\n}\n\n/**\n * cgroup_update_dfl_csses - update css assoc of a subtree in default hierarchy\n * @cgrp: root of the subtree to update csses for\n *\n * @cgrp's control masks have changed and its subtree's css associations\n * need to be updated accordingly.  This function looks up all css_sets\n * which are attached to the subtree, creates the matching updated css_sets\n * and migrates the tasks to the new ones.\n */\nstatic int cgroup_update_dfl_csses(struct cgroup *cgrp)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup *dsct;\n\tstruct css_set *src_cset;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\t/* look up all csses currently attached to @cgrp's subtree */\n\tspin_lock_irq(&css_set_lock);\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tstruct cgrp_cset_link *link;\n\n\t\tlist_for_each_entry(link, &dsct->cset_links, cset_link)\n\t\t\tcgroup_migrate_add_src(link->cset, dsct, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* NULL dst indicates self on default hierarchy */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(src_cset, &mgctx.preloaded_src_csets, mg_preload_node) {\n\t\tstruct task_struct *task, *ntask;\n\n\t\t/* all tasks in src_csets need to be migrated */\n\t\tlist_for_each_entry_safe(task, ntask, &src_cset->tasks, cg_list)\n\t\t\tcgroup_migrate_add_task(task, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_migrate_execute(&mgctx);\nout_finish:\n\tcgroup_migrate_finish(&mgctx);\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\treturn ret;\n}\n\n/**\n * cgroup_lock_and_drain_offline - lock cgroup_mutex and drain offlined csses\n * @cgrp: root of the target subtree\n *\n * Because css offlining is asynchronous, userland may try to re-enable a\n * controller while the previous css is still around.  This function grabs\n * cgroup_mutex and drains the previous css instances of @cgrp's subtree.\n */\nvoid cgroup_lock_and_drain_offline(struct cgroup *cgrp)\n\t__acquires(&cgroup_mutex)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\nrestart:\n\tmutex_lock(&cgroup_mutex);\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\t\t\tDEFINE_WAIT(wait);\n\n\t\t\tif (!css || !percpu_ref_is_dying(&css->refcnt))\n\t\t\t\tcontinue;\n\n\t\t\tcgroup_get_live(dsct);\n\t\t\tprepare_to_wait(&dsct->offline_waitq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tschedule();\n\t\t\tfinish_wait(&dsct->offline_waitq, &wait);\n\n\t\t\tcgroup_put(dsct);\n\t\t\tgoto restart;\n\t\t}\n\t}\n}\n\n/**\n * cgroup_save_control - save control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Save ->subtree_control, ->subtree_ss_mask and ->dom_cgrp to the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_save_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->old_subtree_control = dsct->subtree_control;\n\t\tdsct->old_subtree_ss_mask = dsct->subtree_ss_mask;\n\t\tdsct->old_dom_cgrp = dsct->dom_cgrp;\n\t}\n}\n\n/**\n * cgroup_propagate_control - refresh control masks of a subtree\n * @cgrp: root of the target subtree\n *\n * For @cgrp and its subtree, ensure ->subtree_ss_mask matches\n * ->subtree_control and propagate controller availability through the\n * subtree so that descendants don't have unavailable controllers enabled.\n */\nstatic void cgroup_propagate_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control &= cgroup_control(dsct);\n\t\tdsct->subtree_ss_mask =\n\t\t\tcgroup_calc_subtree_ss_mask(dsct->subtree_control,\n\t\t\t\t\t\t    cgroup_ss_mask(dsct));\n\t}\n}\n\n/**\n * cgroup_restore_control - restore control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Restore ->subtree_control, ->subtree_ss_mask and ->dom_cgrp from the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_restore_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control = dsct->old_subtree_control;\n\t\tdsct->subtree_ss_mask = dsct->old_subtree_ss_mask;\n\t\tdsct->dom_cgrp = dsct->old_dom_cgrp;\n\t}\n}\n\nstatic bool css_visible(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tif (cgroup_control(cgrp) & (1 << ss->id))\n\t\treturn true;\n\tif (!(cgroup_ss_mask(cgrp) & (1 << ss->id)))\n\t\treturn false;\n\treturn cgroup_on_dfl(cgrp) && ss->implicit_on_dfl;\n}\n\n/**\n * cgroup_apply_control_enable - enable or show csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and create new csses or make the existing ones\n * visible.  A css is created invisible if it's being implicitly enabled\n * through dependency.  An invisible css is made visible when the userland\n * explicitly enables it.\n *\n * Returns 0 on success, -errno on failure.  On failure, csses which have\n * been processed already aren't cleaned up.  The caller is responsible for\n * cleaning up with cgroup_apply_control_disable().\n */\nstatic int cgroup_apply_control_enable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid, ret;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!(cgroup_ss_mask(dsct) & (1 << ss->id)))\n\t\t\t\tcontinue;\n\n\t\t\tif (!css) {\n\t\t\t\tcss = css_create(dsct, ss);\n\t\t\t\tif (IS_ERR(css))\n\t\t\t\t\treturn PTR_ERR(css);\n\t\t\t}\n\n\t\t\tif (css_visible(css)) {\n\t\t\t\tret = css_populate_dir(css);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_apply_control_disable - kill or hide csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and kill and hide csses so that they match\n * cgroup_ss_mask() and cgroup_visible_mask().\n *\n * A css is hidden when the userland requests it to be disabled while other\n * subsystems are still depending on it.  The css must not actively control\n * resources and be in the vanilla state if it's made visible again later.\n * Controllers which may be depended upon should provide ->css_reset() for\n * this purpose.\n */\nstatic void cgroup_apply_control_disable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!css)\n\t\t\t\tcontinue;\n\n\t\t\tif (css->parent &&\n\t\t\t    !(cgroup_ss_mask(dsct) & (1 << ss->id))) {\n\t\t\t\tkill_css(css);\n\t\t\t} else if (!css_visible(css)) {\n\t\t\t\tcss_clear_dir(css);\n\t\t\t\tif (ss->css_reset)\n\t\t\t\t\tss->css_reset(css);\n\t\t\t}\n\t\t}\n\t}\n}\n\n/**\n * cgroup_apply_control - apply control mask updates to the subtree\n * @cgrp: root of the target subtree\n *\n * subsystems can be enabled and disabled in a subtree using the following\n * steps.\n *\n * 1. Call cgroup_save_control() to stash the current state.\n * 2. Update ->subtree_control masks in the subtree as desired.\n * 3. Call cgroup_apply_control() to apply the changes.\n * 4. Optionally perform other related operations.\n * 5. Call cgroup_finalize_control() to finish up.\n *\n * This function implements step 3 and propagates the mask changes\n * throughout @cgrp's subtree, updates csses accordingly and perform\n * process migrations.\n */\nstatic int cgroup_apply_control(struct cgroup *cgrp)\n{\n\tint ret;\n\n\tcgroup_propagate_control(cgrp);\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * At this point, cgroup_e_css() results reflect the new csses\n\t * making the following cgroup_update_dfl_csses() properly update\n\t * css associations of all tasks in the subtree.\n\t */\n\tret = cgroup_update_dfl_csses(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\n/**\n * cgroup_finalize_control - finalize control mask update\n * @cgrp: root of the target subtree\n * @ret: the result of the update\n *\n * Finalize control mask update.  See cgroup_apply_control() for more info.\n */\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret)\n{\n\tif (ret) {\n\t\tcgroup_restore_control(cgrp);\n\t\tcgroup_propagate_control(cgrp);\n\t}\n\n\tcgroup_apply_control_disable(cgrp);\n}\n\nstatic int cgroup_vet_subtree_control_enable(struct cgroup *cgrp, u16 enable)\n{\n\tu16 domain_enable = enable & ~cgrp_dfl_threaded_ss_mask;\n\n\t/* if nothing is getting enabled, nothing to worry about */\n\tif (!enable)\n\t\treturn 0;\n\n\t/* can @cgrp host any resources? */\n\tif (!cgroup_is_valid_domain(cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/* mixables don't care */\n\tif (cgroup_is_mixable(cgrp))\n\t\treturn 0;\n\n\tif (domain_enable) {\n\t\t/* can't enable domain controllers inside a thread subtree */\n\t\tif (cgroup_is_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\t/*\n\t\t * Threaded controllers can handle internal competitions\n\t\t * and are always allowed inside a (prospective) thread\n\t\t * subtree.\n\t\t */\n\t\tif (cgroup_can_be_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * Controllers can't be enabled for a cgroup with tasks to avoid\n\t * child cgroups competing against tasks.\n\t */\n\tif (cgroup_has_tasks(cgrp))\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n/* change the enabled child controllers for a cgroup in the default hierarchy */\nstatic ssize_t cgroup_subtree_control_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tu16 enable = 0, disable = 0;\n\tstruct cgroup *cgrp, *child;\n\tstruct cgroup_subsys *ss;\n\tchar *tok;\n\tint ssid, ret;\n\n\t/*\n\t * Parse input - space separated list of subsystem names prefixed\n\t * with either + or -.\n\t */\n\tbuf = strstrip(buf);\n\twhile ((tok = strsep(&buf, \" \"))) {\n\t\tif (tok[0] == '\\0')\n\t\t\tcontinue;\n\t\tdo_each_subsys_mask(ss, ssid, ~cgrp_dfl_inhibit_ss_mask) {\n\t\t\tif (!cgroup_ssid_enabled(ssid) ||\n\t\t\t    strcmp(tok + 1, ss->name))\n\t\t\t\tcontinue;\n\n\t\t\tif (*tok == '+') {\n\t\t\t\tenable |= 1 << ssid;\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t} else if (*tok == '-') {\n\t\t\t\tdisable |= 1 << ssid;\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t} else {\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t} while_each_subsys_mask();\n\t\tif (ssid == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -EINVAL;\n\t}\n\n\tcgrp = cgroup_kn_lock_live(of->kn, true);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tfor_each_subsys(ss, ssid) {\n\t\tif (enable & (1 << ssid)) {\n\t\t\tif (cgrp->subtree_control & (1 << ssid)) {\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!(cgroup_control(cgrp) & (1 << ssid))) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else if (disable & (1 << ssid)) {\n\t\t\tif (!(cgrp->subtree_control & (1 << ssid))) {\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* a child has it enabled? */\n\t\t\tcgroup_for_each_live_child(child, cgrp) {\n\t\t\t\tif (child->subtree_control & (1 << ssid)) {\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!enable && !disable) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tret = cgroup_vet_subtree_control_enable(cgrp, enable);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* save and update control masks and prepare csses */\n\tcgroup_save_control(cgrp);\n\n\tcgrp->subtree_control |= enable;\n\tcgrp->subtree_control &= ~disable;\n\n\tret = cgroup_apply_control(cgrp);\n\tcgroup_finalize_control(cgrp, ret);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tkernfs_activate(cgrp->kn);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\n/**\n * cgroup_enable_threaded - make @cgrp threaded\n * @cgrp: the target cgroup\n *\n * Called when \"threaded\" is written to the cgroup.type interface file and\n * tries to make @cgrp threaded and join the parent's resource domain.\n * This function is never called on the root cgroup as cgroup.type doesn't\n * exist on it.\n */\nstatic int cgroup_enable_threaded(struct cgroup *cgrp)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup *dom_cgrp = parent->dom_cgrp;\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* noop if already threaded */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn 0;\n\n\t/*\n\t * If @cgroup is populated or has domain controllers enabled, it\n\t * can't be switched.  While the below cgroup_can_be_thread_root()\n\t * test can catch the same conditions, that's only when @parent is\n\t * not mixable, so let's check it explicitly.\n\t */\n\tif (cgroup_is_populated(cgrp) ||\n\t    cgrp->subtree_control & ~cgrp_dfl_threaded_ss_mask)\n\t\treturn -EOPNOTSUPP;\n\n\t/* we're joining the parent's domain, ensure its validity */\n\tif (!cgroup_is_valid_domain(dom_cgrp) ||\n\t    !cgroup_can_be_thread_root(dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * The following shouldn't cause actual migrations and should\n\t * always succeed.\n\t */\n\tcgroup_save_control(cgrp);\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp)\n\t\tif (dsct == cgrp || cgroup_is_threaded(dsct))\n\t\t\tdsct->dom_cgrp = dom_cgrp;\n\n\tret = cgroup_apply_control(cgrp);\n\tif (!ret)\n\t\tparent->nr_threaded_children++;\n\n\tcgroup_finalize_control(cgrp, ret);\n\treturn ret;\n}\n\nstatic int cgroup_type_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tif (cgroup_is_threaded(cgrp))\n\t\tseq_puts(seq, \"threaded\\n\");\n\telse if (!cgroup_is_valid_domain(cgrp))\n\t\tseq_puts(seq, \"domain invalid\\n\");\n\telse if (cgroup_is_thread_root(cgrp))\n\t\tseq_puts(seq, \"domain threaded\\n\");\n\telse\n\t\tseq_puts(seq, \"domain\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_type_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\t/* only switching to threaded mode is supported */\n\tif (strcmp(strstrip(buf), \"threaded\"))\n\t\treturn -EINVAL;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\t/* threaded can only be enabled */\n\tret = cgroup_enable_threaded(cgrp);\n\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\nstatic int cgroup_max_descendants_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint descendants = READ_ONCE(cgrp->max_descendants);\n\n\tif (descendants == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", descendants);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_descendants_write(struct kernfs_open_file *of,\n\t\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint descendants;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdescendants = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &descendants);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (descendants < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_descendants = descendants;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_max_depth_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint depth = READ_ONCE(cgrp->max_depth);\n\n\tif (depth == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", depth);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_depth_write(struct kernfs_open_file *of,\n\t\t\t\t      char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tssize_t ret;\n\tint depth;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdepth = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &depth);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (depth < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_depth = depth;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_events_show(struct seq_file *seq, void *v)\n{\n\tseq_printf(seq, \"populated %d\\n\",\n\t\t   cgroup_is_populated(seq_css(seq)->cgroup));\n\treturn 0;\n}\n\nstatic int cgroup_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgroup = seq_css(seq)->cgroup;\n\n\tseq_printf(seq, \"nr_descendants %d\\n\",\n\t\t   cgroup->nr_descendants);\n\tseq_printf(seq, \"nr_dying_descendants %d\\n\",\n\t\t   cgroup->nr_dying_descendants);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused cgroup_extra_stat_show(struct seq_file *seq,\n\t\t\t\t\t\t struct cgroup *cgrp, int ssid)\n{\n\tstruct cgroup_subsys *ss = cgroup_subsys[ssid];\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\tif (!ss->css_extra_stat_show)\n\t\treturn 0;\n\n\tcss = cgroup_tryget_css(cgrp, ss);\n\tif (!css)\n\t\treturn 0;\n\n\tret = ss->css_extra_stat_show(seq, css);\n\tcss_put(css);\n\treturn ret;\n}\n\nstatic int cpu_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup __maybe_unused *cgrp = seq_css(seq)->cgroup;\n\tint ret = 0;\n\n\tcgroup_base_stat_cputime_show(seq);\n#ifdef CONFIG_CGROUP_SCHED\n\tret = cgroup_extra_stat_show(seq, cgrp, cpu_cgrp_id);\n#endif\n\treturn ret;\n}\n\n#ifdef CONFIG_PSI\nstatic int cgroup_io_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_IO);\n}\nstatic int cgroup_memory_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_MEM);\n}\nstatic int cgroup_cpu_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_CPU);\n}\n#endif\n\nstatic int cgroup_file_open(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->open)\n\t\treturn cft->open(of);\n\treturn 0;\n}\n\nstatic void cgroup_file_release(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->release)\n\t\tcft->release(of);\n}\n\nstatic ssize_t cgroup_file_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *cgrp = of->kn->parent->priv;\n\tstruct cftype *cft = of->kn->priv;\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, disallow writes to\n\t * files in an non-init namespace root from inside the namespace\n\t * except for the files explicitly marked delegatable -\n\t * cgroup.procs and cgroup.subtree_control.\n\t */\n\tif ((cgrp->root->flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    !(cft->flags & CFTYPE_NS_DELEGATABLE) &&\n\t    ns != &init_cgroup_ns && ns->root_cset->dfl_cgrp == cgrp)\n\t\treturn -EPERM;\n\n\tif (cft->write)\n\t\treturn cft->write(of, buf, nbytes, off);\n\n\t/*\n\t * kernfs guarantees that a file isn't deleted with operations in\n\t * flight, which means that the matching css is and stays alive and\n\t * doesn't need to be pinned.  The RCU locking is not necessary\n\t * either.  It's just for the convenience of using cgroup_css().\n\t */\n\trcu_read_lock();\n\tcss = cgroup_css(cgrp, cft->ss);\n\trcu_read_unlock();\n\n\tif (cft->write_u64) {\n\t\tunsigned long long v;\n\t\tret = kstrtoull(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_u64(css, cft, v);\n\t} else if (cft->write_s64) {\n\t\tlong long v;\n\t\tret = kstrtoll(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_s64(css, cft, v);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_seqfile_start(struct seq_file *seq, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_start(seq, ppos);\n}\n\nstatic void *cgroup_seqfile_next(struct seq_file *seq, void *v, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_next(seq, v, ppos);\n}\n\nstatic void cgroup_seqfile_stop(struct seq_file *seq, void *v)\n{\n\tif (seq_cft(seq)->seq_stop)\n\t\tseq_cft(seq)->seq_stop(seq, v);\n}\n\nstatic int cgroup_seqfile_show(struct seq_file *m, void *arg)\n{\n\tstruct cftype *cft = seq_cft(m);\n\tstruct cgroup_subsys_state *css = seq_css(m);\n\n\tif (cft->seq_show)\n\t\treturn cft->seq_show(m, arg);\n\n\tif (cft->read_u64)\n\t\tseq_printf(m, \"%llu\\n\", cft->read_u64(css, cft));\n\telse if (cft->read_s64)\n\t\tseq_printf(m, \"%lld\\n\", cft->read_s64(css, cft));\n\telse\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic struct kernfs_ops cgroup_kf_single_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\nstatic struct kernfs_ops cgroup_kf_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_start\t\t= cgroup_seqfile_start,\n\t.seq_next\t\t= cgroup_seqfile_next,\n\t.seq_stop\t\t= cgroup_seqfile_stop,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\n/* set uid and gid of cgroup dirs and files to that of the creator */\nstatic int cgroup_kn_set_ugid(struct kernfs_node *kn)\n{\n\tstruct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,\n\t\t\t       .ia_uid = current_fsuid(),\n\t\t\t       .ia_gid = current_fsgid(), };\n\n\tif (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&\n\t    gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))\n\t\treturn 0;\n\n\treturn kernfs_setattr(kn, &iattr);\n}\n\nstatic void cgroup_file_notify_timer(struct timer_list *timer)\n{\n\tcgroup_file_notify(container_of(timer, struct cgroup_file,\n\t\t\t\t\tnotify_timer));\n}\n\nstatic int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,\n\t\t\t   struct cftype *cft)\n{\n\tchar name[CGROUP_FILE_NAME_MAX];\n\tstruct kernfs_node *kn;\n\tstruct lock_class_key *key = NULL;\n\tint ret;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tkey = &cft->lockdep_key;\n#endif\n\tkn = __kernfs_create_file(cgrp->kn, cgroup_file_name(cgrp, cft, name),\n\t\t\t\t  cgroup_file_mode(cft),\n\t\t\t\t  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID,\n\t\t\t\t  0, cft->kf_ops, cft,\n\t\t\t\t  NULL, key);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\tkernfs_remove(kn);\n\t\treturn ret;\n\t}\n\n\tif (cft->file_offset) {\n\t\tstruct cgroup_file *cfile = (void *)css + cft->file_offset;\n\n\t\ttimer_setup(&cfile->notify_timer, cgroup_file_notify_timer, 0);\n\n\t\tspin_lock_irq(&cgroup_file_kn_lock);\n\t\tcfile->kn = kn;\n\t\tspin_unlock_irq(&cgroup_file_kn_lock);\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_addrm_files - add or remove files to a cgroup directory\n * @css: the target css\n * @cgrp: the target cgroup (usually css->cgroup)\n * @cfts: array of cftypes to be added\n * @is_add: whether to add or remove\n *\n * Depending on @is_add, add or remove files defined by @cfts on @cgrp.\n * For removals, this function never fails.\n */\nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add)\n{\n\tstruct cftype *cft, *cft_end = NULL;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\nrestart:\n\tfor (cft = cfts; cft != cft_end && cft->name[0] != '\\0'; cft++) {\n\t\t/* does cft->flags tell us to skip this file on @cgrp? */\n\t\tif ((cft->flags & __CFTYPE_ONLY_ON_DFL) && !cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & __CFTYPE_NOT_ON_DFL) && cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_NOT_ON_ROOT) && !cgroup_parent(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_ONLY_ON_ROOT) && cgroup_parent(cgrp))\n\t\t\tcontinue;\n\n\t\tif (is_add) {\n\t\t\tret = cgroup_add_file(css, cgrp, cft);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"%s: failed to add %s, err=%d\\n\",\n\t\t\t\t\t__func__, cft->name, ret);\n\t\t\t\tcft_end = cft;\n\t\t\t\tis_add = false;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t} else {\n\t\t\tcgroup_rm_file(cgrp, cft);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int cgroup_apply_cftypes(struct cftype *cfts, bool is_add)\n{\n\tstruct cgroup_subsys *ss = cfts[0].ss;\n\tstruct cgroup *root = &ss->root->cgrp;\n\tstruct cgroup_subsys_state *css;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* add/rm files for all cgroups created before */\n\tcss_for_each_descendant_pre(css, cgroup_css(root, ss)) {\n\t\tstruct cgroup *cgrp = css->cgroup;\n\n\t\tif (!(css->flags & CSS_VISIBLE))\n\t\t\tcontinue;\n\n\t\tret = cgroup_addrm_files(css, cgrp, cfts, is_add);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (is_add && !ret)\n\t\tkernfs_activate(root->kn);\n\treturn ret;\n}\n\nstatic void cgroup_exit_cftypes(struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\t/* free copy for custom atomic_write_len, see init_cftypes() */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE)\n\t\t\tkfree(cft->kf_ops);\n\t\tcft->kf_ops = NULL;\n\t\tcft->ss = NULL;\n\n\t\t/* revert flags set by cgroup core while adding @cfts */\n\t\tcft->flags &= ~(__CFTYPE_ONLY_ON_DFL | __CFTYPE_NOT_ON_DFL);\n\t}\n}\n\nstatic int cgroup_init_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\tstruct kernfs_ops *kf_ops;\n\n\t\tWARN_ON(cft->ss || cft->kf_ops);\n\n\t\tif (cft->seq_start)\n\t\t\tkf_ops = &cgroup_kf_ops;\n\t\telse\n\t\t\tkf_ops = &cgroup_kf_single_ops;\n\n\t\t/*\n\t\t * Ugh... if @cft wants a custom max_write_len, we need to\n\t\t * make a copy of kf_ops to set its atomic_write_len.\n\t\t */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE) {\n\t\t\tkf_ops = kmemdup(kf_ops, sizeof(*kf_ops), GFP_KERNEL);\n\t\t\tif (!kf_ops) {\n\t\t\t\tcgroup_exit_cftypes(cfts);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tkf_ops->atomic_write_len = cft->max_write_len;\n\t\t}\n\n\t\tcft->kf_ops = kf_ops;\n\t\tcft->ss = ss;\n\t}\n\n\treturn 0;\n}\n\nstatic int cgroup_rm_cftypes_locked(struct cftype *cfts)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!cfts || !cfts[0].ss)\n\t\treturn -ENOENT;\n\n\tlist_del(&cfts->node);\n\tcgroup_apply_cftypes(cfts, false);\n\tcgroup_exit_cftypes(cfts);\n\treturn 0;\n}\n\n/**\n * cgroup_rm_cftypes - remove an array of cftypes from a subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Unregister @cfts.  Files described by @cfts are removed from all\n * existing cgroups and all future cgroups won't have them either.  This\n * function can be called anytime whether @cfts' subsys is attached or not.\n *\n * Returns 0 on successful unregistration, -ENOENT if @cfts is not\n * registered.\n */\nint cgroup_rm_cftypes(struct cftype *cfts)\n{\n\tint ret;\n\n\tmutex_lock(&cgroup_mutex);\n\tret = cgroup_rm_cftypes_locked(cfts);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_cftypes - add an array of cftypes to a subsystem\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Register @cfts to @ss.  Files described by @cfts are created for all\n * existing cgroups to which @ss is attached and all future cgroups will\n * have them too.  This function can be called anytime whether @ss is\n * attached or not.\n *\n * Returns 0 on successful registration, -errno on failure.  Note that this\n * function currently returns 0 as long as @cfts registration is successful\n * even if some file creation attempts on existing cgroups fail.\n */\nstatic int cgroup_add_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tint ret;\n\n\tif (!cgroup_ssid_enabled(ss->id))\n\t\treturn 0;\n\n\tif (!cfts || cfts[0].name[0] == '\\0')\n\t\treturn 0;\n\n\tret = cgroup_init_cftypes(ss, cfts);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tlist_add_tail(&cfts->node, &ss->cfts);\n\tret = cgroup_apply_cftypes(cfts, true);\n\tif (ret)\n\t\tcgroup_rm_cftypes_locked(cfts);\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_dfl_cftypes - add an array of cftypes for default hierarchy\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the default hierarchy.\n */\nint cgroup_add_dfl_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_ONLY_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_add_legacy_cftypes - add an array of cftypes for legacy hierarchies\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the legacy hierarchies.\n */\nint cgroup_add_legacy_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_NOT_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_file_notify - generate a file modified event for a cgroup_file\n * @cfile: target cgroup_file\n *\n * @cfile must have been obtained by setting cftype->file_offset.\n */\nvoid cgroup_file_notify(struct cgroup_file *cfile)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cgroup_file_kn_lock, flags);\n\tif (cfile->kn) {\n\t\tunsigned long last = cfile->notified_at;\n\t\tunsigned long next = last + CGROUP_FILE_NOTIFY_MIN_INTV;\n\n\t\tif (time_in_range(jiffies, last, next)) {\n\t\t\ttimer_reduce(&cfile->notify_timer, next);\n\t\t} else {\n\t\t\tkernfs_notify(cfile->kn);\n\t\t\tcfile->notified_at = jiffies;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&cgroup_file_kn_lock, flags);\n}\n\n/**\n * css_next_child - find the next child of a given css\n * @pos: the current position (%NULL to initiate traversal)\n * @parent: css whose children to walk\n *\n * This function returns the next child of @parent and should be called\n * under either cgroup_mutex or RCU read lock.  The only requirement is\n * that @parent and @pos are accessible.  The next sibling is guaranteed to\n * be returned regardless of their states.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *css_next_child(struct cgroup_subsys_state *pos,\n\t\t\t\t\t   struct cgroup_subsys_state *parent)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/*\n\t * @pos could already have been unlinked from the sibling list.\n\t * Once a cgroup is removed, its ->sibling.next is no longer\n\t * updated when its next sibling changes.  CSS_RELEASED is set when\n\t * @pos is taken off list, at which time its next pointer is valid,\n\t * and, as releases are serialized, the one pointed to by the next\n\t * pointer is guaranteed to not have started release yet.  This\n\t * implies that if we observe !CSS_RELEASED on @pos in this RCU\n\t * critical section, the one pointed to by its next pointer is\n\t * guaranteed to not have finished its RCU grace period even if we\n\t * have dropped rcu_read_lock() inbetween iterations.\n\t *\n\t * If @pos has CSS_RELEASED set, its next pointer can't be\n\t * dereferenced; however, as each css is given a monotonically\n\t * increasing unique serial number and always appended to the\n\t * sibling list, the next one can be found by walking the parent's\n\t * children until the first css with higher serial number than\n\t * @pos's.  While this path can be slower, it happens iff iteration\n\t * races against release and the race window is very small.\n\t */\n\tif (!pos) {\n\t\tnext = list_entry_rcu(parent->children.next, struct cgroup_subsys_state, sibling);\n\t} else if (likely(!(pos->flags & CSS_RELEASED))) {\n\t\tnext = list_entry_rcu(pos->sibling.next, struct cgroup_subsys_state, sibling);\n\t} else {\n\t\tlist_for_each_entry_rcu(next, &parent->children, sibling)\n\t\t\tif (next->serial_nr > pos->serial_nr)\n\t\t\t\tbreak;\n\t}\n\n\t/*\n\t * @next, if not pointing to the head, can be dereferenced and is\n\t * the next sibling.\n\t */\n\tif (&next->sibling != &parent->children)\n\t\treturn next;\n\treturn NULL;\n}\n\n/**\n * css_next_descendant_pre - find the next descendant for pre-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_pre().  Find the next descendant\n * to visit for pre-order traversal of @root's descendants.  @root is\n * included in the iteration and the first node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @root are accessible and @pos is a descendant of @root.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_pre(struct cgroup_subsys_state *pos,\n\t\t\tstruct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit @root */\n\tif (!pos)\n\t\treturn root;\n\n\t/* visit the first child if exists */\n\tnext = css_next_child(NULL, pos);\n\tif (next)\n\t\treturn next;\n\n\t/* no child, visit my or the closest ancestor's next sibling */\n\twhile (pos != root) {\n\t\tnext = css_next_child(pos, pos->parent);\n\t\tif (next)\n\t\t\treturn next;\n\t\tpos = pos->parent;\n\t}\n\n\treturn NULL;\n}\n\n/**\n * css_rightmost_descendant - return the rightmost descendant of a css\n * @pos: css of interest\n *\n * Return the rightmost descendant of @pos.  If there's no descendant, @pos\n * is returned.  This can be used during pre-order traversal to skip\n * subtree of @pos.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct rightmost descendant as\n * long as @pos is accessible.\n */\nstruct cgroup_subsys_state *\ncss_rightmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last, *tmp;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\tdo {\n\t\tlast = pos;\n\t\t/* ->prev isn't RCU safe, walk ->next till the end */\n\t\tpos = NULL;\n\t\tcss_for_each_child(tmp, last)\n\t\t\tpos = tmp;\n\t} while (pos);\n\n\treturn last;\n}\n\nstatic struct cgroup_subsys_state *\ncss_leftmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last;\n\n\tdo {\n\t\tlast = pos;\n\t\tpos = css_next_child(NULL, pos);\n\t} while (pos);\n\n\treturn last;\n}\n\n/**\n * css_next_descendant_post - find the next descendant for post-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_post().  Find the next descendant\n * to visit for post-order traversal of @root's descendants.  @root is\n * included in the iteration and the last node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @cgroup are accessible and @pos is a descendant of\n * @cgroup.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_post(struct cgroup_subsys_state *pos,\n\t\t\t struct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit leftmost descendant which may be @root */\n\tif (!pos)\n\t\treturn css_leftmost_descendant(root);\n\n\t/* if we visited @root, we're done */\n\tif (pos == root)\n\t\treturn NULL;\n\n\t/* if there's an unvisited sibling, visit its leftmost descendant */\n\tnext = css_next_child(pos, pos->parent);\n\tif (next)\n\t\treturn css_leftmost_descendant(next);\n\n\t/* no sibling left, visit parent */\n\treturn pos->parent;\n}\n\n/**\n * css_has_online_children - does a css have online children\n * @css: the target css\n *\n * Returns %true if @css has any online children; otherwise, %false.  This\n * function can be called from any context but the caller is responsible\n * for synchronizing against on/offlining as necessary.\n */\nbool css_has_online_children(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys_state *child;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tcss_for_each_child(child, css) {\n\t\tif (child->flags & CSS_ONLINE) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic struct css_set *css_task_iter_next_css_set(struct css_task_iter *it)\n{\n\tstruct list_head *l;\n\tstruct cgrp_cset_link *link;\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* find the next threaded cset */\n\tif (it->tcset_pos) {\n\t\tl = it->tcset_pos->next;\n\n\t\tif (l != it->tcset_head) {\n\t\t\tit->tcset_pos = l;\n\t\t\treturn container_of(l, struct css_set,\n\t\t\t\t\t    threaded_csets_node);\n\t\t}\n\n\t\tit->tcset_pos = NULL;\n\t}\n\n\t/* find the next cset */\n\tl = it->cset_pos;\n\tl = l->next;\n\tif (l == it->cset_head) {\n\t\tit->cset_pos = NULL;\n\t\treturn NULL;\n\t}\n\n\tif (it->ss) {\n\t\tcset = container_of(l, struct css_set, e_cset_node[it->ss->id]);\n\t} else {\n\t\tlink = list_entry(l, struct cgrp_cset_link, cset_link);\n\t\tcset = link->cset;\n\t}\n\n\tit->cset_pos = l;\n\n\t/* initialize threaded css_set walking */\n\tif (it->flags & CSS_TASK_ITER_THREADED) {\n\t\tif (it->cur_dcset)\n\t\t\tput_css_set_locked(it->cur_dcset);\n\t\tit->cur_dcset = cset;\n\t\tget_css_set(cset);\n\n\t\tit->tcset_head = &cset->threaded_csets;\n\t\tit->tcset_pos = &cset->threaded_csets;\n\t}\n\n\treturn cset;\n}\n\n/**\n * css_task_iter_advance_css_set - advance a task itererator to the next css_set\n * @it: the iterator to advance\n *\n * Advance @it to the next css_set to walk.\n */\nstatic void css_task_iter_advance_css_set(struct css_task_iter *it)\n{\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* Advance to the next non-empty css_set */\n\tdo {\n\t\tcset = css_task_iter_next_css_set(it);\n\t\tif (!cset) {\n\t\t\tit->task_pos = NULL;\n\t\t\treturn;\n\t\t}\n\t} while (!css_set_populated(cset));\n\n\tif (!list_empty(&cset->tasks))\n\t\tit->task_pos = cset->tasks.next;\n\telse\n\t\tit->task_pos = cset->mg_tasks.next;\n\n\tit->tasks_head = &cset->tasks;\n\tit->mg_tasks_head = &cset->mg_tasks;\n\n\t/*\n\t * We don't keep css_sets locked across iteration steps and thus\n\t * need to take steps to ensure that iteration can be resumed after\n\t * the lock is re-acquired.  Iteration is performed at two levels -\n\t * css_sets and tasks in them.\n\t *\n\t * Once created, a css_set never leaves its cgroup lists, so a\n\t * pinned css_set is guaranteed to stay put and we can resume\n\t * iteration afterwards.\n\t *\n\t * Tasks may leave @cset across iteration steps.  This is resolved\n\t * by registering each iterator with the css_set currently being\n\t * walked and making css_set_move_task() advance iterators whose\n\t * next task is leaving.\n\t */\n\tif (it->cur_cset) {\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t}\n\tget_css_set(cset);\n\tit->cur_cset = cset;\n\tlist_add(&it->iters_node, &cset->task_iters);\n}\n\nstatic void css_task_iter_advance(struct css_task_iter *it)\n{\n\tstruct list_head *next;\n\n\tlockdep_assert_held(&css_set_lock);\nrepeat:\n\t/*\n\t * Advance iterator to find next entry.  cset->tasks is consumed\n\t * first and then ->mg_tasks.  After ->mg_tasks, we move onto the\n\t * next cset.\n\t */\n\tnext = it->task_pos->next;\n\n\tif (next == it->tasks_head)\n\t\tnext = it->mg_tasks_head->next;\n\n\tif (next == it->mg_tasks_head)\n\t\tcss_task_iter_advance_css_set(it);\n\telse\n\t\tit->task_pos = next;\n\n\t/* if PROCS, skip over tasks which aren't group leaders */\n\tif ((it->flags & CSS_TASK_ITER_PROCS) && it->task_pos &&\n\t    !thread_group_leader(list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t    cg_list)))\n\t\tgoto repeat;\n}\n\n/**\n * css_task_iter_start - initiate task iteration\n * @css: the css to walk tasks of\n * @flags: CSS_TASK_ITER_* flags\n * @it: the task iterator to use\n *\n * Initiate iteration through the tasks of @css.  The caller can call\n * css_task_iter_next() to walk through the tasks until the function\n * returns NULL.  On completion of iteration, css_task_iter_end() must be\n * called.\n */\nvoid css_task_iter_start(struct cgroup_subsys_state *css, unsigned int flags,\n\t\t\t struct css_task_iter *it)\n{\n\t/* no one should try to iterate before mounting cgroups */\n\tWARN_ON_ONCE(!use_task_css_set_links);\n\n\tmemset(it, 0, sizeof(*it));\n\n\tspin_lock_irq(&css_set_lock);\n\n\tit->ss = css->ss;\n\tit->flags = flags;\n\n\tif (it->ss)\n\t\tit->cset_pos = &css->cgroup->e_csets[css->ss->id];\n\telse\n\t\tit->cset_pos = &css->cgroup->cset_links;\n\n\tit->cset_head = it->cset_pos;\n\n\tcss_task_iter_advance_css_set(it);\n\n\tspin_unlock_irq(&css_set_lock);\n}\n\n/**\n * css_task_iter_next - return the next task for the iterator\n * @it: the task iterator being iterated\n *\n * The \"next\" function for task iteration.  @it should have been\n * initialized via css_task_iter_start().  Returns NULL when the iteration\n * reaches the end.\n */\nstruct task_struct *css_task_iter_next(struct css_task_iter *it)\n{\n\tif (it->cur_task) {\n\t\tput_task_struct(it->cur_task);\n\t\tit->cur_task = NULL;\n\t}\n\n\tspin_lock_irq(&css_set_lock);\n\n\tif (it->task_pos) {\n\t\tit->cur_task = list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t  cg_list);\n\t\tget_task_struct(it->cur_task);\n\t\tcss_task_iter_advance(it);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn it->cur_task;\n}\n\n/**\n * css_task_iter_end - finish task iteration\n * @it: the task iterator to finish\n *\n * Finish task iteration started by css_task_iter_start().\n */\nvoid css_task_iter_end(struct css_task_iter *it)\n{\n\tif (it->cur_cset) {\n\t\tspin_lock_irq(&css_set_lock);\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t\tspin_unlock_irq(&css_set_lock);\n\t}\n\n\tif (it->cur_dcset)\n\t\tput_css_set(it->cur_dcset);\n\n\tif (it->cur_task)\n\t\tput_task_struct(it->cur_task);\n}\n\nstatic void cgroup_procs_release(struct kernfs_open_file *of)\n{\n\tif (of->priv) {\n\t\tcss_task_iter_end(of->priv);\n\t\tkfree(of->priv);\n\t}\n}\n\nstatic void *cgroup_procs_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct css_task_iter *it = of->priv;\n\n\treturn css_task_iter_next(it);\n}\n\nstatic void *__cgroup_procs_start(struct seq_file *s, loff_t *pos,\n\t\t\t\t  unsigned int iter_flags)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\tstruct css_task_iter *it = of->priv;\n\n\t/*\n\t * When a seq_file is seeked, it's always traversed sequentially\n\t * from position 0, so we can simply keep iterating on !0 *pos.\n\t */\n\tif (!it) {\n\t\tif (WARN_ON_ONCE((*pos)++))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tit = kzalloc(sizeof(*it), GFP_KERNEL);\n\t\tif (!it)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tof->priv = it;\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t} else if (!(*pos)++) {\n\t\tcss_task_iter_end(it);\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t}\n\n\treturn cgroup_procs_next(s, NULL, NULL);\n}\n\nstatic void *cgroup_procs_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\n\t/*\n\t * All processes of a threaded subtree belong to the domain cgroup\n\t * of the subtree.  Only threads can be distributed across the\n\t * subtree.  Reject reads on cgroup.procs in the subtree proper.\n\t * They're always empty anyway.\n\t */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn __cgroup_procs_start(s, pos, CSS_TASK_ITER_PROCS |\n\t\t\t\t\t    CSS_TASK_ITER_THREADED);\n}\n\nstatic int cgroup_procs_show(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", task_pid_vnr(v));\n\treturn 0;\n}\n\nstatic int cgroup_procs_write_permission(struct cgroup *src_cgrp,\n\t\t\t\t\t struct cgroup *dst_cgrp,\n\t\t\t\t\t struct super_block *sb)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *com_cgrp = src_cgrp;\n\tstruct inode *inode;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* find the common ancestor */\n\twhile (!cgroup_is_descendant(dst_cgrp, com_cgrp))\n\t\tcom_cgrp = cgroup_parent(com_cgrp);\n\n\t/* %current should be authorized to migrate to the common ancestor */\n\tinode = kernfs_get_inode(sb, com_cgrp->procs_file.kn);\n\tif (!inode)\n\t\treturn -ENOMEM;\n\n\tret = inode_permission(inode, MAY_WRITE);\n\tiput(inode);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, %current must be able\n\t * to see both source and destination cgroups from its namespace.\n\t */\n\tif ((cgrp_dfl_root.flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    (!cgroup_is_descendant(src_cgrp, ns->root_cset->dfl_cgrp) ||\n\t     !cgroup_is_descendant(dst_cgrp, ns->root_cset->dfl_cgrp)))\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_procs_write(struct kernfs_open_file *of,\n\t\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, true);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, true);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_threads_start(struct seq_file *s, loff_t *pos)\n{\n\treturn __cgroup_procs_start(s, pos, 0);\n}\n\nstatic ssize_t cgroup_threads_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, false);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* thread migrations follow the cgroup.procs delegation rule */\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\t/* and must be contained in the same domain */\n\tret = -EOPNOTSUPP;\n\tif (src_cgrp->dom_cgrp != dst_cgrp->dom_cgrp)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, false);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\n/* cgroup core interface files for the default hierarchy */\nstatic struct cftype cgroup_base_files[] = {\n\t{\n\t\t.name = \"cgroup.type\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_type_show,\n\t\t.write = cgroup_type_write,\n\t},\n\t{\n\t\t.name = \"cgroup.procs\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.file_offset = offsetof(struct cgroup, procs_file),\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_procs_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_procs_write,\n\t},\n\t{\n\t\t.name = \"cgroup.threads\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_threads_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_threads_write,\n\t},\n\t{\n\t\t.name = \"cgroup.controllers\",\n\t\t.seq_show = cgroup_controllers_show,\n\t},\n\t{\n\t\t.name = \"cgroup.subtree_control\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.seq_show = cgroup_subtree_control_show,\n\t\t.write = cgroup_subtree_control_write,\n\t},\n\t{\n\t\t.name = \"cgroup.events\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct cgroup, events_file),\n\t\t.seq_show = cgroup_events_show,\n\t},\n\t{\n\t\t.name = \"cgroup.max.descendants\",\n\t\t.seq_show = cgroup_max_descendants_show,\n\t\t.write = cgroup_max_descendants_write,\n\t},\n\t{\n\t\t.name = \"cgroup.max.depth\",\n\t\t.seq_show = cgroup_max_depth_show,\n\t\t.write = cgroup_max_depth_write,\n\t},\n\t{\n\t\t.name = \"cgroup.stat\",\n\t\t.seq_show = cgroup_stat_show,\n\t},\n\t{\n\t\t.name = \"cpu.stat\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_stat_show,\n\t},\n#ifdef CONFIG_PSI\n\t{\n\t\t.name = \"io.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_io_pressure_show,\n\t},\n\t{\n\t\t.name = \"memory.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_memory_pressure_show,\n\t},\n\t{\n\t\t.name = \"cpu.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_cpu_pressure_show,\n\t},\n#endif\n\t{ }\t/* terminate */\n};\n\n/*\n * css destruction is four-stage process.\n *\n * 1. Destruction starts.  Killing of the percpu_ref is initiated.\n *    Implemented in kill_css().\n *\n * 2. When the percpu_ref is confirmed to be visible as killed on all CPUs\n *    and thus css_tryget_online() is guaranteed to fail, the css can be\n *    offlined by invoking offline_css().  After offlining, the base ref is\n *    put.  Implemented in css_killed_work_fn().\n *\n * 3. When the percpu_ref reaches zero, the only possible remaining\n *    accessors are inside RCU read sections.  css_release() schedules the\n *    RCU callback.\n *\n * 4. After the grace period, the css can be freed.  Implemented in\n *    css_free_work_fn().\n *\n * It is actually hairier because both step 2 and 4 require process context\n * and thus involve punting to css->destroy_work adding two additional\n * steps to the already complex sequence.\n */\nstatic void css_free_rwork_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css = container_of(to_rcu_work(work),\n\t\t\t\tstruct cgroup_subsys_state, destroy_rwork);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tpercpu_ref_exit(&css->refcnt);\n\n\tif (ss) {\n\t\t/* css free path */\n\t\tstruct cgroup_subsys_state *parent = css->parent;\n\t\tint id = css->id;\n\n\t\tss->css_free(css);\n\t\tcgroup_idr_remove(&ss->css_idr, id);\n\t\tcgroup_put(cgrp);\n\n\t\tif (parent)\n\t\t\tcss_put(parent);\n\t} else {\n\t\t/* cgroup free path */\n\t\tatomic_dec(&cgrp->root->nr_cgrps);\n\t\tcgroup1_pidlist_destroy_all(cgrp);\n\t\tcancel_work_sync(&cgrp->release_agent_work);\n\n\t\tif (cgroup_parent(cgrp)) {\n\t\t\t/*\n\t\t\t * We get a ref to the parent, and put the ref when\n\t\t\t * this cgroup is being freed, so it's guaranteed\n\t\t\t * that the parent won't be destroyed before its\n\t\t\t * children.\n\t\t\t */\n\t\t\tcgroup_put(cgroup_parent(cgrp));\n\t\t\tkernfs_put(cgrp->kn);\n\t\t\tpsi_cgroup_free(cgrp);\n\t\t\tif (cgroup_on_dfl(cgrp))\n\t\t\t\tcgroup_rstat_exit(cgrp);\n\t\t\tkfree(cgrp);\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is root cgroup's refcnt reaching zero,\n\t\t\t * which indicates that the root should be\n\t\t\t * released.\n\t\t\t */\n\t\t\tcgroup_destroy_root(cgrp->root);\n\t\t}\n\t}\n}\n\nstatic void css_release_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tcss->flags |= CSS_RELEASED;\n\tlist_del_rcu(&css->sibling);\n\n\tif (ss) {\n\t\t/* css release path */\n\t\tif (!list_empty(&css->rstat_css_node)) {\n\t\t\tcgroup_rstat_flush(cgrp);\n\t\t\tlist_del_rcu(&css->rstat_css_node);\n\t\t}\n\n\t\tcgroup_idr_replace(&ss->css_idr, NULL, css->id);\n\t\tif (ss->css_released)\n\t\t\tss->css_released(css);\n\t} else {\n\t\tstruct cgroup *tcgrp;\n\n\t\t/* cgroup release path */\n\t\tTRACE_CGROUP_PATH(release, cgrp);\n\n\t\tif (cgroup_on_dfl(cgrp))\n\t\t\tcgroup_rstat_flush(cgrp);\n\n\t\tfor (tcgrp = cgroup_parent(cgrp); tcgrp;\n\t\t     tcgrp = cgroup_parent(tcgrp))\n\t\t\ttcgrp->nr_dying_descendants--;\n\n\t\tcgroup_idr_remove(&cgrp->root->cgroup_idr, cgrp->id);\n\t\tcgrp->id = -1;\n\n\t\t/*\n\t\t * There are two control paths which try to determine\n\t\t * cgroup from dentry without going through kernfs -\n\t\t * cgroupstats_build() and css_tryget_online_from_dir().\n\t\t * Those are supported by RCU protecting clearing of\n\t\t * cgrp->kn->priv backpointer.\n\t\t */\n\t\tif (cgrp->kn)\n\t\t\tRCU_INIT_POINTER(*(void __rcu __force **)&cgrp->kn->priv,\n\t\t\t\t\t NULL);\n\n\t\tcgroup_bpf_put(cgrp);\n\t}\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n}\n\nstatic void css_release(struct percpu_ref *ref)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(ref, struct cgroup_subsys_state, refcnt);\n\n\tINIT_WORK(&css->destroy_work, css_release_work_fn);\n\tqueue_work(cgroup_destroy_wq, &css->destroy_work);\n}\n\nstatic void init_and_link_css(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcgroup_get_live(cgrp);\n\n\tmemset(css, 0, sizeof(*css));\n\tcss->cgroup = cgrp;\n\tcss->ss = ss;\n\tcss->id = -1;\n\tINIT_LIST_HEAD(&css->sibling);\n\tINIT_LIST_HEAD(&css->children);\n\tINIT_LIST_HEAD(&css->rstat_css_node);\n\tcss->serial_nr = css_serial_nr_next++;\n\tatomic_set(&css->online_cnt, 0);\n\n\tif (cgroup_parent(cgrp)) {\n\t\tcss->parent = cgroup_css(cgroup_parent(cgrp), ss);\n\t\tcss_get(css->parent);\n\t}\n\n\tif (cgroup_on_dfl(cgrp) && ss->css_rstat_flush)\n\t\tlist_add_rcu(&css->rstat_css_node, &cgrp->rstat_css_list);\n\n\tBUG_ON(cgroup_css(cgrp, ss));\n}\n\n/* invoke ->css_online() on a new CSS and mark it online if successful */\nstatic int online_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (ss->css_online)\n\t\tret = ss->css_online(css);\n\tif (!ret) {\n\t\tcss->flags |= CSS_ONLINE;\n\t\trcu_assign_pointer(css->cgroup->subsys[ss->id], css);\n\n\t\tatomic_inc(&css->online_cnt);\n\t\tif (css->parent)\n\t\t\tatomic_inc(&css->parent->online_cnt);\n\t}\n\treturn ret;\n}\n\n/* if the CSS is online, invoke ->css_offline() on it and mark it offline */\nstatic void offline_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!(css->flags & CSS_ONLINE))\n\t\treturn;\n\n\tif (ss->css_offline)\n\t\tss->css_offline(css);\n\n\tcss->flags &= ~CSS_ONLINE;\n\tRCU_INIT_POINTER(css->cgroup->subsys[ss->id], NULL);\n\n\twake_up_all(&css->cgroup->offline_waitq);\n}\n\n/**\n * css_create - create a cgroup_subsys_state\n * @cgrp: the cgroup new css will be associated with\n * @ss: the subsys of new css\n *\n * Create a new css associated with @cgrp - @ss pair.  On success, the new\n * css is online and installed in @cgrp.  This function doesn't create the\n * interface files.  Returns 0 on success, -errno on failure.\n */\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup_subsys_state *parent_css = cgroup_css(parent, ss);\n\tstruct cgroup_subsys_state *css;\n\tint err;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcss = ss->css_alloc(parent_css);\n\tif (!css)\n\t\tcss = ERR_PTR(-ENOMEM);\n\tif (IS_ERR(css))\n\t\treturn css;\n\n\tinit_and_link_css(css, ss, cgrp);\n\n\terr = percpu_ref_init(&css->refcnt, css_release, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err_free_css;\n\n\terr = cgroup_idr_alloc(&ss->css_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (err < 0)\n\t\tgoto err_free_css;\n\tcss->id = err;\n\n\t/* @css is ready to be brought online now, make it visible */\n\tlist_add_tail_rcu(&css->sibling, &parent_css->children);\n\tcgroup_idr_replace(&ss->css_idr, css, css->id);\n\n\terr = online_css(css);\n\tif (err)\n\t\tgoto err_list_del;\n\n\tif (ss->broken_hierarchy && !ss->warned_broken_hierarchy &&\n\t    cgroup_parent(parent)) {\n\t\tpr_warn(\"%s (%d) created nested cgroup for controller \\\"%s\\\" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.\\n\",\n\t\t\tcurrent->comm, current->pid, ss->name);\n\t\tif (!strcmp(ss->name, \"memory\"))\n\t\t\tpr_warn(\"\\\"memory\\\" requires setting use_hierarchy to 1 on the root\\n\");\n\t\tss->warned_broken_hierarchy = true;\n\t}\n\n\treturn css;\n\nerr_list_del:\n\tlist_del_rcu(&css->sibling);\nerr_free_css:\n\tlist_del_rcu(&css->rstat_css_node);\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n\treturn ERR_PTR(err);\n}\n\n/*\n * The returned cgroup is fully initialized including its control mask, but\n * it isn't associated with its kernfs_node and doesn't have the control\n * mask applied.\n */\nstatic struct cgroup *cgroup_create(struct cgroup *parent)\n{\n\tstruct cgroup_root *root = parent->root;\n\tstruct cgroup *cgrp, *tcgrp;\n\tint level = parent->level + 1;\n\tint ret;\n\n\t/* allocate the cgroup and its ID, 0 is reserved for the root */\n\tcgrp = kzalloc(struct_size(cgrp, ancestor_ids, (level + 1)),\n\t\t       GFP_KERNEL);\n\tif (!cgrp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = percpu_ref_init(&cgrp->self.refcnt, css_release, 0, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_free_cgrp;\n\n\tif (cgroup_on_dfl(parent)) {\n\t\tret = cgroup_rstat_init(cgrp);\n\t\tif (ret)\n\t\t\tgoto out_cancel_ref;\n\t}\n\n\t/*\n\t * Temporarily set the pointer to NULL, so idr_find() won't return\n\t * a half-baked cgroup.\n\t */\n\tcgrp->id = cgroup_idr_alloc(&root->cgroup_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (cgrp->id < 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out_stat_exit;\n\t}\n\n\tinit_cgroup_housekeeping(cgrp);\n\n\tcgrp->self.parent = &parent->self;\n\tcgrp->root = root;\n\tcgrp->level = level;\n\n\tret = psi_cgroup_alloc(cgrp);\n\tif (ret)\n\t\tgoto out_idr_free;\n\n\tret = cgroup_bpf_inherit(cgrp);\n\tif (ret)\n\t\tgoto out_psi_free;\n\n\tfor (tcgrp = cgrp; tcgrp; tcgrp = cgroup_parent(tcgrp)) {\n\t\tcgrp->ancestor_ids[tcgrp->level] = tcgrp->id;\n\n\t\tif (tcgrp != cgrp)\n\t\t\ttcgrp->nr_descendants++;\n\t}\n\n\tif (notify_on_release(parent))\n\t\tset_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n\n\tif (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &parent->flags))\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags);\n\n\tcgrp->self.serial_nr = css_serial_nr_next++;\n\n\t/* allocation complete, commit to creation */\n\tlist_add_tail_rcu(&cgrp->self.sibling, &cgroup_parent(cgrp)->self.children);\n\tatomic_inc(&root->nr_cgrps);\n\tcgroup_get_live(parent);\n\n\t/*\n\t * @cgrp is now fully operational.  If something fails after this\n\t * point, it'll be released via the normal destruction path.\n\t */\n\tcgroup_idr_replace(&root->cgroup_idr, cgrp, cgrp->id);\n\n\t/*\n\t * On the default hierarchy, a child doesn't automatically inherit\n\t * subtree_control from the parent.  Each is configured manually.\n\t */\n\tif (!cgroup_on_dfl(cgrp))\n\t\tcgrp->subtree_control = cgroup_control(cgrp);\n\n\tcgroup_propagate_control(cgrp);\n\n\treturn cgrp;\n\nout_psi_free:\n\tpsi_cgroup_free(cgrp);\nout_idr_free:\n\tcgroup_idr_remove(&root->cgroup_idr, cgrp->id);\nout_stat_exit:\n\tif (cgroup_on_dfl(parent))\n\t\tcgroup_rstat_exit(cgrp);\nout_cancel_ref:\n\tpercpu_ref_exit(&cgrp->self.refcnt);\nout_free_cgrp:\n\tkfree(cgrp);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool cgroup_check_hierarchy_limits(struct cgroup *parent)\n{\n\tstruct cgroup *cgroup;\n\tint ret = false;\n\tint level = 1;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tfor (cgroup = parent; cgroup; cgroup = cgroup_parent(cgroup)) {\n\t\tif (cgroup->nr_descendants >= cgroup->max_descendants)\n\t\t\tgoto fail;\n\n\t\tif (level > cgroup->max_depth)\n\t\t\tgoto fail;\n\n\t\tlevel++;\n\t}\n\n\tret = true;\nfail:\n\treturn ret;\n}\n\nint cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)\n{\n\tstruct cgroup *parent, *cgrp;\n\tstruct kernfs_node *kn;\n\tint ret;\n\n\t/* do not accept '\\n' to prevent making /proc/<pid>/cgroup unparsable */\n\tif (strchr(name, '\\n'))\n\t\treturn -EINVAL;\n\n\tparent = cgroup_kn_lock_live(parent_kn, false);\n\tif (!parent)\n\t\treturn -ENODEV;\n\n\tif (!cgroup_check_hierarchy_limits(parent)) {\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tcgrp = cgroup_create(parent);\n\tif (IS_ERR(cgrp)) {\n\t\tret = PTR_ERR(cgrp);\n\t\tgoto out_unlock;\n\t}\n\n\t/* create the directory */\n\tkn = kernfs_create_dir(parent->kn, name, mode, cgrp);\n\tif (IS_ERR(kn)) {\n\t\tret = PTR_ERR(kn);\n\t\tgoto out_destroy;\n\t}\n\tcgrp->kn = kn;\n\n\t/*\n\t * This extra ref will be put in cgroup_free_fn() and guarantees\n\t * that @cgrp->kn is always accessible.\n\t */\n\tkernfs_get(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = css_populate_dir(&cgrp->self);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tTRACE_CGROUP_PATH(mkdir, cgrp);\n\n\t/* let's create and online css's */\n\tkernfs_activate(kn);\n\n\tret = 0;\n\tgoto out_unlock;\n\nout_destroy:\n\tcgroup_destroy_locked(cgrp);\nout_unlock:\n\tcgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\n/*\n * This is called when the refcnt of a css is confirmed to be killed.\n * css_tryget_online() is now guaranteed to fail.  Tell the subsystem to\n * initate destruction and put the css ref from kill_css().\n */\nstatic void css_killed_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\n\tmutex_lock(&cgroup_mutex);\n\n\tdo {\n\t\toffline_css(css);\n\t\tcss_put(css);\n\t\t/* @css can't go away while we're holding cgroup_mutex */\n\t\tcss = css->parent;\n\t} while (css && atomic_dec_and_test(&css->online_cnt));\n\n\tmutex_unlock(&cgroup_mutex);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [
            "#define CGROUP_FILE_NOTIFY_MIN_INTV\tDIV_ROUND_UP(HZ, 100)",
            "#define CGROUP_FILE_NAME_MAX\t\t(MAX_CGROUP_TYPE_NAMELEN +\t\\\n\t\t\t\t\t MAX_CFTYPE_NAME + 2)"
          ],
          "globals_used": [
            "static DEFINE_SPINLOCK(cgroup_file_kn_lock);",
            "struct percpu_rw_semaphore cgroup_threadgroup_rwsem;",
            "static struct workqueue_struct *cgroup_destroy_wq;",
            "struct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };",
            "static u16 cgrp_dfl_inhibit_ss_mask;",
            "static u16 cgrp_dfl_threaded_ss_mask;",
            "static u64 css_serial_nr_next = 1;",
            "struct cgroup_namespace init_cgroup_ns = {\n\t.count\t\t= REFCOUNT_INIT(2),\n\t.user_ns\t= &init_user_ns,\n\t.ns.ops\t\t= &cgroupns_operations,\n\t.ns.inum\t= PROC_CGROUP_INIT_INO,\n\t.root_cset\t= &init_css_set,\n};",
            "static struct cftype cgroup_base_files[];",
            "static int cgroup_apply_control(struct cgroup *cgrp);",
            "static void cgroup_finalize_control(struct cgroup *cgrp, int ret);",
            "static void css_task_iter_advance(struct css_task_iter *it);",
            "static int cgroup_destroy_locked(struct cgroup *cgrp);",
            "static struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);",
            "static void css_release(struct percpu_ref *ref);",
            "static void kill_css(struct cgroup_subsys_state *css);",
            "static int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\n#define CGROUP_FILE_NOTIFY_MIN_INTV\tDIV_ROUND_UP(HZ, 100)\n#define CGROUP_FILE_NAME_MAX\t\t(MAX_CGROUP_TYPE_NAMELEN +\t\\\n\t\t\t\t\t MAX_CFTYPE_NAME + 2)\n\nstatic DEFINE_SPINLOCK(cgroup_file_kn_lock);\nstruct percpu_rw_semaphore cgroup_threadgroup_rwsem;\nstatic struct workqueue_struct *cgroup_destroy_wq;\nstruct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };\nstatic u16 cgrp_dfl_inhibit_ss_mask;\nstatic u16 cgrp_dfl_threaded_ss_mask;\nstatic u64 css_serial_nr_next = 1;\nstruct cgroup_namespace init_cgroup_ns = {\n\t.count\t\t= REFCOUNT_INIT(2),\n\t.user_ns\t= &init_user_ns,\n\t.ns.ops\t\t= &cgroupns_operations,\n\t.ns.inum\t= PROC_CGROUP_INIT_INO,\n\t.root_cset\t= &init_css_set,\n};\nstatic struct cftype cgroup_base_files[];\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret);\nstatic void css_task_iter_advance(struct css_task_iter *it);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);\nstatic void css_release(struct percpu_ref *ref);\nstatic void kill_css(struct cgroup_subsys_state *css);\nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add);\n\nint cgroup_attach_task(struct cgroup *dst_cgrp, struct task_struct *leader,\n\t\t       bool threadgroup)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct task_struct *task;\n\tint ret;\n\n\tret = cgroup_migrate_vet_dst(dst_cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/* look up all src csets */\n\tspin_lock_irq(&css_set_lock);\n\trcu_read_lock();\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_src(task_css_set(task), dst_cgrp, &mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\trcu_read_unlock();\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* prepare dst csets and commit */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (!ret)\n\t\tret = cgroup_migrate(leader, threadgroup, &mgctx);\n\n\tcgroup_migrate_finish(&mgctx);\n\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(attach_task, dst_cgrp, leader, threadgroup);\n\n\treturn ret;\n}\n\nstruct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup)\n\t__acquires(&cgroup_threadgroup_rwsem)\n{\n\tstruct task_struct *tsk;\n\tpid_t pid;\n\n\tif (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\trcu_read_lock();\n\tif (pid) {\n\t\ttsk = find_task_by_vpid(pid);\n\t\tif (!tsk) {\n\t\t\ttsk = ERR_PTR(-ESRCH);\n\t\t\tgoto out_unlock_threadgroup;\n\t\t}\n\t} else {\n\t\ttsk = current;\n\t}\n\n\tif (threadgroup)\n\t\ttsk = tsk->group_leader;\n\n\t/*\n\t * kthreads may acquire PF_NO_SETAFFINITY during initialization.\n\t * If userland migrates such a kthread to a non-root cgroup, it can\n\t * become trapped in a cpuset, or RT kthread may be born in a\n\t * cgroup with no rt_runtime allocated.  Just say no.\n\t */\n\tif (tsk->no_cgroup_migration || (tsk->flags & PF_NO_SETAFFINITY)) {\n\t\ttsk = ERR_PTR(-EINVAL);\n\t\tgoto out_unlock_threadgroup;\n\t}\n\n\tget_task_struct(tsk);\n\tgoto out_unlock_rcu;\n\nout_unlock_threadgroup:\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\nout_unlock_rcu:\n\trcu_read_unlock();\n\treturn tsk;\n}\n\nvoid cgroup_procs_write_finish(struct task_struct *task)\n\t__releases(&cgroup_threadgroup_rwsem)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\t/* release reference from cgroup_procs_write_start() */\n\tput_task_struct(task);\n\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tfor_each_subsys(ss, ssid)\n\t\tif (ss->post_attach)\n\t\t\tss->post_attach();\n}\n\nstatic void cgroup_print_ss_mask(struct seq_file *seq, u16 ss_mask)\n{\n\tstruct cgroup_subsys *ss;\n\tbool printed = false;\n\tint ssid;\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tif (printed)\n\t\t\tseq_putc(seq, ' ');\n\t\tseq_printf(seq, \"%s\", ss->name);\n\t\tprinted = true;\n\t} while_each_subsys_mask();\n\tif (printed)\n\t\tseq_putc(seq, '\\n');\n}\n\n/* show controllers which are enabled from the parent */\nstatic int cgroup_controllers_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgroup_control(cgrp));\n\treturn 0;\n}\n\n/* show controllers which are enabled for a given cgroup's children */\nstatic int cgroup_subtree_control_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgrp->subtree_control);\n\treturn 0;\n}\n\n/**\n * cgroup_update_dfl_csses - update css assoc of a subtree in default hierarchy\n * @cgrp: root of the subtree to update csses for\n *\n * @cgrp's control masks have changed and its subtree's css associations\n * need to be updated accordingly.  This function looks up all css_sets\n * which are attached to the subtree, creates the matching updated css_sets\n * and migrates the tasks to the new ones.\n */\nstatic int cgroup_update_dfl_csses(struct cgroup *cgrp)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup *dsct;\n\tstruct css_set *src_cset;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\n\t/* look up all csses currently attached to @cgrp's subtree */\n\tspin_lock_irq(&css_set_lock);\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tstruct cgrp_cset_link *link;\n\n\t\tlist_for_each_entry(link, &dsct->cset_links, cset_link)\n\t\t\tcgroup_migrate_add_src(link->cset, dsct, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* NULL dst indicates self on default hierarchy */\n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(src_cset, &mgctx.preloaded_src_csets, mg_preload_node) {\n\t\tstruct task_struct *task, *ntask;\n\n\t\t/* all tasks in src_csets need to be migrated */\n\t\tlist_for_each_entry_safe(task, ntask, &src_cset->tasks, cg_list)\n\t\t\tcgroup_migrate_add_task(task, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_migrate_execute(&mgctx);\nout_finish:\n\tcgroup_migrate_finish(&mgctx);\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\treturn ret;\n}\n\n/**\n * cgroup_lock_and_drain_offline - lock cgroup_mutex and drain offlined csses\n * @cgrp: root of the target subtree\n *\n * Because css offlining is asynchronous, userland may try to re-enable a\n * controller while the previous css is still around.  This function grabs\n * cgroup_mutex and drains the previous css instances of @cgrp's subtree.\n */\nvoid cgroup_lock_and_drain_offline(struct cgroup *cgrp)\n\t__acquires(&cgroup_mutex)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\nrestart:\n\tmutex_lock(&cgroup_mutex);\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\t\t\tDEFINE_WAIT(wait);\n\n\t\t\tif (!css || !percpu_ref_is_dying(&css->refcnt))\n\t\t\t\tcontinue;\n\n\t\t\tcgroup_get_live(dsct);\n\t\t\tprepare_to_wait(&dsct->offline_waitq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\t\tmutex_unlock(&cgroup_mutex);\n\t\t\tschedule();\n\t\t\tfinish_wait(&dsct->offline_waitq, &wait);\n\n\t\t\tcgroup_put(dsct);\n\t\t\tgoto restart;\n\t\t}\n\t}\n}\n\n/**\n * cgroup_save_control - save control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Save ->subtree_control, ->subtree_ss_mask and ->dom_cgrp to the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_save_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->old_subtree_control = dsct->subtree_control;\n\t\tdsct->old_subtree_ss_mask = dsct->subtree_ss_mask;\n\t\tdsct->old_dom_cgrp = dsct->dom_cgrp;\n\t}\n}\n\n/**\n * cgroup_propagate_control - refresh control masks of a subtree\n * @cgrp: root of the target subtree\n *\n * For @cgrp and its subtree, ensure ->subtree_ss_mask matches\n * ->subtree_control and propagate controller availability through the\n * subtree so that descendants don't have unavailable controllers enabled.\n */\nstatic void cgroup_propagate_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control &= cgroup_control(dsct);\n\t\tdsct->subtree_ss_mask =\n\t\t\tcgroup_calc_subtree_ss_mask(dsct->subtree_control,\n\t\t\t\t\t\t    cgroup_ss_mask(dsct));\n\t}\n}\n\n/**\n * cgroup_restore_control - restore control masks and dom_cgrp of a subtree\n * @cgrp: root of the target subtree\n *\n * Restore ->subtree_control, ->subtree_ss_mask and ->dom_cgrp from the\n * respective old_ prefixed fields for @cgrp's subtree including @cgrp\n * itself.\n */\nstatic void cgroup_restore_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control = dsct->old_subtree_control;\n\t\tdsct->subtree_ss_mask = dsct->old_subtree_ss_mask;\n\t\tdsct->dom_cgrp = dsct->old_dom_cgrp;\n\t}\n}\n\nstatic bool css_visible(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tif (cgroup_control(cgrp) & (1 << ss->id))\n\t\treturn true;\n\tif (!(cgroup_ss_mask(cgrp) & (1 << ss->id)))\n\t\treturn false;\n\treturn cgroup_on_dfl(cgrp) && ss->implicit_on_dfl;\n}\n\n/**\n * cgroup_apply_control_enable - enable or show csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and create new csses or make the existing ones\n * visible.  A css is created invisible if it's being implicitly enabled\n * through dependency.  An invisible css is made visible when the userland\n * explicitly enables it.\n *\n * Returns 0 on success, -errno on failure.  On failure, csses which have\n * been processed already aren't cleaned up.  The caller is responsible for\n * cleaning up with cgroup_apply_control_disable().\n */\nstatic int cgroup_apply_control_enable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid, ret;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!(cgroup_ss_mask(dsct) & (1 << ss->id)))\n\t\t\t\tcontinue;\n\n\t\t\tif (!css) {\n\t\t\t\tcss = css_create(dsct, ss);\n\t\t\t\tif (IS_ERR(css))\n\t\t\t\t\treturn PTR_ERR(css);\n\t\t\t}\n\n\t\t\tif (css_visible(css)) {\n\t\t\t\tret = css_populate_dir(css);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_apply_control_disable - kill or hide csses according to control\n * @cgrp: root of the target subtree\n *\n * Walk @cgrp's subtree and kill and hide csses so that they match\n * cgroup_ss_mask() and cgroup_visible_mask().\n *\n * A css is hidden when the userland requests it to be disabled while other\n * subsystems are still depending on it.  The css must not actively control\n * resources and be in the vanilla state if it's made visible again later.\n * Controllers which may be depended upon should provide ->css_reset() for\n * this purpose.\n */\nstatic void cgroup_apply_control_disable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tWARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (!css)\n\t\t\t\tcontinue;\n\n\t\t\tif (css->parent &&\n\t\t\t    !(cgroup_ss_mask(dsct) & (1 << ss->id))) {\n\t\t\t\tkill_css(css);\n\t\t\t} else if (!css_visible(css)) {\n\t\t\t\tcss_clear_dir(css);\n\t\t\t\tif (ss->css_reset)\n\t\t\t\t\tss->css_reset(css);\n\t\t\t}\n\t\t}\n\t}\n}\n\n/**\n * cgroup_apply_control - apply control mask updates to the subtree\n * @cgrp: root of the target subtree\n *\n * subsystems can be enabled and disabled in a subtree using the following\n * steps.\n *\n * 1. Call cgroup_save_control() to stash the current state.\n * 2. Update ->subtree_control masks in the subtree as desired.\n * 3. Call cgroup_apply_control() to apply the changes.\n * 4. Optionally perform other related operations.\n * 5. Call cgroup_finalize_control() to finish up.\n *\n * This function implements step 3 and propagates the mask changes\n * throughout @cgrp's subtree, updates csses accordingly and perform\n * process migrations.\n */\nstatic int cgroup_apply_control(struct cgroup *cgrp)\n{\n\tint ret;\n\n\tcgroup_propagate_control(cgrp);\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * At this point, cgroup_e_css() results reflect the new csses\n\t * making the following cgroup_update_dfl_csses() properly update\n\t * css associations of all tasks in the subtree.\n\t */\n\tret = cgroup_update_dfl_csses(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\n/**\n * cgroup_finalize_control - finalize control mask update\n * @cgrp: root of the target subtree\n * @ret: the result of the update\n *\n * Finalize control mask update.  See cgroup_apply_control() for more info.\n */\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret)\n{\n\tif (ret) {\n\t\tcgroup_restore_control(cgrp);\n\t\tcgroup_propagate_control(cgrp);\n\t}\n\n\tcgroup_apply_control_disable(cgrp);\n}\n\nstatic int cgroup_vet_subtree_control_enable(struct cgroup *cgrp, u16 enable)\n{\n\tu16 domain_enable = enable & ~cgrp_dfl_threaded_ss_mask;\n\n\t/* if nothing is getting enabled, nothing to worry about */\n\tif (!enable)\n\t\treturn 0;\n\n\t/* can @cgrp host any resources? */\n\tif (!cgroup_is_valid_domain(cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/* mixables don't care */\n\tif (cgroup_is_mixable(cgrp))\n\t\treturn 0;\n\n\tif (domain_enable) {\n\t\t/* can't enable domain controllers inside a thread subtree */\n\t\tif (cgroup_is_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\t/*\n\t\t * Threaded controllers can handle internal competitions\n\t\t * and are always allowed inside a (prospective) thread\n\t\t * subtree.\n\t\t */\n\t\tif (cgroup_can_be_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * Controllers can't be enabled for a cgroup with tasks to avoid\n\t * child cgroups competing against tasks.\n\t */\n\tif (cgroup_has_tasks(cgrp))\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n/* change the enabled child controllers for a cgroup in the default hierarchy */\nstatic ssize_t cgroup_subtree_control_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tu16 enable = 0, disable = 0;\n\tstruct cgroup *cgrp, *child;\n\tstruct cgroup_subsys *ss;\n\tchar *tok;\n\tint ssid, ret;\n\n\t/*\n\t * Parse input - space separated list of subsystem names prefixed\n\t * with either + or -.\n\t */\n\tbuf = strstrip(buf);\n\twhile ((tok = strsep(&buf, \" \"))) {\n\t\tif (tok[0] == '\\0')\n\t\t\tcontinue;\n\t\tdo_each_subsys_mask(ss, ssid, ~cgrp_dfl_inhibit_ss_mask) {\n\t\t\tif (!cgroup_ssid_enabled(ssid) ||\n\t\t\t    strcmp(tok + 1, ss->name))\n\t\t\t\tcontinue;\n\n\t\t\tif (*tok == '+') {\n\t\t\t\tenable |= 1 << ssid;\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t} else if (*tok == '-') {\n\t\t\t\tdisable |= 1 << ssid;\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t} else {\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t} while_each_subsys_mask();\n\t\tif (ssid == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -EINVAL;\n\t}\n\n\tcgrp = cgroup_kn_lock_live(of->kn, true);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tfor_each_subsys(ss, ssid) {\n\t\tif (enable & (1 << ssid)) {\n\t\t\tif (cgrp->subtree_control & (1 << ssid)) {\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!(cgroup_control(cgrp) & (1 << ssid))) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else if (disable & (1 << ssid)) {\n\t\t\tif (!(cgrp->subtree_control & (1 << ssid))) {\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* a child has it enabled? */\n\t\t\tcgroup_for_each_live_child(child, cgrp) {\n\t\t\t\tif (child->subtree_control & (1 << ssid)) {\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!enable && !disable) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tret = cgroup_vet_subtree_control_enable(cgrp, enable);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* save and update control masks and prepare csses */\n\tcgroup_save_control(cgrp);\n\n\tcgrp->subtree_control |= enable;\n\tcgrp->subtree_control &= ~disable;\n\n\tret = cgroup_apply_control(cgrp);\n\tcgroup_finalize_control(cgrp, ret);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tkernfs_activate(cgrp->kn);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\n/**\n * cgroup_enable_threaded - make @cgrp threaded\n * @cgrp: the target cgroup\n *\n * Called when \"threaded\" is written to the cgroup.type interface file and\n * tries to make @cgrp threaded and join the parent's resource domain.\n * This function is never called on the root cgroup as cgroup.type doesn't\n * exist on it.\n */\nstatic int cgroup_enable_threaded(struct cgroup *cgrp)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup *dom_cgrp = parent->dom_cgrp;\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* noop if already threaded */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn 0;\n\n\t/*\n\t * If @cgroup is populated or has domain controllers enabled, it\n\t * can't be switched.  While the below cgroup_can_be_thread_root()\n\t * test can catch the same conditions, that's only when @parent is\n\t * not mixable, so let's check it explicitly.\n\t */\n\tif (cgroup_is_populated(cgrp) ||\n\t    cgrp->subtree_control & ~cgrp_dfl_threaded_ss_mask)\n\t\treturn -EOPNOTSUPP;\n\n\t/* we're joining the parent's domain, ensure its validity */\n\tif (!cgroup_is_valid_domain(dom_cgrp) ||\n\t    !cgroup_can_be_thread_root(dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * The following shouldn't cause actual migrations and should\n\t * always succeed.\n\t */\n\tcgroup_save_control(cgrp);\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp)\n\t\tif (dsct == cgrp || cgroup_is_threaded(dsct))\n\t\t\tdsct->dom_cgrp = dom_cgrp;\n\n\tret = cgroup_apply_control(cgrp);\n\tif (!ret)\n\t\tparent->nr_threaded_children++;\n\n\tcgroup_finalize_control(cgrp, ret);\n\treturn ret;\n}\n\nstatic int cgroup_type_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tif (cgroup_is_threaded(cgrp))\n\t\tseq_puts(seq, \"threaded\\n\");\n\telse if (!cgroup_is_valid_domain(cgrp))\n\t\tseq_puts(seq, \"domain invalid\\n\");\n\telse if (cgroup_is_thread_root(cgrp))\n\t\tseq_puts(seq, \"domain threaded\\n\");\n\telse\n\t\tseq_puts(seq, \"domain\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_type_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\t/* only switching to threaded mode is supported */\n\tif (strcmp(strstrip(buf), \"threaded\"))\n\t\treturn -EINVAL;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\t/* threaded can only be enabled */\n\tret = cgroup_enable_threaded(cgrp);\n\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\nstatic int cgroup_max_descendants_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint descendants = READ_ONCE(cgrp->max_descendants);\n\n\tif (descendants == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", descendants);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_descendants_write(struct kernfs_open_file *of,\n\t\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint descendants;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdescendants = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &descendants);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (descendants < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_descendants = descendants;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_max_depth_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint depth = READ_ONCE(cgrp->max_depth);\n\n\tif (depth == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", depth);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_depth_write(struct kernfs_open_file *of,\n\t\t\t\t      char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tssize_t ret;\n\tint depth;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdepth = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &depth);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (depth < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_depth = depth;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_events_show(struct seq_file *seq, void *v)\n{\n\tseq_printf(seq, \"populated %d\\n\",\n\t\t   cgroup_is_populated(seq_css(seq)->cgroup));\n\treturn 0;\n}\n\nstatic int cgroup_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgroup = seq_css(seq)->cgroup;\n\n\tseq_printf(seq, \"nr_descendants %d\\n\",\n\t\t   cgroup->nr_descendants);\n\tseq_printf(seq, \"nr_dying_descendants %d\\n\",\n\t\t   cgroup->nr_dying_descendants);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused cgroup_extra_stat_show(struct seq_file *seq,\n\t\t\t\t\t\t struct cgroup *cgrp, int ssid)\n{\n\tstruct cgroup_subsys *ss = cgroup_subsys[ssid];\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\tif (!ss->css_extra_stat_show)\n\t\treturn 0;\n\n\tcss = cgroup_tryget_css(cgrp, ss);\n\tif (!css)\n\t\treturn 0;\n\n\tret = ss->css_extra_stat_show(seq, css);\n\tcss_put(css);\n\treturn ret;\n}\n\nstatic int cpu_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup __maybe_unused *cgrp = seq_css(seq)->cgroup;\n\tint ret = 0;\n\n\tcgroup_base_stat_cputime_show(seq);\n#ifdef CONFIG_CGROUP_SCHED\n\tret = cgroup_extra_stat_show(seq, cgrp, cpu_cgrp_id);\n#endif\n\treturn ret;\n}\n\n#ifdef CONFIG_PSI\nstatic int cgroup_io_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_IO);\n}\nstatic int cgroup_memory_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_MEM);\n}\nstatic int cgroup_cpu_pressure_show(struct seq_file *seq, void *v)\n{\n\treturn psi_show(seq, &seq_css(seq)->cgroup->psi, PSI_CPU);\n}\n#endif\n\nstatic int cgroup_file_open(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->open)\n\t\treturn cft->open(of);\n\treturn 0;\n}\n\nstatic void cgroup_file_release(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of->kn->priv;\n\n\tif (cft->release)\n\t\tcft->release(of);\n}\n\nstatic ssize_t cgroup_file_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *cgrp = of->kn->parent->priv;\n\tstruct cftype *cft = of->kn->priv;\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, disallow writes to\n\t * files in an non-init namespace root from inside the namespace\n\t * except for the files explicitly marked delegatable -\n\t * cgroup.procs and cgroup.subtree_control.\n\t */\n\tif ((cgrp->root->flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    !(cft->flags & CFTYPE_NS_DELEGATABLE) &&\n\t    ns != &init_cgroup_ns && ns->root_cset->dfl_cgrp == cgrp)\n\t\treturn -EPERM;\n\n\tif (cft->write)\n\t\treturn cft->write(of, buf, nbytes, off);\n\n\t/*\n\t * kernfs guarantees that a file isn't deleted with operations in\n\t * flight, which means that the matching css is and stays alive and\n\t * doesn't need to be pinned.  The RCU locking is not necessary\n\t * either.  It's just for the convenience of using cgroup_css().\n\t */\n\trcu_read_lock();\n\tcss = cgroup_css(cgrp, cft->ss);\n\trcu_read_unlock();\n\n\tif (cft->write_u64) {\n\t\tunsigned long long v;\n\t\tret = kstrtoull(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_u64(css, cft, v);\n\t} else if (cft->write_s64) {\n\t\tlong long v;\n\t\tret = kstrtoll(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_s64(css, cft, v);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_seqfile_start(struct seq_file *seq, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_start(seq, ppos);\n}\n\nstatic void *cgroup_seqfile_next(struct seq_file *seq, void *v, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_next(seq, v, ppos);\n}\n\nstatic void cgroup_seqfile_stop(struct seq_file *seq, void *v)\n{\n\tif (seq_cft(seq)->seq_stop)\n\t\tseq_cft(seq)->seq_stop(seq, v);\n}\n\nstatic int cgroup_seqfile_show(struct seq_file *m, void *arg)\n{\n\tstruct cftype *cft = seq_cft(m);\n\tstruct cgroup_subsys_state *css = seq_css(m);\n\n\tif (cft->seq_show)\n\t\treturn cft->seq_show(m, arg);\n\n\tif (cft->read_u64)\n\t\tseq_printf(m, \"%llu\\n\", cft->read_u64(css, cft));\n\telse if (cft->read_s64)\n\t\tseq_printf(m, \"%lld\\n\", cft->read_s64(css, cft));\n\telse\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic struct kernfs_ops cgroup_kf_single_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\nstatic struct kernfs_ops cgroup_kf_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_start\t\t= cgroup_seqfile_start,\n\t.seq_next\t\t= cgroup_seqfile_next,\n\t.seq_stop\t\t= cgroup_seqfile_stop,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\n/* set uid and gid of cgroup dirs and files to that of the creator */\nstatic int cgroup_kn_set_ugid(struct kernfs_node *kn)\n{\n\tstruct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,\n\t\t\t       .ia_uid = current_fsuid(),\n\t\t\t       .ia_gid = current_fsgid(), };\n\n\tif (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&\n\t    gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))\n\t\treturn 0;\n\n\treturn kernfs_setattr(kn, &iattr);\n}\n\nstatic void cgroup_file_notify_timer(struct timer_list *timer)\n{\n\tcgroup_file_notify(container_of(timer, struct cgroup_file,\n\t\t\t\t\tnotify_timer));\n}\n\nstatic int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,\n\t\t\t   struct cftype *cft)\n{\n\tchar name[CGROUP_FILE_NAME_MAX];\n\tstruct kernfs_node *kn;\n\tstruct lock_class_key *key = NULL;\n\tint ret;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tkey = &cft->lockdep_key;\n#endif\n\tkn = __kernfs_create_file(cgrp->kn, cgroup_file_name(cgrp, cft, name),\n\t\t\t\t  cgroup_file_mode(cft),\n\t\t\t\t  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID,\n\t\t\t\t  0, cft->kf_ops, cft,\n\t\t\t\t  NULL, key);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\tkernfs_remove(kn);\n\t\treturn ret;\n\t}\n\n\tif (cft->file_offset) {\n\t\tstruct cgroup_file *cfile = (void *)css + cft->file_offset;\n\n\t\ttimer_setup(&cfile->notify_timer, cgroup_file_notify_timer, 0);\n\n\t\tspin_lock_irq(&cgroup_file_kn_lock);\n\t\tcfile->kn = kn;\n\t\tspin_unlock_irq(&cgroup_file_kn_lock);\n\t}\n\n\treturn 0;\n}\n\n/**\n * cgroup_addrm_files - add or remove files to a cgroup directory\n * @css: the target css\n * @cgrp: the target cgroup (usually css->cgroup)\n * @cfts: array of cftypes to be added\n * @is_add: whether to add or remove\n *\n * Depending on @is_add, add or remove files defined by @cfts on @cgrp.\n * For removals, this function never fails.\n */\nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add)\n{\n\tstruct cftype *cft, *cft_end = NULL;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\nrestart:\n\tfor (cft = cfts; cft != cft_end && cft->name[0] != '\\0'; cft++) {\n\t\t/* does cft->flags tell us to skip this file on @cgrp? */\n\t\tif ((cft->flags & __CFTYPE_ONLY_ON_DFL) && !cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & __CFTYPE_NOT_ON_DFL) && cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_NOT_ON_ROOT) && !cgroup_parent(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_ONLY_ON_ROOT) && cgroup_parent(cgrp))\n\t\t\tcontinue;\n\n\t\tif (is_add) {\n\t\t\tret = cgroup_add_file(css, cgrp, cft);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"%s: failed to add %s, err=%d\\n\",\n\t\t\t\t\t__func__, cft->name, ret);\n\t\t\t\tcft_end = cft;\n\t\t\t\tis_add = false;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t} else {\n\t\t\tcgroup_rm_file(cgrp, cft);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int cgroup_apply_cftypes(struct cftype *cfts, bool is_add)\n{\n\tstruct cgroup_subsys *ss = cfts[0].ss;\n\tstruct cgroup *root = &ss->root->cgrp;\n\tstruct cgroup_subsys_state *css;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* add/rm files for all cgroups created before */\n\tcss_for_each_descendant_pre(css, cgroup_css(root, ss)) {\n\t\tstruct cgroup *cgrp = css->cgroup;\n\n\t\tif (!(css->flags & CSS_VISIBLE))\n\t\t\tcontinue;\n\n\t\tret = cgroup_addrm_files(css, cgrp, cfts, is_add);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (is_add && !ret)\n\t\tkernfs_activate(root->kn);\n\treturn ret;\n}\n\nstatic void cgroup_exit_cftypes(struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\t/* free copy for custom atomic_write_len, see init_cftypes() */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE)\n\t\t\tkfree(cft->kf_ops);\n\t\tcft->kf_ops = NULL;\n\t\tcft->ss = NULL;\n\n\t\t/* revert flags set by cgroup core while adding @cfts */\n\t\tcft->flags &= ~(__CFTYPE_ONLY_ON_DFL | __CFTYPE_NOT_ON_DFL);\n\t}\n}\n\nstatic int cgroup_init_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\tstruct kernfs_ops *kf_ops;\n\n\t\tWARN_ON(cft->ss || cft->kf_ops);\n\n\t\tif (cft->seq_start)\n\t\t\tkf_ops = &cgroup_kf_ops;\n\t\telse\n\t\t\tkf_ops = &cgroup_kf_single_ops;\n\n\t\t/*\n\t\t * Ugh... if @cft wants a custom max_write_len, we need to\n\t\t * make a copy of kf_ops to set its atomic_write_len.\n\t\t */\n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE) {\n\t\t\tkf_ops = kmemdup(kf_ops, sizeof(*kf_ops), GFP_KERNEL);\n\t\t\tif (!kf_ops) {\n\t\t\t\tcgroup_exit_cftypes(cfts);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tkf_ops->atomic_write_len = cft->max_write_len;\n\t\t}\n\n\t\tcft->kf_ops = kf_ops;\n\t\tcft->ss = ss;\n\t}\n\n\treturn 0;\n}\n\nstatic int cgroup_rm_cftypes_locked(struct cftype *cfts)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!cfts || !cfts[0].ss)\n\t\treturn -ENOENT;\n\n\tlist_del(&cfts->node);\n\tcgroup_apply_cftypes(cfts, false);\n\tcgroup_exit_cftypes(cfts);\n\treturn 0;\n}\n\n/**\n * cgroup_rm_cftypes - remove an array of cftypes from a subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Unregister @cfts.  Files described by @cfts are removed from all\n * existing cgroups and all future cgroups won't have them either.  This\n * function can be called anytime whether @cfts' subsys is attached or not.\n *\n * Returns 0 on successful unregistration, -ENOENT if @cfts is not\n * registered.\n */\nint cgroup_rm_cftypes(struct cftype *cfts)\n{\n\tint ret;\n\n\tmutex_lock(&cgroup_mutex);\n\tret = cgroup_rm_cftypes_locked(cfts);\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_cftypes - add an array of cftypes to a subsystem\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Register @cfts to @ss.  Files described by @cfts are created for all\n * existing cgroups to which @ss is attached and all future cgroups will\n * have them too.  This function can be called anytime whether @ss is\n * attached or not.\n *\n * Returns 0 on successful registration, -errno on failure.  Note that this\n * function currently returns 0 as long as @cfts registration is successful\n * even if some file creation attempts on existing cgroups fail.\n */\nstatic int cgroup_add_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tint ret;\n\n\tif (!cgroup_ssid_enabled(ss->id))\n\t\treturn 0;\n\n\tif (!cfts || cfts[0].name[0] == '\\0')\n\t\treturn 0;\n\n\tret = cgroup_init_cftypes(ss, cfts);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tlist_add_tail(&cfts->node, &ss->cfts);\n\tret = cgroup_apply_cftypes(cfts, true);\n\tif (ret)\n\t\tcgroup_rm_cftypes_locked(cfts);\n\n\tmutex_unlock(&cgroup_mutex);\n\treturn ret;\n}\n\n/**\n * cgroup_add_dfl_cftypes - add an array of cftypes for default hierarchy\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the default hierarchy.\n */\nint cgroup_add_dfl_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_ONLY_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_add_legacy_cftypes - add an array of cftypes for legacy hierarchies\n * @ss: target cgroup subsystem\n * @cfts: zero-length name terminated array of cftypes\n *\n * Similar to cgroup_add_cftypes() but the added files are only used for\n * the legacy hierarchies.\n */\nint cgroup_add_legacy_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_NOT_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n/**\n * cgroup_file_notify - generate a file modified event for a cgroup_file\n * @cfile: target cgroup_file\n *\n * @cfile must have been obtained by setting cftype->file_offset.\n */\nvoid cgroup_file_notify(struct cgroup_file *cfile)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cgroup_file_kn_lock, flags);\n\tif (cfile->kn) {\n\t\tunsigned long last = cfile->notified_at;\n\t\tunsigned long next = last + CGROUP_FILE_NOTIFY_MIN_INTV;\n\n\t\tif (time_in_range(jiffies, last, next)) {\n\t\t\ttimer_reduce(&cfile->notify_timer, next);\n\t\t} else {\n\t\t\tkernfs_notify(cfile->kn);\n\t\t\tcfile->notified_at = jiffies;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&cgroup_file_kn_lock, flags);\n}\n\n/**\n * css_next_child - find the next child of a given css\n * @pos: the current position (%NULL to initiate traversal)\n * @parent: css whose children to walk\n *\n * This function returns the next child of @parent and should be called\n * under either cgroup_mutex or RCU read lock.  The only requirement is\n * that @parent and @pos are accessible.  The next sibling is guaranteed to\n * be returned regardless of their states.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *css_next_child(struct cgroup_subsys_state *pos,\n\t\t\t\t\t   struct cgroup_subsys_state *parent)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/*\n\t * @pos could already have been unlinked from the sibling list.\n\t * Once a cgroup is removed, its ->sibling.next is no longer\n\t * updated when its next sibling changes.  CSS_RELEASED is set when\n\t * @pos is taken off list, at which time its next pointer is valid,\n\t * and, as releases are serialized, the one pointed to by the next\n\t * pointer is guaranteed to not have started release yet.  This\n\t * implies that if we observe !CSS_RELEASED on @pos in this RCU\n\t * critical section, the one pointed to by its next pointer is\n\t * guaranteed to not have finished its RCU grace period even if we\n\t * have dropped rcu_read_lock() inbetween iterations.\n\t *\n\t * If @pos has CSS_RELEASED set, its next pointer can't be\n\t * dereferenced; however, as each css is given a monotonically\n\t * increasing unique serial number and always appended to the\n\t * sibling list, the next one can be found by walking the parent's\n\t * children until the first css with higher serial number than\n\t * @pos's.  While this path can be slower, it happens iff iteration\n\t * races against release and the race window is very small.\n\t */\n\tif (!pos) {\n\t\tnext = list_entry_rcu(parent->children.next, struct cgroup_subsys_state, sibling);\n\t} else if (likely(!(pos->flags & CSS_RELEASED))) {\n\t\tnext = list_entry_rcu(pos->sibling.next, struct cgroup_subsys_state, sibling);\n\t} else {\n\t\tlist_for_each_entry_rcu(next, &parent->children, sibling)\n\t\t\tif (next->serial_nr > pos->serial_nr)\n\t\t\t\tbreak;\n\t}\n\n\t/*\n\t * @next, if not pointing to the head, can be dereferenced and is\n\t * the next sibling.\n\t */\n\tif (&next->sibling != &parent->children)\n\t\treturn next;\n\treturn NULL;\n}\n\n/**\n * css_next_descendant_pre - find the next descendant for pre-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_pre().  Find the next descendant\n * to visit for pre-order traversal of @root's descendants.  @root is\n * included in the iteration and the first node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @root are accessible and @pos is a descendant of @root.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_pre(struct cgroup_subsys_state *pos,\n\t\t\tstruct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit @root */\n\tif (!pos)\n\t\treturn root;\n\n\t/* visit the first child if exists */\n\tnext = css_next_child(NULL, pos);\n\tif (next)\n\t\treturn next;\n\n\t/* no child, visit my or the closest ancestor's next sibling */\n\twhile (pos != root) {\n\t\tnext = css_next_child(pos, pos->parent);\n\t\tif (next)\n\t\t\treturn next;\n\t\tpos = pos->parent;\n\t}\n\n\treturn NULL;\n}\n\n/**\n * css_rightmost_descendant - return the rightmost descendant of a css\n * @pos: css of interest\n *\n * Return the rightmost descendant of @pos.  If there's no descendant, @pos\n * is returned.  This can be used during pre-order traversal to skip\n * subtree of @pos.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct rightmost descendant as\n * long as @pos is accessible.\n */\nstruct cgroup_subsys_state *\ncss_rightmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last, *tmp;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\tdo {\n\t\tlast = pos;\n\t\t/* ->prev isn't RCU safe, walk ->next till the end */\n\t\tpos = NULL;\n\t\tcss_for_each_child(tmp, last)\n\t\t\tpos = tmp;\n\t} while (pos);\n\n\treturn last;\n}\n\nstatic struct cgroup_subsys_state *\ncss_leftmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last;\n\n\tdo {\n\t\tlast = pos;\n\t\tpos = css_next_child(NULL, pos);\n\t} while (pos);\n\n\treturn last;\n}\n\n/**\n * css_next_descendant_post - find the next descendant for post-order walk\n * @pos: the current position (%NULL to initiate traversal)\n * @root: css whose descendants to walk\n *\n * To be used by css_for_each_descendant_post().  Find the next descendant\n * to visit for post-order traversal of @root's descendants.  @root is\n * included in the iteration and the last node to be visited.\n *\n * While this function requires cgroup_mutex or RCU read locking, it\n * doesn't require the whole traversal to be contained in a single critical\n * section.  This function will return the correct next descendant as long\n * as both @pos and @cgroup are accessible and @pos is a descendant of\n * @cgroup.\n *\n * If a subsystem synchronizes ->css_online() and the start of iteration, a\n * css which finished ->css_online() is guaranteed to be visible in the\n * future iterations and will stay visible until the last reference is put.\n * A css which hasn't finished ->css_online() or already finished\n * ->css_offline() may show up during traversal.  It's each subsystem's\n * responsibility to synchronize against on/offlining.\n */\nstruct cgroup_subsys_state *\ncss_next_descendant_post(struct cgroup_subsys_state *pos,\n\t\t\t struct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t/* if first iteration, visit leftmost descendant which may be @root */\n\tif (!pos)\n\t\treturn css_leftmost_descendant(root);\n\n\t/* if we visited @root, we're done */\n\tif (pos == root)\n\t\treturn NULL;\n\n\t/* if there's an unvisited sibling, visit its leftmost descendant */\n\tnext = css_next_child(pos, pos->parent);\n\tif (next)\n\t\treturn css_leftmost_descendant(next);\n\n\t/* no sibling left, visit parent */\n\treturn pos->parent;\n}\n\n/**\n * css_has_online_children - does a css have online children\n * @css: the target css\n *\n * Returns %true if @css has any online children; otherwise, %false.  This\n * function can be called from any context but the caller is responsible\n * for synchronizing against on/offlining as necessary.\n */\nbool css_has_online_children(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys_state *child;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tcss_for_each_child(child, css) {\n\t\tif (child->flags & CSS_ONLINE) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic struct css_set *css_task_iter_next_css_set(struct css_task_iter *it)\n{\n\tstruct list_head *l;\n\tstruct cgrp_cset_link *link;\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* find the next threaded cset */\n\tif (it->tcset_pos) {\n\t\tl = it->tcset_pos->next;\n\n\t\tif (l != it->tcset_head) {\n\t\t\tit->tcset_pos = l;\n\t\t\treturn container_of(l, struct css_set,\n\t\t\t\t\t    threaded_csets_node);\n\t\t}\n\n\t\tit->tcset_pos = NULL;\n\t}\n\n\t/* find the next cset */\n\tl = it->cset_pos;\n\tl = l->next;\n\tif (l == it->cset_head) {\n\t\tit->cset_pos = NULL;\n\t\treturn NULL;\n\t}\n\n\tif (it->ss) {\n\t\tcset = container_of(l, struct css_set, e_cset_node[it->ss->id]);\n\t} else {\n\t\tlink = list_entry(l, struct cgrp_cset_link, cset_link);\n\t\tcset = link->cset;\n\t}\n\n\tit->cset_pos = l;\n\n\t/* initialize threaded css_set walking */\n\tif (it->flags & CSS_TASK_ITER_THREADED) {\n\t\tif (it->cur_dcset)\n\t\t\tput_css_set_locked(it->cur_dcset);\n\t\tit->cur_dcset = cset;\n\t\tget_css_set(cset);\n\n\t\tit->tcset_head = &cset->threaded_csets;\n\t\tit->tcset_pos = &cset->threaded_csets;\n\t}\n\n\treturn cset;\n}\n\n/**\n * css_task_iter_advance_css_set - advance a task itererator to the next css_set\n * @it: the iterator to advance\n *\n * Advance @it to the next css_set to walk.\n */\nstatic void css_task_iter_advance_css_set(struct css_task_iter *it)\n{\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t/* Advance to the next non-empty css_set */\n\tdo {\n\t\tcset = css_task_iter_next_css_set(it);\n\t\tif (!cset) {\n\t\t\tit->task_pos = NULL;\n\t\t\treturn;\n\t\t}\n\t} while (!css_set_populated(cset));\n\n\tif (!list_empty(&cset->tasks))\n\t\tit->task_pos = cset->tasks.next;\n\telse\n\t\tit->task_pos = cset->mg_tasks.next;\n\n\tit->tasks_head = &cset->tasks;\n\tit->mg_tasks_head = &cset->mg_tasks;\n\n\t/*\n\t * We don't keep css_sets locked across iteration steps and thus\n\t * need to take steps to ensure that iteration can be resumed after\n\t * the lock is re-acquired.  Iteration is performed at two levels -\n\t * css_sets and tasks in them.\n\t *\n\t * Once created, a css_set never leaves its cgroup lists, so a\n\t * pinned css_set is guaranteed to stay put and we can resume\n\t * iteration afterwards.\n\t *\n\t * Tasks may leave @cset across iteration steps.  This is resolved\n\t * by registering each iterator with the css_set currently being\n\t * walked and making css_set_move_task() advance iterators whose\n\t * next task is leaving.\n\t */\n\tif (it->cur_cset) {\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t}\n\tget_css_set(cset);\n\tit->cur_cset = cset;\n\tlist_add(&it->iters_node, &cset->task_iters);\n}\n\nstatic void css_task_iter_advance(struct css_task_iter *it)\n{\n\tstruct list_head *next;\n\n\tlockdep_assert_held(&css_set_lock);\nrepeat:\n\t/*\n\t * Advance iterator to find next entry.  cset->tasks is consumed\n\t * first and then ->mg_tasks.  After ->mg_tasks, we move onto the\n\t * next cset.\n\t */\n\tnext = it->task_pos->next;\n\n\tif (next == it->tasks_head)\n\t\tnext = it->mg_tasks_head->next;\n\n\tif (next == it->mg_tasks_head)\n\t\tcss_task_iter_advance_css_set(it);\n\telse\n\t\tit->task_pos = next;\n\n\t/* if PROCS, skip over tasks which aren't group leaders */\n\tif ((it->flags & CSS_TASK_ITER_PROCS) && it->task_pos &&\n\t    !thread_group_leader(list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t    cg_list)))\n\t\tgoto repeat;\n}\n\n/**\n * css_task_iter_start - initiate task iteration\n * @css: the css to walk tasks of\n * @flags: CSS_TASK_ITER_* flags\n * @it: the task iterator to use\n *\n * Initiate iteration through the tasks of @css.  The caller can call\n * css_task_iter_next() to walk through the tasks until the function\n * returns NULL.  On completion of iteration, css_task_iter_end() must be\n * called.\n */\nvoid css_task_iter_start(struct cgroup_subsys_state *css, unsigned int flags,\n\t\t\t struct css_task_iter *it)\n{\n\t/* no one should try to iterate before mounting cgroups */\n\tWARN_ON_ONCE(!use_task_css_set_links);\n\n\tmemset(it, 0, sizeof(*it));\n\n\tspin_lock_irq(&css_set_lock);\n\n\tit->ss = css->ss;\n\tit->flags = flags;\n\n\tif (it->ss)\n\t\tit->cset_pos = &css->cgroup->e_csets[css->ss->id];\n\telse\n\t\tit->cset_pos = &css->cgroup->cset_links;\n\n\tit->cset_head = it->cset_pos;\n\n\tcss_task_iter_advance_css_set(it);\n\n\tspin_unlock_irq(&css_set_lock);\n}\n\n/**\n * css_task_iter_next - return the next task for the iterator\n * @it: the task iterator being iterated\n *\n * The \"next\" function for task iteration.  @it should have been\n * initialized via css_task_iter_start().  Returns NULL when the iteration\n * reaches the end.\n */\nstruct task_struct *css_task_iter_next(struct css_task_iter *it)\n{\n\tif (it->cur_task) {\n\t\tput_task_struct(it->cur_task);\n\t\tit->cur_task = NULL;\n\t}\n\n\tspin_lock_irq(&css_set_lock);\n\n\tif (it->task_pos) {\n\t\tit->cur_task = list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t  cg_list);\n\t\tget_task_struct(it->cur_task);\n\t\tcss_task_iter_advance(it);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn it->cur_task;\n}\n\n/**\n * css_task_iter_end - finish task iteration\n * @it: the task iterator to finish\n *\n * Finish task iteration started by css_task_iter_start().\n */\nvoid css_task_iter_end(struct css_task_iter *it)\n{\n\tif (it->cur_cset) {\n\t\tspin_lock_irq(&css_set_lock);\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t\tspin_unlock_irq(&css_set_lock);\n\t}\n\n\tif (it->cur_dcset)\n\t\tput_css_set(it->cur_dcset);\n\n\tif (it->cur_task)\n\t\tput_task_struct(it->cur_task);\n}\n\nstatic void cgroup_procs_release(struct kernfs_open_file *of)\n{\n\tif (of->priv) {\n\t\tcss_task_iter_end(of->priv);\n\t\tkfree(of->priv);\n\t}\n}\n\nstatic void *cgroup_procs_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct css_task_iter *it = of->priv;\n\n\treturn css_task_iter_next(it);\n}\n\nstatic void *__cgroup_procs_start(struct seq_file *s, loff_t *pos,\n\t\t\t\t  unsigned int iter_flags)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\tstruct css_task_iter *it = of->priv;\n\n\t/*\n\t * When a seq_file is seeked, it's always traversed sequentially\n\t * from position 0, so we can simply keep iterating on !0 *pos.\n\t */\n\tif (!it) {\n\t\tif (WARN_ON_ONCE((*pos)++))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tit = kzalloc(sizeof(*it), GFP_KERNEL);\n\t\tif (!it)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tof->priv = it;\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t} else if (!(*pos)++) {\n\t\tcss_task_iter_end(it);\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t}\n\n\treturn cgroup_procs_next(s, NULL, NULL);\n}\n\nstatic void *cgroup_procs_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\n\t/*\n\t * All processes of a threaded subtree belong to the domain cgroup\n\t * of the subtree.  Only threads can be distributed across the\n\t * subtree.  Reject reads on cgroup.procs in the subtree proper.\n\t * They're always empty anyway.\n\t */\n\tif (cgroup_is_threaded(cgrp))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn __cgroup_procs_start(s, pos, CSS_TASK_ITER_PROCS |\n\t\t\t\t\t    CSS_TASK_ITER_THREADED);\n}\n\nstatic int cgroup_procs_show(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", task_pid_vnr(v));\n\treturn 0;\n}\n\nstatic int cgroup_procs_write_permission(struct cgroup *src_cgrp,\n\t\t\t\t\t struct cgroup *dst_cgrp,\n\t\t\t\t\t struct super_block *sb)\n{\n\tstruct cgroup_namespace *ns = current->nsproxy->cgroup_ns;\n\tstruct cgroup *com_cgrp = src_cgrp;\n\tstruct inode *inode;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t/* find the common ancestor */\n\twhile (!cgroup_is_descendant(dst_cgrp, com_cgrp))\n\t\tcom_cgrp = cgroup_parent(com_cgrp);\n\n\t/* %current should be authorized to migrate to the common ancestor */\n\tinode = kernfs_get_inode(sb, com_cgrp->procs_file.kn);\n\tif (!inode)\n\t\treturn -ENOMEM;\n\n\tret = inode_permission(inode, MAY_WRITE);\n\tiput(inode);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * If namespaces are delegation boundaries, %current must be able\n\t * to see both source and destination cgroups from its namespace.\n\t */\n\tif ((cgrp_dfl_root.flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    (!cgroup_is_descendant(src_cgrp, ns->root_cset->dfl_cgrp) ||\n\t     !cgroup_is_descendant(dst_cgrp, ns->root_cset->dfl_cgrp)))\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_procs_write(struct kernfs_open_file *of,\n\t\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, true);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, true);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\nstatic void *cgroup_threads_start(struct seq_file *s, loff_t *pos)\n{\n\treturn __cgroup_procs_start(s, pos, 0);\n}\n\nstatic ssize_t cgroup_threads_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, false);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/* find the source cgroup */\n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\t/* thread migrations follow the cgroup.procs delegation rule */\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp,\n\t\t\t\t\t    of->file->f_path.dentry->d_sb);\n\tif (ret)\n\t\tgoto out_finish;\n\n\t/* and must be contained in the same domain */\n\tret = -EOPNOTSUPP;\n\tif (src_cgrp->dom_cgrp != dst_cgrp->dom_cgrp)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, false);\n\nout_finish:\n\tcgroup_procs_write_finish(task);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\n/* cgroup core interface files for the default hierarchy */\nstatic struct cftype cgroup_base_files[] = {\n\t{\n\t\t.name = \"cgroup.type\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_type_show,\n\t\t.write = cgroup_type_write,\n\t},\n\t{\n\t\t.name = \"cgroup.procs\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.file_offset = offsetof(struct cgroup, procs_file),\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_procs_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_procs_write,\n\t},\n\t{\n\t\t.name = \"cgroup.threads\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_threads_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_threads_write,\n\t},\n\t{\n\t\t.name = \"cgroup.controllers\",\n\t\t.seq_show = cgroup_controllers_show,\n\t},\n\t{\n\t\t.name = \"cgroup.subtree_control\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.seq_show = cgroup_subtree_control_show,\n\t\t.write = cgroup_subtree_control_write,\n\t},\n\t{\n\t\t.name = \"cgroup.events\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct cgroup, events_file),\n\t\t.seq_show = cgroup_events_show,\n\t},\n\t{\n\t\t.name = \"cgroup.max.descendants\",\n\t\t.seq_show = cgroup_max_descendants_show,\n\t\t.write = cgroup_max_descendants_write,\n\t},\n\t{\n\t\t.name = \"cgroup.max.depth\",\n\t\t.seq_show = cgroup_max_depth_show,\n\t\t.write = cgroup_max_depth_write,\n\t},\n\t{\n\t\t.name = \"cgroup.stat\",\n\t\t.seq_show = cgroup_stat_show,\n\t},\n\t{\n\t\t.name = \"cpu.stat\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_stat_show,\n\t},\n#ifdef CONFIG_PSI\n\t{\n\t\t.name = \"io.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_io_pressure_show,\n\t},\n\t{\n\t\t.name = \"memory.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_memory_pressure_show,\n\t},\n\t{\n\t\t.name = \"cpu.pressure\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_cpu_pressure_show,\n\t},\n#endif\n\t{ }\t/* terminate */\n};\n\n/*\n * css destruction is four-stage process.\n *\n * 1. Destruction starts.  Killing of the percpu_ref is initiated.\n *    Implemented in kill_css().\n *\n * 2. When the percpu_ref is confirmed to be visible as killed on all CPUs\n *    and thus css_tryget_online() is guaranteed to fail, the css can be\n *    offlined by invoking offline_css().  After offlining, the base ref is\n *    put.  Implemented in css_killed_work_fn().\n *\n * 3. When the percpu_ref reaches zero, the only possible remaining\n *    accessors are inside RCU read sections.  css_release() schedules the\n *    RCU callback.\n *\n * 4. After the grace period, the css can be freed.  Implemented in\n *    css_free_work_fn().\n *\n * It is actually hairier because both step 2 and 4 require process context\n * and thus involve punting to css->destroy_work adding two additional\n * steps to the already complex sequence.\n */\nstatic void css_free_rwork_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css = container_of(to_rcu_work(work),\n\t\t\t\tstruct cgroup_subsys_state, destroy_rwork);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tpercpu_ref_exit(&css->refcnt);\n\n\tif (ss) {\n\t\t/* css free path */\n\t\tstruct cgroup_subsys_state *parent = css->parent;\n\t\tint id = css->id;\n\n\t\tss->css_free(css);\n\t\tcgroup_idr_remove(&ss->css_idr, id);\n\t\tcgroup_put(cgrp);\n\n\t\tif (parent)\n\t\t\tcss_put(parent);\n\t} else {\n\t\t/* cgroup free path */\n\t\tatomic_dec(&cgrp->root->nr_cgrps);\n\t\tcgroup1_pidlist_destroy_all(cgrp);\n\t\tcancel_work_sync(&cgrp->release_agent_work);\n\n\t\tif (cgroup_parent(cgrp)) {\n\t\t\t/*\n\t\t\t * We get a ref to the parent, and put the ref when\n\t\t\t * this cgroup is being freed, so it's guaranteed\n\t\t\t * that the parent won't be destroyed before its\n\t\t\t * children.\n\t\t\t */\n\t\t\tcgroup_put(cgroup_parent(cgrp));\n\t\t\tkernfs_put(cgrp->kn);\n\t\t\tpsi_cgroup_free(cgrp);\n\t\t\tif (cgroup_on_dfl(cgrp))\n\t\t\t\tcgroup_rstat_exit(cgrp);\n\t\t\tkfree(cgrp);\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is root cgroup's refcnt reaching zero,\n\t\t\t * which indicates that the root should be\n\t\t\t * released.\n\t\t\t */\n\t\t\tcgroup_destroy_root(cgrp->root);\n\t\t}\n\t}\n}\n\nstatic void css_release_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tmutex_lock(&cgroup_mutex);\n\n\tcss->flags |= CSS_RELEASED;\n\tlist_del_rcu(&css->sibling);\n\n\tif (ss) {\n\t\t/* css release path */\n\t\tif (!list_empty(&css->rstat_css_node)) {\n\t\t\tcgroup_rstat_flush(cgrp);\n\t\t\tlist_del_rcu(&css->rstat_css_node);\n\t\t}\n\n\t\tcgroup_idr_replace(&ss->css_idr, NULL, css->id);\n\t\tif (ss->css_released)\n\t\t\tss->css_released(css);\n\t} else {\n\t\tstruct cgroup *tcgrp;\n\n\t\t/* cgroup release path */\n\t\tTRACE_CGROUP_PATH(release, cgrp);\n\n\t\tif (cgroup_on_dfl(cgrp))\n\t\t\tcgroup_rstat_flush(cgrp);\n\n\t\tfor (tcgrp = cgroup_parent(cgrp); tcgrp;\n\t\t     tcgrp = cgroup_parent(tcgrp))\n\t\t\ttcgrp->nr_dying_descendants--;\n\n\t\tcgroup_idr_remove(&cgrp->root->cgroup_idr, cgrp->id);\n\t\tcgrp->id = -1;\n\n\t\t/*\n\t\t * There are two control paths which try to determine\n\t\t * cgroup from dentry without going through kernfs -\n\t\t * cgroupstats_build() and css_tryget_online_from_dir().\n\t\t * Those are supported by RCU protecting clearing of\n\t\t * cgrp->kn->priv backpointer.\n\t\t */\n\t\tif (cgrp->kn)\n\t\t\tRCU_INIT_POINTER(*(void __rcu __force **)&cgrp->kn->priv,\n\t\t\t\t\t NULL);\n\n\t\tcgroup_bpf_put(cgrp);\n\t}\n\n\tmutex_unlock(&cgroup_mutex);\n\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n}\n\nstatic void css_release(struct percpu_ref *ref)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(ref, struct cgroup_subsys_state, refcnt);\n\n\tINIT_WORK(&css->destroy_work, css_release_work_fn);\n\tqueue_work(cgroup_destroy_wq, &css->destroy_work);\n}\n\nstatic void init_and_link_css(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcgroup_get_live(cgrp);\n\n\tmemset(css, 0, sizeof(*css));\n\tcss->cgroup = cgrp;\n\tcss->ss = ss;\n\tcss->id = -1;\n\tINIT_LIST_HEAD(&css->sibling);\n\tINIT_LIST_HEAD(&css->children);\n\tINIT_LIST_HEAD(&css->rstat_css_node);\n\tcss->serial_nr = css_serial_nr_next++;\n\tatomic_set(&css->online_cnt, 0);\n\n\tif (cgroup_parent(cgrp)) {\n\t\tcss->parent = cgroup_css(cgroup_parent(cgrp), ss);\n\t\tcss_get(css->parent);\n\t}\n\n\tif (cgroup_on_dfl(cgrp) && ss->css_rstat_flush)\n\t\tlist_add_rcu(&css->rstat_css_node, &cgrp->rstat_css_list);\n\n\tBUG_ON(cgroup_css(cgrp, ss));\n}\n\n/* invoke ->css_online() on a new CSS and mark it online if successful */\nstatic int online_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (ss->css_online)\n\t\tret = ss->css_online(css);\n\tif (!ret) {\n\t\tcss->flags |= CSS_ONLINE;\n\t\trcu_assign_pointer(css->cgroup->subsys[ss->id], css);\n\n\t\tatomic_inc(&css->online_cnt);\n\t\tif (css->parent)\n\t\t\tatomic_inc(&css->parent->online_cnt);\n\t}\n\treturn ret;\n}\n\n/* if the CSS is online, invoke ->css_offline() on it and mark it offline */\nstatic void offline_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!(css->flags & CSS_ONLINE))\n\t\treturn;\n\n\tif (ss->css_offline)\n\t\tss->css_offline(css);\n\n\tcss->flags &= ~CSS_ONLINE;\n\tRCU_INIT_POINTER(css->cgroup->subsys[ss->id], NULL);\n\n\twake_up_all(&css->cgroup->offline_waitq);\n}\n\n/**\n * css_create - create a cgroup_subsys_state\n * @cgrp: the cgroup new css will be associated with\n * @ss: the subsys of new css\n *\n * Create a new css associated with @cgrp - @ss pair.  On success, the new\n * css is online and installed in @cgrp.  This function doesn't create the\n * interface files.  Returns 0 on success, -errno on failure.\n */\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup_subsys_state *parent_css = cgroup_css(parent, ss);\n\tstruct cgroup_subsys_state *css;\n\tint err;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcss = ss->css_alloc(parent_css);\n\tif (!css)\n\t\tcss = ERR_PTR(-ENOMEM);\n\tif (IS_ERR(css))\n\t\treturn css;\n\n\tinit_and_link_css(css, ss, cgrp);\n\n\terr = percpu_ref_init(&css->refcnt, css_release, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err_free_css;\n\n\terr = cgroup_idr_alloc(&ss->css_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (err < 0)\n\t\tgoto err_free_css;\n\tcss->id = err;\n\n\t/* @css is ready to be brought online now, make it visible */\n\tlist_add_tail_rcu(&css->sibling, &parent_css->children);\n\tcgroup_idr_replace(&ss->css_idr, css, css->id);\n\n\terr = online_css(css);\n\tif (err)\n\t\tgoto err_list_del;\n\n\tif (ss->broken_hierarchy && !ss->warned_broken_hierarchy &&\n\t    cgroup_parent(parent)) {\n\t\tpr_warn(\"%s (%d) created nested cgroup for controller \\\"%s\\\" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.\\n\",\n\t\t\tcurrent->comm, current->pid, ss->name);\n\t\tif (!strcmp(ss->name, \"memory\"))\n\t\t\tpr_warn(\"\\\"memory\\\" requires setting use_hierarchy to 1 on the root\\n\");\n\t\tss->warned_broken_hierarchy = true;\n\t}\n\n\treturn css;\n\nerr_list_del:\n\tlist_del_rcu(&css->sibling);\nerr_free_css:\n\tlist_del_rcu(&css->rstat_css_node);\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n\treturn ERR_PTR(err);\n}\n\n/*\n * The returned cgroup is fully initialized including its control mask, but\n * it isn't associated with its kernfs_node and doesn't have the control\n * mask applied.\n */\nstatic struct cgroup *cgroup_create(struct cgroup *parent)\n{\n\tstruct cgroup_root *root = parent->root;\n\tstruct cgroup *cgrp, *tcgrp;\n\tint level = parent->level + 1;\n\tint ret;\n\n\t/* allocate the cgroup and its ID, 0 is reserved for the root */\n\tcgrp = kzalloc(struct_size(cgrp, ancestor_ids, (level + 1)),\n\t\t       GFP_KERNEL);\n\tif (!cgrp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = percpu_ref_init(&cgrp->self.refcnt, css_release, 0, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_free_cgrp;\n\n\tif (cgroup_on_dfl(parent)) {\n\t\tret = cgroup_rstat_init(cgrp);\n\t\tif (ret)\n\t\t\tgoto out_cancel_ref;\n\t}\n\n\t/*\n\t * Temporarily set the pointer to NULL, so idr_find() won't return\n\t * a half-baked cgroup.\n\t */\n\tcgrp->id = cgroup_idr_alloc(&root->cgroup_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (cgrp->id < 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out_stat_exit;\n\t}\n\n\tinit_cgroup_housekeeping(cgrp);\n\n\tcgrp->self.parent = &parent->self;\n\tcgrp->root = root;\n\tcgrp->level = level;\n\n\tret = psi_cgroup_alloc(cgrp);\n\tif (ret)\n\t\tgoto out_idr_free;\n\n\tret = cgroup_bpf_inherit(cgrp);\n\tif (ret)\n\t\tgoto out_psi_free;\n\n\tfor (tcgrp = cgrp; tcgrp; tcgrp = cgroup_parent(tcgrp)) {\n\t\tcgrp->ancestor_ids[tcgrp->level] = tcgrp->id;\n\n\t\tif (tcgrp != cgrp)\n\t\t\ttcgrp->nr_descendants++;\n\t}\n\n\tif (notify_on_release(parent))\n\t\tset_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n\n\tif (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &parent->flags))\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags);\n\n\tcgrp->self.serial_nr = css_serial_nr_next++;\n\n\t/* allocation complete, commit to creation */\n\tlist_add_tail_rcu(&cgrp->self.sibling, &cgroup_parent(cgrp)->self.children);\n\tatomic_inc(&root->nr_cgrps);\n\tcgroup_get_live(parent);\n\n\t/*\n\t * @cgrp is now fully operational.  If something fails after this\n\t * point, it'll be released via the normal destruction path.\n\t */\n\tcgroup_idr_replace(&root->cgroup_idr, cgrp, cgrp->id);\n\n\t/*\n\t * On the default hierarchy, a child doesn't automatically inherit\n\t * subtree_control from the parent.  Each is configured manually.\n\t */\n\tif (!cgroup_on_dfl(cgrp))\n\t\tcgrp->subtree_control = cgroup_control(cgrp);\n\n\tcgroup_propagate_control(cgrp);\n\n\treturn cgrp;\n\nout_psi_free:\n\tpsi_cgroup_free(cgrp);\nout_idr_free:\n\tcgroup_idr_remove(&root->cgroup_idr, cgrp->id);\nout_stat_exit:\n\tif (cgroup_on_dfl(parent))\n\t\tcgroup_rstat_exit(cgrp);\nout_cancel_ref:\n\tpercpu_ref_exit(&cgrp->self.refcnt);\nout_free_cgrp:\n\tkfree(cgrp);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool cgroup_check_hierarchy_limits(struct cgroup *parent)\n{\n\tstruct cgroup *cgroup;\n\tint ret = false;\n\tint level = 1;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tfor (cgroup = parent; cgroup; cgroup = cgroup_parent(cgroup)) {\n\t\tif (cgroup->nr_descendants >= cgroup->max_descendants)\n\t\t\tgoto fail;\n\n\t\tif (level > cgroup->max_depth)\n\t\t\tgoto fail;\n\n\t\tlevel++;\n\t}\n\n\tret = true;\nfail:\n\treturn ret;\n}\n\nint cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)\n{\n\tstruct cgroup *parent, *cgrp;\n\tstruct kernfs_node *kn;\n\tint ret;\n\n\t/* do not accept '\\n' to prevent making /proc/<pid>/cgroup unparsable */\n\tif (strchr(name, '\\n'))\n\t\treturn -EINVAL;\n\n\tparent = cgroup_kn_lock_live(parent_kn, false);\n\tif (!parent)\n\t\treturn -ENODEV;\n\n\tif (!cgroup_check_hierarchy_limits(parent)) {\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tcgrp = cgroup_create(parent);\n\tif (IS_ERR(cgrp)) {\n\t\tret = PTR_ERR(cgrp);\n\t\tgoto out_unlock;\n\t}\n\n\t/* create the directory */\n\tkn = kernfs_create_dir(parent->kn, name, mode, cgrp);\n\tif (IS_ERR(kn)) {\n\t\tret = PTR_ERR(kn);\n\t\tgoto out_destroy;\n\t}\n\tcgrp->kn = kn;\n\n\t/*\n\t * This extra ref will be put in cgroup_free_fn() and guarantees\n\t * that @cgrp->kn is always accessible.\n\t */\n\tkernfs_get(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = css_populate_dir(&cgrp->self);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tTRACE_CGROUP_PATH(mkdir, cgrp);\n\n\t/* let's create and online css's */\n\tkernfs_activate(kn);\n\n\tret = 0;\n\tgoto out_unlock;\n\nout_destroy:\n\tcgroup_destroy_locked(cgrp);\nout_unlock:\n\tcgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\n/*\n * This is called when the refcnt of a css is confirmed to be killed.\n * css_tryget_online() is now guaranteed to fail.  Tell the subsystem to\n * initate destruction and put the css ref from kill_css().\n */\nstatic void css_killed_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\n\tmutex_lock(&cgroup_mutex);\n\n\tdo {\n\t\toffline_css(css);\n\t\tcss_put(css);\n\t\t/* @css can't go away while we're holding cgroup_mutex */\n\t\tcss = css->parent;\n\t} while (css && atomic_dec_and_test(&css->online_cnt));\n\n\tmutex_unlock(&cgroup_mutex);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_unlock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 67
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_cgroup_from_root",
          "args": [
            "from",
            "root"
          ],
          "line": 66
        },
        "resolved": true,
        "details": {
          "function_name": "task_cgroup_from_root",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup.c",
          "lines": "1357-1366",
          "snippet": "struct cgroup *task_cgroup_from_root(struct task_struct *task,\n\t\t\t\t     struct cgroup_root *root)\n{\n\t/*\n\t * No need to lock the task - since we hold cgroup_mutex the\n\t * task can't change groups, so the only thing that can happen\n\t * is that it exits and its css is set back to init_css_set.\n\t */\n\treturn cset_cgroup_from_root(task_css_set(task), root);\n}",
          "includes": [
            "#include <linux/cgroup_subsys.h>",
            "#include <linux/cgroup_subsys.h>",
            "#include <trace/events/cgroup.h>",
            "#include <net/sock.h>",
            "#include <linux/psi.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/file.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/proc_ns.h>",
            "#include <linux/cpuset.h>",
            "#include <linux/atomic.h>",
            "#include <linux/kthread.h>",
            "#include <linux/idr.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/string.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/slab.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mount.h>",
            "#include <linux/mutex.h>",
            "#include <linux/magic.h>",
            "#include <linux/kernel.h>",
            "#include <linux/init_task.h>",
            "#include <linux/errno.h>",
            "#include <linux/cred.h>",
            "#include \"cgroup-internal.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void css_task_iter_advance(struct css_task_iter *it);",
            "static void kill_css(struct cgroup_subsys_state *css);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/cgroup_subsys.h>\n#include <linux/cgroup_subsys.h>\n#include <trace/events/cgroup.h>\n#include <net/sock.h>\n#include <linux/psi.h>\n#include <linux/sched/cputime.h>\n#include <linux/file.h>\n#include <linux/nsproxy.h>\n#include <linux/proc_ns.h>\n#include <linux/cpuset.h>\n#include <linux/atomic.h>\n#include <linux/kthread.h>\n#include <linux/idr.h>\n#include <linux/hashtable.h>\n#include <linux/string.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched/task.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/mount.h>\n#include <linux/mutex.h>\n#include <linux/magic.h>\n#include <linux/kernel.h>\n#include <linux/init_task.h>\n#include <linux/errno.h>\n#include <linux/cred.h>\n#include \"cgroup-internal.h\"\n\nstatic void css_task_iter_advance(struct css_task_iter *it);\nstatic void kill_css(struct cgroup_subsys_state *css);\n\nstruct cgroup *task_cgroup_from_root(struct task_struct *task,\n\t\t\t\t     struct cgroup_root *root)\n{\n\t/*\n\t * No need to lock the task - since we hold cgroup_mutex the\n\t * task can't change groups, so the only thing that can happen\n\t * is that it exits and its css is set back to init_css_set.\n\t */\n\treturn cset_cgroup_from_root(task_css_set(task), root);\n}"
        }
      },
      {
        "call_info": {
          "callee": "spin_lock_irq",
          "args": [
            "&css_set_lock"
          ],
          "line": 65
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "percpu_down_write",
          "args": [
            "&cgroup_threadgroup_rwsem"
          ],
          "line": 58
        },
        "resolved": true,
        "details": {
          "function_name": "percpu_down_write",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/percpu-rwsem.c",
          "lines": "140-163",
          "snippet": "void percpu_down_write(struct percpu_rw_semaphore *sem)\n{\n\t/* Notify readers to take the slow path. */\n\trcu_sync_enter(&sem->rss);\n\n\tdown_write(&sem->rw_sem);\n\n\t/*\n\t * Notify new readers to block; up until now, and thus throughout the\n\t * longish rcu_sync_enter() above, new readers could still come in.\n\t */\n\tWRITE_ONCE(sem->readers_block, 1);\n\n\tsmp_mb(); /* D matches A */\n\n\t/*\n\t * If they don't see our writer of readers_block, then we are\n\t * guaranteed to see their sem->read_count increment, and therefore\n\t * will wait for them.\n\t */\n\n\t/* Wait for all now active readers to complete. */\n\trcuwait_wait_event(&sem->writer, readers_active_check(sem));\n}",
          "includes": [
            "#include <linux/errno.h>",
            "#include <linux/sched.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/percpu.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/atomic.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/lockdep.h>\n#include <linux/percpu.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n\nvoid percpu_down_write(struct percpu_rw_semaphore *sem)\n{\n\t/* Notify readers to take the slow path. */\n\trcu_sync_enter(&sem->rss);\n\n\tdown_write(&sem->rw_sem);\n\n\t/*\n\t * Notify new readers to block; up until now, and thus throughout the\n\t * longish rcu_sync_enter() above, new readers could still come in.\n\t */\n\tWRITE_ONCE(sem->readers_block, 1);\n\n\tsmp_mb(); /* D matches A */\n\n\t/*\n\t * If they don't see our writer of readers_block, then we are\n\t * guaranteed to see their sem->read_count increment, and therefore\n\t * will wait for them.\n\t */\n\n\t/* Wait for all now active readers to complete. */\n\trcuwait_wait_event(&sem->writer, readers_active_check(sem));\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_lock",
          "args": [
            "&cgroup_mutex"
          ],
          "line": 57
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_lock_interruptible",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1512-1524",
          "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nint cgroup_attach_task_all(struct task_struct *from, struct task_struct *tsk)\n{\n\tstruct cgroup_root *root;\n\tint retval = 0;\n\n\tmutex_lock(&cgroup_mutex);\n\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n\tfor_each_root(root) {\n\t\tstruct cgroup *from_cgrp;\n\n\t\tif (root == &cgrp_dfl_root)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\tfrom_cgrp = task_cgroup_from_root(from, root);\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\tretval = cgroup_attach_task(from_cgrp, tsk, false);\n\t\tif (retval)\n\t\t\tbreak;\n\t}\n\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tmutex_unlock(&cgroup_mutex);\n\n\treturn retval;\n}"
  },
  {
    "function_name": "cgroup1_ssid_disabled",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cgroup/cgroup-v1.c",
    "lines": "42-45",
    "snippet": "bool cgroup1_ssid_disabled(int ssid)\n{\n\treturn cgroup_no_v1_mask & (1 << ssid);\n}",
    "includes": [
      "#include <trace/events/cgroup.h>",
      "#include <linux/cgroupstats.h>",
      "#include <linux/pid_namespace.h>",
      "#include <linux/delayacct.h>",
      "#include <linux/vmalloc.h>",
      "#include <linux/slab.h>",
      "#include <linux/magic.h>",
      "#include <linux/sched/task.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/mm.h>",
      "#include <linux/delay.h>",
      "#include <linux/sort.h>",
      "#include <linux/kmod.h>",
      "#include <linux/ctype.h>",
      "#include \"cgroup-internal.h\""
    ],
    "macros_used": [],
    "globals_used": [
      "static u16 cgroup_no_v1_mask;"
    ],
    "called_functions": [],
    "contextual_snippet": "#include <trace/events/cgroup.h>\n#include <linux/cgroupstats.h>\n#include <linux/pid_namespace.h>\n#include <linux/delayacct.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/magic.h>\n#include <linux/sched/task.h>\n#include <linux/sched/signal.h>\n#include <linux/mm.h>\n#include <linux/delay.h>\n#include <linux/sort.h>\n#include <linux/kmod.h>\n#include <linux/ctype.h>\n#include \"cgroup-internal.h\"\n\nstatic u16 cgroup_no_v1_mask;\n\nbool cgroup1_ssid_disabled(int ssid)\n{\n\treturn cgroup_no_v1_mask & (1 << ssid);\n}"
  }
]