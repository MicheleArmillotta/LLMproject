[
  {
    "function_name": "membarrier_register_private_expedited",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/membarrier.c",
    "lines": "221-254",
    "snippet": "static int membarrier_register_private_expedited(int flags)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY;\n\n\tif (flags & MEMBARRIER_FLAG_SYNC_CORE) {\n\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))\n\t\t\treturn -EINVAL;\n\t\tstate = MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;\n\t}\n\n\t/*\n\t * We need to consider threads belonging to different thread\n\t * groups, which use the same mm. (CLONE_VM but not\n\t * CLONE_THREAD).\n\t */\n\tif (atomic_read(&mm->membarrier_state) & state)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_PRIVATE_EXPEDITED, &mm->membarrier_state);\n\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)\n\t\tatomic_or(MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE,\n\t\t\t  &mm->membarrier_state);\n\tif (!(atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1)) {\n\t\t/*\n\t\t * Ensure all future scheduler executions will observe the\n\t\t * new thread flag state for this process.\n\t\t */\n\t\tsynchronize_sched();\n\t}\n\tatomic_or(state, &mm->membarrier_state);\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_or",
          "args": [
            "state",
            "&mm->membarrier_state"
          ],
          "line": 251
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "synchronize_sched",
          "args": [],
          "line": 249
        },
        "resolved": true,
        "details": {
          "function_name": "synchronize_sched_expedited_wait",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_exp.h",
          "lines": "468-540",
          "snippet": "static void synchronize_sched_expedited_wait(void)\n{\n\tint cpu;\n\tunsigned long jiffies_stall;\n\tunsigned long jiffies_start;\n\tunsigned long mask;\n\tint ndetected;\n\tstruct rcu_node *rnp;\n\tstruct rcu_node *rnp_root = rcu_get_root();\n\tint ret;\n\n\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"startwait\"));\n\tjiffies_stall = rcu_jiffies_till_stall_check();\n\tjiffies_start = jiffies;\n\n\tfor (;;) {\n\t\tret = swait_event_timeout_exclusive(\n\t\t\t\trcu_state.expedited_wq,\n\t\t\t\tsync_rcu_preempt_exp_done_unlocked(rnp_root),\n\t\t\t\tjiffies_stall);\n\t\tif (ret > 0 || sync_rcu_preempt_exp_done_unlocked(rnp_root))\n\t\t\treturn;\n\t\tWARN_ON(ret < 0);  /* workqueues should not be signaled. */\n\t\tif (rcu_cpu_stall_suppress)\n\t\t\tcontinue;\n\t\tpanic_on_rcu_stall();\n\t\tpr_err(\"INFO: %s detected expedited stalls on CPUs/tasks: {\",\n\t\t       rcu_state.name);\n\t\tndetected = 0;\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tndetected += rcu_print_task_exp_stall(rnp);\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tstruct rcu_data *rdp;\n\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tndetected++;\n\t\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\t\tpr_cont(\" %d-%c%c%c\", cpu,\n\t\t\t\t\t\"O.\"[!!cpu_online(cpu)],\n\t\t\t\t\t\"o.\"[!!(rdp->grpmask & rnp->expmaskinit)],\n\t\t\t\t\t\"N.\"[!!(rdp->grpmask & rnp->expmaskinitnext)]);\n\t\t\t}\n\t\t}\n\t\tpr_cont(\" } %lu jiffies s: %lu root: %#lx/%c\\n\",\n\t\t\tjiffies - jiffies_start, rcu_state.expedited_sequence,\n\t\t\trnp_root->expmask, \".T\"[!!rnp_root->exp_tasks]);\n\t\tif (ndetected) {\n\t\t\tpr_err(\"blocking rcu_node structures:\");\n\t\t\trcu_for_each_node_breadth_first(rnp) {\n\t\t\t\tif (rnp == rnp_root)\n\t\t\t\t\tcontinue; /* printed unconditionally */\n\t\t\t\tif (sync_rcu_preempt_exp_done_unlocked(rnp))\n\t\t\t\t\tcontinue;\n\t\t\t\tpr_cont(\" l=%u:%d-%d:%#lx/%c\",\n\t\t\t\t\trnp->level, rnp->grplo, rnp->grphi,\n\t\t\t\t\trnp->expmask,\n\t\t\t\t\t\".T\"[!!rnp->exp_tasks]);\n\t\t\t}\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tdump_cpu_task(cpu);\n\t\t\t}\n\t\t}\n\t\tjiffies_stall = 3 * rcu_jiffies_till_stall_check() + 3;\n\t}\n}",
          "includes": [
            "#include <linux/lockdep.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/lockdep.h>\n\nstatic void synchronize_sched_expedited_wait(void)\n{\n\tint cpu;\n\tunsigned long jiffies_stall;\n\tunsigned long jiffies_start;\n\tunsigned long mask;\n\tint ndetected;\n\tstruct rcu_node *rnp;\n\tstruct rcu_node *rnp_root = rcu_get_root();\n\tint ret;\n\n\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"startwait\"));\n\tjiffies_stall = rcu_jiffies_till_stall_check();\n\tjiffies_start = jiffies;\n\n\tfor (;;) {\n\t\tret = swait_event_timeout_exclusive(\n\t\t\t\trcu_state.expedited_wq,\n\t\t\t\tsync_rcu_preempt_exp_done_unlocked(rnp_root),\n\t\t\t\tjiffies_stall);\n\t\tif (ret > 0 || sync_rcu_preempt_exp_done_unlocked(rnp_root))\n\t\t\treturn;\n\t\tWARN_ON(ret < 0);  /* workqueues should not be signaled. */\n\t\tif (rcu_cpu_stall_suppress)\n\t\t\tcontinue;\n\t\tpanic_on_rcu_stall();\n\t\tpr_err(\"INFO: %s detected expedited stalls on CPUs/tasks: {\",\n\t\t       rcu_state.name);\n\t\tndetected = 0;\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tndetected += rcu_print_task_exp_stall(rnp);\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tstruct rcu_data *rdp;\n\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tndetected++;\n\t\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\t\tpr_cont(\" %d-%c%c%c\", cpu,\n\t\t\t\t\t\"O.\"[!!cpu_online(cpu)],\n\t\t\t\t\t\"o.\"[!!(rdp->grpmask & rnp->expmaskinit)],\n\t\t\t\t\t\"N.\"[!!(rdp->grpmask & rnp->expmaskinitnext)]);\n\t\t\t}\n\t\t}\n\t\tpr_cont(\" } %lu jiffies s: %lu root: %#lx/%c\\n\",\n\t\t\tjiffies - jiffies_start, rcu_state.expedited_sequence,\n\t\t\trnp_root->expmask, \".T\"[!!rnp_root->exp_tasks]);\n\t\tif (ndetected) {\n\t\t\tpr_err(\"blocking rcu_node structures:\");\n\t\t\trcu_for_each_node_breadth_first(rnp) {\n\t\t\t\tif (rnp == rnp_root)\n\t\t\t\t\tcontinue; /* printed unconditionally */\n\t\t\t\tif (sync_rcu_preempt_exp_done_unlocked(rnp))\n\t\t\t\t\tcontinue;\n\t\t\t\tpr_cont(\" l=%u:%d-%d:%#lx/%c\",\n\t\t\t\t\trnp->level, rnp->grplo, rnp->grphi,\n\t\t\t\t\trnp->expmask,\n\t\t\t\t\t\".T\"[!!rnp->exp_tasks]);\n\t\t\t}\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tdump_cpu_task(cpu);\n\t\t\t}\n\t\t}\n\t\tjiffies_stall = 3 * rcu_jiffies_till_stall_check() + 3;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_nr_threads",
          "args": [
            "p"
          ],
          "line": 244
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&mm->mm_users"
          ],
          "line": 244
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_or",
          "args": [
            "MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE",
            "&mm->membarrier_state"
          ],
          "line": 242
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_or",
          "args": [
            "MEMBARRIER_STATE_PRIVATE_EXPEDITED",
            "&mm->membarrier_state"
          ],
          "line": 240
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&mm->membarrier_state"
          ],
          "line": 238
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE"
          ],
          "line": 228
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_private_expedited(int flags)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tint state = MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY;\n\n\tif (flags & MEMBARRIER_FLAG_SYNC_CORE) {\n\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))\n\t\t\treturn -EINVAL;\n\t\tstate = MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY;\n\t}\n\n\t/*\n\t * We need to consider threads belonging to different thread\n\t * groups, which use the same mm. (CLONE_VM but not\n\t * CLONE_THREAD).\n\t */\n\tif (atomic_read(&mm->membarrier_state) & state)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_PRIVATE_EXPEDITED, &mm->membarrier_state);\n\tif (flags & MEMBARRIER_FLAG_SYNC_CORE)\n\t\tatomic_or(MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE,\n\t\t\t  &mm->membarrier_state);\n\tif (!(atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1)) {\n\t\t/*\n\t\t * Ensure all future scheduler executions will observe the\n\t\t * new thread flag state for this process.\n\t\t */\n\t\tsynchronize_sched();\n\t}\n\tatomic_or(state, &mm->membarrier_state);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "membarrier_register_global_expedited",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/membarrier.c",
    "lines": "189-219",
    "snippet": "static int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tif (atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1) {\n\t\t/*\n\t\t * For single mm user, single threaded process, we can\n\t\t * simply issue a memory barrier after setting\n\t\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED to guarantee that\n\t\t * no memory access following registration is reordered\n\t\t * before registration.\n\t\t */\n\t\tsmp_mb();\n\t} else {\n\t\t/*\n\t\t * For multi-mm user threads, we need to ensure all\n\t\t * future scheduler executions will observe the new\n\t\t * thread flag state for this mm.\n\t\t */\n\t\tsynchronize_sched();\n\t}\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "atomic_or",
          "args": [
            "MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY",
            "&mm->membarrier_state"
          ],
          "line": 215
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "synchronize_sched",
          "args": [],
          "line": 213
        },
        "resolved": true,
        "details": {
          "function_name": "synchronize_sched_expedited_wait",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_exp.h",
          "lines": "468-540",
          "snippet": "static void synchronize_sched_expedited_wait(void)\n{\n\tint cpu;\n\tunsigned long jiffies_stall;\n\tunsigned long jiffies_start;\n\tunsigned long mask;\n\tint ndetected;\n\tstruct rcu_node *rnp;\n\tstruct rcu_node *rnp_root = rcu_get_root();\n\tint ret;\n\n\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"startwait\"));\n\tjiffies_stall = rcu_jiffies_till_stall_check();\n\tjiffies_start = jiffies;\n\n\tfor (;;) {\n\t\tret = swait_event_timeout_exclusive(\n\t\t\t\trcu_state.expedited_wq,\n\t\t\t\tsync_rcu_preempt_exp_done_unlocked(rnp_root),\n\t\t\t\tjiffies_stall);\n\t\tif (ret > 0 || sync_rcu_preempt_exp_done_unlocked(rnp_root))\n\t\t\treturn;\n\t\tWARN_ON(ret < 0);  /* workqueues should not be signaled. */\n\t\tif (rcu_cpu_stall_suppress)\n\t\t\tcontinue;\n\t\tpanic_on_rcu_stall();\n\t\tpr_err(\"INFO: %s detected expedited stalls on CPUs/tasks: {\",\n\t\t       rcu_state.name);\n\t\tndetected = 0;\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tndetected += rcu_print_task_exp_stall(rnp);\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tstruct rcu_data *rdp;\n\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tndetected++;\n\t\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\t\tpr_cont(\" %d-%c%c%c\", cpu,\n\t\t\t\t\t\"O.\"[!!cpu_online(cpu)],\n\t\t\t\t\t\"o.\"[!!(rdp->grpmask & rnp->expmaskinit)],\n\t\t\t\t\t\"N.\"[!!(rdp->grpmask & rnp->expmaskinitnext)]);\n\t\t\t}\n\t\t}\n\t\tpr_cont(\" } %lu jiffies s: %lu root: %#lx/%c\\n\",\n\t\t\tjiffies - jiffies_start, rcu_state.expedited_sequence,\n\t\t\trnp_root->expmask, \".T\"[!!rnp_root->exp_tasks]);\n\t\tif (ndetected) {\n\t\t\tpr_err(\"blocking rcu_node structures:\");\n\t\t\trcu_for_each_node_breadth_first(rnp) {\n\t\t\t\tif (rnp == rnp_root)\n\t\t\t\t\tcontinue; /* printed unconditionally */\n\t\t\t\tif (sync_rcu_preempt_exp_done_unlocked(rnp))\n\t\t\t\t\tcontinue;\n\t\t\t\tpr_cont(\" l=%u:%d-%d:%#lx/%c\",\n\t\t\t\t\trnp->level, rnp->grplo, rnp->grphi,\n\t\t\t\t\trnp->expmask,\n\t\t\t\t\t\".T\"[!!rnp->exp_tasks]);\n\t\t\t}\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tdump_cpu_task(cpu);\n\t\t\t}\n\t\t}\n\t\tjiffies_stall = 3 * rcu_jiffies_till_stall_check() + 3;\n\t}\n}",
          "includes": [
            "#include <linux/lockdep.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/lockdep.h>\n\nstatic void synchronize_sched_expedited_wait(void)\n{\n\tint cpu;\n\tunsigned long jiffies_stall;\n\tunsigned long jiffies_start;\n\tunsigned long mask;\n\tint ndetected;\n\tstruct rcu_node *rnp;\n\tstruct rcu_node *rnp_root = rcu_get_root();\n\tint ret;\n\n\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"startwait\"));\n\tjiffies_stall = rcu_jiffies_till_stall_check();\n\tjiffies_start = jiffies;\n\n\tfor (;;) {\n\t\tret = swait_event_timeout_exclusive(\n\t\t\t\trcu_state.expedited_wq,\n\t\t\t\tsync_rcu_preempt_exp_done_unlocked(rnp_root),\n\t\t\t\tjiffies_stall);\n\t\tif (ret > 0 || sync_rcu_preempt_exp_done_unlocked(rnp_root))\n\t\t\treturn;\n\t\tWARN_ON(ret < 0);  /* workqueues should not be signaled. */\n\t\tif (rcu_cpu_stall_suppress)\n\t\t\tcontinue;\n\t\tpanic_on_rcu_stall();\n\t\tpr_err(\"INFO: %s detected expedited stalls on CPUs/tasks: {\",\n\t\t       rcu_state.name);\n\t\tndetected = 0;\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tndetected += rcu_print_task_exp_stall(rnp);\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tstruct rcu_data *rdp;\n\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tndetected++;\n\t\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\t\tpr_cont(\" %d-%c%c%c\", cpu,\n\t\t\t\t\t\"O.\"[!!cpu_online(cpu)],\n\t\t\t\t\t\"o.\"[!!(rdp->grpmask & rnp->expmaskinit)],\n\t\t\t\t\t\"N.\"[!!(rdp->grpmask & rnp->expmaskinitnext)]);\n\t\t\t}\n\t\t}\n\t\tpr_cont(\" } %lu jiffies s: %lu root: %#lx/%c\\n\",\n\t\t\tjiffies - jiffies_start, rcu_state.expedited_sequence,\n\t\t\trnp_root->expmask, \".T\"[!!rnp_root->exp_tasks]);\n\t\tif (ndetected) {\n\t\t\tpr_err(\"blocking rcu_node structures:\");\n\t\t\trcu_for_each_node_breadth_first(rnp) {\n\t\t\t\tif (rnp == rnp_root)\n\t\t\t\t\tcontinue; /* printed unconditionally */\n\t\t\t\tif (sync_rcu_preempt_exp_done_unlocked(rnp))\n\t\t\t\t\tcontinue;\n\t\t\t\tpr_cont(\" l=%u:%d-%d:%#lx/%c\",\n\t\t\t\t\trnp->level, rnp->grplo, rnp->grphi,\n\t\t\t\t\trnp->expmask,\n\t\t\t\t\t\".T\"[!!rnp->exp_tasks]);\n\t\t\t}\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(rnp->expmask & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tdump_cpu_task(cpu);\n\t\t\t}\n\t\t}\n\t\tjiffies_stall = 3 * rcu_jiffies_till_stall_check() + 3;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 206
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "get_nr_threads",
          "args": [
            "p"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&mm->mm_users"
          ],
          "line": 198
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_or",
          "args": [
            "MEMBARRIER_STATE_GLOBAL_EXPEDITED",
            "&mm->membarrier_state"
          ],
          "line": 197
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&mm->membarrier_state"
          ],
          "line": 194
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_register_global_expedited(void)\n{\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (atomic_read(&mm->membarrier_state) &\n\t    MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY)\n\t\treturn 0;\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED, &mm->membarrier_state);\n\tif (atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1) {\n\t\t/*\n\t\t * For single mm user, single threaded process, we can\n\t\t * simply issue a memory barrier after setting\n\t\t * MEMBARRIER_STATE_GLOBAL_EXPEDITED to guarantee that\n\t\t * no memory access following registration is reordered\n\t\t * before registration.\n\t\t */\n\t\tsmp_mb();\n\t} else {\n\t\t/*\n\t\t * For multi-mm user threads, we need to ensure all\n\t\t * future scheduler executions will observe the new\n\t\t * thread flag state for this mm.\n\t\t */\n\t\tsynchronize_sched();\n\t}\n\tatomic_or(MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY,\n\t\t  &mm->membarrier_state);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "membarrier_private_expedited",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/membarrier.c",
    "lines": "110-187",
    "snippet": "static int membarrier_private_expedited(int flags)\n{\n\tint cpu;\n\tbool fallback = false;\n\tcpumask_var_t tmpmask;\n\n\tif (flags & MEMBARRIER_FLAG_SYNC_CORE) {\n\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))\n\t\t\treturn -EINVAL;\n\t\tif (!(atomic_read(&current->mm->membarrier_state) &\n\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))\n\t\t\treturn -EPERM;\n\t} else {\n\t\tif (!(atomic_read(&current->mm->membarrier_state) &\n\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))\n\t\t\treturn -EPERM;\n\t}\n\n\tif (num_online_cpus() == 1)\n\t\treturn 0;\n\n\t/*\n\t * Matches memory barriers around rq->curr modification in\n\t * scheduler.\n\t */\n\tsmp_mb();\t/* system call entry is not a mb. */\n\n\t/*\n\t * Expedited membarrier commands guarantee that they won't\n\t * block, hence the GFP_NOWAIT allocation flag and fallback\n\t * implementation.\n\t */\n\tif (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {\n\t\t/* Fallback for OOM. */\n\t\tfallback = true;\n\t}\n\n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tstruct task_struct *p;\n\n\t\t/*\n\t\t * Skipping the current CPU is OK even through we can be\n\t\t * migrated at any point. The current CPU, at the point\n\t\t * where we read raw_smp_processor_id(), is ensured to\n\t\t * be in program order with respect to the caller\n\t\t * thread. Therefore, we can skip this CPU from the\n\t\t * iteration.\n\t\t */\n\t\tif (cpu == raw_smp_processor_id())\n\t\t\tcontinue;\n\t\trcu_read_lock();\n\t\tp = task_rcu_dereference(&cpu_rq(cpu)->curr);\n\t\tif (p && p->mm == current->mm) {\n\t\t\tif (!fallback)\n\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);\n\t\t\telse\n\t\t\t\tsmp_call_function_single(cpu, ipi_mb, NULL, 1);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tif (!fallback) {\n\t\tpreempt_disable();\n\t\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);\n\t\tpreempt_enable();\n\t\tfree_cpumask_var(tmpmask);\n\t}\n\tcpus_read_unlock();\n\n\t/*\n\t * Memory barrier on the caller thread _after_ we finished\n\t * waiting for the last IPI. Matches memory barriers around\n\t * rq->curr modification in scheduler.\n\t */\n\tsmp_mb();\t/* exit from system call is not a mb */\n\n\treturn 0;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 184
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpus_read_unlock",
          "args": [],
          "line": 177
        },
        "resolved": true,
        "details": {
          "function_name": "cpus_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cpu.c",
          "lines": "297-300",
          "snippet": "void cpus_read_unlock(void)\n{\n\tpercpu_up_read(&cpu_hotplug_lock);\n}",
          "includes": [
            "#include \"smpboot.h\"",
            "#include <trace/events/cpuhp.h>",
            "#include <trace/events/power.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/slab.h>",
            "#include <linux/relay.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/nmi.h>",
            "#include <linux/irq.h>",
            "#include <linux/tick.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/suspend.h>",
            "#include <linux/gfp.h>",
            "#include <linux/mutex.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/kthread.h>",
            "#include <linux/bug.h>",
            "#include <linux/export.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/oom.h>",
            "#include <linux/cpu.h>",
            "#include <linux/unistd.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/notifier.h>",
            "#include <linux/init.h>",
            "#include <linux/smp.h>",
            "#include <linux/proc_fs.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"smpboot.h\"\n#include <trace/events/cpuhp.h>\n#include <trace/events/power.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/slab.h>\n#include <linux/relay.h>\n#include <linux/smpboot.h>\n#include <linux/nmi.h>\n#include <linux/irq.h>\n#include <linux/tick.h>\n#include <linux/lockdep.h>\n#include <linux/suspend.h>\n#include <linux/gfp.h>\n#include <linux/mutex.h>\n#include <linux/stop_machine.h>\n#include <linux/kthread.h>\n#include <linux/bug.h>\n#include <linux/export.h>\n#include <linux/rcupdate.h>\n#include <linux/oom.h>\n#include <linux/cpu.h>\n#include <linux/unistd.h>\n#include <linux/sched/task.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/signal.h>\n#include <linux/notifier.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/proc_fs.h>\n\nvoid cpus_read_unlock(void)\n{\n\tpercpu_up_read(&cpu_hotplug_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "free_cpumask_var",
          "args": [
            "tmpmask"
          ],
          "line": 175
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "preempt_enable",
          "args": [],
          "line": 174
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_call_function_many",
          "args": [
            "tmpmask",
            "ipi_mb",
            "NULL",
            "1"
          ],
          "line": 173
        },
        "resolved": true,
        "details": {
          "function_name": "smp_call_function_many",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/smp.c",
          "lines": "403-471",
          "snippet": "void smp_call_function_many(const struct cpumask *mask,\n\t\t\t    smp_call_func_t func, void *info, bool wait)\n{\n\tstruct call_function_data *cfd;\n\tint cpu, next_cpu, this_cpu = smp_processor_id();\n\n\t/*\n\t * Can deadlock when called with interrupts disabled.\n\t * We allow cpu's that are not yet online though, as no one else can\n\t * send smp call function interrupt to this cpu and as such deadlocks\n\t * can't happen.\n\t */\n\tWARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()\n\t\t     && !oops_in_progress && !early_boot_irqs_disabled);\n\n\t/* Try to fastpath.  So, what's a CPU they want? Ignoring this one. */\n\tcpu = cpumask_first_and(mask, cpu_online_mask);\n\tif (cpu == this_cpu)\n\t\tcpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\n\t/* No online cpus?  We're done. */\n\tif (cpu >= nr_cpu_ids)\n\t\treturn;\n\n\t/* Do we have another CPU which isn't us? */\n\tnext_cpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\tif (next_cpu == this_cpu)\n\t\tnext_cpu = cpumask_next_and(next_cpu, mask, cpu_online_mask);\n\n\t/* Fastpath: do that cpu by itself. */\n\tif (next_cpu >= nr_cpu_ids) {\n\t\tsmp_call_function_single(cpu, func, info, wait);\n\t\treturn;\n\t}\n\n\tcfd = this_cpu_ptr(&cfd_data);\n\n\tcpumask_and(cfd->cpumask, mask, cpu_online_mask);\n\t__cpumask_clear_cpu(this_cpu, cfd->cpumask);\n\n\t/* Some callers race with other cpus changing the passed mask */\n\tif (unlikely(!cpumask_weight(cfd->cpumask)))\n\t\treturn;\n\n\tcpumask_clear(cfd->cpumask_ipi);\n\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\tcall_single_data_t *csd = per_cpu_ptr(cfd->csd, cpu);\n\n\t\tcsd_lock(csd);\n\t\tif (wait)\n\t\t\tcsd->flags |= CSD_FLAG_SYNCHRONOUS;\n\t\tcsd->func = func;\n\t\tcsd->info = info;\n\t\tif (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))\n\t\t\t__cpumask_set_cpu(cpu, cfd->cpumask_ipi);\n\t}\n\n\t/* Send a message to all CPUs in the map */\n\tarch_send_call_function_ipi_mask(cfd->cpumask_ipi);\n\n\tif (wait) {\n\t\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\t\tcall_single_data_t *csd;\n\n\t\t\tcsd = per_cpu_ptr(cfd->csd, cpu);\n\t\t\tcsd_lock_wait(csd);\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"smpboot.h\"",
            "#include <linux/hypervisor.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched.h>",
            "#include <linux/cpu.h>",
            "#include <linux/smp.h>",
            "#include <linux/gfp.h>",
            "#include <linux/init.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/kernel.h>",
            "#include <linux/rculist.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/irq_work.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_function_data, cfd_data);",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);",
            "unsigned int nr_cpu_ids"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"smpboot.h\"\n#include <linux/hypervisor.h>\n#include <linux/sched/idle.h>\n#include <linux/sched.h>\n#include <linux/cpu.h>\n#include <linux/smp.h>\n#include <linux/gfp.h>\n#include <linux/init.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/rculist.h>\n#include <linux/rcupdate.h>\n#include <linux/irq_work.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct call_function_data, cfd_data);\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);\nunsigned int nr_cpu_ids;\n\nvoid smp_call_function_many(const struct cpumask *mask,\n\t\t\t    smp_call_func_t func, void *info, bool wait)\n{\n\tstruct call_function_data *cfd;\n\tint cpu, next_cpu, this_cpu = smp_processor_id();\n\n\t/*\n\t * Can deadlock when called with interrupts disabled.\n\t * We allow cpu's that are not yet online though, as no one else can\n\t * send smp call function interrupt to this cpu and as such deadlocks\n\t * can't happen.\n\t */\n\tWARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()\n\t\t     && !oops_in_progress && !early_boot_irqs_disabled);\n\n\t/* Try to fastpath.  So, what's a CPU they want? Ignoring this one. */\n\tcpu = cpumask_first_and(mask, cpu_online_mask);\n\tif (cpu == this_cpu)\n\t\tcpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\n\t/* No online cpus?  We're done. */\n\tif (cpu >= nr_cpu_ids)\n\t\treturn;\n\n\t/* Do we have another CPU which isn't us? */\n\tnext_cpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\tif (next_cpu == this_cpu)\n\t\tnext_cpu = cpumask_next_and(next_cpu, mask, cpu_online_mask);\n\n\t/* Fastpath: do that cpu by itself. */\n\tif (next_cpu >= nr_cpu_ids) {\n\t\tsmp_call_function_single(cpu, func, info, wait);\n\t\treturn;\n\t}\n\n\tcfd = this_cpu_ptr(&cfd_data);\n\n\tcpumask_and(cfd->cpumask, mask, cpu_online_mask);\n\t__cpumask_clear_cpu(this_cpu, cfd->cpumask);\n\n\t/* Some callers race with other cpus changing the passed mask */\n\tif (unlikely(!cpumask_weight(cfd->cpumask)))\n\t\treturn;\n\n\tcpumask_clear(cfd->cpumask_ipi);\n\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\tcall_single_data_t *csd = per_cpu_ptr(cfd->csd, cpu);\n\n\t\tcsd_lock(csd);\n\t\tif (wait)\n\t\t\tcsd->flags |= CSD_FLAG_SYNCHRONOUS;\n\t\tcsd->func = func;\n\t\tcsd->info = info;\n\t\tif (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))\n\t\t\t__cpumask_set_cpu(cpu, cfd->cpumask_ipi);\n\t}\n\n\t/* Send a message to all CPUs in the map */\n\tarch_send_call_function_ipi_mask(cfd->cpumask_ipi);\n\n\tif (wait) {\n\t\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\t\tcall_single_data_t *csd;\n\n\t\t\tcsd = per_cpu_ptr(cfd->csd, cpu);\n\t\t\tcsd_lock_wait(csd);\n\t\t}\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "preempt_disable",
          "args": [],
          "line": 172
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_preempt_disabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "3571-3576",
          "snippet": "void __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic void __sched;\nstatic void __sched;\n\nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 169
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_call_function_single",
          "args": [
            "cpu",
            "ipi_mb",
            "NULL",
            "1"
          ],
          "line": 167
        },
        "resolved": true,
        "details": {
          "function_name": "smp_call_function_single",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/up.c",
          "lines": "11-23",
          "snippet": "int smp_call_function_single(int cpu, void (*func) (void *info), void *info,\n\t\t\t\tint wait)\n{\n\tunsigned long flags;\n\n\tWARN_ON(cpu != 0);\n\n\tlocal_irq_save(flags);\n\tfunc(info);\n\tlocal_irq_restore(flags);\n\n\treturn 0;\n}",
          "includes": [
            "#include <linux/hypervisor.h>",
            "#include <linux/smp.h>",
            "#include <linux/export.h>",
            "#include <linux/kernel.h>",
            "#include <linux/interrupt.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/hypervisor.h>\n#include <linux/smp.h>\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n\nint smp_call_function_single(int cpu, void (*func) (void *info), void *info,\n\t\t\t\tint wait)\n{\n\tunsigned long flags;\n\n\tWARN_ON(cpu != 0);\n\n\tlocal_irq_save(flags);\n\tfunc(info);\n\tlocal_irq_restore(flags);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__cpumask_set_cpu",
          "args": [
            "cpu",
            "tmpmask"
          ],
          "line": 165
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_rcu_dereference",
          "args": [
            "&cpu_rq(cpu)->curr"
          ],
          "line": 162
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 162
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 161
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_smp_processor_id",
          "args": [],
          "line": 159
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpus_read_lock",
          "args": [],
          "line": 147
        },
        "resolved": true,
        "details": {
          "function_name": "cpus_read_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cpu.c",
          "lines": "285-288",
          "snippet": "void cpus_read_lock(void)\n{\n\tpercpu_down_read(&cpu_hotplug_lock);\n}",
          "includes": [
            "#include \"smpboot.h\"",
            "#include <trace/events/cpuhp.h>",
            "#include <trace/events/power.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/slab.h>",
            "#include <linux/relay.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/nmi.h>",
            "#include <linux/irq.h>",
            "#include <linux/tick.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/suspend.h>",
            "#include <linux/gfp.h>",
            "#include <linux/mutex.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/kthread.h>",
            "#include <linux/bug.h>",
            "#include <linux/export.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/oom.h>",
            "#include <linux/cpu.h>",
            "#include <linux/unistd.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/notifier.h>",
            "#include <linux/init.h>",
            "#include <linux/smp.h>",
            "#include <linux/proc_fs.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"smpboot.h\"\n#include <trace/events/cpuhp.h>\n#include <trace/events/power.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/slab.h>\n#include <linux/relay.h>\n#include <linux/smpboot.h>\n#include <linux/nmi.h>\n#include <linux/irq.h>\n#include <linux/tick.h>\n#include <linux/lockdep.h>\n#include <linux/suspend.h>\n#include <linux/gfp.h>\n#include <linux/mutex.h>\n#include <linux/stop_machine.h>\n#include <linux/kthread.h>\n#include <linux/bug.h>\n#include <linux/export.h>\n#include <linux/rcupdate.h>\n#include <linux/oom.h>\n#include <linux/cpu.h>\n#include <linux/unistd.h>\n#include <linux/sched/task.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/signal.h>\n#include <linux/notifier.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/proc_fs.h>\n\nvoid cpus_read_lock(void)\n{\n\tpercpu_down_read(&cpu_hotplug_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "zalloc_cpumask_var",
          "args": [
            "&tmpmask",
            "GFP_NOWAIT"
          ],
          "line": 142
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 135
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "num_online_cpus",
          "args": [],
          "line": 128
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&current->mm->membarrier_state"
          ],
          "line": 123
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&current->mm->membarrier_state"
          ],
          "line": 119
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "IS_ENABLED",
          "args": [
            "CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE"
          ],
          "line": 117
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_private_expedited(int flags)\n{\n\tint cpu;\n\tbool fallback = false;\n\tcpumask_var_t tmpmask;\n\n\tif (flags & MEMBARRIER_FLAG_SYNC_CORE) {\n\t\tif (!IS_ENABLED(CONFIG_ARCH_HAS_MEMBARRIER_SYNC_CORE))\n\t\t\treturn -EINVAL;\n\t\tif (!(atomic_read(&current->mm->membarrier_state) &\n\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY))\n\t\t\treturn -EPERM;\n\t} else {\n\t\tif (!(atomic_read(&current->mm->membarrier_state) &\n\t\t      MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY))\n\t\t\treturn -EPERM;\n\t}\n\n\tif (num_online_cpus() == 1)\n\t\treturn 0;\n\n\t/*\n\t * Matches memory barriers around rq->curr modification in\n\t * scheduler.\n\t */\n\tsmp_mb();\t/* system call entry is not a mb. */\n\n\t/*\n\t * Expedited membarrier commands guarantee that they won't\n\t * block, hence the GFP_NOWAIT allocation flag and fallback\n\t * implementation.\n\t */\n\tif (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {\n\t\t/* Fallback for OOM. */\n\t\tfallback = true;\n\t}\n\n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tstruct task_struct *p;\n\n\t\t/*\n\t\t * Skipping the current CPU is OK even through we can be\n\t\t * migrated at any point. The current CPU, at the point\n\t\t * where we read raw_smp_processor_id(), is ensured to\n\t\t * be in program order with respect to the caller\n\t\t * thread. Therefore, we can skip this CPU from the\n\t\t * iteration.\n\t\t */\n\t\tif (cpu == raw_smp_processor_id())\n\t\t\tcontinue;\n\t\trcu_read_lock();\n\t\tp = task_rcu_dereference(&cpu_rq(cpu)->curr);\n\t\tif (p && p->mm == current->mm) {\n\t\t\tif (!fallback)\n\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);\n\t\t\telse\n\t\t\t\tsmp_call_function_single(cpu, ipi_mb, NULL, 1);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tif (!fallback) {\n\t\tpreempt_disable();\n\t\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);\n\t\tpreempt_enable();\n\t\tfree_cpumask_var(tmpmask);\n\t}\n\tcpus_read_unlock();\n\n\t/*\n\t * Memory barrier on the caller thread _after_ we finished\n\t * waiting for the last IPI. Matches memory barriers around\n\t * rq->curr modification in scheduler.\n\t */\n\tsmp_mb();\t/* exit from system call is not a mb */\n\n\treturn 0;\n}"
  },
  {
    "function_name": "membarrier_global_expedited",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/membarrier.c",
    "lines": "42-108",
    "snippet": "static int membarrier_global_expedited(void)\n{\n\tint cpu;\n\tbool fallback = false;\n\tcpumask_var_t tmpmask;\n\n\tif (num_online_cpus() == 1)\n\t\treturn 0;\n\n\t/*\n\t * Matches memory barriers around rq->curr modification in\n\t * scheduler.\n\t */\n\tsmp_mb();\t/* system call entry is not a mb. */\n\n\t/*\n\t * Expedited membarrier commands guarantee that they won't\n\t * block, hence the GFP_NOWAIT allocation flag and fallback\n\t * implementation.\n\t */\n\tif (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {\n\t\t/* Fallback for OOM. */\n\t\tfallback = true;\n\t}\n\n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tstruct task_struct *p;\n\n\t\t/*\n\t\t * Skipping the current CPU is OK even through we can be\n\t\t * migrated at any point. The current CPU, at the point\n\t\t * where we read raw_smp_processor_id(), is ensured to\n\t\t * be in program order with respect to the caller\n\t\t * thread. Therefore, we can skip this CPU from the\n\t\t * iteration.\n\t\t */\n\t\tif (cpu == raw_smp_processor_id())\n\t\t\tcontinue;\n\n\t\trcu_read_lock();\n\t\tp = task_rcu_dereference(&cpu_rq(cpu)->curr);\n\t\tif (p && p->mm && (atomic_read(&p->mm->membarrier_state) &\n\t\t\t\t   MEMBARRIER_STATE_GLOBAL_EXPEDITED)) {\n\t\t\tif (!fallback)\n\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);\n\t\t\telse\n\t\t\t\tsmp_call_function_single(cpu, ipi_mb, NULL, 1);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tif (!fallback) {\n\t\tpreempt_disable();\n\t\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);\n\t\tpreempt_enable();\n\t\tfree_cpumask_var(tmpmask);\n\t}\n\tcpus_read_unlock();\n\n\t/*\n\t * Memory barrier on the caller thread _after_ we finished\n\t * waiting for the last IPI. Matches memory barriers around\n\t * rq->curr modification in scheduler.\n\t */\n\tsmp_mb();\t/* exit from system call is not a mb */\n\treturn 0;\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 106
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpus_read_unlock",
          "args": [],
          "line": 99
        },
        "resolved": true,
        "details": {
          "function_name": "cpus_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cpu.c",
          "lines": "297-300",
          "snippet": "void cpus_read_unlock(void)\n{\n\tpercpu_up_read(&cpu_hotplug_lock);\n}",
          "includes": [
            "#include \"smpboot.h\"",
            "#include <trace/events/cpuhp.h>",
            "#include <trace/events/power.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/slab.h>",
            "#include <linux/relay.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/nmi.h>",
            "#include <linux/irq.h>",
            "#include <linux/tick.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/suspend.h>",
            "#include <linux/gfp.h>",
            "#include <linux/mutex.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/kthread.h>",
            "#include <linux/bug.h>",
            "#include <linux/export.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/oom.h>",
            "#include <linux/cpu.h>",
            "#include <linux/unistd.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/notifier.h>",
            "#include <linux/init.h>",
            "#include <linux/smp.h>",
            "#include <linux/proc_fs.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"smpboot.h\"\n#include <trace/events/cpuhp.h>\n#include <trace/events/power.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/slab.h>\n#include <linux/relay.h>\n#include <linux/smpboot.h>\n#include <linux/nmi.h>\n#include <linux/irq.h>\n#include <linux/tick.h>\n#include <linux/lockdep.h>\n#include <linux/suspend.h>\n#include <linux/gfp.h>\n#include <linux/mutex.h>\n#include <linux/stop_machine.h>\n#include <linux/kthread.h>\n#include <linux/bug.h>\n#include <linux/export.h>\n#include <linux/rcupdate.h>\n#include <linux/oom.h>\n#include <linux/cpu.h>\n#include <linux/unistd.h>\n#include <linux/sched/task.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/signal.h>\n#include <linux/notifier.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/proc_fs.h>\n\nvoid cpus_read_unlock(void)\n{\n\tpercpu_up_read(&cpu_hotplug_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "free_cpumask_var",
          "args": [
            "tmpmask"
          ],
          "line": 97
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "preempt_enable",
          "args": [],
          "line": 96
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_call_function_many",
          "args": [
            "tmpmask",
            "ipi_mb",
            "NULL",
            "1"
          ],
          "line": 95
        },
        "resolved": true,
        "details": {
          "function_name": "smp_call_function_many",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/smp.c",
          "lines": "403-471",
          "snippet": "void smp_call_function_many(const struct cpumask *mask,\n\t\t\t    smp_call_func_t func, void *info, bool wait)\n{\n\tstruct call_function_data *cfd;\n\tint cpu, next_cpu, this_cpu = smp_processor_id();\n\n\t/*\n\t * Can deadlock when called with interrupts disabled.\n\t * We allow cpu's that are not yet online though, as no one else can\n\t * send smp call function interrupt to this cpu and as such deadlocks\n\t * can't happen.\n\t */\n\tWARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()\n\t\t     && !oops_in_progress && !early_boot_irqs_disabled);\n\n\t/* Try to fastpath.  So, what's a CPU they want? Ignoring this one. */\n\tcpu = cpumask_first_and(mask, cpu_online_mask);\n\tif (cpu == this_cpu)\n\t\tcpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\n\t/* No online cpus?  We're done. */\n\tif (cpu >= nr_cpu_ids)\n\t\treturn;\n\n\t/* Do we have another CPU which isn't us? */\n\tnext_cpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\tif (next_cpu == this_cpu)\n\t\tnext_cpu = cpumask_next_and(next_cpu, mask, cpu_online_mask);\n\n\t/* Fastpath: do that cpu by itself. */\n\tif (next_cpu >= nr_cpu_ids) {\n\t\tsmp_call_function_single(cpu, func, info, wait);\n\t\treturn;\n\t}\n\n\tcfd = this_cpu_ptr(&cfd_data);\n\n\tcpumask_and(cfd->cpumask, mask, cpu_online_mask);\n\t__cpumask_clear_cpu(this_cpu, cfd->cpumask);\n\n\t/* Some callers race with other cpus changing the passed mask */\n\tif (unlikely(!cpumask_weight(cfd->cpumask)))\n\t\treturn;\n\n\tcpumask_clear(cfd->cpumask_ipi);\n\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\tcall_single_data_t *csd = per_cpu_ptr(cfd->csd, cpu);\n\n\t\tcsd_lock(csd);\n\t\tif (wait)\n\t\t\tcsd->flags |= CSD_FLAG_SYNCHRONOUS;\n\t\tcsd->func = func;\n\t\tcsd->info = info;\n\t\tif (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))\n\t\t\t__cpumask_set_cpu(cpu, cfd->cpumask_ipi);\n\t}\n\n\t/* Send a message to all CPUs in the map */\n\tarch_send_call_function_ipi_mask(cfd->cpumask_ipi);\n\n\tif (wait) {\n\t\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\t\tcall_single_data_t *csd;\n\n\t\t\tcsd = per_cpu_ptr(cfd->csd, cpu);\n\t\t\tcsd_lock_wait(csd);\n\t\t}\n\t}\n}",
          "includes": [
            "#include \"smpboot.h\"",
            "#include <linux/hypervisor.h>",
            "#include <linux/sched/idle.h>",
            "#include <linux/sched.h>",
            "#include <linux/cpu.h>",
            "#include <linux/smp.h>",
            "#include <linux/gfp.h>",
            "#include <linux/init.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/kernel.h>",
            "#include <linux/rculist.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/irq_work.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_function_data, cfd_data);",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);",
            "unsigned int nr_cpu_ids"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"smpboot.h\"\n#include <linux/hypervisor.h>\n#include <linux/sched/idle.h>\n#include <linux/sched.h>\n#include <linux/cpu.h>\n#include <linux/smp.h>\n#include <linux/gfp.h>\n#include <linux/init.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/rculist.h>\n#include <linux/rcupdate.h>\n#include <linux/irq_work.h>\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct call_function_data, cfd_data);\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);\nunsigned int nr_cpu_ids;\n\nvoid smp_call_function_many(const struct cpumask *mask,\n\t\t\t    smp_call_func_t func, void *info, bool wait)\n{\n\tstruct call_function_data *cfd;\n\tint cpu, next_cpu, this_cpu = smp_processor_id();\n\n\t/*\n\t * Can deadlock when called with interrupts disabled.\n\t * We allow cpu's that are not yet online though, as no one else can\n\t * send smp call function interrupt to this cpu and as such deadlocks\n\t * can't happen.\n\t */\n\tWARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()\n\t\t     && !oops_in_progress && !early_boot_irqs_disabled);\n\n\t/* Try to fastpath.  So, what's a CPU they want? Ignoring this one. */\n\tcpu = cpumask_first_and(mask, cpu_online_mask);\n\tif (cpu == this_cpu)\n\t\tcpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\n\t/* No online cpus?  We're done. */\n\tif (cpu >= nr_cpu_ids)\n\t\treturn;\n\n\t/* Do we have another CPU which isn't us? */\n\tnext_cpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\tif (next_cpu == this_cpu)\n\t\tnext_cpu = cpumask_next_and(next_cpu, mask, cpu_online_mask);\n\n\t/* Fastpath: do that cpu by itself. */\n\tif (next_cpu >= nr_cpu_ids) {\n\t\tsmp_call_function_single(cpu, func, info, wait);\n\t\treturn;\n\t}\n\n\tcfd = this_cpu_ptr(&cfd_data);\n\n\tcpumask_and(cfd->cpumask, mask, cpu_online_mask);\n\t__cpumask_clear_cpu(this_cpu, cfd->cpumask);\n\n\t/* Some callers race with other cpus changing the passed mask */\n\tif (unlikely(!cpumask_weight(cfd->cpumask)))\n\t\treturn;\n\n\tcpumask_clear(cfd->cpumask_ipi);\n\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\tcall_single_data_t *csd = per_cpu_ptr(cfd->csd, cpu);\n\n\t\tcsd_lock(csd);\n\t\tif (wait)\n\t\t\tcsd->flags |= CSD_FLAG_SYNCHRONOUS;\n\t\tcsd->func = func;\n\t\tcsd->info = info;\n\t\tif (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))\n\t\t\t__cpumask_set_cpu(cpu, cfd->cpumask_ipi);\n\t}\n\n\t/* Send a message to all CPUs in the map */\n\tarch_send_call_function_ipi_mask(cfd->cpumask_ipi);\n\n\tif (wait) {\n\t\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\t\tcall_single_data_t *csd;\n\n\t\t\tcsd = per_cpu_ptr(cfd->csd, cpu);\n\t\t\tcsd_lock_wait(csd);\n\t\t}\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "preempt_disable",
          "args": [],
          "line": 94
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_preempt_disabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "3571-3576",
          "snippet": "void __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic void __sched;\nstatic void __sched;\n\nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "rcu_read_unlock",
          "args": [],
          "line": 91
        },
        "resolved": true,
        "details": {
          "function_name": "__rcu_read_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/tree_plugin.h",
          "lines": "419-441",
          "snippet": "void __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}",
          "includes": [
            "#include \"../locking/rtmutex_common.h\"",
            "#include \"../time/tick-internal.h\"",
            "#include <uapi/linux/sched/types.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/oom.h>",
            "#include <linux/gfp.h>",
            "#include <linux/delay.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"../locking/rtmutex_common.h\"\n#include \"../time/tick-internal.h\"\n#include <uapi/linux/sched/types.h>\n#include <linux/sched/isolation.h>\n#include <linux/smpboot.h>\n#include <linux/sched/debug.h>\n#include <linux/oom.h>\n#include <linux/gfp.h>\n#include <linux/delay.h>\n\nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (t->rcu_read_lock_nesting != 1) {\n\t\t--t->rcu_read_lock_nesting;\n\t} else {\n\t\tbarrier();  /* critical section before exit code. */\n\t\tt->rcu_read_lock_nesting = INT_MIN;\n\t\tbarrier();  /* assign before ->rcu_read_unlock_special load */\n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t\tbarrier();  /* ->rcu_read_unlock_special load before assign */\n\t\tt->rcu_read_lock_nesting = 0;\n\t}\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\tint rrln = READ_ONCE(t->rcu_read_lock_nesting);\n\n\t\tWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\n\t}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n}"
        }
      },
      {
        "call_info": {
          "callee": "smp_call_function_single",
          "args": [
            "cpu",
            "ipi_mb",
            "NULL",
            "1"
          ],
          "line": 89
        },
        "resolved": true,
        "details": {
          "function_name": "smp_call_function_single",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/up.c",
          "lines": "11-23",
          "snippet": "int smp_call_function_single(int cpu, void (*func) (void *info), void *info,\n\t\t\t\tint wait)\n{\n\tunsigned long flags;\n\n\tWARN_ON(cpu != 0);\n\n\tlocal_irq_save(flags);\n\tfunc(info);\n\tlocal_irq_restore(flags);\n\n\treturn 0;\n}",
          "includes": [
            "#include <linux/hypervisor.h>",
            "#include <linux/smp.h>",
            "#include <linux/export.h>",
            "#include <linux/kernel.h>",
            "#include <linux/interrupt.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/hypervisor.h>\n#include <linux/smp.h>\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n\nint smp_call_function_single(int cpu, void (*func) (void *info), void *info,\n\t\t\t\tint wait)\n{\n\tunsigned long flags;\n\n\tWARN_ON(cpu != 0);\n\n\tlocal_irq_save(flags);\n\tfunc(info);\n\tlocal_irq_restore(flags);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "__cpumask_set_cpu",
          "args": [
            "cpu",
            "tmpmask"
          ],
          "line": 87
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "atomic_read",
          "args": [
            "&p->mm->membarrier_state"
          ],
          "line": 84
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_rcu_dereference",
          "args": [
            "&cpu_rq(cpu)->curr"
          ],
          "line": 83
        },
        "resolved": true,
        "details": {
          "function_name": "task_rcu_dereference",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/exit.c",
          "lines": "234-291",
          "snippet": "struct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}",
          "includes": [
            "#include <asm/mmu_context.h>",
            "#include <asm/pgtable.h>",
            "#include <asm/unistd.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/rcuwait.h>",
            "#include <linux/random.h>",
            "#include <linux/kcov.h>",
            "#include <linux/shm.h>",
            "#include <linux/writeback.h>",
            "#include <linux/oom.h>",
            "#include <linux/hw_breakpoint.h>",
            "#include <trace/events/sched.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/init_task.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/tracehook.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/resource.h>",
            "#include <linux/audit.h> /* for audit_free() */",
            "#include <linux/pipe_fs_i.h>",
            "#include <linux/futex.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/signal.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/kthread.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/mount.h>",
            "#include <linux/profile.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/pid_namespace.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/freezer.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/acct.h>",
            "#include <linux/cpu.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/tty.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/capability.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/mmu_context.h>\n#include <asm/pgtable.h>\n#include <asm/unistd.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/rcuwait.h>\n#include <linux/random.h>\n#include <linux/kcov.h>\n#include <linux/shm.h>\n#include <linux/writeback.h>\n#include <linux/oom.h>\n#include <linux/hw_breakpoint.h>\n#include <trace/events/sched.h>\n#include <linux/perf_event.h>\n#include <linux/init_task.h>\n#include <linux/fs_struct.h>\n#include <linux/tracehook.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/resource.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/pipe_fs_i.h>\n#include <linux/futex.h>\n#include <linux/mutex.h>\n#include <linux/cn_proc.h>\n#include <linux/posix-timers.h>\n#include <linux/signal.h>\n#include <linux/syscalls.h>\n#include <linux/cgroup.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/mempolicy.h>\n#include <linux/kthread.h>\n#include <linux/proc_fs.h>\n#include <linux/mount.h>\n#include <linux/profile.h>\n#include <linux/ptrace.h>\n#include <linux/pid_namespace.h>\n#include <linux/nsproxy.h>\n#include <linux/binfmts.h>\n#include <linux/freezer.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/tsacct_kern.h>\n#include <linux/acct.h>\n#include <linux/cpu.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/tty.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_rq",
          "args": [
            "cpu"
          ],
          "line": 83
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rcu_read_lock",
          "args": [],
          "line": 82
        },
        "resolved": true,
        "details": {
          "function_name": "rcu_read_lock_bh_held",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/rcu/update.c",
          "lines": "300-309",
          "snippet": "int rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}",
          "includes": [
            "#include \"rcu.h\"",
            "#include <linux/sched/isolation.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/tick.h>",
            "#include <linux/kthread.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/delay.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/percpu.h>",
            "#include <linux/bitops.h>",
            "#include <linux/atomic.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/smp.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/init.h>",
            "#include <linux/kernel.h>",
            "#include <linux/types.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rcu.h\"\n#include <linux/sched/isolation.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/moduleparam.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/bitops.h>\n#include <linux/atomic.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/signal.h>\n#include <linux/interrupt.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n\nint rcu_read_lock_bh_held(void)\n{\n\tif (!debug_lockdep_rcu_enabled())\n\t\treturn 1;\n\tif (!rcu_is_watching())\n\t\treturn 0;\n\tif (!rcu_lockdep_current_cpu_online())\n\t\treturn 0;\n\treturn in_softirq() || irqs_disabled();\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_smp_processor_id",
          "args": [],
          "line": 79
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "cpus_read_lock",
          "args": [],
          "line": 67
        },
        "resolved": true,
        "details": {
          "function_name": "cpus_read_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/cpu.c",
          "lines": "285-288",
          "snippet": "void cpus_read_lock(void)\n{\n\tpercpu_down_read(&cpu_hotplug_lock);\n}",
          "includes": [
            "#include \"smpboot.h\"",
            "#include <trace/events/cpuhp.h>",
            "#include <trace/events/power.h>",
            "#include <linux/percpu-rwsem.h>",
            "#include <linux/slab.h>",
            "#include <linux/relay.h>",
            "#include <linux/smpboot.h>",
            "#include <linux/nmi.h>",
            "#include <linux/irq.h>",
            "#include <linux/tick.h>",
            "#include <linux/lockdep.h>",
            "#include <linux/suspend.h>",
            "#include <linux/gfp.h>",
            "#include <linux/mutex.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/kthread.h>",
            "#include <linux/bug.h>",
            "#include <linux/export.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/oom.h>",
            "#include <linux/cpu.h>",
            "#include <linux/unistd.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/notifier.h>",
            "#include <linux/init.h>",
            "#include <linux/smp.h>",
            "#include <linux/proc_fs.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"smpboot.h\"\n#include <trace/events/cpuhp.h>\n#include <trace/events/power.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/slab.h>\n#include <linux/relay.h>\n#include <linux/smpboot.h>\n#include <linux/nmi.h>\n#include <linux/irq.h>\n#include <linux/tick.h>\n#include <linux/lockdep.h>\n#include <linux/suspend.h>\n#include <linux/gfp.h>\n#include <linux/mutex.h>\n#include <linux/stop_machine.h>\n#include <linux/kthread.h>\n#include <linux/bug.h>\n#include <linux/export.h>\n#include <linux/rcupdate.h>\n#include <linux/oom.h>\n#include <linux/cpu.h>\n#include <linux/unistd.h>\n#include <linux/sched/task.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/signal.h>\n#include <linux/notifier.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/proc_fs.h>\n\nvoid cpus_read_lock(void)\n{\n\tpercpu_down_read(&cpu_hotplug_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "zalloc_cpumask_var",
          "args": [
            "&tmpmask",
            "GFP_NOWAIT"
          ],
          "line": 62
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 55
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "num_online_cpus",
          "args": [],
          "line": 48
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nstatic int membarrier_global_expedited(void)\n{\n\tint cpu;\n\tbool fallback = false;\n\tcpumask_var_t tmpmask;\n\n\tif (num_online_cpus() == 1)\n\t\treturn 0;\n\n\t/*\n\t * Matches memory barriers around rq->curr modification in\n\t * scheduler.\n\t */\n\tsmp_mb();\t/* system call entry is not a mb. */\n\n\t/*\n\t * Expedited membarrier commands guarantee that they won't\n\t * block, hence the GFP_NOWAIT allocation flag and fallback\n\t * implementation.\n\t */\n\tif (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {\n\t\t/* Fallback for OOM. */\n\t\tfallback = true;\n\t}\n\n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tstruct task_struct *p;\n\n\t\t/*\n\t\t * Skipping the current CPU is OK even through we can be\n\t\t * migrated at any point. The current CPU, at the point\n\t\t * where we read raw_smp_processor_id(), is ensured to\n\t\t * be in program order with respect to the caller\n\t\t * thread. Therefore, we can skip this CPU from the\n\t\t * iteration.\n\t\t */\n\t\tif (cpu == raw_smp_processor_id())\n\t\t\tcontinue;\n\n\t\trcu_read_lock();\n\t\tp = task_rcu_dereference(&cpu_rq(cpu)->curr);\n\t\tif (p && p->mm && (atomic_read(&p->mm->membarrier_state) &\n\t\t\t\t   MEMBARRIER_STATE_GLOBAL_EXPEDITED)) {\n\t\t\tif (!fallback)\n\t\t\t\t__cpumask_set_cpu(cpu, tmpmask);\n\t\t\telse\n\t\t\t\tsmp_call_function_single(cpu, ipi_mb, NULL, 1);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tif (!fallback) {\n\t\tpreempt_disable();\n\t\tsmp_call_function_many(tmpmask, ipi_mb, NULL, 1);\n\t\tpreempt_enable();\n\t\tfree_cpumask_var(tmpmask);\n\t}\n\tcpus_read_unlock();\n\n\t/*\n\t * Memory barrier on the caller thread _after_ we finished\n\t * waiting for the last IPI. Matches memory barriers around\n\t * rq->curr modification in scheduler.\n\t */\n\tsmp_mb();\t/* exit from system call is not a mb */\n\treturn 0;\n}"
  },
  {
    "function_name": "ipi_mb",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/membarrier.c",
    "lines": "37-40",
    "snippet": "static void ipi_mb(void *info)\n{\n\tsmp_mb();\t/* IPIs should be serializing but paranoid. */\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "smp_mb",
          "args": [],
          "line": 39
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"sched.h\"\n\nstatic void ipi_mb(void *info)\n{\n\tsmp_mb();\t/* IPIs should be serializing but paranoid. */\n}"
  },
  {
    "function_name": "membarrier",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/membarrier.c",
    "lines": "283-318",
    "snippet": "SYSCALL_DEFINE2(membarrier, int, cmd, int, flags)\n{\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\tswitch (cmd) {\n\tcase MEMBARRIER_CMD_QUERY:\n\t{\n\t\tint cmd_mask = MEMBARRIER_CMD_BITMASK;\n\n\t\tif (tick_nohz_full_enabled())\n\t\t\tcmd_mask &= ~MEMBARRIER_CMD_GLOBAL;\n\t\treturn cmd_mask;\n\t}\n\tcase MEMBARRIER_CMD_GLOBAL:\n\t\t/* MEMBARRIER_CMD_GLOBAL is not compatible with nohz_full. */\n\t\tif (tick_nohz_full_enabled())\n\t\t\treturn -EINVAL;\n\t\tif (num_online_cpus() > 1)\n\t\t\tsynchronize_sched();\n\t\treturn 0;\n\tcase MEMBARRIER_CMD_GLOBAL_EXPEDITED:\n\t\treturn membarrier_global_expedited();\n\tcase MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED:\n\t\treturn membarrier_register_global_expedited();\n\tcase MEMBARRIER_CMD_PRIVATE_EXPEDITED:\n\t\treturn membarrier_private_expedited(0);\n\tcase MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED:\n\t\treturn membarrier_register_private_expedited(0);\n\tcase MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE:\n\t\treturn membarrier_private_expedited(MEMBARRIER_FLAG_SYNC_CORE);\n\tcase MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE:\n\t\treturn membarrier_register_private_expedited(MEMBARRIER_FLAG_SYNC_CORE);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
    "includes": [
      "#include \"sched.h\""
    ],
    "macros_used": [
      "#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\\n\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\\n\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\\n\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\\n\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\\n\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK)"
    ],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"sched.h\"\n\n#define MEMBARRIER_CMD_BITMASK\t\t\t\t\t\t\\\n\t(MEMBARRIER_CMD_GLOBAL | MEMBARRIER_CMD_GLOBAL_EXPEDITED\t\\\n\t| MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED\t\t\t\\\n\t| MEMBARRIER_CMD_PRIVATE_EXPEDITED\t\t\t\t\\\n\t| MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED\t\t\t\\\n\t| MEMBARRIER_PRIVATE_EXPEDITED_SYNC_CORE_BITMASK)\n\nSYSCALL_DEFINE2(membarrier, int, cmd, int, flags)\n{\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\tswitch (cmd) {\n\tcase MEMBARRIER_CMD_QUERY:\n\t{\n\t\tint cmd_mask = MEMBARRIER_CMD_BITMASK;\n\n\t\tif (tick_nohz_full_enabled())\n\t\t\tcmd_mask &= ~MEMBARRIER_CMD_GLOBAL;\n\t\treturn cmd_mask;\n\t}\n\tcase MEMBARRIER_CMD_GLOBAL:\n\t\t/* MEMBARRIER_CMD_GLOBAL is not compatible with nohz_full. */\n\t\tif (tick_nohz_full_enabled())\n\t\t\treturn -EINVAL;\n\t\tif (num_online_cpus() > 1)\n\t\t\tsynchronize_sched();\n\t\treturn 0;\n\tcase MEMBARRIER_CMD_GLOBAL_EXPEDITED:\n\t\treturn membarrier_global_expedited();\n\tcase MEMBARRIER_CMD_REGISTER_GLOBAL_EXPEDITED:\n\t\treturn membarrier_register_global_expedited();\n\tcase MEMBARRIER_CMD_PRIVATE_EXPEDITED:\n\t\treturn membarrier_private_expedited(0);\n\tcase MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED:\n\t\treturn membarrier_register_private_expedited(0);\n\tcase MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE:\n\t\treturn membarrier_private_expedited(MEMBARRIER_FLAG_SYNC_CORE);\n\tcase MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED_SYNC_CORE:\n\t\treturn membarrier_register_private_expedited(MEMBARRIER_FLAG_SYNC_CORE);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}"
  }
]