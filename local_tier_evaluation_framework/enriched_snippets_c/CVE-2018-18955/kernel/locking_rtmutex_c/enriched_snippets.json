[
  {
    "function_name": "rt_mutex_cleanup_proxy_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1862-1897",
    "snippet": "bool rt_mutex_cleanup_proxy_lock(struct rt_mutex *lock,\n\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tbool cleanup = false;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/*\n\t * Do an unconditional try-lock, this deals with the lock stealing\n\t * state where __rt_mutex_futex_unlock() -> mark_wakeup_next_waiter()\n\t * sets a NULL owner.\n\t *\n\t * We're not interested in the return value, because the subsequent\n\t * test on rt_mutex_owner() will infer that. If the trylock succeeded,\n\t * we will own the lock and it will have removed the waiter. If we\n\t * failed the trylock, we're still not owner and we need to remove\n\t * ourselves.\n\t */\n\ttry_to_take_rt_mutex(lock, current, waiter);\n\t/*\n\t * Unless we're the owner; we're still enqueued on the wait_list.\n\t * So check if we became owner, if not, take us off the wait_list.\n\t */\n\tif (rt_mutex_owner(lock) != current) {\n\t\tremove_waiter(lock, waiter);\n\t\tcleanup = true;\n\t}\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn cleanup;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1894
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 1892
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "68-136",
          "snippet": "static void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "remove_waiter",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 1885
        },
        "resolved": true,
        "details": {
          "function_name": "remove_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1068-1119",
          "snippet": "static void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1884
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "waiter"
          ],
          "line": 1879
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "808-920",
          "snippet": "static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1867
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool rt_mutex_cleanup_proxy_lock(struct rt_mutex *lock,\n\t\t\t\t struct rt_mutex_waiter *waiter)\n{\n\tbool cleanup = false;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/*\n\t * Do an unconditional try-lock, this deals with the lock stealing\n\t * state where __rt_mutex_futex_unlock() -> mark_wakeup_next_waiter()\n\t * sets a NULL owner.\n\t *\n\t * We're not interested in the return value, because the subsequent\n\t * test on rt_mutex_owner() will infer that. If the trylock succeeded,\n\t * we will own the lock and it will have removed the waiter. If we\n\t * failed the trylock, we're still not owner and we need to remove\n\t * ourselves.\n\t */\n\ttry_to_take_rt_mutex(lock, current, waiter);\n\t/*\n\t * Unless we're the owner; we're still enqueued on the wait_list.\n\t * So check if we became owner, if not, take us off the wait_list.\n\t */\n\tif (rt_mutex_owner(lock) != current) {\n\t\tremove_waiter(lock, waiter);\n\t\tcleanup = true;\n\t}\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn cleanup;\n}"
  },
  {
    "function_name": "rt_mutex_wait_proxy_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1823-1841",
    "snippet": "int rt_mutex_wait_proxy_lock(struct rt_mutex *lock,\n\t\t\t       struct hrtimer_sleeper *to,\n\t\t\t       struct rt_mutex_waiter *waiter)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/* sleep on the mutex */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1838
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 1837
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "68-136",
          "snippet": "static void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_slowlock",
          "args": [
            "lock",
            "TASK_INTERRUPTIBLE",
            "to",
            "waiter"
          ],
          "line": 1832
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1167-1205",
          "snippet": "static int __sched\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t    struct hrtimer_sleeper *timeout,\n\t\t    struct rt_mutex_waiter *waiter)\n{\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * TASK_INTERRUPTIBLE checks for signals and\n\t\t * timeout. Ignored otherwise.\n\t\t */\n\t\tif (likely(state == TASK_INTERRUPTIBLE)) {\n\t\t\t/* Signal pending? */\n\t\t\tif (signal_pending(current))\n\t\t\t\tret = -EINTR;\n\t\t\tif (timeout && !timeout->task)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tdebug_rt_mutex_print_deadlock(waiter);\n\n\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int __sched\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t    struct hrtimer_sleeper *timeout,\n\t\t    struct rt_mutex_waiter *waiter)\n{\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * TASK_INTERRUPTIBLE checks for signals and\n\t\t * timeout. Ignored otherwise.\n\t\t */\n\t\tif (likely(state == TASK_INTERRUPTIBLE)) {\n\t\t\t/* Signal pending? */\n\t\t\tif (signal_pending(current))\n\t\t\t\tret = -EINTR;\n\t\t\tif (timeout && !timeout->task)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tdebug_rt_mutex_print_deadlock(waiter);\n\n\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "TASK_INTERRUPTIBLE"
          ],
          "line": 1831
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1829
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint rt_mutex_wait_proxy_lock(struct rt_mutex *lock,\n\t\t\t       struct hrtimer_sleeper *to,\n\t\t\t       struct rt_mutex_waiter *waiter)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\t/* sleep on the mutex */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might\n\t * have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_next_owner",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1798-1804",
    "snippet": "struct task_struct *rt_mutex_next_owner(struct rt_mutex *lock)\n{\n\tif (!rt_mutex_has_waiters(lock))\n\t\treturn NULL;\n\n\treturn rt_mutex_top_waiter(lock)->task;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1803
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "84-88",
          "snippet": "static inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1800
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstruct task_struct *rt_mutex_next_owner(struct rt_mutex *lock)\n{\n\tif (!rt_mutex_has_waiters(lock))\n\t\treturn NULL;\n\n\treturn rt_mutex_top_waiter(lock)->task;\n}"
  },
  {
    "function_name": "rt_mutex_start_proxy_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1773-1784",
    "snippet": "int rt_mutex_start_proxy_lock(struct rt_mutex *lock,\n\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\tret = __rt_mutex_start_proxy_lock(lock, waiter, task);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1781
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_start_proxy_lock",
          "args": [
            "lock",
            "waiter",
            "task"
          ],
          "line": 1780
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_start_proxy_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1729-1758",
          "snippet": "int __rt_mutex_start_proxy_lock(struct rt_mutex *lock,\n\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\tif (unlikely(ret))\n\t\tremove_waiter(lock, waiter);\n\n\tdebug_rt_mutex_print_deadlock(waiter);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __rt_mutex_start_proxy_lock(struct rt_mutex *lock,\n\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\tif (unlikely(ret))\n\t\tremove_waiter(lock, waiter);\n\n\tdebug_rt_mutex_print_deadlock(waiter);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1779
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint rt_mutex_start_proxy_lock(struct rt_mutex *lock,\n\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\tret = __rt_mutex_start_proxy_lock(lock, waiter, task);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "__rt_mutex_start_proxy_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1729-1758",
    "snippet": "int __rt_mutex_start_proxy_lock(struct rt_mutex *lock,\n\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\tif (unlikely(ret))\n\t\tremove_waiter(lock, waiter);\n\n\tdebug_rt_mutex_print_deadlock(waiter);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "debug_rt_mutex_print_deadlock",
          "args": [
            "waiter"
          ],
          "line": 1755
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_print_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "87-137",
          "snippet": "void debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter)\n{\n\tstruct task_struct *task;\n\n\tif (!waiter->deadlock_lock || !debug_locks)\n\t\treturn;\n\n\trcu_read_lock();\n\ttask = pid_task(waiter->deadlock_task_pid, PIDTYPE_PID);\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (!debug_locks_off()) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"============================================\\n\");\n\tpr_warn(\"WARNING: circular locking deadlock detected!\\n\");\n\tpr_warn(\"%s\\n\", print_tainted());\n\tpr_warn(\"--------------------------------------------\\n\");\n\tprintk(\"%s/%d is deadlocking current task %s/%d\\n\\n\",\n\t       task->comm, task_pid_nr(task),\n\t       current->comm, task_pid_nr(current));\n\n\tprintk(\"\\n1) %s/%d is trying to acquire this lock:\\n\",\n\t       current->comm, task_pid_nr(current));\n\tprintk_lock(waiter->lock, 1);\n\n\tprintk(\"\\n2) %s/%d is blocked on this lock:\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tprintk_lock(waiter->deadlock_lock, 1);\n\n\tdebug_show_held_locks(current);\n\tdebug_show_held_locks(task);\n\n\tprintk(\"\\n%s/%d's [blocked] stackdump:\\n\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tshow_stack(task, NULL);\n\tprintk(\"\\n%s/%d's [current] stackdump:\\n\\n\",\n\t\tcurrent->comm, task_pid_nr(current));\n\tdump_stack();\n\tdebug_show_all_locks();\n\trcu_read_unlock();\n\n\tprintk(\"[ turning off deadlock detection.\"\n\t       \"Please report this trace. ]\\n\\n\");\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter)\n{\n\tstruct task_struct *task;\n\n\tif (!waiter->deadlock_lock || !debug_locks)\n\t\treturn;\n\n\trcu_read_lock();\n\ttask = pid_task(waiter->deadlock_task_pid, PIDTYPE_PID);\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (!debug_locks_off()) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"============================================\\n\");\n\tpr_warn(\"WARNING: circular locking deadlock detected!\\n\");\n\tpr_warn(\"%s\\n\", print_tainted());\n\tpr_warn(\"--------------------------------------------\\n\");\n\tprintk(\"%s/%d is deadlocking current task %s/%d\\n\\n\",\n\t       task->comm, task_pid_nr(task),\n\t       current->comm, task_pid_nr(current));\n\n\tprintk(\"\\n1) %s/%d is trying to acquire this lock:\\n\",\n\t       current->comm, task_pid_nr(current));\n\tprintk_lock(waiter->lock, 1);\n\n\tprintk(\"\\n2) %s/%d is blocked on this lock:\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tprintk_lock(waiter->deadlock_lock, 1);\n\n\tdebug_show_held_locks(current);\n\tdebug_show_held_locks(task);\n\n\tprintk(\"\\n%s/%d's [blocked] stackdump:\\n\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tshow_stack(task, NULL);\n\tprintk(\"\\n%s/%d's [current] stackdump:\\n\\n\",\n\t\tcurrent->comm, task_pid_nr(current));\n\tdump_stack();\n\tdebug_show_all_locks();\n\trcu_read_unlock();\n\n\tprintk(\"[ turning off deadlock detection.\"\n\t       \"Please report this trace. ]\\n\\n\");\n}"
        }
      },
      {
        "call_info": {
          "callee": "remove_waiter",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 1753
        },
        "resolved": true,
        "details": {
          "function_name": "remove_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1068-1119",
          "snippet": "static void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret"
          ],
          "line": 1752
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1742
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocks_on_rt_mutex",
          "args": [
            "lock",
            "waiter",
            "task",
            "RT_MUTEX_FULL_CHAINWALK"
          ],
          "line": 1739
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocks_on_rt_mutex",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "929-1010",
          "snippet": "static int task_blocks_on_rt_mutex(struct rt_mutex *lock,\n\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t */\n\tif (owner == task)\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int task_blocks_on_rt_mutex(struct rt_mutex *lock,\n\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t */\n\tif (owner == task)\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}"
        }
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "task",
            "NULL"
          ],
          "line": 1735
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "808-920",
          "snippet": "static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __rt_mutex_start_proxy_lock(struct rt_mutex *lock,\n\t\t\t      struct rt_mutex_waiter *waiter,\n\t\t\t      struct task_struct *task)\n{\n\tint ret;\n\n\tif (try_to_take_rt_mutex(lock, task, NULL))\n\t\treturn 1;\n\n\t/* We enforce deadlock detection for futexes */\n\tret = task_blocks_on_rt_mutex(lock, waiter, task,\n\t\t\t\t      RT_MUTEX_FULL_CHAINWALK);\n\n\tif (ret && !rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * Reset the return value. We might have\n\t\t * returned with -EDEADLK and the owner\n\t\t * released the lock while we were walking the\n\t\t * pi chain.  Let the waiter sort it out.\n\t\t */\n\t\tret = 0;\n\t}\n\n\tif (unlikely(ret))\n\t\tremove_waiter(lock, waiter);\n\n\tdebug_rt_mutex_print_deadlock(waiter);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_proxy_unlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1722-1727",
    "snippet": "void rt_mutex_proxy_unlock(struct rt_mutex *lock,\n\t\t\t   struct task_struct *proxy_owner)\n{\n\tdebug_rt_mutex_proxy_unlock(lock);\n\trt_mutex_set_owner(lock, NULL);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_set_owner",
          "args": [
            "lock",
            "NULL"
          ],
          "line": 1726
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_set_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "51-60",
          "snippet": "static void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_proxy_unlock",
          "args": [
            "lock"
          ],
          "line": 1725
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_proxy_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "153-156",
          "snippet": "void debug_rt_mutex_proxy_unlock(struct rt_mutex *lock)\n{\n\tDEBUG_LOCKS_WARN_ON(!rt_mutex_owner(lock));\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_proxy_unlock(struct rt_mutex *lock)\n{\n\tDEBUG_LOCKS_WARN_ON(!rt_mutex_owner(lock));\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_proxy_unlock(struct rt_mutex *lock,\n\t\t\t   struct task_struct *proxy_owner)\n{\n\tdebug_rt_mutex_proxy_unlock(lock);\n\trt_mutex_set_owner(lock, NULL);\n}"
  },
  {
    "function_name": "rt_mutex_init_proxy_locked",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1702-1708",
    "snippet": "void rt_mutex_init_proxy_locked(struct rt_mutex *lock,\n\t\t\t\tstruct task_struct *proxy_owner)\n{\n\t__rt_mutex_init(lock, NULL, NULL);\n\tdebug_rt_mutex_proxy_lock(lock, proxy_owner);\n\trt_mutex_set_owner(lock, proxy_owner);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_set_owner",
          "args": [
            "lock",
            "proxy_owner"
          ],
          "line": 1707
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_set_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "51-60",
          "snippet": "static void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_proxy_lock",
          "args": [
            "lock",
            "proxy_owner"
          ],
          "line": 1706
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_proxy_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "148-151",
          "snippet": "void\ndebug_rt_mutex_proxy_lock(struct rt_mutex *lock, struct task_struct *powner)\n{\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid\ndebug_rt_mutex_proxy_lock(struct rt_mutex *lock, struct task_struct *powner)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_init",
          "args": [
            "lock",
            "NULL",
            "NULL"
          ],
          "line": 1705
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_init",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1676-1685",
          "snippet": "void __rt_mutex_init(struct rt_mutex *lock, const char *name,\n\t\t     struct lock_class_key *key)\n{\n\tlock->owner = NULL;\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\n\tif (name && key)\n\t\tdebug_rt_mutex_init(lock, name, key);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __rt_mutex_init(struct rt_mutex *lock, const char *name,\n\t\t     struct lock_class_key *key)\n{\n\tlock->owner = NULL;\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\n\tif (name && key)\n\t\tdebug_rt_mutex_init(lock, name, key);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_init_proxy_locked(struct rt_mutex *lock,\n\t\t\t\tstruct task_struct *proxy_owner)\n{\n\t__rt_mutex_init(lock, NULL, NULL);\n\tdebug_rt_mutex_proxy_lock(lock, proxy_owner);\n\trt_mutex_set_owner(lock, proxy_owner);\n}"
  },
  {
    "function_name": "__rt_mutex_init",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1676-1685",
    "snippet": "void __rt_mutex_init(struct rt_mutex *lock, const char *name,\n\t\t     struct lock_class_key *key)\n{\n\tlock->owner = NULL;\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\n\tif (name && key)\n\t\tdebug_rt_mutex_init(lock, name, key);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "debug_rt_mutex_init",
          "args": [
            "lock",
            "name",
            "key"
          ],
          "line": 1684
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_init",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "170-181",
          "snippet": "void debug_rt_mutex_init(struct rt_mutex *lock, const char *name, struct lock_class_key *key)\n{\n\t/*\n\t * Make sure we are not reinitializing a held lock:\n\t */\n\tdebug_check_no_locks_freed((void *)lock, sizeof(*lock));\n\tlock->name = name;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tlockdep_init_map(&lock->dep_map, name, key, 0);\n#endif\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_init(struct rt_mutex *lock, const char *name, struct lock_class_key *key)\n{\n\t/*\n\t * Make sure we are not reinitializing a held lock:\n\t */\n\tdebug_check_no_locks_freed((void *)lock, sizeof(*lock));\n\tlock->name = name;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tlockdep_init_map(&lock->dep_map, name, key, 0);\n#endif\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_init",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1680
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __rt_mutex_init(struct rt_mutex *lock, const char *name,\n\t\t     struct lock_class_key *key)\n{\n\tlock->owner = NULL;\n\traw_spin_lock_init(&lock->wait_lock);\n\tlock->waiters = RB_ROOT_CACHED;\n\n\tif (name && key)\n\t\tdebug_rt_mutex_init(lock, name, key);\n}"
  },
  {
    "function_name": "rt_mutex_destroy",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1658-1664",
    "snippet": "void rt_mutex_destroy(struct rt_mutex *lock)\n{\n\tWARN_ON(rt_mutex_is_locked(lock));\n#ifdef CONFIG_DEBUG_RT_MUTEXES\n\tlock->magic = NULL;\n#endif\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WARN_ON",
          "args": [
            "rt_mutex_is_locked(lock)"
          ],
          "line": 1660
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_is_locked",
          "args": [
            "lock"
          ],
          "line": 1660
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_destroy(struct rt_mutex *lock)\n{\n\tWARN_ON(rt_mutex_is_locked(lock));\n#ifdef CONFIG_DEBUG_RT_MUTEXES\n\tlock->magic = NULL;\n#endif\n}"
  },
  {
    "function_name": "rt_mutex_futex_unlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1636-1648",
    "snippet": "void __sched rt_mutex_futex_unlock(struct rt_mutex *lock)\n{\n\tDEFINE_WAKE_Q(wake_q);\n\tunsigned long flags;\n\tbool postunlock;\n\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tpostunlock = __rt_mutex_futex_unlock(lock, &wake_q);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wake_q);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_postunlock",
          "args": [
            "&wake_q"
          ],
          "line": 1647
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_postunlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1446-1452",
          "snippet": "void rt_mutex_postunlock(struct wake_q_head *wake_q)\n{\n\twake_up_q(wake_q);\n\n\t/* Pairs with preempt_disable() in rt_mutex_slowunlock() */\n\tpreempt_enable();\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_postunlock(struct wake_q_head *wake_q)\n{\n\twake_up_q(wake_q);\n\n\t/* Pairs with preempt_disable() in rt_mutex_slowunlock() */\n\tpreempt_enable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1644
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_futex_unlock",
          "args": [
            "lock",
            "&wake_q"
          ],
          "line": 1643
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_futex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1613-1634",
          "snippet": "bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,\n\t\t\t\t    struct wake_q_head *wake_q)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wake_q, lock);\n\n\treturn true; /* call postunlock() */\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,\n\t\t\t\t    struct wake_q_head *wake_q)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wake_q, lock);\n\n\treturn true; /* call postunlock() */\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1642
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 1638
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_futex_unlock(struct rt_mutex *lock)\n{\n\tDEFINE_WAKE_Q(wake_q);\n\tunsigned long flags;\n\tbool postunlock;\n\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tpostunlock = __rt_mutex_futex_unlock(lock, &wake_q);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wake_q);\n}"
  },
  {
    "function_name": "__rt_mutex_futex_unlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1613-1634",
    "snippet": "bool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,\n\t\t\t\t    struct wake_q_head *wake_q)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wake_q, lock);\n\n\treturn true; /* call postunlock() */\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mark_wakeup_next_waiter",
          "args": [
            "wake_q",
            "lock"
          ],
          "line": 1631
        },
        "resolved": true,
        "details": {
          "function_name": "mark_wakeup_next_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1018-1060",
          "snippet": "static void mark_wakeup_next_waiter(struct wake_q_head *wake_q,\n\t\t\t\t    struct rt_mutex *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_postunlock();\n\t */\n\tpreempt_disable();\n\twake_q_add(wake_q, waiter->task);\n\traw_spin_unlock(&current->pi_lock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void mark_wakeup_next_waiter(struct wake_q_head *wake_q,\n\t\t\t\t    struct rt_mutex *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_postunlock();\n\t */\n\tpreempt_disable();\n\twake_q_add(wake_q, waiter->task);\n\traw_spin_unlock(&current->pi_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1620
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_unlock",
          "args": [
            "lock"
          ],
          "line": 1618
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "143-146",
          "snippet": "void debug_rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1616
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nbool __sched __rt_mutex_futex_unlock(struct rt_mutex *lock,\n\t\t\t\t    struct wake_q_head *wake_q)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\tif (!rt_mutex_has_waiters(lock)) {\n\t\tlock->owner = NULL;\n\t\treturn false; /* done */\n\t}\n\n\t/*\n\t * We've already deboosted, mark_wakeup_next_waiter() will\n\t * retain preempt_disabled when we drop the wait_lock, to\n\t * avoid inversion prior to the wakeup.  preempt_disable()\n\t * therein pairs with rt_mutex_postunlock().\n\t */\n\tmark_wakeup_next_waiter(wake_q, lock);\n\n\treturn true; /* call postunlock() */\n}"
  },
  {
    "function_name": "rt_mutex_unlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1602-1606",
    "snippet": "void __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_fastunlock",
          "args": [
            "lock",
            "rt_mutex_slowunlock"
          ],
          "line": 1605
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_fastunlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1454-1466",
          "snippet": "static inline void\nrt_mutex_fastunlock(struct rt_mutex *lock,\n\t\t    bool (*slowfn)(struct rt_mutex *lock,\n\t\t\t\t   struct wake_q_head *wqh))\n{\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\tif (slowfn(lock, &wake_q))\n\t\trt_mutex_postunlock(&wake_q);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void\nrt_mutex_fastunlock(struct rt_mutex *lock,\n\t\t    bool (*slowfn)(struct rt_mutex *lock,\n\t\t\t\t   struct wake_q_head *wqh))\n{\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\tif (slowfn(lock, &wake_q))\n\t\trt_mutex_postunlock(&wake_q);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&lock->dep_map",
            "1",
            "_RET_IP_"
          ],
          "line": 1604
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\trt_mutex_fastunlock(lock, rt_mutex_slowunlock);\n}"
  },
  {
    "function_name": "rt_mutex_trylock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1582-1594",
    "snippet": "int __sched rt_mutex_trylock(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tif (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))\n\t\treturn 0;\n\n\tret = rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);\n\tif (ret)\n\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_acquire",
          "args": [
            "&lock->dep_map",
            "0",
            "1",
            "_RET_IP_"
          ],
          "line": 1591
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_fasttrylock",
          "args": [
            "lock",
            "rt_mutex_slowtrylock"
          ],
          "line": 1589
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_fasttrylock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1433-1441",
          "snippet": "static inline int\nrt_mutex_fasttrylock(struct rt_mutex *lock,\n\t\t     int (*slowfn)(struct rt_mutex *lock))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn slowfn(lock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_fasttrylock(struct rt_mutex *lock,\n\t\t     int (*slowfn)(struct rt_mutex *lock))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn slowfn(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "WARN_ON_ONCE",
          "args": [
            "in_irq() || in_nmi() || in_serving_softirq()"
          ],
          "line": 1586
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "in_serving_softirq",
          "args": [],
          "line": 1586
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "in_nmi",
          "args": [],
          "line": 1586
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "in_irq",
          "args": [],
          "line": 1586
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_trylock(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tif (WARN_ON_ONCE(in_irq() || in_nmi() || in_serving_softirq()))\n\t\treturn 0;\n\n\tret = rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);\n\tif (ret)\n\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_timed_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1553-1568",
    "snippet": "int\nrt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,\n\t\t\t\t       RT_MUTEX_MIN_CHAINWALK,\n\t\t\t\t       rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&lock->dep_map",
            "1",
            "_RET_IP_"
          ],
          "line": 1565
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_timed_fastlock",
          "args": [
            "lock",
            "TASK_INTERRUPTIBLE",
            "timeout",
            "RT_MUTEX_MIN_CHAINWALK",
            "rt_mutex_slowlock"
          ],
          "line": 1561
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_timed_fastlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1418-1431",
          "snippet": "static inline int\nrt_mutex_timed_fastlock(struct rt_mutex *lock, int state,\n\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\tenum rtmutex_chainwalk chwalk,\n\t\t\tint (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\t      struct hrtimer_sleeper *timeout,\n\t\t\t\t      enum rtmutex_chainwalk chwalk))\n{\n\tif (chwalk == RT_MUTEX_MIN_CHAINWALK &&\n\t    likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, timeout, chwalk);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_timed_fastlock(struct rt_mutex *lock, int state,\n\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\tenum rtmutex_chainwalk chwalk,\n\t\t\tint (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\t      struct hrtimer_sleeper *timeout,\n\t\t\t\t      enum rtmutex_chainwalk chwalk))\n{\n\tif (chwalk == RT_MUTEX_MIN_CHAINWALK &&\n\t    likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, timeout, chwalk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_acquire",
          "args": [
            "&lock->dep_map",
            "0",
            "0",
            "_RET_IP_"
          ],
          "line": 1560
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "might_sleep",
          "args": [],
          "line": 1558
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint\nrt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,\n\t\t\t\t       RT_MUTEX_MIN_CHAINWALK,\n\t\t\t\t       rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "__rt_mutex_futex_trylock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1535-1538",
    "snippet": "int __sched __rt_mutex_futex_trylock(struct rt_mutex *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_slowtrylock",
          "args": [
            "lock"
          ],
          "line": 1537
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowtrylock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1292-1303",
          "snippet": "static inline int __rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int __rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched __rt_mutex_futex_trylock(struct rt_mutex *lock)\n{\n\treturn __rt_mutex_slowtrylock(lock);\n}"
  },
  {
    "function_name": "rt_mutex_futex_trylock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1530-1533",
    "snippet": "int __sched rt_mutex_futex_trylock(struct rt_mutex *lock)\n{\n\treturn rt_mutex_slowtrylock(lock);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_slowtrylock",
          "args": [
            "lock"
          ],
          "line": 1532
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_slowtrylock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1308-1332",
          "snippet": "static inline int rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_futex_trylock(struct rt_mutex *lock)\n{\n\treturn rt_mutex_slowtrylock(lock);\n}"
  },
  {
    "function_name": "rt_mutex_lock_interruptible",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1512-1524",
    "snippet": "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "mutex_release",
          "args": [
            "&lock->dep_map",
            "1",
            "_RET_IP_"
          ],
          "line": 1521
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_fastlock",
          "args": [
            "lock",
            "TASK_INTERRUPTIBLE",
            "rt_mutex_slowlock"
          ],
          "line": 1519
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_fastlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1406-1416",
          "snippet": "static inline int\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\n\t\t  int (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\t\tenum rtmutex_chainwalk chwalk))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\n\t\t  int (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\t\tenum rtmutex_chainwalk chwalk))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_acquire",
          "args": [
            "&lock->dep_map",
            "0",
            "0",
            "_RET_IP_"
          ],
          "line": 1518
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "might_sleep",
          "args": [],
          "line": 1516
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tret = rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\n\tif (ret)\n\t\tmutex_release(&lock->dep_map, 1, _RET_IP_);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1496-1499",
    "snippet": "void __sched rt_mutex_lock(struct rt_mutex *lock)\n{\n\t__rt_mutex_lock(lock, 0);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_lock",
          "args": [
            "lock",
            "0"
          ],
          "line": 1498
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1468-1474",
          "snippet": "static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)\n{\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\trt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)\n{\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\trt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_lock(struct rt_mutex *lock)\n{\n\t__rt_mutex_lock(lock, 0);\n}"
  },
  {
    "function_name": "rt_mutex_lock_nested",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1483-1486",
    "snippet": "void __sched rt_mutex_lock_nested(struct rt_mutex *lock, unsigned int subclass)\n{\n\t__rt_mutex_lock(lock, subclass);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__rt_mutex_lock",
          "args": [
            "lock",
            "subclass"
          ],
          "line": 1485
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1468-1474",
          "snippet": "static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)\n{\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\trt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)\n{\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\trt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid __sched rt_mutex_lock_nested(struct rt_mutex *lock, unsigned int subclass)\n{\n\t__rt_mutex_lock(lock, subclass);\n}"
  },
  {
    "function_name": "__rt_mutex_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1468-1474",
    "snippet": "static inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)\n{\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\trt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_fastlock",
          "args": [
            "lock",
            "TASK_UNINTERRUPTIBLE",
            "rt_mutex_slowlock"
          ],
          "line": 1473
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_fastlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1406-1416",
          "snippet": "static inline int\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\n\t\t  int (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\t\tenum rtmutex_chainwalk chwalk))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\n\t\t  int (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\t\tenum rtmutex_chainwalk chwalk))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mutex_acquire",
          "args": [
            "&lock->dep_map",
            "subclass",
            "0",
            "_RET_IP_"
          ],
          "line": 1472
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "might_sleep",
          "args": [],
          "line": 1470
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void __rt_mutex_lock(struct rt_mutex *lock, unsigned int subclass)\n{\n\tmight_sleep();\n\n\tmutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\trt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);\n}"
  },
  {
    "function_name": "rt_mutex_fastunlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1454-1466",
    "snippet": "static inline void\nrt_mutex_fastunlock(struct rt_mutex *lock,\n\t\t    bool (*slowfn)(struct rt_mutex *lock,\n\t\t\t\t   struct wake_q_head *wqh))\n{\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\tif (slowfn(lock, &wake_q))\n\t\trt_mutex_postunlock(&wake_q);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_postunlock",
          "args": [
            "&wake_q"
          ],
          "line": 1465
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_postunlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1446-1452",
          "snippet": "void rt_mutex_postunlock(struct wake_q_head *wake_q)\n{\n\twake_up_q(wake_q);\n\n\t/* Pairs with preempt_disable() in rt_mutex_slowunlock() */\n\tpreempt_enable();\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_postunlock(struct wake_q_head *wake_q)\n{\n\twake_up_q(wake_q);\n\n\t/* Pairs with preempt_disable() in rt_mutex_slowunlock() */\n\tpreempt_enable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "slowfn",
          "args": [
            "lock",
            "&wake_q"
          ],
          "line": 1464
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_release(lock, current, NULL)"
          ],
          "line": 1461
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_release",
          "args": [
            "lock",
            "current",
            "NULL"
          ],
          "line": 1461
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "DEFINE_WAKE_Q",
          "args": [
            "wake_q"
          ],
          "line": 1459
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void\nrt_mutex_fastunlock(struct rt_mutex *lock,\n\t\t    bool (*slowfn)(struct rt_mutex *lock,\n\t\t\t\t   struct wake_q_head *wqh))\n{\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\tif (slowfn(lock, &wake_q))\n\t\trt_mutex_postunlock(&wake_q);\n}"
  },
  {
    "function_name": "rt_mutex_postunlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1446-1452",
    "snippet": "void rt_mutex_postunlock(struct wake_q_head *wake_q)\n{\n\twake_up_q(wake_q);\n\n\t/* Pairs with preempt_disable() in rt_mutex_slowunlock() */\n\tpreempt_enable();\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "preempt_enable",
          "args": [],
          "line": 1451
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "wake_up_q",
          "args": [
            "wake_q"
          ],
          "line": 1448
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_q",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "423-443",
          "snippet": "void wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\tBUG_ON(!task);\n\t\t/* Task can safely be re-inserted now: */\n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t/*\n\t\t * wake_up_process() executes a full barrier, which pairs with\n\t\t * the queueing in wake_q_add() so as not to miss wakeups.\n\t\t */\n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_postunlock(struct wake_q_head *wake_q)\n{\n\twake_up_q(wake_q);\n\n\t/* Pairs with preempt_disable() in rt_mutex_slowunlock() */\n\tpreempt_enable();\n}"
  },
  {
    "function_name": "rt_mutex_fasttrylock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1433-1441",
    "snippet": "static inline int\nrt_mutex_fasttrylock(struct rt_mutex *lock,\n\t\t     int (*slowfn)(struct rt_mutex *lock))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn slowfn(lock);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "slowfn",
          "args": [
            "lock"
          ],
          "line": 1440
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_acquire(lock, NULL, current)"
          ],
          "line": 1437
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_acquire",
          "args": [
            "lock",
            "NULL",
            "current"
          ],
          "line": 1437
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_fasttrylock(struct rt_mutex *lock,\n\t\t     int (*slowfn)(struct rt_mutex *lock))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn slowfn(lock);\n}"
  },
  {
    "function_name": "rt_mutex_timed_fastlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1418-1431",
    "snippet": "static inline int\nrt_mutex_timed_fastlock(struct rt_mutex *lock, int state,\n\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\tenum rtmutex_chainwalk chwalk,\n\t\t\tint (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\t      struct hrtimer_sleeper *timeout,\n\t\t\t\t      enum rtmutex_chainwalk chwalk))\n{\n\tif (chwalk == RT_MUTEX_MIN_CHAINWALK &&\n\t    likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, timeout, chwalk);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "slowfn",
          "args": [
            "lock",
            "state",
            "timeout",
            "chwalk"
          ],
          "line": 1430
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_acquire(lock, NULL, current)"
          ],
          "line": 1427
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_acquire",
          "args": [
            "lock",
            "NULL",
            "current"
          ],
          "line": 1427
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_timed_fastlock(struct rt_mutex *lock, int state,\n\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\tenum rtmutex_chainwalk chwalk,\n\t\t\tint (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\t      struct hrtimer_sleeper *timeout,\n\t\t\t\t      enum rtmutex_chainwalk chwalk))\n{\n\tif (chwalk == RT_MUTEX_MIN_CHAINWALK &&\n\t    likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, timeout, chwalk);\n}"
  },
  {
    "function_name": "rt_mutex_fastlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1406-1416",
    "snippet": "static inline int\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\n\t\t  int (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\t\tenum rtmutex_chainwalk chwalk))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "slowfn",
          "args": [
            "lock",
            "state",
            "NULL",
            "RT_MUTEX_MIN_CHAINWALK"
          ],
          "line": 1415
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "rt_mutex_cmpxchg_acquire(lock, NULL, current)"
          ],
          "line": 1412
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_acquire",
          "args": [
            "lock",
            "NULL",
            "current"
          ],
          "line": 1412
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\n\t\t  int (*slowfn)(struct rt_mutex *lock, int state,\n\t\t\t\tstruct hrtimer_sleeper *timeout,\n\t\t\t\tenum rtmutex_chainwalk chwalk))\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);\n}"
  },
  {
    "function_name": "rt_mutex_slowunlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1339-1398",
    "snippet": "static bool __sched rt_mutex_slowunlock(struct rt_mutex *lock,\n\t\t\t\t\tstruct wake_q_head *wake_q)\n{\n\tunsigned long flags;\n\n\t/* irqsave required to support early boot calls */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\t/*\n\t * We must be careful here if the fast path is enabled. If we\n\t * have no waiters queued we cannot set owner to NULL here\n\t * because of:\n\t *\n\t * foo->lock->owner = NULL;\n\t *\t\t\trtmutex_lock(foo->lock);   <- fast path\n\t *\t\t\tfree = atomic_dec_and_test(foo->refcnt);\n\t *\t\t\trtmutex_unlock(foo->lock); <- fast path\n\t *\t\t\tif (free)\n\t *\t\t\t\tkfree(foo);\n\t * raw_spin_unlock(foo->lock->wait_lock);\n\t *\n\t * So for the fastpath enabled kernel:\n\t *\n\t * Nothing can set the waiters bit as long as we hold\n\t * lock->wait_lock. So we do the following sequence:\n\t *\n\t *\towner = rt_mutex_owner(lock);\n\t *\tclear_rt_mutex_waiters(lock);\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t *\tif (cmpxchg(&lock->owner, owner, 0) == owner)\n\t *\t\treturn;\n\t *\tgoto retry;\n\t *\n\t * The fastpath disabled variant is simple as all access to\n\t * lock->owner is serialized by lock->wait_lock:\n\t *\n\t *\tlock->owner = NULL;\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t */\n\twhile (!rt_mutex_has_waiters(lock)) {\n\t\t/* Drops lock->wait_lock ! */\n\t\tif (unlock_rt_mutex_safe(lock, flags) == true)\n\t\t\treturn false;\n\t\t/* Relock the rtmutex and try again */\n\t\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\t}\n\n\t/*\n\t * The wakeup next waiter path does not suffer from the above\n\t * race. See the comments there.\n\t *\n\t * Queue the next waiter for wakeup once we release the wait_lock.\n\t */\n\tmark_wakeup_next_waiter(wake_q, lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn true; /* call rt_mutex_postunlock() */\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1395
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mark_wakeup_next_waiter",
          "args": [
            "wake_q",
            "lock"
          ],
          "line": 1394
        },
        "resolved": true,
        "details": {
          "function_name": "mark_wakeup_next_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1018-1060",
          "snippet": "static void mark_wakeup_next_waiter(struct wake_q_head *wake_q,\n\t\t\t\t    struct rt_mutex *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_postunlock();\n\t */\n\tpreempt_disable();\n\twake_q_add(wake_q, waiter->task);\n\traw_spin_unlock(&current->pi_lock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void mark_wakeup_next_waiter(struct wake_q_head *wake_q,\n\t\t\t\t    struct rt_mutex *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_postunlock();\n\t */\n\tpreempt_disable();\n\twake_q_add(wake_q, waiter->task);\n\traw_spin_unlock(&current->pi_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1385
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlock_rt_mutex_safe",
          "args": [
            "lock",
            "flags"
          ],
          "line": 1382
        },
        "resolved": true,
        "details": {
          "function_name": "unlock_rt_mutex_safe",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "217-224",
          "snippet": "static inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,\n\t\t\t\t\tunsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,\n\t\t\t\t\tunsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1380
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_unlock",
          "args": [
            "lock"
          ],
          "line": 1347
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_unlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "143-146",
          "snippet": "void debug_rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_unlock(struct rt_mutex *lock)\n{\n\tDEBUG_LOCKS_WARN_ON(rt_mutex_owner(lock) != current);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic bool __sched rt_mutex_slowunlock(struct rt_mutex *lock,\n\t\t\t\t\tstruct wake_q_head *wake_q)\n{\n\tunsigned long flags;\n\n\t/* irqsave required to support early boot calls */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\t/*\n\t * We must be careful here if the fast path is enabled. If we\n\t * have no waiters queued we cannot set owner to NULL here\n\t * because of:\n\t *\n\t * foo->lock->owner = NULL;\n\t *\t\t\trtmutex_lock(foo->lock);   <- fast path\n\t *\t\t\tfree = atomic_dec_and_test(foo->refcnt);\n\t *\t\t\trtmutex_unlock(foo->lock); <- fast path\n\t *\t\t\tif (free)\n\t *\t\t\t\tkfree(foo);\n\t * raw_spin_unlock(foo->lock->wait_lock);\n\t *\n\t * So for the fastpath enabled kernel:\n\t *\n\t * Nothing can set the waiters bit as long as we hold\n\t * lock->wait_lock. So we do the following sequence:\n\t *\n\t *\towner = rt_mutex_owner(lock);\n\t *\tclear_rt_mutex_waiters(lock);\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t *\tif (cmpxchg(&lock->owner, owner, 0) == owner)\n\t *\t\treturn;\n\t *\tgoto retry;\n\t *\n\t * The fastpath disabled variant is simple as all access to\n\t * lock->owner is serialized by lock->wait_lock:\n\t *\n\t *\tlock->owner = NULL;\n\t *\traw_spin_unlock(&lock->wait_lock);\n\t */\n\twhile (!rt_mutex_has_waiters(lock)) {\n\t\t/* Drops lock->wait_lock ! */\n\t\tif (unlock_rt_mutex_safe(lock, flags) == true)\n\t\t\treturn false;\n\t\t/* Relock the rtmutex and try again */\n\t\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\t}\n\n\t/*\n\t * The wakeup next waiter path does not suffer from the above\n\t * race. See the comments there.\n\t *\n\t * Queue the next waiter for wakeup once we release the wait_lock.\n\t */\n\tmark_wakeup_next_waiter(wake_q, lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn true; /* call rt_mutex_postunlock() */\n}"
  },
  {
    "function_name": "rt_mutex_slowtrylock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1308-1332",
    "snippet": "static inline int rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1329
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__rt_mutex_slowtrylock",
          "args": [
            "lock"
          ],
          "line": 1327
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowtrylock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1292-1303",
          "snippet": "static inline int __rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int __rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1325
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1318
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/*\n\t * If the lock already has an owner we fail to get the lock.\n\t * This can be done without taking the @lock->wait_lock as\n\t * it is only being read, and this is a trylock anyway.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * The mutex has currently no owner. Lock the wait lock and try to\n\t * acquire the lock. We use irqsave here to support early boot calls.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "__rt_mutex_slowtrylock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1292-1303",
    "snippet": "static inline int __rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 1300
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "68-136",
          "snippet": "static void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "NULL"
          ],
          "line": 1294
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "808-920",
          "snippet": "static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int __rt_mutex_slowtrylock(struct rt_mutex *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t/*\n\t * try_to_take_rt_mutex() sets the lock waiters bit\n\t * unconditionally. Clean this up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_slowlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1230-1290",
    "snippet": "static int __sched\nrt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t  struct hrtimer_sleeper *timeout,\n\t\t  enum rtmutex_chainwalk chwalk)\n{\n\tstruct rt_mutex_waiter waiter;\n\tunsigned long flags;\n\tint ret = 0;\n\n\trt_mutex_init_waiter(&waiter);\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\t/* Try to acquire the lock again: */\n\tif (try_to_take_rt_mutex(lock, current, NULL)) {\n\t\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\t\treturn 0;\n\t}\n\n\tset_current_state(state);\n\n\t/* Setup the timer, when timeout != NULL */\n\tif (unlikely(timeout))\n\t\thrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\n\n\tret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);\n\n\tif (likely(!ret))\n\t\t/* sleep on the mutex */\n\t\tret = __rt_mutex_slowlock(lock, state, timeout, &waiter);\n\n\tif (unlikely(ret)) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_waiter(lock, &waiter);\n\t\trt_mutex_handle_deadlock(ret, chwalk, &waiter);\n\t}\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit\n\t * unconditionally. We might have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\t/* Remove pending timer: */\n\tif (unlikely(timeout))\n\t\thrtimer_cancel(&timeout->timer);\n\n\tdebug_rt_mutex_free_waiter(&waiter);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "debug_rt_mutex_free_waiter",
          "args": [
            "&waiter"
          ],
          "line": 1287
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_free_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "164-168",
          "snippet": "void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)\n{\n\tput_pid(waiter->deadlock_task_pid);\n\tmemset(waiter, 0x22, sizeof(*waiter));\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)\n{\n\tput_pid(waiter->deadlock_task_pid);\n\tmemset(waiter, 0x22, sizeof(*waiter));\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_cancel",
          "args": [
            "&timeout->timer"
          ],
          "line": 1285
        },
        "resolved": true,
        "details": {
          "function_name": "hrtimer_cancel",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/time/hrtimer.c",
          "lines": "1167-1176",
          "snippet": "int hrtimer_cancel(struct hrtimer *timer)\n{\n\tfor (;;) {\n\t\tint ret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret >= 0)\n\t\t\treturn ret;\n\t\tcpu_relax();\n\t}\n}",
          "includes": [
            "#include \"tick-internal.h\"",
            "#include <trace/events/timer.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/compat.h>",
            "#include <linux/freezer.h>",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/nohz.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/sysctl.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/debugobjects.h>",
            "#include <linux/err.h>",
            "#include <linux/seq_file.h>",
            "#include <linux/tick.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/notifier.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/percpu.h>",
            "#include <linux/export.h>",
            "#include <linux/cpu.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"tick-internal.h\"\n#include <trace/events/timer.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <linux/freezer.h>\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/signal.h>\n#include <linux/debugobjects.h>\n#include <linux/err.h>\n#include <linux/seq_file.h>\n#include <linux/tick.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/notifier.h>\n#include <linux/hrtimer.h>\n#include <linux/percpu.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n\nint hrtimer_cancel(struct hrtimer *timer)\n{\n\tfor (;;) {\n\t\tint ret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret >= 0)\n\t\t\treturn ret;\n\t\tcpu_relax();\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "timeout"
          ],
          "line": 1284
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1281
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "fixup_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 1279
        },
        "resolved": true,
        "details": {
          "function_name": "fixup_rt_mutex_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "68-136",
          "snippet": "static void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_handle_deadlock",
          "args": [
            "ret",
            "chwalk",
            "&waiter"
          ],
          "line": 1272
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_handle_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1207-1225",
          "snippet": "static void rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\t/*\n\t * Yell lowdly and stop the task right here.\n\t */\n\trt_mutex_print_deadlock(w);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\t/*\n\t * Yell lowdly and stop the task right here.\n\t */\n\trt_mutex_print_deadlock(w);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "remove_waiter",
          "args": [
            "lock",
            "&waiter"
          ],
          "line": 1271
        },
        "resolved": true,
        "details": {
          "function_name": "remove_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1068-1119",
          "snippet": "static void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 1270
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "ret"
          ],
          "line": 1269
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "__rt_mutex_slowlock",
          "args": [
            "lock",
            "state",
            "timeout",
            "&waiter"
          ],
          "line": 1267
        },
        "resolved": true,
        "details": {
          "function_name": "__rt_mutex_slowlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1167-1205",
          "snippet": "static int __sched\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t    struct hrtimer_sleeper *timeout,\n\t\t    struct rt_mutex_waiter *waiter)\n{\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * TASK_INTERRUPTIBLE checks for signals and\n\t\t * timeout. Ignored otherwise.\n\t\t */\n\t\tif (likely(state == TASK_INTERRUPTIBLE)) {\n\t\t\t/* Signal pending? */\n\t\t\tif (signal_pending(current))\n\t\t\t\tret = -EINTR;\n\t\t\tif (timeout && !timeout->task)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tdebug_rt_mutex_print_deadlock(waiter);\n\n\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int __sched\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t    struct hrtimer_sleeper *timeout,\n\t\t    struct rt_mutex_waiter *waiter)\n{\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * TASK_INTERRUPTIBLE checks for signals and\n\t\t * timeout. Ignored otherwise.\n\t\t */\n\t\tif (likely(state == TASK_INTERRUPTIBLE)) {\n\t\t\t/* Signal pending? */\n\t\t\tif (signal_pending(current))\n\t\t\t\tret = -EINTR;\n\t\t\tif (timeout && !timeout->task)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tdebug_rt_mutex_print_deadlock(waiter);\n\n\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "!ret"
          ],
          "line": 1265
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_blocks_on_rt_mutex",
          "args": [
            "lock",
            "&waiter",
            "current",
            "chwalk"
          ],
          "line": 1263
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocks_on_rt_mutex",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "929-1010",
          "snippet": "static int task_blocks_on_rt_mutex(struct rt_mutex *lock,\n\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t */\n\tif (owner == task)\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int task_blocks_on_rt_mutex(struct rt_mutex *lock,\n\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t */\n\tif (owner == task)\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}"
        }
      },
      {
        "call_info": {
          "callee": "hrtimer_start_expires",
          "args": [
            "&timeout->timer",
            "HRTIMER_MODE_ABS"
          ],
          "line": 1261
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "unlikely",
          "args": [
            "timeout"
          ],
          "line": 1260
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 1257
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "NULL"
          ],
          "line": 1252
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "808-920",
          "snippet": "static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 1249
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_init_waiter",
          "args": [
            "&waiter"
          ],
          "line": 1239
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_init_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "1149-1155",
          "snippet": "void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->task = NULL;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->task = NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int __sched\nrt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t  struct hrtimer_sleeper *timeout,\n\t\t  enum rtmutex_chainwalk chwalk)\n{\n\tstruct rt_mutex_waiter waiter;\n\tunsigned long flags;\n\tint ret = 0;\n\n\trt_mutex_init_waiter(&waiter);\n\n\t/*\n\t * Technically we could use raw_spin_[un]lock_irq() here, but this can\n\t * be called in early boot if the cmpxchg() fast path is disabled\n\t * (debug, no architecture support). In this case we will acquire the\n\t * rtmutex with lock->wait_lock held. But we cannot unconditionally\n\t * enable interrupts in that early boot case. So we need to use the\n\t * irqsave/restore variants.\n\t */\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\t/* Try to acquire the lock again: */\n\tif (try_to_take_rt_mutex(lock, current, NULL)) {\n\t\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\t\treturn 0;\n\t}\n\n\tset_current_state(state);\n\n\t/* Setup the timer, when timeout != NULL */\n\tif (unlikely(timeout))\n\t\thrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\n\n\tret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);\n\n\tif (likely(!ret))\n\t\t/* sleep on the mutex */\n\t\tret = __rt_mutex_slowlock(lock, state, timeout, &waiter);\n\n\tif (unlikely(ret)) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_waiter(lock, &waiter);\n\t\trt_mutex_handle_deadlock(ret, chwalk, &waiter);\n\t}\n\n\t/*\n\t * try_to_take_rt_mutex() sets the waiter bit\n\t * unconditionally. We might have to fix that up.\n\t */\n\tfixup_rt_mutex_waiters(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\t/* Remove pending timer: */\n\tif (unlikely(timeout))\n\t\thrtimer_cancel(&timeout->timer);\n\n\tdebug_rt_mutex_free_waiter(&waiter);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_handle_deadlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1207-1225",
    "snippet": "static void rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\t/*\n\t * Yell lowdly and stop the task right here.\n\t */\n\trt_mutex_print_deadlock(w);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "schedule",
          "args": [],
          "line": 1223
        },
        "resolved": true,
        "details": {
          "function_name": "audit_schedule_prune",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/audit_tree.c",
          "lines": "919-922",
          "snippet": "static void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}",
          "includes": [
            "#include <linux/slab.h>",
            "#include <linux/refcount.h>",
            "#include <linux/kthread.h>",
            "#include <linux/mount.h>",
            "#include <linux/namei.h>",
            "#include <linux/fsnotify_backend.h>",
            "#include \"audit.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct task_struct *prune_thread;",
            "static void audit_schedule_prune(void);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/slab.h>\n#include <linux/refcount.h>\n#include <linux/kthread.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/fsnotify_backend.h>\n#include \"audit.h\"\n\nstatic struct task_struct *prune_thread;\nstatic void audit_schedule_prune(void);\n\nstatic void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}"
        }
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "TASK_INTERRUPTIBLE"
          ],
          "line": 1222
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_print_deadlock",
          "args": [
            "w"
          ],
          "line": 1220
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_print_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.h",
          "lines": "26-29",
          "snippet": "static inline void rt_mutex_print_deadlock(struct rt_mutex_waiter *w)\n{\n\tWARN(1, \"rtmutex deadlock detected\\n\");\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline void rt_mutex_print_deadlock(struct rt_mutex_waiter *w)\n{\n\tWARN(1, \"rtmutex deadlock detected\\n\");\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t/*\n\t * If the result is not -EDEADLOCK or the caller requested\n\t * deadlock detection, nothing to do here.\n\t */\n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\t/*\n\t * Yell lowdly and stop the task right here.\n\t */\n\trt_mutex_print_deadlock(w);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}"
  },
  {
    "function_name": "__rt_mutex_slowlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1167-1205",
    "snippet": "static int __sched\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t    struct hrtimer_sleeper *timeout,\n\t\t    struct rt_mutex_waiter *waiter)\n{\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * TASK_INTERRUPTIBLE checks for signals and\n\t\t * timeout. Ignored otherwise.\n\t\t */\n\t\tif (likely(state == TASK_INTERRUPTIBLE)) {\n\t\t\t/* Signal pending? */\n\t\t\tif (signal_pending(current))\n\t\t\t\tret = -EINTR;\n\t\t\tif (timeout && !timeout->task)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tdebug_rt_mutex_print_deadlock(waiter);\n\n\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "__set_current_state",
          "args": [
            "TASK_RUNNING"
          ],
          "line": 1203
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "set_current_state",
          "args": [
            "state"
          ],
          "line": 1200
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1199
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "schedule",
          "args": [],
          "line": 1197
        },
        "resolved": true,
        "details": {
          "function_name": "audit_schedule_prune",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/audit_tree.c",
          "lines": "919-922",
          "snippet": "static void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}",
          "includes": [
            "#include <linux/slab.h>",
            "#include <linux/refcount.h>",
            "#include <linux/kthread.h>",
            "#include <linux/mount.h>",
            "#include <linux/namei.h>",
            "#include <linux/fsnotify_backend.h>",
            "#include \"audit.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static struct task_struct *prune_thread;",
            "static void audit_schedule_prune(void);"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/slab.h>\n#include <linux/refcount.h>\n#include <linux/kthread.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/fsnotify_backend.h>\n#include \"audit.h\"\n\nstatic struct task_struct *prune_thread;\nstatic void audit_schedule_prune(void);\n\nstatic void audit_schedule_prune(void)\n{\n\twake_up_process(prune_thread);\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_print_deadlock",
          "args": [
            "waiter"
          ],
          "line": 1195
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_print_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "87-137",
          "snippet": "void debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter)\n{\n\tstruct task_struct *task;\n\n\tif (!waiter->deadlock_lock || !debug_locks)\n\t\treturn;\n\n\trcu_read_lock();\n\ttask = pid_task(waiter->deadlock_task_pid, PIDTYPE_PID);\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (!debug_locks_off()) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"============================================\\n\");\n\tpr_warn(\"WARNING: circular locking deadlock detected!\\n\");\n\tpr_warn(\"%s\\n\", print_tainted());\n\tpr_warn(\"--------------------------------------------\\n\");\n\tprintk(\"%s/%d is deadlocking current task %s/%d\\n\\n\",\n\t       task->comm, task_pid_nr(task),\n\t       current->comm, task_pid_nr(current));\n\n\tprintk(\"\\n1) %s/%d is trying to acquire this lock:\\n\",\n\t       current->comm, task_pid_nr(current));\n\tprintk_lock(waiter->lock, 1);\n\n\tprintk(\"\\n2) %s/%d is blocked on this lock:\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tprintk_lock(waiter->deadlock_lock, 1);\n\n\tdebug_show_held_locks(current);\n\tdebug_show_held_locks(task);\n\n\tprintk(\"\\n%s/%d's [blocked] stackdump:\\n\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tshow_stack(task, NULL);\n\tprintk(\"\\n%s/%d's [current] stackdump:\\n\\n\",\n\t\tcurrent->comm, task_pid_nr(current));\n\tdump_stack();\n\tdebug_show_all_locks();\n\trcu_read_unlock();\n\n\tprintk(\"[ turning off deadlock detection.\"\n\t       \"Please report this trace. ]\\n\\n\");\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_print_deadlock(struct rt_mutex_waiter *waiter)\n{\n\tstruct task_struct *task;\n\n\tif (!waiter->deadlock_lock || !debug_locks)\n\t\treturn;\n\n\trcu_read_lock();\n\ttask = pid_task(waiter->deadlock_task_pid, PIDTYPE_PID);\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (!debug_locks_off()) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"============================================\\n\");\n\tpr_warn(\"WARNING: circular locking deadlock detected!\\n\");\n\tpr_warn(\"%s\\n\", print_tainted());\n\tpr_warn(\"--------------------------------------------\\n\");\n\tprintk(\"%s/%d is deadlocking current task %s/%d\\n\\n\",\n\t       task->comm, task_pid_nr(task),\n\t       current->comm, task_pid_nr(current));\n\n\tprintk(\"\\n1) %s/%d is trying to acquire this lock:\\n\",\n\t       current->comm, task_pid_nr(current));\n\tprintk_lock(waiter->lock, 1);\n\n\tprintk(\"\\n2) %s/%d is blocked on this lock:\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tprintk_lock(waiter->deadlock_lock, 1);\n\n\tdebug_show_held_locks(current);\n\tdebug_show_held_locks(task);\n\n\tprintk(\"\\n%s/%d's [blocked] stackdump:\\n\\n\",\n\t\ttask->comm, task_pid_nr(task));\n\tshow_stack(task, NULL);\n\tprintk(\"\\n%s/%d's [current] stackdump:\\n\\n\",\n\t\tcurrent->comm, task_pid_nr(current));\n\tdump_stack();\n\tdebug_show_all_locks();\n\trcu_read_unlock();\n\n\tprintk(\"[ turning off deadlock detection.\"\n\t       \"Please report this trace. ]\\n\\n\");\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1193
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "signal_pending",
          "args": [
            "current"
          ],
          "line": 1185
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "likely",
          "args": [
            "state == TASK_INTERRUPTIBLE"
          ],
          "line": 1183
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "try_to_take_rt_mutex",
          "args": [
            "lock",
            "current",
            "waiter"
          ],
          "line": 1176
        },
        "resolved": true,
        "details": {
          "function_name": "try_to_take_rt_mutex",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "808-920",
          "snippet": "static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int __sched\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\n\t\t    struct hrtimer_sleeper *timeout,\n\t\t    struct rt_mutex_waiter *waiter)\n{\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t/* Try to acquire the lock: */\n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * TASK_INTERRUPTIBLE checks for signals and\n\t\t * timeout. Ignored otherwise.\n\t\t */\n\t\tif (likely(state == TASK_INTERRUPTIBLE)) {\n\t\t\t/* Signal pending? */\n\t\t\tif (signal_pending(current))\n\t\t\t\tret = -EINTR;\n\t\t\tif (timeout && !timeout->task)\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tdebug_rt_mutex_print_deadlock(waiter);\n\n\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}"
  },
  {
    "function_name": "rt_mutex_init_waiter",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1149-1155",
    "snippet": "void rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->task = NULL;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RB_CLEAR_NODE",
          "args": [
            "&waiter->tree_entry"
          ],
          "line": 1153
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RB_CLEAR_NODE",
          "args": [
            "&waiter->pi_tree_entry"
          ],
          "line": 1152
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_init_waiter",
          "args": [
            "waiter"
          ],
          "line": 1151
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_init_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "158-162",
          "snippet": "void debug_rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tmemset(waiter, 0x11, sizeof(*waiter));\n\twaiter->deadlock_task_pid = NULL;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tmemset(waiter, 0x11, sizeof(*waiter));\n\twaiter->deadlock_task_pid = NULL;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)\n{\n\tdebug_rt_mutex_init_waiter(waiter);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n\twaiter->task = NULL;\n}"
  },
  {
    "function_name": "rt_mutex_adjust_pi",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1126-1147",
    "snippet": "void rt_mutex_adjust_pi(struct task_struct *task)\n{\n\tstruct rt_mutex_waiter *waiter;\n\tstruct rt_mutex *next_lock;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&task->pi_lock, flags);\n\n\twaiter = task->pi_blocked_on;\n\tif (!waiter || rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\t\treturn;\n\t}\n\tnext_lock = waiter->lock;\n\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(task);\n\n\trt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,\n\t\t\t\t   next_lock, NULL, task);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio_chain",
          "args": [
            "task",
            "RT_MUTEX_MIN_CHAINWALK",
            "NULL",
            "next_lock",
            "NULL",
            "task"
          ],
          "line": 1145
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio_chain",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "448-796",
          "snippet": "static int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "int max_lock_depth = 1024;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint max_lock_depth = 1024;\n\nstatic int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "task"
          ],
          "line": 1143
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&task->pi_lock",
            "flags"
          ],
          "line": 1140
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_equal",
          "args": [
            "waiter",
            "task_to_waiter(task)"
          ],
          "line": 1135
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_equal",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "252-269",
          "snippet": "static inline int\nrt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t      struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t      struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_to_waiter",
          "args": [
            "task"
          ],
          "line": 1135
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irqsave",
          "args": [
            "&task->pi_lock",
            "flags"
          ],
          "line": 1132
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irqsave_nested",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "359-370",
          "snippet": "unsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nunsigned long __lockfunc _raw_spin_lock_irqsave_nested(raw_spinlock_t *lock,\n\t\t\t\t\t\t   int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED_FLAGS(lock, do_raw_spin_trylock, do_raw_spin_lock,\n\t\t\t\tdo_raw_spin_lock_flags, &flags);\n\treturn flags;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nvoid rt_mutex_adjust_pi(struct task_struct *task)\n{\n\tstruct rt_mutex_waiter *waiter;\n\tstruct rt_mutex *next_lock;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&task->pi_lock, flags);\n\n\twaiter = task->pi_blocked_on;\n\tif (!waiter || rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\t\treturn;\n\t}\n\tnext_lock = waiter->lock;\n\traw_spin_unlock_irqrestore(&task->pi_lock, flags);\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(task);\n\n\trt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,\n\t\t\t\t   next_lock, NULL, task);\n}"
  },
  {
    "function_name": "remove_waiter",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1068-1119",
    "snippet": "static void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1118
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio_chain",
          "args": [
            "owner",
            "RT_MUTEX_MIN_CHAINWALK",
            "lock",
            "next_lock",
            "NULL",
            "current"
          ],
          "line": 1115
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio_chain",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "448-796",
          "snippet": "static int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "int max_lock_depth = 1024;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint max_lock_depth = 1024;\n\nstatic int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1113
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "owner"
          ],
          "line": 1111
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 1101
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocked_on_lock",
          "args": [
            "owner"
          ],
          "line": 1099
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocked_on_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "380-383",
          "snippet": "static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "owner"
          ],
          "line": 1096
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "337-347",
          "snippet": "static void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "owner",
            "rt_mutex_top_waiter(lock)"
          ],
          "line": 1094
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "304-325",
          "snippet": "static void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1094
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "84-88",
          "snippet": "static inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 1093
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "owner",
            "waiter"
          ],
          "line": 1091
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "327-335",
          "snippet": "static void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 1089
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1075
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 1072
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void remove_waiter(struct rt_mutex *lock,\n\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t/*\n\t * Only update priority if the waiter was the highest priority\n\t * waiter of the lock and there is an owner to update.\n\t */\n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(owner);\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t/*\n\t * Don't walk the chain, if the owner task is not blocked\n\t * itself.\n\t */\n\tif (!next_lock)\n\t\treturn;\n\n\t/* gets dropped in rt_mutex_adjust_prio_chain()! */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}"
  },
  {
    "function_name": "mark_wakeup_next_waiter",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "1018-1060",
    "snippet": "static void mark_wakeup_next_waiter(struct wake_q_head *wake_q,\n\t\t\t\t    struct rt_mutex *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_postunlock();\n\t */\n\tpreempt_disable();\n\twake_q_add(wake_q, waiter->task);\n\traw_spin_unlock(&current->pi_lock);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&current->pi_lock"
          ],
          "line": 1059
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_q_add",
          "args": [
            "wake_q",
            "waiter->task"
          ],
          "line": 1058
        },
        "resolved": true,
        "details": {
          "function_name": "wake_q_add",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "399-421",
          "snippet": "void wake_q_add(struct wake_q_head *head, struct task_struct *task)\n{\n\tstruct wake_q_node *node = &task->wake_q;\n\n\t/*\n\t * Atomically grab the task, if ->wake_q is !nil already it means\n\t * its already queued (either by us or someone else) and will get the\n\t * wakeup due to that.\n\t *\n\t * This cmpxchg() executes a full barrier, which pairs with the full\n\t * barrier executed by the wakeup in wake_up_q().\n\t */\n\tif (cmpxchg(&node->next, NULL, WAKE_Q_TAIL))\n\t\treturn;\n\n\tget_task_struct(task);\n\n\t/*\n\t * The head is context local, there can be no concurrency.\n\t */\n\t*head->lastp = node;\n\thead->lastp = &node->next;\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid wake_q_add(struct wake_q_head *head, struct task_struct *task)\n{\n\tstruct wake_q_node *node = &task->wake_q;\n\n\t/*\n\t * Atomically grab the task, if ->wake_q is !nil already it means\n\t * its already queued (either by us or someone else) and will get the\n\t * wakeup due to that.\n\t *\n\t * This cmpxchg() executes a full barrier, which pairs with the full\n\t * barrier executed by the wakeup in wake_up_q().\n\t */\n\tif (cmpxchg(&node->next, NULL, WAKE_Q_TAIL))\n\t\treturn;\n\n\tget_task_struct(task);\n\n\t/*\n\t * The head is context local, there can be no concurrency.\n\t */\n\t*head->lastp = node;\n\thead->lastp = &node->next;\n}"
        }
      },
      {
        "call_info": {
          "callee": "preempt_disable",
          "args": [],
          "line": 1057
        },
        "resolved": true,
        "details": {
          "function_name": "schedule_preempt_disabled",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "3571-3576",
          "snippet": "void __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static void __sched",
            "static void __sched"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic void __sched;\nstatic void __sched;\n\nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "current"
          ],
          "line": 1035
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "337-347",
          "snippet": "static void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "current",
            "waiter"
          ],
          "line": 1034
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "327-335",
          "snippet": "static void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 1025
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "84-88",
          "snippet": "static inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&current->pi_lock"
          ],
          "line": 1023
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void mark_wakeup_next_waiter(struct wake_q_head *wake_q,\n\t\t\t\t    struct rt_mutex *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t/*\n\t * Remove it from current->pi_waiters and deboost.\n\t *\n\t * We must in fact deboost here in order to ensure we call\n\t * rt_mutex_setprio() to update p->pi_top_task before the\n\t * task unblocks.\n\t */\n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(current);\n\n\t/*\n\t * As we are waking up the top waiter, and the waiter stays\n\t * queued on the lock until it gets the lock, this lock\n\t * obviously has waiters. Just set the bit here and this has\n\t * the added benefit of forcing all new tasks into the\n\t * slow path making sure no task of lower priority than\n\t * the top waiter can steal this lock.\n\t */\n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t/*\n\t * We deboosted before waking the top waiter task such that we don't\n\t * run two tasks with the 'same' priority (and ensure the\n\t * p->pi_top_task pointer points to a blocked task). This however can\n\t * lead to priority inversion if we would get preempted after the\n\t * deboost but before waking our donor task, hence the preempt_disable()\n\t * before unlock.\n\t *\n\t * Pairs with preempt_enable() in rt_mutex_postunlock();\n\t */\n\tpreempt_disable();\n\twake_q_add(wake_q, waiter->task);\n\traw_spin_unlock(&current->pi_lock);\n}"
  },
  {
    "function_name": "task_blocks_on_rt_mutex",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "929-1010",
    "snippet": "static int task_blocks_on_rt_mutex(struct rt_mutex *lock,\n\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t */\n\tif (owner == task)\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1007
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio_chain",
          "args": [
            "owner",
            "chwalk",
            "lock",
            "next_lock",
            "waiter",
            "task"
          ],
          "line": 1004
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio_chain",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "448-796",
          "snippet": "static int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "int max_lock_depth = 1024;"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint max_lock_depth = 1024;\n\nstatic int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 1002
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "owner"
          ],
          "line": 1000
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 986
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocked_on_lock",
          "args": [
            "owner"
          ],
          "line": 984
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocked_on_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "380-383",
          "snippet": "static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_cond_detect_deadlock",
          "args": [
            "waiter",
            "chwalk"
          ],
          "line": 979
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cond_detect_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "362-373",
          "snippet": "static bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t\t\t  enum rtmutex_chainwalk chwalk)\n{\n\t/*\n\t * This is just a wrapper function for the following call,\n\t * because debug_rt_mutex_detect_deadlock() smells like a magic\n\t * debug feature and I wanted to keep the cond function in the\n\t * main source file along with the comments instead of having\n\t * two of the same in the headers.\n\t */\n\treturn debug_rt_mutex_detect_deadlock(waiter, chwalk);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t\t\t  enum rtmutex_chainwalk chwalk)\n{\n\t/*\n\t * This is just a wrapper function for the following call,\n\t * because debug_rt_mutex_detect_deadlock() smells like a magic\n\t * debug feature and I wanted to keep the cond function in the\n\t * main source file along with the comments instead of having\n\t * two of the same in the headers.\n\t */\n\treturn debug_rt_mutex_detect_deadlock(waiter, chwalk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "owner"
          ],
          "line": 976
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "337-347",
          "snippet": "static void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "owner",
            "waiter"
          ],
          "line": 974
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "304-325",
          "snippet": "static void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "owner",
            "top_waiter"
          ],
          "line": 973
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "327-335",
          "snippet": "static void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 972
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "84-88",
          "snippet": "static inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&owner->pi_lock"
          ],
          "line": 971
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 960
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 939
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 934
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int task_blocks_on_rt_mutex(struct rt_mutex *lock,\n\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Early deadlock detection. We really don't want the task to\n\t * enqueue on itself just to untangle the mess later. It's not\n\t * only an optimization. We drop the locks, so another waiter\n\t * can come in before the chain walk detects the deadlock. So\n\t * the other will detect the deadlock and return -EDEADLOCK,\n\t * which is wrong, as the other waiter is not in a deadlock\n\t * situation.\n\t */\n\tif (owner == task)\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\t/* Get the top priority waiter on the lock */\n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t/* Store the lock on which owner is blocked or NULL */\n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t/*\n\t * Even if full deadlock detection is on, if the owner is not\n\t * blocked itself, we can avoid finding this out in the chain\n\t * walk.\n\t */\n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t/*\n\t * The owner can't disappear while holding a lock,\n\t * so the owner struct is protected by wait_lock.\n\t * Gets dropped in rt_mutex_adjust_prio_chain()!\n\t */\n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}"
  },
  {
    "function_name": "try_to_take_rt_mutex",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "808-920",
    "snippet": "static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_set_owner",
          "args": [
            "lock",
            "task"
          ],
          "line": 917
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_set_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "51-60",
          "snippet": "static void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}"
        }
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_lock",
          "args": [
            "lock"
          ],
          "line": 911
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "139-141",
          "snippet": "void debug_rt_mutex_lock(struct rt_mutex *lock)\n{\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_lock(struct rt_mutex *lock)\n{\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 907
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "task",
            "rt_mutex_top_waiter(lock)"
          ],
          "line": 906
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "304-325",
          "snippet": "static void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 906
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "84-88",
          "snippet": "static inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 905
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 898
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_less",
          "args": [
            "task_to_waiter(task)",
            "rt_mutex_top_waiter(lock)"
          ],
          "line": 872
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_less",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "233-250",
          "snippet": "static inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_to_waiter",
          "args": [
            "task"
          ],
          "line": 872
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue",
          "args": [
            "lock",
            "waiter"
          ],
          "line": 855
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "327-335",
          "snippet": "static void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 835
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "mark_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 830
        },
        "resolved": true,
        "details": {
          "function_name": "mark_rt_mutex_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "208-212",
          "snippet": "static inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 811
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\n\t\t\t\tstruct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t/*\n\t * Before testing whether we can acquire @lock, we set the\n\t * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all\n\t * other tasks which try to modify @lock into the slow path\n\t * and they serialize on @lock->wait_lock.\n\t *\n\t * The RT_MUTEX_HAS_WAITERS bit can have a transitional state\n\t * as explained at the top of this file if and only if:\n\t *\n\t * - There is a lock owner. The caller must fixup the\n\t *   transient state if it does a trylock or leaves the lock\n\t *   function due to a signal or timeout.\n\t *\n\t * - @task acquires the lock and there are no other\n\t *   waiters. This is undone in rt_mutex_set_owner(@task) at\n\t *   the end of this function.\n\t */\n\tmark_rt_mutex_waiters(lock);\n\n\t/*\n\t * If @lock has an owner, give up.\n\t */\n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t/*\n\t * If @waiter != NULL, @task has already enqueued the waiter\n\t * into @lock waiter tree. If @waiter == NULL then this is a\n\t * trylock attempt.\n\t */\n\tif (waiter) {\n\t\t/*\n\t\t * If waiter is not the highest priority waiter of\n\t\t * @lock, give up.\n\t\t */\n\t\tif (waiter != rt_mutex_top_waiter(lock))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * We can acquire the lock. Remove the waiter from the\n\t\t * lock waiters tree.\n\t\t */\n\t\trt_mutex_dequeue(lock, waiter);\n\n\t} else {\n\t\t/*\n\t\t * If the lock has waiters already we check whether @task is\n\t\t * eligible to take over the lock.\n\t\t *\n\t\t * If there are no other waiters, @task can acquire\n\t\t * the lock.  @task->pi_blocked_on is NULL, so it does\n\t\t * not need to be dequeued.\n\t\t */\n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t/*\n\t\t\t * If @task->prio is greater than or equal to\n\t\t\t * the top waiter priority (kernel view),\n\t\t\t * @task lost.\n\t\t\t */\n\t\t\tif (!rt_mutex_waiter_less(task_to_waiter(task),\n\t\t\t\t\t\t  rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * The current top waiter stays enqueued. We\n\t\t\t * don't have to change anything in the lock\n\t\t\t * waiters order.\n\t\t\t */\n\t\t} else {\n\t\t\t/*\n\t\t\t * No waiters. Take the lock without the\n\t\t\t * pi_lock dance.@task->pi_blocked_on is NULL\n\t\t\t * and we have no waiters to enqueue in @task\n\t\t\t * pi waiters tree.\n\t\t\t */\n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t/*\n\t * Clear @task->pi_blocked_on. Requires protection by\n\t * @task->pi_lock. Redundant operation for the @waiter == NULL\n\t * case, but conditionals are more expensive than a redundant\n\t * store.\n\t */\n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t/*\n\t * Finish the lock acquisition. @task is the new owner. If\n\t * other waiters exist we have to insert the highest priority\n\t * waiter into @task->pi_waiters tree.\n\t */\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t/* We got the lock. */\n\tdebug_rt_mutex_lock(lock);\n\n\t/*\n\t * This either preserves the RT_MUTEX_HAS_WAITERS bit if there\n\t * are still waiters or clears it.\n\t */\n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}"
  },
  {
    "function_name": "rt_mutex_adjust_prio_chain",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "448-796",
    "snippet": "static int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [
      "int max_lock_depth = 1024;"
    ],
    "called_functions": [
      {
        "call_info": {
          "callee": "put_task_struct",
          "args": [
            "task"
          ],
          "line": 793
        },
        "resolved": true,
        "details": {
          "function_name": "__put_task_struct",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/fork.c",
          "lines": "716-731",
          "snippet": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}",
          "includes": [
            "#include <linux/init_task.h>",
            "#include <trace/events/task.h>",
            "#include <trace/events/sched.h>",
            "#include <asm/tlbflush.h>",
            "#include <asm/cacheflush.h>",
            "#include <asm/mmu_context.h>",
            "#include <linux/uaccess.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/pgtable.h>",
            "#include <linux/stackleak.h>",
            "#include <linux/thread_info.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/kcov.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/compiler.h>",
            "#include <linux/aio.h>",
            "#include <linux/uprobes.h>",
            "#include <linux/signalfd.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/oom.h>",
            "#include <linux/user-return-notifier.h>",
            "#include <linux/posix-timers.h>",
            "#include <linux/perf_event.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/magic.h>",
            "#include <linux/fs_struct.h>",
            "#include <linux/blkdev.h>",
            "#include <linux/tty.h>",
            "#include <linux/random.h>",
            "#include <linux/taskstats_kern.h>",
            "#include <linux/delayacct.h>",
            "#include <linux/freezer.h>",
            "#include <linux/cn_proc.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/acct.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/profile.h>",
            "#include <linux/proc_fs.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/audit.h>",
            "#include <linux/mount.h>",
            "#include <linux/ptrace.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/task_io_accounting_ops.h>",
            "#include <linux/kthread.h>",
            "#include <linux/compat.h>",
            "#include <linux/futex.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/swap.h>",
            "#include <linux/seccomp.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/security.h>",
            "#include <linux/cgroup.h>",
            "#include <linux/cpu.h>",
            "#include <linux/capability.h>",
            "#include <linux/nsproxy.h>",
            "#include <linux/vmacache.h>",
            "#include <linux/mm.h>",
            "#include <linux/fs.h>",
            "#include <linux/hmm.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/mman.h>",
            "#include <linux/binfmts.h>",
            "#include <linux/key.h>",
            "#include <linux/iocontext.h>",
            "#include <linux/fdtable.h>",
            "#include <linux/file.h>",
            "#include <linux/sem.h>",
            "#include <linux/mempolicy.h>",
            "#include <linux/personality.h>",
            "#include <linux/completion.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/module.h>",
            "#include <linux/unistd.h>",
            "#include <linux/init.h>",
            "#include <linux/rtmutex.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/task_stack.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/sched/stat.h>",
            "#include <linux/sched/numa_balancing.h>",
            "#include <linux/sched/user.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/autogroup.h>",
            "#include <linux/slab.h>"
          ],
          "macros_used": [],
          "globals_used": [
            "static __latent_entropy struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include <linux/init_task.h>\n#include <trace/events/task.h>\n#include <trace/events/sched.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/pgtable.h>\n#include <linux/stackleak.h>\n#include <linux/thread_info.h>\n#include <linux/livepatch.h>\n#include <linux/kcov.h>\n#include <linux/sysctl.h>\n#include <linux/compiler.h>\n#include <linux/aio.h>\n#include <linux/uprobes.h>\n#include <linux/signalfd.h>\n#include <linux/khugepaged.h>\n#include <linux/oom.h>\n#include <linux/user-return-notifier.h>\n#include <linux/posix-timers.h>\n#include <linux/perf_event.h>\n#include <linux/sched/mm.h>\n#include <linux/magic.h>\n#include <linux/fs_struct.h>\n#include <linux/blkdev.h>\n#include <linux/tty.h>\n#include <linux/random.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/freezer.h>\n#include <linux/cn_proc.h>\n#include <linux/tsacct_kern.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/acct.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/profile.h>\n#include <linux/proc_fs.h>\n#include <linux/ftrace.h>\n#include <linux/memcontrol.h>\n#include <linux/audit.h>\n#include <linux/mount.h>\n#include <linux/ptrace.h>\n#include <linux/rcupdate.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/kthread.h>\n#include <linux/compat.h>\n#include <linux/futex.h>\n#include <linux/jiffies.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/seccomp.h>\n#include <linux/hugetlb.h>\n#include <linux/security.h>\n#include <linux/cgroup.h>\n#include <linux/cpu.h>\n#include <linux/capability.h>\n#include <linux/nsproxy.h>\n#include <linux/vmacache.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/hmm.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mman.h>\n#include <linux/binfmts.h>\n#include <linux/key.h>\n#include <linux/iocontext.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/sem.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/completion.h>\n#include <linux/vmalloc.h>\n#include <linux/module.h>\n#include <linux/unistd.h>\n#include <linux/init.h>\n#include <linux/rtmutex.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/task.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/user.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/autogroup.h>\n#include <linux/slab.h>\n\nstatic __latent_entropy struct;\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irq",
          "args": [
            "&task->pi_lock"
          ],
          "line": 791
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "190-193",
          "snippet": "void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 767
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "198-201",
          "snippet": "void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_unlock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_top_waiter",
          "args": [
            "lock"
          ],
          "line": 764
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_top_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "84-88",
          "snippet": "static inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\nrt_mutex_top_waiter(struct rt_mutex *lock)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_blocked_on_lock",
          "args": [
            "task"
          ],
          "line": 759
        },
        "resolved": true,
        "details": {
          "function_name": "task_blocked_on_lock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "380-383",
          "snippet": "static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_adjust_prio",
          "args": [
            "task"
          ],
          "line": 741
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_adjust_prio",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "337-347",
          "snippet": "static void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_enqueue_pi",
          "args": [
            "task",
            "waiter"
          ],
          "line": 740
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_enqueue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "304-325",
          "snippet": "static void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_dequeue_pi",
          "args": [
            "task",
            "waiter"
          ],
          "line": 738
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_dequeue_pi",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "327-335",
          "snippet": "static void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock",
          "args": [
            "&task->pi_lock"
          ],
          "line": 713
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "166-169",
          "snippet": "void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "task"
          ],
          "line": 712
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 711
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "wake_up_process",
          "args": [
            "rt_mutex_top_waiter(lock)->task"
          ],
          "line": 705
        },
        "resolved": true,
        "details": {
          "function_name": "wake_up_process",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "2127-2130",
          "snippet": "int wake_up_process(struct task_struct *p)\n{\n\treturn try_to_wake_up(p, TASK_NORMAL, 0);\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nint wake_up_process(struct task_struct *p)\n{\n\treturn try_to_wake_up(p, TASK_NORMAL, 0);\n}"
        }
      },
      {
        "call_info": {
          "callee": "get_task_struct",
          "args": [
            "task"
          ],
          "line": 631
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "debug_rt_mutex_deadlock",
          "args": [
            "chwalk",
            "orig_waiter",
            "lock"
          ],
          "line": 601
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex-debug.c",
          "lines": "71-85",
          "snippet": "void debug_rt_mutex_deadlock(enum rtmutex_chainwalk chwalk,\n\t\t\t     struct rt_mutex_waiter *act_waiter,\n\t\t\t     struct rt_mutex *lock)\n{\n\tstruct task_struct *task;\n\n\tif (!debug_locks || chwalk == RT_MUTEX_FULL_CHAINWALK || !act_waiter)\n\t\treturn;\n\n\ttask = rt_mutex_owner(act_waiter->lock);\n\tif (task && task != current) {\n\t\tact_waiter->deadlock_task_pid = get_pid(task_pid(task));\n\t\tact_waiter->deadlock_lock = lock;\n\t}\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/debug_locks.h>",
            "#include <linux/fs.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/syscalls.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "#include <linux/delay.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/debug_locks.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n#include <linux/interrupt.h>\n#include <linux/syscalls.h>\n#include <linux/kallsyms.h>\n#include <linux/spinlock.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/rt.h>\n#include <linux/sched.h>\n\nvoid debug_rt_mutex_deadlock(enum rtmutex_chainwalk chwalk,\n\t\t\t     struct rt_mutex_waiter *act_waiter,\n\t\t\t     struct rt_mutex *lock)\n{\n\tstruct task_struct *task;\n\n\tif (!debug_locks || chwalk == RT_MUTEX_FULL_CHAINWALK || !act_waiter)\n\t\treturn;\n\n\ttask = rt_mutex_owner(act_waiter->lock);\n\tif (task && task != current) {\n\t\tact_waiter->deadlock_task_pid = get_pid(task_pid(task));\n\t\tact_waiter->deadlock_lock = lock;\n\t}\n}"
        }
      },
      {
        "call_info": {
          "callee": "cpu_relax",
          "args": [],
          "line": 587
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_trylock",
          "args": [
            "&lock->wait_lock"
          ],
          "line": 585
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_trylock_bh",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "134-137",
          "snippet": "int __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)\n{\n\treturn __raw_spin_trylock_bh(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nint __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)\n{\n\treturn __raw_spin_trylock_bh(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_equal",
          "args": [
            "waiter",
            "task_to_waiter(task)"
          ],
          "line": 569
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_equal",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "252-269",
          "snippet": "static inline int\nrt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t      struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t      struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_to_waiter",
          "args": [
            "task"
          ],
          "line": 569
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "task_top_pi_waiter",
          "args": [
            "task"
          ],
          "line": 554
        },
        "resolved": true,
        "details": {
          "function_name": "task_top_pi_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "95-99",
          "snippet": "static inline struct rt_mutex_waiter *\ntask_top_pi_waiter(struct task_struct *p)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\ntask_top_pi_waiter(struct task_struct *p)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_has_pi_waiters",
          "args": [
            "task"
          ],
          "line": 546
        },
        "resolved": true,
        "details": {
          "function_name": "task_has_pi_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "90-93",
          "snippet": "static inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "raw_spin_lock_irq",
          "args": [
            "&task->pi_lock"
          ],
          "line": 502
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_lock_irq",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "158-161",
          "snippet": "void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\t__raw_spin_lock_irq(lock);\n}"
        }
      },
      {
        "call_info": {
          "callee": "printk",
          "args": [
            "KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\"",
            "max_lock_depth",
            "top_task->comm",
            "task_pid_nr(top_task)"
          ],
          "line": 483
        },
        "resolved": true,
        "details": {
          "function_name": "__warn_printk",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/panic.c",
          "lines": "590-599",
          "snippet": "void __warn_printk(const char *fmt, ...)\n{\n\tva_list args;\n\n\tpr_warn(CUT_HERE);\n\n\tva_start(args, fmt);\n\tvprintk(fmt, args);\n\tva_end(args);\n}",
          "includes": [
            "#include <asm/sections.h>",
            "#include <linux/debugfs.h>",
            "#include <linux/ratelimit.h>",
            "#include <linux/bug.h>",
            "#include <linux/console.h>",
            "#include <linux/nmi.h>",
            "#include <linux/init.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/sched.h>",
            "#include <linux/kexec.h>",
            "#include <linux/delay.h>",
            "#include <linux/reboot.h>",
            "#include <linux/ftrace.h>",
            "#include <linux/random.h>",
            "#include <linux/module.h>",
            "#include <linux/notifier.h>",
            "#include <linux/kallsyms.h>",
            "#include <linux/kmsg_dump.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/debug_locks.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <asm/sections.h>\n#include <linux/debugfs.h>\n#include <linux/ratelimit.h>\n#include <linux/bug.h>\n#include <linux/console.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/sysrq.h>\n#include <linux/sched.h>\n#include <linux/kexec.h>\n#include <linux/delay.h>\n#include <linux/reboot.h>\n#include <linux/ftrace.h>\n#include <linux/random.h>\n#include <linux/module.h>\n#include <linux/notifier.h>\n#include <linux/kallsyms.h>\n#include <linux/kmsg_dump.h>\n#include <linux/interrupt.h>\n#include <linux/sched/debug.h>\n#include <linux/debug_locks.h>\n\nvoid __warn_printk(const char *fmt, ...)\n{\n\tva_list args;\n\n\tpr_warn(CUT_HERE);\n\n\tva_start(args, fmt);\n\tvprintk(fmt, args);\n\tva_end(args);\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_pid_nr",
          "args": [
            "top_task"
          ],
          "line": 485
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_cond_detect_deadlock",
          "args": [
            "orig_waiter",
            "chwalk"
          ],
          "line": 462
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_cond_detect_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "362-373",
          "snippet": "static bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t\t\t  enum rtmutex_chainwalk chwalk)\n{\n\t/*\n\t * This is just a wrapper function for the following call,\n\t * because debug_rt_mutex_detect_deadlock() smells like a magic\n\t * debug feature and I wanted to keep the cond function in the\n\t * main source file along with the comments instead of having\n\t * two of the same in the headers.\n\t */\n\treturn debug_rt_mutex_detect_deadlock(waiter, chwalk);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t\t\t  enum rtmutex_chainwalk chwalk)\n{\n\t/*\n\t * This is just a wrapper function for the following call,\n\t * because debug_rt_mutex_detect_deadlock() smells like a magic\n\t * debug feature and I wanted to keep the cond function in the\n\t * main source file along with the comments instead of having\n\t * two of the same in the headers.\n\t */\n\treturn debug_rt_mutex_detect_deadlock(waiter, chwalk);\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nint max_lock_depth = 1024;\n\nstatic int rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t      struct rt_mutex *orig_lock,\n\t\t\t\t      struct rt_mutex *next_lock,\n\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t/*\n\t * The (de)boosting is a step by step approach with a lot of\n\t * pitfalls. We want this to be preemptible and we want hold a\n\t * maximum of two locks per step. So we have to check\n\t * carefully whether things change under us.\n\t */\n again:\n\t/*\n\t * We limit the lock chain length for each invocation.\n\t */\n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t/*\n\t\t * Print this only once. If the admin changes the limit,\n\t\t * print a new message when reaching the limit again.\n\t\t */\n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t/*\n\t * We are fully preemptible here and only hold the refcount on\n\t * @task. So everything can have changed under us since the\n\t * caller or our own code below (goto retry/again) dropped all\n\t * locks.\n\t */\n retry:\n\t/*\n\t * [1] Task cannot go away as we did a get_task() before !\n\t */\n\traw_spin_lock_irq(&task->pi_lock);\n\n\t/*\n\t * [2] Get the waiter on which @task is blocked on.\n\t */\n\twaiter = task->pi_blocked_on;\n\n\t/*\n\t * [3] check_exit_conditions_1() protected by task->pi_lock.\n\t */\n\n\t/*\n\t * Check whether the end of the boosting chain has been\n\t * reached or the state of the chain has changed while we\n\t * dropped the locks.\n\t */\n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Check the orig_waiter state. After we dropped the locks,\n\t * the previous owner of the lock might have released the lock.\n\t */\n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * We dropped all locks after taking a refcount on @task, so\n\t * the task might have moved on in the lock chain or even left\n\t * the chain completely and blocks now on an unrelated lock or\n\t * on @orig_lock.\n\t *\n\t * We stored the lock on which @task was blocked in @next_lock,\n\t * so we can detect the chain change.\n\t */\n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t/*\n\t * Drop out, when the task has no waiters. Note,\n\t * top_waiter can be NULL, when we are in the deboosting\n\t * mode!\n\t */\n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t/*\n\t\t * If deadlock detection is off, we stop here if we\n\t\t * are not the top pi waiter of the task. If deadlock\n\t\t * detection is enabled we continue, but stop the\n\t\t * requeueing in the chain walk.\n\t\t */\n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t/*\n\t * If the waiter priority is the same as the task priority\n\t * then there is no further priority adjustment necessary.  If\n\t * deadlock detection is off, we stop the chain walk. If its\n\t * enabled we continue, but stop the requeueing in the chain\n\t * walk.\n\t */\n\tif (rt_mutex_waiter_equal(waiter, task_to_waiter(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t/*\n\t * [4] Get the next lock\n\t */\n\tlock = waiter->lock;\n\t/*\n\t * [5] We need to trylock here as we are holding task->pi_lock,\n\t * which is the reverse lock order versus the other rtmutex\n\t * operations.\n\t */\n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * [6] check_exit_conditions_2() protected by task->pi_lock and\n\t * lock->wait_lock.\n\t *\n\t * Deadlock detection. If the lock is the same as the original\n\t * lock which caused us to walk the lock chain or if the\n\t * current lock is owned by the task which initiated the chain\n\t * walk, we detected a deadlock.\n\t */\n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tdebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tret = -EDEADLK;\n\t\tgoto out_unlock_pi;\n\t}\n\n\t/*\n\t * If we just follow the lock chain for deadlock detection, no\n\t * need to do all the requeue operations. To avoid a truckload\n\t * of conditionals around the various places below, just do the\n\t * minimum chain walk checks.\n\t */\n\tif (!requeue) {\n\t\t/*\n\t\t * No requeue[7] here. Just release @task [8]\n\t\t */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t/*\n\t\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t\t * If there is no owner of the lock, end of chain.\n\t\t */\n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* [10] Grab the next task, i.e. owner of @lock */\n\t\ttask = rt_mutex_owner(lock);\n\t\tget_task_struct(task);\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t/*\n\t\t * No requeue [11] here. We just do deadlock detection.\n\t\t *\n\t\t * [12] Store whether owner is blocked\n\t\t * itself. Decision is made after dropping the locks\n\t\t */\n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t/*\n\t\t * Get the top waiter for the next iteration\n\t\t */\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t/* [13] Drop locks */\n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t/* If owner is not blocked, end of chain. */\n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t/*\n\t * Store the current top waiter before doing the requeue\n\t * operation on @lock. We need it for the boost/deboost\n\t * decision below.\n\t */\n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [7] Requeue the waiter in the lock waiter tree. */\n\trt_mutex_dequeue(lock, waiter);\n\n\t/*\n\t * Update the waiter prio fields now that we're dequeued.\n\t *\n\t * These values can have changed through either:\n\t *\n\t *   sys_sched_set_scheduler() / sys_sched_setattr()\n\t *\n\t * or\n\t *\n\t *   DL CBS enforcement advancing the effective deadline.\n\t *\n\t * Even though pi_waiters also uses these fields, and that tree is only\n\t * updated in [11], we can do this here, since we hold [L], which\n\t * serializes all pi_waiters access and rb_erase() does not care about\n\t * the values of the node being removed.\n\t */\n\twaiter->prio = task->prio;\n\twaiter->deadline = task->dl.deadline;\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t/* [8] Release the task */\n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t/*\n\t * [9] check_exit_conditions_3 protected by lock->wait_lock.\n\t *\n\t * We must abort the chain walk if there is no lock owner even\n\t * in the dead lock detection case, as we have nothing to\n\t * follow here. This is the end of the chain we are walking.\n\t */\n\tif (!rt_mutex_owner(lock)) {\n\t\t/*\n\t\t * If the requeue [7] above changed the top waiter,\n\t\t * then we need to wake the new top waiter up to try\n\t\t * to get the lock.\n\t\t */\n\t\tif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\n\t\t\twake_up_process(rt_mutex_top_waiter(lock)->task);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t/* [10] Grab the next task, i.e. the owner of @lock */\n\ttask = rt_mutex_owner(lock);\n\tget_task_struct(task);\n\traw_spin_lock(&task->pi_lock);\n\n\t/* [11] requeue the pi waiters if necessary */\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t/*\n\t\t * The waiter became the new top (highest priority)\n\t\t * waiter on the lock. Replace the previous top waiter\n\t\t * in the owner tasks pi waiters tree with this waiter\n\t\t * and adjust the priority of the owner.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t/*\n\t\t * The waiter was the top waiter on the lock, but is\n\t\t * no longer the top prority waiter. Replace waiter in\n\t\t * the owner tasks pi waiters tree with the new top\n\t\t * (highest priority) waiter and adjust the priority\n\t\t * of the owner.\n\t\t * The new top waiter is stored in @waiter so that\n\t\t * @waiter == @top_waiter evaluates to true below and\n\t\t * we continue to deboost the rest of the chain.\n\t\t */\n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(task);\n\t} else {\n\t\t/*\n\t\t * Nothing changed. No need to do any priority\n\t\t * adjustment.\n\t\t */\n\t}\n\n\t/*\n\t * [12] check_exit_conditions_4() protected by task->pi_lock\n\t * and lock->wait_lock. The actual decisions are made after we\n\t * dropped the locks.\n\t *\n\t * Check whether the task which owns the current lock is pi\n\t * blocked itself. If yes we store a pointer to the lock for\n\t * the lock chain change detection above. After we dropped\n\t * task->pi_lock next_lock cannot be dereferenced anymore.\n\t */\n\tnext_lock = task_blocked_on_lock(task);\n\t/*\n\t * Store the top waiter of @lock for the end of chain walk\n\t * decision below.\n\t */\n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t/* [13] Drop the locks */\n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t/*\n\t * Make the actual exit decisions [12], based on the stored\n\t * values.\n\t *\n\t * We reached the end of the lock chain. Stop right here. No\n\t * point to go back just to figure that out.\n\t */\n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t/*\n\t * If the current waiter is not the top waiter on the lock,\n\t * then we can stop the chain walk here if we are not in full\n\t * deadlock detection mode.\n\t */\n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}"
  },
  {
    "function_name": "task_blocked_on_lock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "380-383",
    "snippet": "static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}"
  },
  {
    "function_name": "rt_mutex_cond_detect_deadlock",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "362-373",
    "snippet": "static bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t\t\t  enum rtmutex_chainwalk chwalk)\n{\n\t/*\n\t * This is just a wrapper function for the following call,\n\t * because debug_rt_mutex_detect_deadlock() smells like a magic\n\t * debug feature and I wanted to keep the cond function in the\n\t * main source file along with the comments instead of having\n\t * two of the same in the headers.\n\t */\n\treturn debug_rt_mutex_detect_deadlock(waiter, chwalk);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "debug_rt_mutex_detect_deadlock",
          "args": [
            "waiter",
            "chwalk"
          ],
          "line": 372
        },
        "resolved": true,
        "details": {
          "function_name": "debug_rt_mutex_detect_deadlock",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.h",
          "lines": "31-35",
          "snippet": "static inline bool debug_rt_mutex_detect_deadlock(struct rt_mutex_waiter *w,\n\t\t\t\t\t\t  enum rtmutex_chainwalk walk)\n{\n\treturn walk == RT_MUTEX_FULL_CHAINWALK;\n}",
          "includes": [],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "static inline bool debug_rt_mutex_detect_deadlock(struct rt_mutex_waiter *w,\n\t\t\t\t\t\t  enum rtmutex_chainwalk walk)\n{\n\treturn walk == RT_MUTEX_FULL_CHAINWALK;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t\t\t  enum rtmutex_chainwalk chwalk)\n{\n\t/*\n\t * This is just a wrapper function for the following call,\n\t * because debug_rt_mutex_detect_deadlock() smells like a magic\n\t * debug feature and I wanted to keep the cond function in the\n\t * main source file along with the comments instead of having\n\t * two of the same in the headers.\n\t */\n\treturn debug_rt_mutex_detect_deadlock(waiter, chwalk);\n}"
  },
  {
    "function_name": "rt_mutex_adjust_prio",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "337-347",
    "snippet": "static void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_setprio",
          "args": [
            "p",
            "pi_task"
          ],
          "line": 346
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_setprio",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/sched/core.c",
          "lines": "3742-3857",
          "snippet": "void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)\n{\n\tint prio, oldprio, queued, running, queue_flag =\n\t\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;\n\tconst struct sched_class *prev_class;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\t/* XXX used to be waiter->prio, not waiter->task->prio */\n\tprio = __rt_effective_prio(pi_task, p->normal_prio);\n\n\t/*\n\t * If nothing changed; bail early.\n\t */\n\tif (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))\n\t\treturn;\n\n\trq = __task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\t/*\n\t * Set under pi_lock && rq->lock, such that the value can be used under\n\t * either lock.\n\t *\n\t * Note that there is loads of tricky to make this pointer cache work\n\t * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to\n\t * ensure a task is de-boosted (pi_task is set to NULL) before the\n\t * task is allowed to run again (and can exit). This ensures the pointer\n\t * points to a blocked task -- which guaratees the task is present.\n\t */\n\tp->pi_top_task = pi_task;\n\n\t/*\n\t * For FIFO/RR we only need to set prio, if that matches we're done.\n\t */\n\tif (prio == p->prio && !dl_prio(prio))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Idle task boosting is a nono in general. There is one\n\t * exception, when PREEMPT_RT and NOHZ is active:\n\t *\n\t * The idle task calls get_next_timer_interrupt() and holds\n\t * the timer wheel base->lock on the CPU and another CPU wants\n\t * to access the timer (probably to cancel it). We can safely\n\t * ignore the boosting request, as the idle CPU runs this code\n\t * with interrupts disabled and will complete the lock\n\t * protected section without being interrupted. So there is no\n\t * real need to boost.\n\t */\n\tif (unlikely(p == rq->idle)) {\n\t\tWARN_ON(p != rq->curr);\n\t\tWARN_ON(p->pi_blocked_on);\n\t\tgoto out_unlock;\n\t}\n\n\ttrace_sched_pi_setprio(p, pi_task);\n\toldprio = p->prio;\n\n\tif (oldprio == prio)\n\t\tqueue_flag &= ~DEQUEUE_MOVE;\n\n\tprev_class = p->sched_class;\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\tif (queued)\n\t\tdequeue_task(rq, p, queue_flag);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\t/*\n\t * Boosting condition are:\n\t * 1. -rt task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A\n\t *\n\t * 2. -dl task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A and could preempt the\n\t *          running task\n\t */\n\tif (dl_prio(prio)) {\n\t\tif (!dl_prio(p->normal_prio) ||\n\t\t    (pi_task && dl_entity_preempt(&pi_task->dl, &p->dl))) {\n\t\t\tp->dl.dl_boosted = 1;\n\t\t\tqueue_flag |= ENQUEUE_REPLENISH;\n\t\t} else\n\t\t\tp->dl.dl_boosted = 0;\n\t\tp->sched_class = &dl_sched_class;\n\t} else if (rt_prio(prio)) {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.dl_boosted = 0;\n\t\tif (oldprio < prio)\n\t\t\tqueue_flag |= ENQUEUE_HEAD;\n\t\tp->sched_class = &rt_sched_class;\n\t} else {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.dl_boosted = 0;\n\t\tif (rt_prio(oldprio))\n\t\t\tp->rt.timeout = 0;\n\t\tp->sched_class = &fair_sched_class;\n\t}\n\n\tp->prio = prio;\n\n\tif (queued)\n\t\tenqueue_task(rq, p, queue_flag);\n\tif (running)\n\t\tset_curr_task(rq, p);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\nout_unlock:\n\t/* Avoid rq from going away on us: */\n\tpreempt_disable();\n\t__task_rq_unlock(rq, &rf);\n\n\tbalance_callback(rq);\n\tpreempt_enable();\n}",
          "includes": [
            "#include \"features.h\"",
            "#include <trace/events/sched.h>",
            "#include \"pelt.h\"",
            "#include \"../smpboot.h\"",
            "#include \"../workqueue_internal.h\"",
            "#include <asm/tlb.h>",
            "#include <asm/switch_to.h>",
            "#include <linux/kcov.h>",
            "#include <linux/nospec.h>",
            "#include \"sched.h\""
          ],
          "macros_used": [],
          "globals_used": [
            "static __always_inline struct"
          ],
          "called_functions": [],
          "contextual_snippet": "#include \"features.h\"\n#include <trace/events/sched.h>\n#include \"pelt.h\"\n#include \"../smpboot.h\"\n#include \"../workqueue_internal.h\"\n#include <asm/tlb.h>\n#include <asm/switch_to.h>\n#include <linux/kcov.h>\n#include <linux/nospec.h>\n#include \"sched.h\"\n\nstatic __always_inline struct;\n\nvoid rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)\n{\n\tint prio, oldprio, queued, running, queue_flag =\n\t\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;\n\tconst struct sched_class *prev_class;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\t/* XXX used to be waiter->prio, not waiter->task->prio */\n\tprio = __rt_effective_prio(pi_task, p->normal_prio);\n\n\t/*\n\t * If nothing changed; bail early.\n\t */\n\tif (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))\n\t\treturn;\n\n\trq = __task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\t/*\n\t * Set under pi_lock && rq->lock, such that the value can be used under\n\t * either lock.\n\t *\n\t * Note that there is loads of tricky to make this pointer cache work\n\t * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to\n\t * ensure a task is de-boosted (pi_task is set to NULL) before the\n\t * task is allowed to run again (and can exit). This ensures the pointer\n\t * points to a blocked task -- which guaratees the task is present.\n\t */\n\tp->pi_top_task = pi_task;\n\n\t/*\n\t * For FIFO/RR we only need to set prio, if that matches we're done.\n\t */\n\tif (prio == p->prio && !dl_prio(prio))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Idle task boosting is a nono in general. There is one\n\t * exception, when PREEMPT_RT and NOHZ is active:\n\t *\n\t * The idle task calls get_next_timer_interrupt() and holds\n\t * the timer wheel base->lock on the CPU and another CPU wants\n\t * to access the timer (probably to cancel it). We can safely\n\t * ignore the boosting request, as the idle CPU runs this code\n\t * with interrupts disabled and will complete the lock\n\t * protected section without being interrupted. So there is no\n\t * real need to boost.\n\t */\n\tif (unlikely(p == rq->idle)) {\n\t\tWARN_ON(p != rq->curr);\n\t\tWARN_ON(p->pi_blocked_on);\n\t\tgoto out_unlock;\n\t}\n\n\ttrace_sched_pi_setprio(p, pi_task);\n\toldprio = p->prio;\n\n\tif (oldprio == prio)\n\t\tqueue_flag &= ~DEQUEUE_MOVE;\n\n\tprev_class = p->sched_class;\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\tif (queued)\n\t\tdequeue_task(rq, p, queue_flag);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\t/*\n\t * Boosting condition are:\n\t * 1. -rt task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A\n\t *\n\t * 2. -dl task is running and holds mutex A\n\t *      --> -dl task blocks on mutex A and could preempt the\n\t *          running task\n\t */\n\tif (dl_prio(prio)) {\n\t\tif (!dl_prio(p->normal_prio) ||\n\t\t    (pi_task && dl_entity_preempt(&pi_task->dl, &p->dl))) {\n\t\t\tp->dl.dl_boosted = 1;\n\t\t\tqueue_flag |= ENQUEUE_REPLENISH;\n\t\t} else\n\t\t\tp->dl.dl_boosted = 0;\n\t\tp->sched_class = &dl_sched_class;\n\t} else if (rt_prio(prio)) {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.dl_boosted = 0;\n\t\tif (oldprio < prio)\n\t\t\tqueue_flag |= ENQUEUE_HEAD;\n\t\tp->sched_class = &rt_sched_class;\n\t} else {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.dl_boosted = 0;\n\t\tif (rt_prio(oldprio))\n\t\t\tp->rt.timeout = 0;\n\t\tp->sched_class = &fair_sched_class;\n\t}\n\n\tp->prio = prio;\n\n\tif (queued)\n\t\tenqueue_task(rq, p, queue_flag);\n\tif (running)\n\t\tset_curr_task(rq, p);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\nout_unlock:\n\t/* Avoid rq from going away on us: */\n\tpreempt_disable();\n\t__task_rq_unlock(rq, &rf);\n\n\tbalance_callback(rq);\n\tpreempt_enable();\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_top_pi_waiter",
          "args": [
            "p"
          ],
          "line": 344
        },
        "resolved": true,
        "details": {
          "function_name": "task_top_pi_waiter",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "95-99",
          "snippet": "static inline struct rt_mutex_waiter *\ntask_top_pi_waiter(struct task_struct *p)\n{\n\treturn NULL;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline struct rt_mutex_waiter *\ntask_top_pi_waiter(struct task_struct *p)\n{\n\treturn NULL;\n}"
        }
      },
      {
        "call_info": {
          "callee": "task_has_pi_waiters",
          "args": [
            "p"
          ],
          "line": 343
        },
        "resolved": true,
        "details": {
          "function_name": "task_has_pi_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "90-93",
          "snippet": "static inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int task_has_pi_waiters(struct task_struct *p)\n{\n\treturn false;\n}"
        }
      },
      {
        "call_info": {
          "callee": "lockdep_assert_held",
          "args": [
            "&p->pi_lock"
          ],
          "line": 341
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void rt_mutex_adjust_prio(struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}"
  },
  {
    "function_name": "rt_mutex_dequeue_pi",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "327-335",
    "snippet": "static void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RB_CLEAR_NODE",
          "args": [
            "&waiter->pi_tree_entry"
          ],
          "line": 334
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_erase_cached",
          "args": [
            "&waiter->pi_tree_entry",
            "&task->pi_waiters"
          ],
          "line": 333
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RB_EMPTY_NODE",
          "args": [
            "&waiter->pi_tree_entry"
          ],
          "line": 330
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree_entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree_entry);\n}"
  },
  {
    "function_name": "rt_mutex_enqueue_pi",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "304-325",
    "snippet": "static void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_insert_color_cached",
          "args": [
            "&waiter->pi_tree_entry",
            "&task->pi_waiters",
            "leftmost"
          ],
          "line": 324
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_link_node",
          "args": [
            "&waiter->pi_tree_entry",
            "parent",
            "link"
          ],
          "line": 323
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_less",
          "args": [
            "waiter",
            "entry"
          ],
          "line": 315
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_less",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "233-250",
          "snippet": "static inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "parent",
            "structrt_mutex_waiter",
            "pi_tree_entry"
          ],
          "line": 314
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &task->pi_waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->pi_tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->pi_tree_entry, &task->pi_waiters, leftmost);\n}"
  },
  {
    "function_name": "rt_mutex_dequeue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "294-302",
    "snippet": "static void\nrt_mutex_dequeue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->tree_entry, &lock->waiters);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "RB_CLEAR_NODE",
          "args": [
            "&waiter->tree_entry"
          ],
          "line": 301
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_erase_cached",
          "args": [
            "&waiter->tree_entry",
            "&lock->waiters"
          ],
          "line": 300
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "RB_EMPTY_NODE",
          "args": [
            "&waiter->tree_entry"
          ],
          "line": 297
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_dequeue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)\n{\n\tif (RB_EMPTY_NODE(&waiter->tree_entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->tree_entry, &lock->waiters);\n\tRB_CLEAR_NODE(&waiter->tree_entry);\n}"
  },
  {
    "function_name": "rt_mutex_enqueue",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "271-292",
    "snippet": "static void\nrt_mutex_enqueue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &lock->waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->tree_entry, &lock->waiters, leftmost);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rb_insert_color_cached",
          "args": [
            "&waiter->tree_entry",
            "&lock->waiters",
            "leftmost"
          ],
          "line": 291
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rb_link_node",
          "args": [
            "&waiter->tree_entry",
            "parent",
            "link"
          ],
          "line": 290
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_waiter_less",
          "args": [
            "waiter",
            "entry"
          ],
          "line": 282
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_waiter_less",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "233-250",
          "snippet": "static inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
        }
      },
      {
        "call_info": {
          "callee": "rb_entry",
          "args": [
            "parent",
            "structrt_mutex_waiter",
            "tree_entry"
          ],
          "line": 281
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_enqueue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)\n{\n\tstruct rb_node **link = &lock->waiters.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct rt_mutex_waiter *entry;\n\tbool leftmost = true;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tentry = rb_entry(parent, struct rt_mutex_waiter, tree_entry);\n\t\tif (rt_mutex_waiter_less(waiter, entry)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&waiter->tree_entry, parent, link);\n\trb_insert_color_cached(&waiter->tree_entry, &lock->waiters, leftmost);\n}"
  },
  {
    "function_name": "rt_mutex_waiter_equal",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "252-269",
    "snippet": "static inline int\nrt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t      struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "dl_prio",
          "args": [
            "left->prio"
          ],
          "line": 265
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_waiter_equal(struct rt_mutex_waiter *left,\n\t\t      struct rt_mutex_waiter *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 0 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}"
  },
  {
    "function_name": "rt_mutex_waiter_less",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "233-250",
    "snippet": "static inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "dl_time_before",
          "args": [
            "left->deadline",
            "right->deadline"
          ],
          "line": 247
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "dl_prio",
          "args": [
            "left->prio"
          ],
          "line": 246
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline int\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\n\t\t     struct rt_mutex_waiter *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t/*\n\t * If both waiters have dl_prio(), we check the deadlines of the\n\t * associated tasks.\n\t * If left waiter has a dl_prio(), and we didn't return 1 above,\n\t * then right waiter has a dl_prio() too.\n\t */\n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}"
  },
  {
    "function_name": "unlock_rt_mutex_safe",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "217-224",
    "snippet": "static inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,\n\t\t\t\t\tunsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 222
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__releases",
          "args": [
            "lock->wait_lock"
          ],
          "line": 219
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,\n\t\t\t\t\tunsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}"
  },
  {
    "function_name": "mark_rt_mutex_waiters",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "208-212",
    "snippet": "static inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}"
  },
  {
    "function_name": "unlock_rt_mutex_safe",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "168-201",
    "snippet": "static inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,\n\t\t\t\t\tunsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\n\tclear_rt_mutex_waiters(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\t/*\n\t * If a new waiter comes in between the unlock and the cmpxchg\n\t * we have two situations:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t * cmpxchg(p, owner, 0) == owner\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\t\t\t\t\tacquire(lock);\n\t * or:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\n\t * cmpxchg(p, owner, 0) != owner\n\t *\t\t\t\t\tenqueue_waiter();\n\t *\t\t\t\t\tunlock(wait_lock);\n\t * lock(wait_lock);\n\t * wake waiter();\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tacquire(lock);\n\t */\n\treturn rt_mutex_cmpxchg_release(lock, owner, NULL);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_cmpxchg_release",
          "args": [
            "lock",
            "owner",
            "NULL"
          ],
          "line": 200
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "raw_spin_unlock_irqrestore",
          "args": [
            "&lock->wait_lock",
            "flags"
          ],
          "line": 175
        },
        "resolved": true,
        "details": {
          "function_name": "_raw_spin_unlock_irqrestore",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/spinlock.c",
          "lines": "182-185",
          "snippet": "void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}",
          "includes": [
            "#include <linux/export.h>",
            "#include <linux/debug_locks.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/preempt.h>",
            "#include <linux/linkage.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include <linux/export.h>\n#include <linux/debug_locks.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n\nvoid __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n{\n\t__raw_spin_unlock_irqrestore(lock, flags);\n}"
        }
      },
      {
        "call_info": {
          "callee": "clear_rt_mutex_waiters",
          "args": [
            "lock"
          ],
          "line": 174
        },
        "resolved": true,
        "details": {
          "function_name": "clear_rt_mutex_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
          "lines": "62-66",
          "snippet": "static inline void clear_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "#include \"rtmutex_common.h\"",
            "#include <linux/timer.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/sched/deadline.h>",
            "#include <linux/sched/rt.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/export.h>",
            "#include <linux/spinlock.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void clear_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "rt_mutex_owner",
          "args": [
            "lock"
          ],
          "line": 172
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_owner",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "108-113",
          "snippet": "static inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [
            "#define RT_MUTEX_HAS_WAITERS\t1UL"
          ],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\n#define RT_MUTEX_HAS_WAITERS\t1UL\n\nstatic inline struct task_struct *rt_mutex_owner(struct rt_mutex *lock)\n{\n\tunsigned long owner = (unsigned long) READ_ONCE(lock->owner);\n\n\treturn (struct task_struct *) (owner & ~RT_MUTEX_HAS_WAITERS);\n}"
        }
      },
      {
        "call_info": {
          "callee": "__releases",
          "args": [
            "lock->wait_lock"
          ],
          "line": 170
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,\n\t\t\t\t\tunsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\n\tclear_rt_mutex_waiters(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\t/*\n\t * If a new waiter comes in between the unlock and the cmpxchg\n\t * we have two situations:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t * cmpxchg(p, owner, 0) == owner\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\t\t\t\t\tacquire(lock);\n\t * or:\n\t *\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tmark_rt_mutex_waiters(lock);\n\t *\n\t * cmpxchg(p, owner, 0) != owner\n\t *\t\t\t\t\tenqueue_waiter();\n\t *\t\t\t\t\tunlock(wait_lock);\n\t * lock(wait_lock);\n\t * wake waiter();\n\t * unlock(wait_lock);\n\t *\t\t\t\t\tlock(wait_lock);\n\t *\t\t\t\t\tacquire(lock);\n\t */\n\treturn rt_mutex_cmpxchg_release(lock, owner, NULL);\n}"
  },
  {
    "function_name": "mark_rt_mutex_waiters",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "152-160",
    "snippet": "static inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tdo {\n\t\towner = *p;\n\t} while (cmpxchg_relaxed(p, owner,\n\t\t\t\t owner | RT_MUTEX_HAS_WAITERS) != owner);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "cmpxchg_relaxed",
          "args": [
            "p",
            "owner",
            "owner | RT_MUTEX_HAS_WAITERS"
          ],
          "line": 158
        },
        "resolved": false,
        "reason": "library_or_external"
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tdo {\n\t\towner = *p;\n\t} while (cmpxchg_relaxed(p, owner,\n\t\t\t\t owner | RT_MUTEX_HAS_WAITERS) != owner);\n}"
  },
  {
    "function_name": "fixup_rt_mutex_waiters",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "68-136",
    "snippet": "static void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "WRITE_ONCE",
          "args": [
            "*p",
            "owner & ~RT_MUTEX_HAS_WAITERS"
          ],
          "line": 135
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "READ_ONCE",
          "args": [
            "*p"
          ],
          "line": 133
        },
        "resolved": false,
        "reason": "library_or_external"
      },
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 72
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void fixup_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t/*\n\t * The rbtree has no waiters enqueued, now make sure that the\n\t * lock->owner still has the waiters bit set, otherwise the\n\t * following can happen:\n\t *\n\t * CPU 0\tCPU 1\t\tCPU2\n\t * l->owner=T1\n\t *\t\trt_mutex_lock(l)\n\t *\t\tlock(l->lock)\n\t *\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\tenqueue(T2)\n\t *\t\tboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\tblock()\n\t *\n\t *\t\t\t\trt_mutex_lock(l)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tl->owner = T1 | HAS_WAITERS;\n\t *\t\t\t\tenqueue(T3)\n\t *\t\t\t\tboost()\n\t *\t\t\t\t  unlock(l->lock)\n\t *\t\t\t\tblock()\n\t *\t\tsignal(->T2)\tsignal(->T3)\n\t *\t\tlock(l->lock)\n\t *\t\tdequeue(T2)\n\t *\t\tdeboost()\n\t *\t\t  unlock(l->lock)\n\t *\t\t\t\tlock(l->lock)\n\t *\t\t\t\tdequeue(T3)\n\t *\t\t\t\t ==> wait list is empty\n\t *\t\t\t\tdeboost()\n\t *\t\t\t\t unlock(l->lock)\n\t *\t\tlock(l->lock)\n\t *\t\tfixup_rt_mutex_waiters()\n\t *\t\t  if (wait_list_empty(l) {\n\t *\t\t    l->owner = owner\n\t *\t\t    owner = l->owner & ~HAS_WAITERS;\n\t *\t\t      ==> l->owner = T1\n\t *\t\t  }\n\t *\t\t\t\tlock(l->lock)\n\t * rt_mutex_unlock(l)\t\tfixup_rt_mutex_waiters()\n\t *\t\t\t\t  if (wait_list_empty(l) {\n\t *\t\t\t\t    owner = l->owner & ~HAS_WAITERS;\n\t * cmpxchg(l->owner, T1, NULL)\n\t *  ===> Success (l->owner = NULL)\n\t *\n\t *\t\t\t\t    l->owner = owner\n\t *\t\t\t\t      ==> l->owner = T1\n\t *\t\t\t\t  }\n\t *\n\t * With the check for the waiter bit in place T3 on CPU2 will not\n\t * overwrite. All tasks fiddling with the waiters bit are\n\t * serialized by l->lock, so nothing else can modify the waiters\n\t * bit. If the bit is set then nothing can change l->owner either\n\t * so the simple RMW is safe. The cmpxchg() will simply fail if it\n\t * happens in the middle of the RMW because the waiters bit is\n\t * still set.\n\t */\n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS)\n\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n}"
  },
  {
    "function_name": "clear_rt_mutex_waiters",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "62-66",
    "snippet": "static inline void clear_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic inline void clear_rt_mutex_waiters(struct rt_mutex *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}"
  },
  {
    "function_name": "rt_mutex_set_owner",
    "container": null,
    "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex.c",
    "lines": "51-60",
    "snippet": "static void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}",
    "includes": [
      "#include \"rtmutex_common.h\"",
      "#include <linux/timer.h>",
      "#include <linux/sched/debug.h>",
      "#include <linux/sched/wake_q.h>",
      "#include <linux/sched/deadline.h>",
      "#include <linux/sched/rt.h>",
      "#include <linux/sched/signal.h>",
      "#include <linux/export.h>",
      "#include <linux/spinlock.h>"
    ],
    "macros_used": [],
    "globals_used": [],
    "called_functions": [
      {
        "call_info": {
          "callee": "rt_mutex_has_waiters",
          "args": [
            "lock"
          ],
          "line": 56
        },
        "resolved": true,
        "details": {
          "function_name": "rt_mutex_has_waiters",
          "container": null,
          "file": "/home/michele/Desktop/ricerca/output_repos_c/CVE-2018-18955/repo/kernel/locking/rtmutex_common.h",
          "lines": "79-82",
          "snippet": "static inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}",
          "includes": [
            "# include \"rtmutex.h\"",
            "# include \"rtmutex-debug.h\"",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/rtmutex.h>"
          ],
          "macros_used": [],
          "globals_used": [],
          "called_functions": [],
          "contextual_snippet": "# include \"rtmutex.h\"\n# include \"rtmutex-debug.h\"\n#include <linux/sched/wake_q.h>\n#include <linux/rtmutex.h>\n\nstatic inline int rt_mutex_has_waiters(struct rt_mutex *lock)\n{\n\treturn false;\n}"
        }
      }
    ],
    "contextual_snippet": "#include \"rtmutex_common.h\"\n#include <linux/timer.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n\nstatic void\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\tlock->owner = (struct task_struct *)val;\n}"
  }
]